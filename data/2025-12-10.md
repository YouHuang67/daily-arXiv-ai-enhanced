<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 163]
- [cs.AI](#cs.AI) [Total: 30]
- [cs.CL](#cs.CL) [Total: 66]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices](https://arxiv.org/abs/2512.05969)
*Hokin Deng*

Main category: cs.CV

TL;DR: 论文展示了视频生成模型具备推理能力，在象棋、迷宫、数独等任务上达到60%成功率，并建立了"任务对"实验范式和评估框架VMEvalKit。


<details>
  <summary>Details</summary>
Motivation: 探索视频生成模型是否具备推理能力，建立可扩展的评估范式来系统测试模型在复杂任务上的表现。

Method: 采用"任务对"实验设计，构建包含39个模型的代码框架VMEvalKit，支持自动评估并与人工判断强相关。

Result: Sora-2等领先模型在象棋、迷宫、数独、心理旋转、瑞文矩阵等推理任务上达到60%的成功率。

Conclusion: 视频生成模型已具备初步推理能力，建立的评估范式具有高度可扩展性，为通过强化学习提升视频模型推理能力提供了机会。

Abstract: We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.

</details>


### [2] [Adaptive Dataset Quantization: A New Direction for Dataset Pruning](https://arxiv.org/abs/2512.05987)
*Chenyue Yu,Jianyu Yu*

Main category: cs.CV

TL;DR: 本文提出一种针对边缘设备大规模数据集存储和通信成本的数据集量化方法，通过减少样本内冗余而非传统方法关注的样本间冗余，在保持模型训练性能的同时实现显著的数据集压缩。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，大规模数据集的存储和通信成本高昂。传统的数据集剪枝和蒸馏方法主要关注样本间冗余，但忽略了样本内的冗余信息。需要一种新方法来压缩每个样本内部的冗余内容，同时保留关键特征。

Method: 首先对每个样本应用线性对称量化以获得初始量化范围和尺度。然后引入自适应量化分配算法，为具有不同精度要求的样本分配不同的量化比率，同时保持总压缩比率恒定。该方法首次使用有限比特表示数据集以减少存储。

Result: 在CIFAR-10、CIFAR-100和ImageNet-1K数据集上的实验验证了方法的有效性。在相同压缩比率下，该方法在保持模型训练性能的同时，显著优于传统量化和数据集剪枝基线方法。

Conclusion: 提出的数据集量化方法通过减少样本内冗余，有效解决了边缘设备中大规模数据集的存储和通信成本问题。自适应量化分配算法能够根据样本精度需求灵活分配量化比率，在保持总压缩比的同时优化性能。

Abstract: This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.

</details>


### [3] [EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head](https://arxiv.org/abs/2512.05991)
*Chang Liu,Tianjiao Jing,Chengcheng Ma,Xuanqi Zhou,Zhengxuan Lian,Qin Jin,Hongliang Yuan,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: EmoDiffTalk：首个支持连续多模态情感编辑的3D高斯泼溅说话头生成框架，通过情感感知高斯扩散和文本到动作单元控制器实现精细面部动画


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的真实感3D说话头在情感表达操控方面存在显著不足，特别是在使用多模态控制进行细粒度和扩展性动态情感编辑方面

Method: 提出情感感知高斯扩散方法，包括：1）动作单元提示的高斯扩散过程用于细粒度面部动画；2）准确的文本到动作单元情感控制器，通过文本输入提供精确且扩展性的动态情感编辑

Result: 在EmoTalk3D和RenderMe-360数据集上的实验表明，EmoDiffTalk在情感细微度、唇部同步保真度和可控性方面优于先前工作，为高质量、扩散驱动、多模态可编辑3D说话头合成建立了原则性途径

Conclusion: EmoDiffTalk是首批支持在基于动作单元的表情空间内进行连续多模态情感编辑的3D高斯泼溅说话头生成框架之一，为高质量可编辑3D说话头合成开辟了新方向

Abstract: Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.

</details>


### [4] [Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology](https://arxiv.org/abs/2512.05993)
*Ruchika Verma,Shrishtee Kandoi,Robina Afzal,Shengjia Chen,Jannes Jegminat,Michael W. Karlovich,Melissa Umphlett,Timothy E. Richardson,Kevin Clare,Quazi Hossain,Jorge Samanamud,Phyllis L. Faust,Elan D. Louis,Ann C. McKee,Thor D. Stein,Jonathan D. Cherry,Jesse Mez,Anya C. McGoldrick,Dalilah D. Quintana Mora,Melissa J. Nirenberg,Ruth H. Walker,Yolfrankcis Mendez,Susan Morgello,Dennis W. Dickson,Melissa E. Murray,Carlos Cordon-Cardo,Nadejda M. Tsankova,Jamie M. Walker,Diana K. Dangoor,Stephanie McQuillan,Emma L. Thorn,Claudia De Sanctis,Shuying Li,Thomas J. Fuchs,Kurt Farrell,John F. Crary,Gabriele Campanella*

Main category: cs.CV

TL;DR: NeuroFM：首个专门针对神经病理学领域训练的病理学基础模型，在神经退行性疾病分析任务上优于通用病理学基础模型


<details>
  <summary>Details</summary>
Motivation: 现有病理学基础模型主要基于外科病理数据训练，这些数据过度代表非神经系统组织和疾病，缺乏神经病理学特有的细胞类型、结构和病理特征，导致在神经退行性疾病分析上存在领域不匹配问题

Method: 开发NeuroFM，专门在涵盖多种神经退行性病理的脑组织全切片图像上进行训练，针对神经病理学领域进行专业化建模

Result: NeuroFM在多个神经病理学特定下游任务上表现优于通用基础模型，包括混合性痴呆疾病分类、海马区域分割、神经退行性共济失调识别（涵盖小脑性特发性震颤和脊髓小脑性共济失调亚型）

Conclusion: 领域专业化基础模型在神经病理学特征捕捉方面优于通用模型，NeuroFM为数字病理学专业领域的特定模型开发树立了先例，能够为脑疾病诊断和研究提供更准确可靠的AI分析

Abstract: Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.

</details>


### [5] [FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting](https://arxiv.org/abs/2512.05996)
*Yi Liu,Jingyu Song,Vedanth Kallakuri,Katherine A. Skinner*

Main category: cs.CV

TL;DR: FishDetector-R1是一个基于多模态大语言模型的弱监督框架，用于水下鱼类检测、分割和计数，在DeepFish数据集上显著提升了性能指标。


<details>
  <summary>Details</summary>
Motivation: 水下鱼类图像分析对生态监测至关重要，但由于视觉质量下降和标注成本高昂而面临困难，需要开发弱监督的解决方案。

Method: 提出了统一的MLLM框架，包含两个关键组件：1）新颖的检测到计数提示，确保空间一致的检测和计数；2）基于可验证奖励的强化学习（RLVR），利用稀疏点标签的可扩展范式。

Result: 在DeepFish数据集上，AP提升20%，mIoU提升10%，MAE降低30%，GAME降低35%。消融研究验证了奖励设计的有效性，且在其他水下数据集上表现出良好的跨域鲁棒性。

Conclusion: FishDetector-R1通过弱监督为准确的水下视觉理解提供了可靠且可扩展的解决方案，在生态监测中具有重要应用价值。

Abstract: Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.

</details>


### [6] [High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing](https://arxiv.org/abs/2512.06012)
*Emmanuel Akeweje,Conall Kirk,Chi-Wai Chan,Denis Dowling,Mimi Zhang*

Main category: cs.CV

TL;DR: 开发基于机器学习的自动框架，通过高通量成像和聚类分析大规模表征金属粉末形态，为SLM工艺提供实时原料监控


<details>
  <summary>Details</summary>
Motivation: 传统粉末表征方法通量低且定性，无法捕捉工业规模批次的异质性，而SLM零件质量严重依赖原料形态

Method: 开发三种聚类流程：自编码器流程、形状描述符流程和函数数据流程，在约126,000个粉末图像数据集上评估，使用傅里叶描述符+k-means方法

Result: 傅里叶描述符+k-means流程最有效，获得最低的Davies-Bouldin指数和最高的Calinski-Harabasz分数，同时在标准工作站上每个颗粒保持亚毫秒级运行时间

Conclusion: 该无监督学习框架实现了粉末形态的快速自动评估，支持跟踪重复使用周期中的形状演变，为SLM工作流程中的实时原料监控提供了途径

Abstract: Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.

</details>


### [7] [VAT: Vision Action Transformer by Unlocking Full Representation of ViT](https://arxiv.org/abs/2512.06013)
*Wenhao Li,Chengwei Ma,Weixin Mao*

Main category: cs.CV

TL;DR: Vision Action Transformer (VAT) 通过利用ViT所有层的特征层次结构，实现感知与动作生成的深度融合，在机器人模仿学习中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有机器人学习方法中，Vision Transformers通常只使用最后一层的特征，丢弃了有价值的信息，导致表示能力不足

Method: 提出Vision Action Transformer (VAT)，从ViT扩展而来，处理专门的动作令牌与所有Transformer层的视觉特征，实现感知与动作生成的渐进式深度融合

Result: 在四个LIBERO基准测试中达到98.15%的平均成功率，超越了OpenVLA-OFT等先前方法，建立了新的SOTA

Conclusion: VAT不仅是一个强大的模仿学习模型，还证明了利用视觉模型完整"表示轨迹"对于推进机器人策略的重要性

Abstract: In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.

</details>


### [8] [Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets](https://arxiv.org/abs/2512.06014)
*Jiho Shin,Dominic Marshall,Matthieu Komorowski*

Main category: cs.CV

TL;DR: 该研究对两个大规模胸部X光嵌入模型（CXR-Foundation和MedImageInsight）在公开数据集上进行标准化基准测试，发现MedImageInsight性能略优，而CXR-Foundation在跨数据集稳定性方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学图像表示学习方面表现出色，但它们在跨数据集上的比较行为尚未得到充分探索。本研究旨在对两个大型胸部X光嵌入模型进行标准化基准测试，为医学基础模型的评估建立可复现的基线。

Method: 使用统一的预处理流程和固定的下游分类器，在MIMIC-CR和NIH ChestX-ray14数据集上评估两个模型。直接从预训练编码器提取嵌入，使用轻量级LightGBM分类器在多个疾病标签上进行训练，报告平均AUROC和F1分数及95%置信区间。

Result: MedImageInsight在大多数任务中性能略高，而CXR-Foundation表现出更强的跨数据集稳定性。MedImageInsight嵌入的无监督聚类显示出与定量结果一致的疾病特异性结构。

Conclusion: 研究结果强调了医学基础模型标准化评估的必要性，并为未来多模态和临床整合研究建立了可复现的基线。

Abstract: Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.

</details>


### [9] [PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation](https://arxiv.org/abs/2512.06020)
*Wenyi Mo,Tianyu Zhang,Yalong Bai,Ligong Han,Ying Ba,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 提出一个多模态框架，利用MLLM提取用户偏好表征并注入扩散模型，通过偏好导向的视觉问答和判别任务学习用户特征，使用MMD对齐损失确保与扩散编码器兼容，实现个性化图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉用户细微的审美偏好，缺乏有效编码个性化视觉信号的机制，需要开发能更好理解并反映用户个人喜好的图像生成方法。

Method: 1) 训练MLLM进行偏好导向的视觉问答以捕捉语义线索；2) 引入用户间和用户内判别任务分离偏好相关特征；3) 设计基于最大均值差异的对齐损失连接模态差异；4) 将学习到的嵌入用于条件化扩散生成器。

Result: 实验表明该方法在图像质量和偏好对齐方面显著优于现有基线，验证了表征提取和对齐策略在个性化生成中的有效性。

Conclusion: 通过多模态框架提取丰富的用户表征并有效对齐到扩散模型，实现了既能忠实于文本提示又能准确反映用户个人偏好的高质量图像生成。

Abstract: Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.

</details>


### [10] [The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation](https://arxiv.org/abs/2512.06032)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.CV

TL;DR: 本文分析了SAM2和SAM3之间的根本性差异，解释了SAM2的基于提示的分割专业知识为何无法迁移到SAM3的多模态概念驱动范式，揭示了从几何分割到语义概念理解的根本转变。


<details>
  <summary>Details</summary>
Motivation: 研究SAM2和SAM3之间的根本性不连续性，解释为什么SAM2的基于提示的分割专业知识无法迁移到SAM3的多模态概念驱动范式，阐明从纯几何分割到语义概念理解的根本转变。

Method: 通过五个核心组件进行结构化分析：(1)概念性断裂：对比SAM2的空间提示语义与SAM3的多模态融合和文本条件掩码生成；(2)架构差异：详细对比纯视觉-时间设计与视觉语言编码器、几何和示例编码器、融合模块、DETR风格解码器、对象查询和专家混合的集成；(3)数据集和标注差异：对比SA-V视频掩码与SAM3的多模态概念标注语料库；(4)训练和超参数区别：展示为什么SAM2优化知识不适用于SAM3；(5)评估、指标和失败模式：概述从几何IoU指标到语义开放词汇评估的转变。

Result: 分析确立了SAM3作为一种新的分割基础模型类别，揭示了从基于提示的几何分割到概念驱动的语义分割的根本转变，展示了两种模型在架构、训练、评估等方面的本质差异。

Conclusion: SAM3代表了分割基础模型的新类别，标志着从几何分割到概念驱动分割时代的转变，为新兴的概念驱动分割领域指明了未来发展方向。

Abstract: This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.

</details>


### [11] [EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing](https://arxiv.org/abs/2512.06065)
*Runjia Li,Moayed Haji-Ali,Ashkan Mirzaei,Chaoyang Wang,Arpit Sahni,Ivan Skorokhodov,Aliaksandr Siarohin,Tomas Jakab,Junlin Han,Sergey Tulyakov,Philip Torr,Willi Menapace*

Main category: cs.CV

TL;DR: 提出EgoEdit生态系统，用于第一人称视角视频的指令引导编辑，解决现有方法在快速自我运动和手物交互方面的挑战，支持实时交互式AR应用。


<details>
  <summary>Details</summary>
Motivation: 现有AI视频编辑器在第三人称视频上表现良好，但第一人称视角存在独特挑战：快速自我运动、频繁的手物交互造成显著领域差距，且现有离线编辑流程延迟高，限制了实时交互。

Method: 构建完整的生态系统：1) EgoEditData - 专门为第一人称编辑场景设计的手动策划数据集，包含丰富的手物交互并明确保留手部；2) EgoEdit - 支持指令跟随的第一人称视频编辑器，可在单GPU上实现实时流式推理；3) EgoEditBench - 针对指令忠实度、手部和交互保留、自我运动下时间稳定性的评估套件。

Result: EgoEdit在第一人称和通用编辑任务中都产生时间稳定、指令忠实的结果，具有交互式延迟。在第一人称编辑基准上取得明显优势（现有方法在此表现不佳），同时在通用编辑任务上保持与最强基线相当的性能。

Conclusion: 提出的EgoEdit生态系统成功解决了第一人称视频编辑的独特挑战，实现了实时交互式AR应用所需的低延迟和高质量编辑。EgoEditData和EgoEditBench将公开供研究社区使用。

Abstract: We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit

</details>


### [12] [Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light](https://arxiv.org/abs/2512.06080)
*Tzofi Klinghoffer,Siddharth Somasundaram,Xiaoyu Xiang,Yuchen Fan,Christian Richardt,Akshat Dave,Ramesh Raskar,Rakesh Ranjan*

Main category: cs.CV

TL;DR: 利用单光子激光雷达通过多跳光传输恢复遮挡区域和镜面反射场景的3D几何结构，提出数据驱动方法处理多路复用照明下的复杂光传输问题。


<details>
  <summary>Details</summary>
Motivation: 单视角3D场景重建面临遮挡区域和镜面材料（如镜子）的挑战，传统单光子激光雷达通常需要逐点扫描，而多路复用照明下的复杂光传输难以解析求解。

Method: 提出数据驱动方法处理单光子激光雷达中的光传输逆问题，创建首个大规模室内场景激光雷达瞬态数据集（约10万样本），学习复杂光传输先验，将测量的双跳光分解为各激光点的贡献。

Result: 实验证明该方法能够从单次测量中推断具有遮挡和镜面反射场景的3D几何结构，代码和数据集已开源。

Conclusion: 通过数据驱动方法处理多路复用照明下的复杂光传输，单光子激光雷达能够从单次测量中恢复密集深度、遮挡几何和材料属性，为实际应用提供了可行方案。

Abstract: 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.

</details>


### [13] [BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving](https://arxiv.org/abs/2512.06096)
*Karthik Mohan,Sonam Singh,Amit Arvind Kale*

Main category: cs.CV

TL;DR: BeLLA是一个端到端架构，将统一的360°鸟瞰图表示与大型语言模型连接，用于自动驾驶问答任务，在需要空间推理的问题上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自动驾驶研究中存在局限性：要么使用单视角编码器无法利用多摄像头系统的空间结构，要么使用聚合的多视角特征缺乏统一的空间表示，难以进行自我中心方向、物体关系和更广泛上下文的推理。

Method: 提出BeLLA端到端架构，将统一的360°鸟瞰图表示与大型语言模型连接，用于自动驾驶问答。该方法利用BEV表示提供统一的空间表征，结合LLM进行推理。

Result: 在NuScenes-QA和DriveLM两个基准测试中，BeLLA在需要空间推理的问题上（如相对物体定位和附近物体行为理解）持续优于现有方法，某些任务上获得高达+9.3%的绝对提升。在其他类别中也表现具有竞争力。

Conclusion: BeLLA通过结合统一的360°BEV表示和大型语言模型，显著提升了自动驾驶问答任务中的空间推理能力，特别是在需要理解物体相对位置和行为的问题上表现突出。

Abstract: The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360° BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.

</details>


### [14] [SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection](https://arxiv.org/abs/2512.06103)
*Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: 提出SpectraIrisPAD框架，利用多光谱成像和DINOv2 Vision Transformer进行虹膜呈现攻击检测，显著提升检测鲁棒性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 虹膜识别虽准确但易受呈现攻击，传统近红外系统存在局限性，多光谱成像能提供互补反射信息以增强攻击检测的泛化能力

Method: 基于DINOv2 Vision Transformer架构，引入可学习光谱位置编码、令牌融合和对比学习，提取区分性波段特征；创建包含5个近红外波段的MSIrPAD数据集

Result: SpectraIrisPAD在未见攻击评估协议下全面优于现有基线方法，在各项性能指标上均表现最佳，展示出卓越的鲁棒性和泛化能力

Conclusion: 多光谱成像结合深度学习能有效提升虹膜呈现攻击检测性能，提出的框架和数据集为虹膜生物识别系统的安全性提供了重要保障

Abstract: Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\,nm, 830\,nm, 850\,nm, 870\,nm, and 980\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.

</details>


### [15] [Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation](https://arxiv.org/abs/2512.06105)
*Junwen Zheng,Xinran Xu,Li Rong Wang,Chang Cai,Lucinda Siyun Tan,Dingyuan Wang,Hong Liang Tey,Xiuyi Fan*

Main category: cs.CV

TL;DR: 提出CEFM框架，通过对比学习将临床ABC标准映射到视觉特征空间，生成结构化文本解释，提高黑色素瘤分类的可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在黑色素瘤分类中已达到专家水平，但模型不透明和缺乏可解释性阻碍了临床采用，医生难以信任黑盒模型的决策过程。

Method: CEFM框架利用对比学习作为核心机制，通过双投影头将临床诊断标准（ABC规则）映射到Vision Transformer嵌入空间，将临床语义与视觉特征对齐，然后通过自然语言生成将对齐的表征转化为结构化文本解释。

Result: 在公共数据集上达到92.79%准确率和0.961的AUC，在多个可解释性指标上有显著提升。定性分析显示学习到的嵌入空间排列与医生应用ABC规则的方式一致。

Conclusion: CEFM框架有效弥合了高性能分类与临床信任之间的差距，通过将临床标准与视觉特征对齐并生成可理解的解释，提高了黑色素瘤诊断系统的透明度和可信度。

Abstract: Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.

</details>


### [16] [Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation](https://arxiv.org/abs/2512.06158)
*Su Sun,Cheng Zhao,Himangi Mittal,Gaurav Mittal,Rohith Kukkala,Yingjie Victor Chen,Mei Chen*

Main category: cs.CV

TL;DR: Track4DGen：两阶段框架，通过结合多视角视频扩散模型、基础点追踪器和混合4D高斯溅射重建器，从稀疏输入生成动态4D对象，解决外观漂移和时间不一致问题。


<details>
  <summary>Details</summary>
Motivation: 从稀疏输入生成动态4D对象具有挑战性，需要同时保持外观和运动在视角和时间上的一致性，同时抑制伪影和时间漂移。现有方法仅依赖像素或潜在空间的视频扩散损失监督，缺乏明确的时间感知特征级追踪指导。

Method: Track4DGen采用两阶段框架：第一阶段在多视角视频扩散生成器中强制密集特征级点对应关系，产生时间一致的特征；第二阶段使用混合运动编码重建4D高斯溅射，结合扩散特征（携带追踪先验）、Hex-plane特征和4D球谐函数进行高保真动态建模。

Result: Track4DGen在多视角视频生成和4D生成基准测试中均超越基线方法，产生时间稳定、可文本编辑的4D资产。此外，还创建了Sketchfab28高质量数据集用于对象中心4D生成基准测试。

Conclusion: 通过显式注入追踪器衍生的运动先验到多视角视频生成和4D高斯溅射的中间特征表示中，Track4DGen有效解决了外观漂移和视角不一致问题，为动态4D对象生成提供了有效解决方案。

Abstract: Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.

</details>


### [17] [Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection](https://arxiv.org/abs/2512.06171)
*Jessica Plassmann,Nicolas Schuler,Michael Schuth,Georg von Freymann*

Main category: cs.CV

TL;DR: 本文提出了一种自动化工作流程，利用深度学习从剪切散斑测量中生成缺陷标注，减少人工标注工作量，支持可扩展的数据集创建。


<details>
  <summary>Details</summary>
Motivation: 剪切散斑技术虽然对表面位移梯度敏感，能有效检测安全关键部件的亚表面缺陷，但其工业应用受到高质量标注数据集缺乏的限制。人工标注劳动密集、主观性强且难以标准化。

Method: 引入自动化工作流程，利用深度学习从剪切散斑测量中生成缺陷标注，包括高分辨率分割和边界框标签。

Result: 与专家标注数据对比评估显示，该方法具有足够的准确性，能够支持弱监督训练，减少人工工作量。

Conclusion: 该自动化工作流程能够支持可扩展的数据集创建，为稳健的缺陷检测提供基础。

Abstract: Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.

</details>


### [18] [Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction](https://arxiv.org/abs/2512.06174)
*Shilin Hu,Jingyi Xu,Akshat Dave,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 该论文提出了一种将显式物理建模（几何与光照）嵌入深度学习阴影生成的新框架，通过单目RGB图像获取近似3D几何和主导光方向，基于阴影形成物理原理生成初始阴影估计，再用扩散模型进行精细化处理。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的阴影生成方法很少利用显式的物理建模，而阴影形成本质上遵循物理规律（遮挡物阻挡光线形成阴影）。论文旨在将几何和光照的显式物理建模融入深度学习框架，生成既视觉逼真又物理一致的阴影。

Method: 1. 从单目RGB图像获取密集点云表示的近似3D几何；2. 预测主导光方向；3. 基于阴影形成物理原理计算初始阴影位置和形状；4. 将物理基础估计输入扩散框架进行精细化，生成高保真阴影外观，同时保持与场景几何和光照的一致性。

Result: 在DESOBAV2数据集上训练，模型生成的阴影既视觉逼真又物理一致，优于现有方法，特别是在几何复杂或光照模糊的场景中表现更佳。

Conclusion: 通过将显式物理建模（几何和光照）嵌入深度学习框架，结合物理基础估计和扩散模型精细化，能够生成视觉逼真且物理一致的阴影，在复杂场景中表现优于现有方法。

Abstract: Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.

</details>


### [19] [Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction](https://arxiv.org/abs/2512.06179)
*Shilin Hu,Jingyi Xu,Sagnik Das,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: 提出联合检测投射阴影和附着阴影的框架，通过光照和几何推理实现两种阴影的相互增强检测


<details>
  <summary>Details</summary>
Motivation: 现有阴影检测方法主要针对投射阴影，缺乏专门的附着阴影数据集和模型，而附着阴影对理解物体三维结构至关重要

Method: 构建包含阴影检测模块和光照估计模块的系统，通过闭环推理过程迭代优化阴影分割和光照估计

Result: 实验结果表明该方法显著提升附着阴影检测性能（BER降低至少33%），同时保持对完整阴影和投射阴影的良好检测效果

Conclusion: 通过光照-几何联合推理框架能够有效解决附着阴影检测问题，为场景理解提供更全面的阴影分析能力

Abstract: Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.

</details>


### [20] [Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying](https://arxiv.org/abs/2512.06190)
*Shichen Li,Ahmadreza Eslaminia,Chenhui Shao*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态颜色轨迹预测方法，通过整合高维时间颜色信息与干燥工艺参数，实现准确且数据高效的颜色轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖低维颜色特征，无法充分捕捉食品样品复杂的动态颜色轨迹，且现有建模方法缺乏对未见工艺条件的泛化能力。

Method: 开发了一种多模态颜色轨迹预测方法，整合高维时间颜色信息与干燥工艺参数，实现准确且数据高效的颜色轨迹预测。

Result: 在未见干燥条件下，模型在饼干干燥中达到RMSE 2.12，苹果干燥中达到RMSE 1.29，相比基线模型误差减少超过90%。

Conclusion: 实验结果表明该模型具有优越的准确性、鲁棒性和广泛的适用性，能够有效预测食品干燥过程中的颜色变化轨迹。

Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.

</details>


### [21] [The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning](https://arxiv.org/abs/2512.06206)
*Akis Linardos,Sarthak Pati,Ujjwal Baid,Brandon Edwards,Patrick Foley,Kevin Ta,Verena Chung,Micah Sheller,Muhammad Irfan Khan,Mojtaba Jafaritadi,Elina Kontio,Suleiman Khan,Leon Mächler,Ivan Ezhov,Suprosanna Shit,Johannes C. Paetzold,Gustav Grimberg,Manuel A. Nickel,David Naccache,Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni,Daewoon Kim,Leonard L. Klausmann,Prashant Shah,Bjoern Menze,Dimitrios Makris,Spyridon Bakas*

Main category: cs.CV

TL;DR: MICCAI FeTS 2024挑战赛评估了用于胶质瘤亚区分割的联邦学习方法，PID控制器方法在分割性能和通信效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 该挑战赛旨在推动医学影像中联邦学习的发展，特别关注胶质瘤亚区分割任务，并评估新的权重聚合方法以提高鲁棒性和效率。

Method: 使用标准化的联邦学习设置和多机构数据集（来自BraTS基准），包含1251个训练案例、219个验证案例和570个隐藏测试案例。采用累积评分系统，综合考虑分割性能（DSC和HD95）和通信效率（收敛分数）。

Result: 基于PID控制器的方法获得最高排名，ET、TC、WT的DSC平均值分别为0.733、0.761、0.751，HD95值分别为33.922mm、33.623mm、32.309mm，收敛分数为0.764，通信效率最高。

Conclusion: 该研究推进了医学影像联邦学习的发展，超越了先前挑战赛的最佳方法，证明PID控制器是稳定和优化联邦学习中权重聚合的有效机制。

Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.

</details>


### [22] [Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study](https://arxiv.org/abs/2512.06221)
*Alena Makarova*

Main category: cs.CV

TL;DR: 该研究对结合奇异值分解(SVD)和小波差分缩减(WDR)的图像压缩方法进行了可重复性验证，发现原论文声称的优于JPEG2000和单独WDR的结果无法复现，实际性能并未超越JPEG2000。


<details>
  <summary>Details</summary>
Motivation: 验证SVD+WDR图像压缩技术的原始性能声称，检查其可重复性，并识别原论文中缺失的实现细节对结果的影响。

Method: 重新实现SVD+WDR方法，仔细填补原论文中缺失的实现细节（如量化和阈值初始化），尽可能复现原始实验，并在新图像上进行额外测试，使用PSNR和SSIM作为评估指标。

Result: 复现结果显示SVD+WDR方法在PSNR方面通常无法超越JPEG2000或WDR，仅在SSIM方面相对于JPEG2000有部分改进，与原论文声称的优越性能不符。

Conclusion: 原论文中的模糊描述（如量化和阈值初始化细节）显著影响了方法的可重复性和报告性能，强调了在学术研究中提供完整实现细节的重要性。

Abstract: This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.

</details>


### [23] [Opinion: Learning Intuitive Physics May Require More than Visual Data](https://arxiv.org/abs/2512.06232)
*Ellen Su,Solim Legris,Todd M. Gureckis,Mengye Ren*

Main category: cs.CV

TL;DR: 研究探讨数据分布而非数据量是否是学习直觉物理的关键，通过在发展现实的儿童视频数据集上训练模型，发现仅使用发展现实数据不足以让当前架构学习直觉物理表示。


<details>
  <summary>Details</summary>
Motivation: 人类通过基于直觉物理理解的丰富内部模型来熟练导航世界，而尽管在大量互联网视频数据上训练，最先进的深度学习模型在直觉物理基准测试中仍达不到人类水平。本研究旨在探究数据分布（而非数据量）是否是学习这些物理原理的关键。

Method: 在SAYCam数据集上预训练视频联合嵌入预测架构（V-JEPA）模型。SAYCam是一个发展现实、自我中心的视频数据集，部分捕捉了三个儿童的日常视觉体验，其数据量仅为最先进模型训练数据量的0.01%。

Result: 在这个发展现实的数据集上训练并未导致IntPhys2基准测试性能的显著提升。结果表明，仅使用发展现实数据集训练不足以让当前架构学习支持直觉物理的表示。

Conclusion: 仅改变视觉数据量和分布可能不足以构建具有人工直觉物理的系统。需要更深入的方法来使AI系统获得类似人类的物理直觉理解能力。

Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.

</details>


### [24] [Language-driven Fine-grained Retrieval](https://arxiv.org/abs/2512.06255)
*Shijie Wang,Xin Yu,Yadan Luo,Zijian Wang,Pengfei Zhang,Zi Huang*

Main category: cs.CV

TL;DR: LaFG是一个语言驱动的细粒度图像检索框架，通过大语言模型将类别名称转换为属性级监督，解决了传统方法依赖稀疏one-hot标签、难以泛化到未见类别的问题。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度图像检索方法使用基于类别名称的稀疏one-hot标签作为监督，虽然对已见类别有效，但忽略了类别名称中丰富的语义信息，阻碍了跨类别细节可比性的建模，限制了模型对未见类别的泛化能力。

Method: LaFG框架：1）使用大语言模型将类别名称转换为详细的属性导向描述；2）利用冻结的视觉语言模型将这些描述投影到视觉对齐空间，聚类形成数据集范围的属性词汇表；3）通过全局提示模板选择类别相关属性，聚合成类别特定的语言原型；4）用这些原型监督检索模型。

Result: 论文未在摘要中提供具体实验结果，但方法旨在通过语言驱动的属性级监督，提升细粒度图像检索模型对未见类别的泛化能力。

Conclusion: LaFG通过将类别名称转换为丰富的属性级监督，利用语言模型挖掘语义信息，为细粒度图像检索提供了更有效的监督信号，有望提升模型对未见类别的泛化性能。

Abstract: Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer

</details>


### [25] [Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs](https://arxiv.org/abs/2512.06258)
*Chaoyang Wang,Yangfan He,Yiyang Zhou,Yixuan Wang,Jiaqi Liu,Peng Xia,Zhengzhong Tu,Mohit Bansal,Huaxiu Yao*

Main category: cs.CV

TL;DR: 论文揭示了大型视觉语言模型存在路径选择偏差问题：即使知道正确答案，也常通过错误推理路径得出结果。作者提出了PSO两阶段优化框架，通过路径选择优化提升推理性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型存在一个关键但未被充分探索的缺陷：即使模型知道正确答案，也经常通过错误的推理路径到达那里。核心问题不是缺乏知识，而是在广阔的推理搜索空间中的路径选择偏差。模型虽然能够采样正确的解决方案轨迹，但不成比例地倾向于不稳定或逻辑不一致的路径，导致结果不稳定和不可靠。

Method: 提出了PSO（路径选择优化）两阶段后训练框架。第一阶段：使用带有模板和答案奖励的组相对策略优化来培养结构化、逐步推理。第二阶段：进行在线偏好优化，模型从GRPO生成的数据中采样推理路径，自我评估它们，并向首选轨迹对齐。错误或次优路径同时存储在负向回放记忆中作为硬负样本，定期重新访问以防止模型重复先前错误并促进持续推理改进。

Result: 广泛实验表明，PSO有效修剪无效推理路径，显著提高推理准确性（平均提升7.4%），并产生更稳定和一致的思维链。

Conclusion: 论文揭示了LVLMs中路径选择偏差的关键问题，并提出了有效的PSO框架来解决这一问题。该方法不仅提高了推理准确性，还增强了推理过程的稳定性和一致性，为改进大型视觉语言模型的推理能力提供了新思路。

Abstract: We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.

</details>


### [26] [TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06269)
*Quan Tran,Tuan Dang*

Main category: cs.CV

TL;DR: 本文提出了一种通过约束多视角三角测量增强全局几何一致性的方法，解决了3D高斯泼溅中仅依赖光度损失导致的几何不一致问题，在DTU数据集上取得了0.50mm的平均倒角距离，优于同类显式方法。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术虽然能实时合成新视角且渲染逼真，但仅依赖光度损失构建3D高斯会导致重建不一致，产生"漂浮物"伪影和非结构化几何，阻碍高质量表面提取。

Method: 提出通过约束多视角三角测量增强全局几何一致性的方法，利用多个估计视角达成物理世界3D表示的共识，通过自监督方式从相邻视角束重新三角化得到鲁棒共识点，并惩罚渲染3D点与共识点的偏差来优化过程。

Result: 在多个数据集上验证了方法的有效性，取得了最先进的结果。在DTU数据集上，平均倒角距离达到0.50mm，优于同类显式方法。

Conclusion: 提出的方法通过增强全局几何一致性，有效解决了3D高斯泼溅中的几何不一致问题，提高了重建质量，并将开源代码以便社区验证和可重复性。

Abstract: 3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in "floater" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.

</details>


### [27] [FacePhys: State of the Heart Learning](https://arxiv.org/abs/2512.06275)
*Kegang Wang,Jiankai Tang,Yuntao Wang,Xin Liu,Yuxuan Fan,Jiatong Ji,Yuanchun Shi,Daniel McDuff*

Main category: cs.CV

TL;DR: FacePhys是一种基于时空状态空间对偶性的内存高效远程光电容积描记算法，解决了模型可扩展性、跨数据集泛化和实时操作的三难问题，实现了49%的错误率降低和实时推理。


<details>
  <summary>Details</summary>
Motivation: 基于摄像头的生命体征测量技术（特别是远程光电容积描记）为舒适、普适的健康监测提供了机会，但实际部署受到前端设备计算限制和数据压缩传输导致信号质量下降的制约。

Method: 提出FacePhys算法，基于时空状态空间对偶性，利用可转移的心脏状态捕捉视频帧间的细微周期性变化，同时保持最小的计算开销，支持长视频序列训练和低延迟推理。

Result: FacePhys实现了最先进的性能，错误率降低49%，内存占用仅3.6MB，每帧延迟9.46ms，比现有方法提升83%到99%，在实际部署中提供可靠的实时性能。

Conclusion: FacePhys通过创新的时空状态空间对偶性设计，成功解决了rPPG技术在实际部署中的计算限制和精度问题，为实时健康监测应用提供了可行的解决方案。

Abstract: Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\% to 99\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.

</details>


### [28] [RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension](https://arxiv.org/abs/2512.06276)
*Tianyi Gao,Hao Li,Han Fang,Xin Wei,Xiaodong Dong,Hongbo Sun,Ye Yuan,Zhongjiang He,Jinglin Xu,Jingmin Xin,Hao Sun*

Main category: cs.CV

TL;DR: RefBench-PRO：一个将指代表达理解分解为感知和推理两个维度、六个子任务的综合基准，包含自动数据生成流程和基于强化学习的Ref-R1方法，用于评估多模态大语言模型的指代表达理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有REC基准主要评估感知能力，缺乏可解释的评分机制，无法揭示多模态大语言模型在不同认知能力上的定位能力。需要一个新的基准来全面评估MLLM在指代表达理解中的表现。

Method: 1. 提出RefBench-PRO基准，将指代表达分解为感知和推理两个核心维度，进一步细分为六个渐进挑战性任务：属性、位置、交互、常识、关系和拒绝。2. 开发全自动数据生成流程，为六个子维度生成多样化的指代表达。3. 提出Ref-R1学习方案，采用基于动态IoU的GRPO强化学习方法来提高复杂推理条件下的定位精度。

Result: 大量实验表明，RefBench-PRO能够对MLLM在指代表达理解上进行可解释的评估，在感知和推理方面都提出了更大的挑战。该方法为REC建立了更强的基线。

Conclusion: RefBench-PRO是一个全面的REC基准，通过分解指代表达的认知维度并提供可解释的评估，能够更全面地评估MLLM的指代表达理解能力。提出的Ref-R1方法在复杂推理条件下提高了定位精度，为REC研究提供了新的基准和方法。

Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.

</details>


### [29] [A Sleep Monitoring System Based on Audio, Video and Depth Information](https://arxiv.org/abs/2512.06282)
*Lyn Chao-ling Chen,Kuan-Wen Chen,Yi-Ping Hung*

Main category: cs.CV

TL;DR: 开发基于事件方法的非侵入式睡眠监测系统，通过红外深度传感器、RGB摄像头和四麦克风阵列检测运动、开关灯和噪音三类事件，用于家庭环境下的睡眠障碍定量评估。


<details>
  <summary>Details</summary>
Motivation: 需要一种非侵入式的定量评估睡眠障碍的方法，能够在家庭环境中监测睡眠干扰事件，为睡眠质量评估提供客观数据。

Method: 使用红外深度传感器、RGB摄像头和四麦克风阵列设备，在低光照环境下监测睡眠。建立深度信号的背景模型检测运动幅度，建立彩色图像的背景模型检测光照变化，采用事件检测算法从三类传感器处理数据中检测事件发生。

Result: 系统在睡眠条件下进行了测试，实验结果验证了系统的可靠性，能够有效检测运动、光照变化和噪音三类睡眠干扰事件。

Conclusion: 基于事件的多传感器非侵入式睡眠监测系统能够有效量化评估睡眠障碍，为家庭环境下的睡眠质量监测提供了可靠的技术方案。

Abstract: For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.

</details>


### [30] [ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models](https://arxiv.org/abs/2512.06328)
*Jiahao Li,Yusheng Luo,Yunzhong Lou,Xiangdong Zhou*

Main category: cs.CV

TL;DR: ReCAD是一个强化学习框架，通过引导预训练大模型生成精确的参数化CAD模型，显著提升了文本到CAD和图像到CAD任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖监督微调注入知识，对可编辑性支持有限，且未能充分利用预训练大模型的强大生成先验。需要开发一个能利用大模型内在生成能力、支持复杂CAD操作并确保几何精度和语义保真度的框架。

Method: 1. 微调视觉语言模型，使其具备基本CAD模型生成能力，将CAD脚本重写为参数化代码用于监督生成准确文本描述；2. 提出新颖的强化学习策略，以参数化代码为指导增强模型在挑战性问题上的推理能力；3. 采用分层基元学习过程，在统一奖励函数下逐步教授结构化组合技能。

Result: ReCAD在文本到CAD和图像到CAD任务中均达到最先进水平，显著提升了几何精度。在图像到CAD任务中，将平均Chamfer距离从73.47降至29.61（分布内）和从272.06降至80.23（分布外），大幅超越现有基线方法。

Conclusion: ReCAD框架成功利用强化学习引导预训练大模型生成精确的参数化CAD模型，通过参数化代码指导和分层基元学习实现了几何精度和语义保真度的双重提升，为多模态输入到CAD模型的生成提供了有效解决方案。

Abstract: We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.

</details>


### [31] [S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening](https://arxiv.org/abs/2512.06330)
*Haoyu Zhang,Junhan Luo,Yugang Cao,Siran Peng,Jie Huang,Liangjian-Deng*

Main category: cs.CV

TL;DR: S2WMamba：一种通过2D/1D小波变换显式解耦频率信息，结合Mamba跨模态交互的轻量级全色锐化方法，在多个数据集上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 全色锐化中联合处理PAN和MS图像时，空间细节与光谱保真度容易纠缠在一起，导致性能受限。需要一种能够显式解耦频率信息并进行有效跨模态交互的方法。

Method: 1) 对PAN图像应用2D Haar DWT定位空间边缘和纹理；2) 对每个像素的光谱应用通道级1D Haar DWT分离低频/高频分量以限制光谱失真；3) 构建光谱分支（注入小波提取的空间细节到MS特征）和空间分支（使用1D金字塔的光谱细化PAN特征）；4) 通过基于Mamba的跨调制实现分支间信息交换，建模长程依赖且保持线性复杂度；5) 使用多尺度动态门（乘法+加法）自适应融合分支输出。

Result: 在WV3、GF2和QB数据集上，S2WMamba匹配或超越了FusionMamba、CANNet、U2Net、ARConv等强基线，PSNR提升最高达0.23 dB，在WV3全分辨率上达到HQNR 0.956。消融实验验证了2D/1D DWT布局、并行双分支和融合门设计的有效性。

Conclusion: S2WMamba通过显式频率解耦和轻量级跨模态交互，有效解决了全色锐化中空间细节与光谱保真度的纠缠问题，在多个数据集上取得了优异的性能表现。

Abstract: Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.

</details>


### [32] [Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate](https://arxiv.org/abs/2512.06344)
*Kaile Wang,Lijun He,Haisheng Fu,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: MTGC框架通过多模态引导增强超低码率生成式图像压缩的语义一致性，解决生成幻觉问题


<details>
  <summary>Details</summary>
Motivation: 生成式图像压缩在超低码率下存在语义偏差问题，限制了其在6G语义通信中的可靠部署，需要增强语义一致性

Method: 提出MTGC框架，集成三种引导模态：文本描述、高压缩图像和语义伪词；设计任务感知语义压缩模块提取任务相关语义；采用双路径协同引导机制的多模态引导扩散解码器

Result: 在DIV2K数据集上DISTS指标下降10.59%，在超低码率下显著提升语义一致性、感知质量和像素级保真度

Conclusion: MTGC框架有效解决了超低码率生成式图像压缩的语义偏差问题，为6G语义通信提供了可靠的图像压缩解决方案

Abstract: Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.

</details>


### [33] [CLUENet: Cluster Attention Makes Neural Networks Have Eyes](https://arxiv.org/abs/2512.06345)
*Xiangshuai Song,Jun-Jie Huang,Tianrui Liu,Ke Liang,Chang Tang*

Main category: cs.CV

TL;DR: CLUENet是一种基于聚类范式的透明视觉语义理解架构，通过全局软聚合硬分配、温度缩放余弦注意力、门控残差连接、硬共享特征调度和改进的聚类池化策略，在准确率、效率和可解释性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 卷积和注意力模型在视觉任务中虽然成功，但其固定的感受野和复杂架构限制了它们对不规则空间模式的建模能力，并影响了模型的可解释性。聚类范式提供了良好的可解释性和灵活的语义建模，但存在准确率有限、效率低和训练时梯度消失等问题。

Method: 提出了CLUENet架构，包含三个关键创新：(1) 全局软聚合和硬分配，结合温度缩放余弦注意力和门控残差连接以增强局部建模；(2) 块间硬共享特征调度；(3) 改进的聚类池化策略。

Result: 在CIFAR-100和Mini-ImageNet数据集上的实验表明，CLUENet超越了现有的聚类方法和主流视觉模型，在分类性能和视觉可解释性方面都有显著提升。

Conclusion: CLUENet为视觉语义理解提供了一个透明深度架构，在准确性、效率和透明度之间取得了令人信服的平衡，解决了传统聚类方法的局限性。

Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.

</details>


### [34] [TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search](https://arxiv.org/abs/2512.06353)
*Kaicheng Yang,Kaisen Yang,Baiting Wu,Xun Zhang,Qianrui Yang,Haotong Qin,He Zhang,Yulun Zhang*

Main category: cs.CV

TL;DR: TreeQ是一个针对扩散变换器（DiT）的统一量化框架，通过树结构搜索、环境噪声引导和通用Monarch分支三个创新技术，实现了DiT模型在超低比特量化下的高性能，首次在DiT上实现了接近无损的4位PTQ性能。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器（DiTs）虽然在图像生成方面表现出色，但其实际部署面临高计算和内存需求挑战。现有的混合精度量化（MPQ）方法在U-Net上取得了成功，但在DiT架构上的应用仍然有限且未充分探索。

Method: 提出了TreeQ统一框架，包含三个关键技术：1）树结构搜索（TSS），利用DiT的线性特性在O(n)时间内遍历解空间；2）环境噪声引导（ENG），使用单一超参数统一PTQ和QAT的优化目标；3）通用Monarch分支（GMB），通过结构化稀疏分支防止超低比特下的信息瓶颈。

Result: TreeQ在DiT-XL/2模型上，在W3A3和W4A4的PTQ/PEFT设置下实现了最先进的性能。首次在DiT模型上实现了接近无损的4位PTQ性能。

Conclusion: TreeQ框架成功解决了DiT量化的关键挑战，通过创新的搜索策略、优化目标统一和信息瓶颈缓解技术，为DiT模型的高效部署提供了有效的解决方案。

Abstract: Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ

</details>


### [35] [Rectifying Latent Space for Generative Single-Image Reflection Removal](https://arxiv.org/abs/2512.06358)
*Mingjia Li,Jin Hu,Hainuo Wang,Qiming Hu,Jiarui Wang,Xiaojie Guo*

Main category: cs.CV

TL;DR: 本文提出了一种基于潜在扩散模型的单图像反射去除方法，通过重新构建潜在空间结构来更好地处理高度模糊的复合图像，在多个基准测试中达到新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 单图像反射去除是一个高度不适定问题，现有方法难以推理被污染区域的组成结构，导致在真实场景中的恢复和泛化能力不足。本文发现关键问题在于语义编码器的潜在空间缺乏解释复合图像作为其组成层线性叠加的内在结构。

Method: 方法包含三个协同组件：1）反射等变VAE，将潜在空间与反射形成的线性物理特性对齐；2）可学习的任务特定文本嵌入，提供精确指导并绕过模糊语言；3）深度引导的早期分支采样策略，利用生成随机性获得有希望的结果。

Result: 在多个基准测试中实现了新的SOTA性能，并且在具有挑战性的真实世界案例中表现出良好的泛化能力。

Conclusion: 通过重新构建编辑目的的潜在扩散模型，使其能够有效感知和处理高度模糊的分层图像输入，解决了单图像反射去除中的关键挑战，取得了显著的性能提升和泛化能力。

Abstract: Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.

</details>


### [36] [Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection](https://arxiv.org/abs/2512.06363)
*Jiabao Guo,Yadian Wang,Hui Ma,Yuhao Fu,Ju Jia,Hui Liu,Shengeng Tang,Lechao Cheng,Yunfeng Diao,Ajian Liu*

Main category: cs.CV

TL;DR: 本文提出SPL-UAD框架，通过解耦物理攻击和数字攻击的优化分支，实现统一的攻击检测，解决了现有方法中两类攻击检测优化方向冲突的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的人脸识别系统同时面临物理呈现攻击和数字伪造攻击的双重威胁。现有方法主要使用CLIP加正则化约束来提升模型在两类任务上的泛化能力，但这些方法在相同类别提示空间下存在物理和数字攻击检测优化方向冲突的问题。

Method: 提出SPL-UAD框架：1）构建可学习的并行提示分支，通过自适应欺骗上下文提示生成增强，实现对每种攻击类型的独立优化控制；2）设计线索感知增强，利用双提示机制在数据上生成具有挑战性的样本挖掘任务，显著提升模型对未见攻击类型的鲁棒性。

Result: 在大型UniAttackDataPlus数据集上的大量实验表明，所提方法在统一攻击检测任务中取得了显著的性能提升。

Conclusion: SPL-UAD框架通过解耦物理和数字攻击的优化分支，有效解决了现有方法中的优化冲突问题，实现了更全面的生物特征数据保护，在统一攻击检测任务上表现出优越性能。

Abstract: Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.

</details>


### [37] [VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2512.06373)
*Yuji Wang,Wenlong Liu,Jingxuan Niu,Haoji Zhang,Yansong Tang*

Main category: cs.CV

TL;DR: VG-Refiner：首个工具精炼的指代接地推理框架，通过两阶段思考-再思考机制处理不可靠工具输出，提升指代和接地任务的准确性


<details>
  <summary>Details</summary>
Motivation: 现有工具集成视觉推理方法主要关注通过强化学习集成各种视觉工具，但缺乏处理不可靠或错误工具输出的有效响应机制。这在指代和接地任务中尤为突出，不准确的检测工具预测常导致模型产生幻觉推理。

Method: 提出VG-Refiner框架，引入两阶段思考-再思考机制，使模型能够显式分析和响应工具反馈；设计精炼奖励机制，鼓励对不良工具结果进行有效修正；提出两个新指标和公平评估协议来系统衡量模型的精炼能力。

Result: 使用少量任务特定数据增强VG-Refiner的精炼能力，在指代和推理接地基准测试中实现了准确性和修正能力的显著提升，同时保持了预训练模型的通用能力。

Conclusion: VG-Refiner是首个针对工具精炼指代接地推理的框架，通过显式处理工具反馈和精炼机制，有效解决了现有TiVR方法在处理不可靠工具输出时的局限性，显著提升了指代和接地任务的性能。

Abstract: Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.

</details>


### [38] [Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework](https://arxiv.org/abs/2512.06376)
*Xinhao Xiang,Abhijeet Rastogi,Jiawei Zhang*

Main category: cs.CV

TL;DR: 论文提出诊断框架评估AI生成驾驶视频对自动驾驶模型训练和评估的可靠性，识别AIGV常见故障模式，构建ADGV-Bench基准，开发ADGVE评估器，证明筛选后的AIGV可作为真实数据的有益补充。


<details>
  <summary>Details</summary>
Motivation: 文本到视频模型能生成高分辨率驾驶场景，AI生成驾驶视频为自动驾驶提供低成本、可扩展的数据替代方案，但关键问题在于这些视频是否能可靠支持自动驾驶模型的训练和评估。

Method: 1. 提出AIGV故障模式分类法（视觉伪影、物理不合理运动、交通语义违规）；2. 构建ADGV-Bench基准，包含人工质量标注和密集标签；3. 开发ADGVE评估器，结合静态语义、时序线索、车道遵守信号和VLM引导推理生成质量评分。

Result: 盲目添加原始AIGV会降低感知性能，但使用ADGVE筛选后能同时提升通用视频质量评估指标和下游自动驾驶模型性能，使AIGV成为真实世界数据的有益补充。

Conclusion: 研究揭示了AIGV的风险和潜力，为在自动驾驶流程中安全利用大规模视频生成提供了实用工具，筛选后的AIGV能有效补充真实数据。

Abstract: Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.

</details>


### [39] [Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement](https://arxiv.org/abs/2512.06400)
*Jing Tao,Yonghong Zong,Banglei Guana,Pengju Sun,Taihang Lei,Yang Shanga,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出基于区域感知的红外与可见光融合框架，使用空间变化曝光相机结合多曝光与多模态成像，在极端环境下保持可见光几何保真度并融入热辐射信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法在融合红外和可见光谱时，往往损害可见光图像质量，影响测量精度，特别是在极端条件下难以同时保持几何保真度和热辐射信息。

Method: 采用基于区域感知的融合框架，结合空间变化曝光相机进行多曝光和多模态成像。首先进行区域感知的特征融合确保精确的多模态配准，然后进行自适应融合与对比度增强，最后通过区域显著性图引导的结构相似性补偿机制优化红外-可见光谱融合。

Result: 在合成和真实数据上的实验表明，该方法在图像清晰度和性能方面优于现有最先进方法，定量和视觉评估均证实了其优越性。

Conclusion: 提出的区域感知融合框架能够有效解决极端环境下红外与可见光融合的挑战，在保持可见光几何保真度的同时成功融入热辐射信息，为摄影测量应用提供了可靠的多模态成像解决方案。

Abstract: In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.

</details>


### [40] [Rethinking Training Dynamics in Scale-wise Autoregressive Generation](https://arxiv.org/abs/2512.06421)
*Gengze Zhou,Chongjian Ge,Hao Tan,Feng Liu,Yicong Hong*

Main category: cs.CV

TL;DR: 本文提出Self-Autoregressive Refinement (SAR)方法，通过Stagger-Scale Rollout机制和Contrastive Student-Forcing Loss，解决自回归生成模型中曝光偏差问题，提升图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 自回归生成模型在媒体合成方面取得进展，但尺度级自回归模型存在曝光偏差问题，影响生成质量。作者识别出两个主要原因：训练-测试不匹配（推理时模型依赖自身不完美预测）和尺度学习难度不平衡（某些尺度优化复杂度过高）。

Method: 提出Self-Autoregressive Refinement (SAR)方法，包含两个核心组件：1) Stagger-Scale Rollout (SSR)机制，执行轻量级自回归展开，让模型接触自身中间预测以对齐训练-测试模式；2) Contrastive Student-Forcing Loss (CSFL)，为自生成上下文提供充分监督以确保训练稳定。

Result: 实验表明，将SAR应用于预训练的自回归模型能持续提升生成质量且计算开销最小。例如，在ImageNet 256上训练的FlexVAR-d16模型，SAR在10个epoch内（32xA100 GPU上5小时）实现了5.2%的FID降低。

Conclusion: SAR作为一种高效、可扩展且有效的后训练方法，有望成为视觉自回归生成的可靠解决方案。

Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.

</details>


### [41] [When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition](https://arxiv.org/abs/2512.06426)
*Nzakiese Mbongo,Kailash A. Hambarde,Hugo Proença*

Main category: cs.CV

TL;DR: 提出双路径Transformer框架，利用CLIP联合建模视觉和属性线索，实现远距离性别识别，在多个指标上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 远距离图像中的性别识别面临空间分辨率有限、视角多变和面部特征缺失等挑战，需要更鲁棒的解决方案。

Method: 采用双路径Transformer框架：1) 视觉路径通过选择性微调CLIP图像编码器上层；2) 属性路径通过软生物特征提示（发型、服装、配饰）在CLIP文本-图像空间中推理性别。加入空间通道注意力模块增强遮挡和低分辨率下的判别定位。

Result: 构建U-DetAGReID数据集进行大规模评估，在多个指标（macro-F1、准确率、AUC）上超越现有行人属性和重识别基线方法，对距离、角度和高度变化具有一致鲁棒性。

Conclusion: 语言引导的双路径学习为无约束远距离场景下的负责任性别识别提供了原则性、可扩展的基础，注意力可视化证实了可解释的属性定位和负责任的弃权行为。

Abstract: Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.

</details>


### [42] [DragMesh: Interactive 3D Generation Made Easy](https://arxiv.org/abs/2512.06424)
*Tianshan Zhang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: DragMesh提出了一种实时交互式3D关节运动生成框架，通过解耦的动力学推理和运动生成方法，结合双四元数VAE和FiLM条件注入，实现了既符合物理约束又实时可用的关节运动生成。


<details>
  <summary>Details</summary>
Motivation: 当前3D内容生成方法在静态内容上表现出色，但在理解物体如何移动和响应交互方面仍面临挑战。现有关节运动方法要么物理一致但速度太慢无法实时使用，要么生成速度快但违反基本运动学约束，需要一种既符合物理约束又实时可用的解决方案。

Method: 1. 解耦的动力学推理框架：先通过语义意图推理确定关节类型，再通过KPP-Net进行几何回归确定轴和原点；2. 开发双四元数VAE（DQ-VAE）利用双四元数的紧凑、连续和无奇异性特性表示刚体运动；3. 使用FiLM条件注入在DQ-VAE的每一层注入关节先验，确保严格遵循运动学约束；4. 采用数值稳定的叉积损失保证轴对齐。

Result: DragMesh实现了实时性能，能够在无需重新训练的情况下对新颖物体进行合理的生成式关节运动，为生成式3D智能提供了实用的一步。

Conclusion: DragMesh通过解耦的动力学推理和运动生成框架，结合双四元数表示和持续的多尺度指导，成功解决了实时交互式3D关节运动的挑战，实现了既符合物理约束又具有实时性能的生成式关节运动系统。

Abstract: While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.

</details>


### [43] [Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion](https://arxiv.org/abs/2512.06504)
*Andrii Lysyi,Anatoliy Sachenko,Pavlo Radiuk,Mykola Lysyi,Oleksandr Melnychenko,Diana Zahorodnia*

Main category: cs.CV

TL;DR: 开发了一个用于光伏基础设施自动检测的智能集成框架，解决了传统方法的缺陷，包括热调色板偏差、数据冗余和高通信带宽需求。


<details>
  <summary>Details</summary>
Motivation: 传统光伏检测方法存在热调色板偏差、数据冗余和高通信带宽需求等关键缺陷，需要开发一个全面的多模态系统来自动化整个监测工作流程。

Method: 采用协同架构：1) 通过强制表示一致性学习调色板不变的热嵌入；2) 通过门控机制与对比度归一化的RGB流融合；3) 使用罗德里格斯更新的闭环自适应重采集控制器；4) 基于DBSCAN和半正矢距离的地理空间去重模块。

Result: 在公开PVF-10基准测试中达到0.903的平均精度(mAP@0.5)，比单模态基线提高12-15%；现场验证召回率达96%；去重过程减少重复引起的误报15-20%；仅相关遥测将空中数据传输减少60-70%。

Conclusion: 本研究为主动式光伏检测建立了强大的新范式，系统性能显著优于现有方法，并已通过现场验证，具备实际应用准备度。

Abstract: The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.

</details>


### [44] [SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities](https://arxiv.org/abs/2512.06562)
*Dung Thuy Nguyen,Quang Nguyen,Preston K. Robinette,Eli Jiang,Taylor T. Johnson,Kevin Leach*

Main category: cs.CV

TL;DR: SUGAR是一个用于生成模型遗忘的框架，能够在不重新训练整个模型的情况下，同时或顺序移除多个身份，同时保持模型质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 随着3D感知生成模型的发展，能够合成高保真的人类身份图像，这引发了关于用户同意和从模型输出空间中移除特定个体的紧迫问题。

Method: SUGAR为每个身份学习个性化的替代潜在表示，将重建结果转移到视觉上连贯的替代方案，而不是将不需要的身份投射到不现实的输出或依赖静态模板面部。进一步引入了持续效用保护目标，防止随着更多身份被遗忘而导致的性能退化。

Result: SUGAR在移除多达200个身份方面实现了最先进的性能，同时与现有基线相比，在保留效用方面提供了高达700%的改进。

Conclusion: SUGAR提供了一个可扩展的生成遗忘框架，能够有效移除多个身份，同时保持生成模型的质量和多样性，解决了生成模型中用户同意和身份移除的重要问题。

Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.

</details>


### [45] [Towards Stable Cross-Domain Depression Recognition under Missing Modalities](https://arxiv.org/abs/2512.06447)
*Jiuyi Chen,Mingkui Tan,Haifeng Lu,Qiuna Xu,Zhihua Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 提出基于多模态大语言模型的稳定跨域抑郁识别框架SCD-MLLM，解决现有方法缺乏统一框架、跨域泛化能力弱、对缺失模态不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 抑郁症筛查具有重要公共卫生意义，但现有多模态自动抑郁检测方法缺乏统一的通用框架，难以适应不同抑郁识别场景，且对现实数据中常见的缺失模态缺乏稳定性。

Method: 提出SCD-MLLM框架，包含两个核心组件：1) 多源数据输入适配器(MDIA)，使用掩码机制和任务特定提示将异构数据转换为统一标记序列；2) 模态感知自适应融合模块(MAFM)，通过共享投影机制自适应融合视听特征，增强对缺失模态的鲁棒性。

Result: 在五个公开抑郁数据集(CMDC、AVEC2014、DAIC-WOZ、DVlog、EATD)上进行多数据集联合训练实验，在完整和部分模态设置下均优于现有SOTA模型及商业LLM(Gemini和GPT)，表现出优异的跨域泛化能力、多模态抑郁线索捕捉能力和缺失模态稳定性。

Conclusion: SCD-MLLM为多模态抑郁检测提供了一个统一、稳定且可泛化的框架，能够有效处理现实世界中的异构数据和缺失模态问题，在跨域抑郁识别中展现出卓越性能。

Abstract: Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.

</details>


### [46] [Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution](https://arxiv.org/abs/2512.06642)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.CV

TL;DR: 使用掩码自编码器（MAE）在强引力透镜模拟图像上进行预训练，学习可泛化的表示，用于暗物质模型分类和图像超分辨率任务。


<details>
  <summary>Details</summary>
Motivation: 强引力透镜可以揭示暗物质亚结构的影响，但从噪声大、分辨率低的图像中分析这些效应具有挑战性。需要一种能够从物理丰富的模拟数据中学习通用表示的方法，以支持多个下游任务。

Method: 提出基于DeepLense ML4SCI基准的模拟强透镜图像的MAE预训练策略。使用Vision Transformer编码器，通过掩码图像建模目标进行预训练，然后为两个下游任务分别微调编码器：1）暗物质模型分类（冷暗物质、轴子样或无亚结构）；2）图像超分辨率（16×16到64×64）。

Result: 在90%掩码率下，微调分类器达到宏观AUC 0.968和准确率88.65%，优于从头训练的基线（AUC 0.957，准确率82.46%）。超分辨率任务中，MAE预训练模型重建图像的PSNR约33 dB，SSIM 0.961，略优于从头训练。掩码率分析显示：更高掩码率改善分类但轻微降低重建保真度。

Conclusion: MAE预训练在物理丰富的模拟数据上提供了一个灵活、可重用的编码器，适用于多个强透镜分析任务，能够匹配或超越从头训练的模型性能。

Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.

</details>


### [47] [The Role of Entropy in Visual Grounding: Analysis and Optimization](https://arxiv.org/abs/2512.06726)
*Shuo Li,Jiajun Sun,Zhihao Zhang,Xiaoran Fan,Senjie Jin,Hui Li,Yuming Yang,Junjie Ye,Lixing Shen,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: 本文提出ECVGPO算法，通过熵控制优化多模态大语言模型在视觉定位任务中的性能，平衡探索与利用的权衡


<details>
  <summary>Details</summary>
Motivation: 虽然基于强化学习的多模态大语言模型微调取得了显著进展，但熵在视觉定位等感知导向任务中的作用和特性，以及有效控制策略尚未得到充分探索

Method: 提出ECVGPO（熵控制视觉定位策略优化）算法，这是一种可解释的熵调节算法，通过分析视觉定位任务中熵的特性，设计有效的熵控制策略

Result: 实验表明ECVGPO在各种基准测试和模型上都取得了广泛的性能提升

Conclusion: 通过熵控制可以更好地平衡探索与利用的权衡，ECVGPO算法为视觉定位任务提供了有效的熵调节方法

Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.

</details>


### [48] [On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization](https://arxiv.org/abs/2512.06530)
*Mohammed Wattad,Tamir Shor,Alex Bronstein*

Main category: cs.CV

TL;DR: 学习型k空间采样模式在跨域MRI重建中展现出良好的泛化能力，通过引入采集不确定性增强域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多关注针对单一数据集或模态优化的采集模式，缺乏对跨成像域可迁移性的考虑。本文旨在探索学习型k空间采样在域偏移下的泛化能力。

Method: 提出一种增强域鲁棒性的新方法：在训练过程中引入采集不确定性，通过随机扰动k空间轨迹来模拟不同扫描仪和成像条件的变异性。

Result: 通过跨数据集和采集范式的系统评估，表明使用学习型采样模式训练的模型在跨域设置下表现出改进的泛化性能。

Conclusion: k空间轨迹设计不仅是加速机制，更是提高MRI重建中域泛化能力的主动自由度，学习型采样模式具有跨域应用潜力。

Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.

</details>


### [49] [Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2512.06746)
*Ruoxin Chen,Jiahui Gao,Kaiqing Lin,Keyue Zhang,Yandan Zhao,Isabel Guan,Taiping Yao,Shouhong Ding*

Main category: cs.CV

TL;DR: 论文提出AlignGemini检测器，通过任务-模型对齐原则将AIGI检测分为语义一致性检查和像素伪影检测两个互补任务，分别用VLM和像素伪影专家处理，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型(VLM)的AI生成图像检测方法需要大量资源且存在严重幻觉问题。研究发现VLM对语义敏感但对像素伪影不敏感，而传统像素伪影检测器缺乏语义意识，任务与模型之间存在错配。

Method: 提出任务-模型对齐原则，将AIGI检测形式化为两个互补任务：语义一致性检查和像素伪影检测。实现为AlignGemini双分支检测器，一个分支使用纯语义监督微调VLM，另一个分支使用纯像素伪影监督训练像素伪影专家。

Result: 在五个野外基准测试中，AlignGemini实现了平均准确率+9.5%的提升，证明了任务-模型对齐原则在可泛化AIGI检测中的有效性。

Conclusion: 任务-模型对齐是提升AI生成图像检测泛化能力的有效途径，通过将检测分解为互补的语义和像素伪影任务，并分别使用最适合的模型处理，可以克服现有方法的局限性。

Abstract: Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.

</details>


### [50] [GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation](https://arxiv.org/abs/2512.06565)
*Xiujin Liu*

Main category: cs.CV

TL;DR: GNC-Pose是一个无需学习的单目6D物体姿态估计方法，通过渲染初始化、几何感知对应点加权和GNC优化实现，在YCB数据集上达到与学习方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的6D姿态估计方法通常需要大量训练数据或学习特征，本文旨在开发一个完全无需学习的、鲁棒的6D姿态估计管道，能够在严重异常值污染下稳定工作。

Method: 1) 通过特征匹配和渲染对齐获得粗略的2D-3D对应点；2) 引入几何感知的聚类加权机制，基于3D结构一致性为每个点分配置信度；3) 采用渐进非凸性(GNC)优化原则；4) 最后使用LM细化进一步提高精度。

Result: 在YCB物体和模型集上测试，尽管不需要学习特征、训练数据或类别特定先验，GNC-Pose在精度上与基于学习的方法和无需学习的方法都具有竞争力。

Conclusion: GNC-Pose为无需学习的6D姿态估计提供了一个简单、鲁棒且实用的解决方案，通过几何先验和加权策略显著提高了在严重异常值污染下的优化稳定性。

Abstract: We present GNC-Pose, a fully learning-free monocular 6D object pose estimation pipeline for textured objects that combines rendering-based initialization, geometry-aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D-3D correspondences obtained through feature matching and rendering-based alignment, our method builds upon the Graduated Non-Convexity (GNC) principle and introduces a geometry-aware, cluster-based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC-Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC-Pose achieves competitive accuracy compared with both learning-based and learning-free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.

</details>


### [51] [VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors](https://arxiv.org/abs/2512.06759)
*Wenbo Lyu,Yingjun Du,Jinglin Zhao,Xianton Zhen,Ling Shao*

Main category: cs.CV

TL;DR: VisChainBench是一个用于评估大型视觉语言模型在多图像、多轮场景下视觉推理能力的大规模基准测试，包含1,457个任务和超过20,000张图像，覆盖日常场景和工程故障排除等领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注静态或横向比较，如发现视觉差异或评估适当性，过度依赖语言线索，忽视了渐进式、上下文相关的推理以及视觉到视觉的推理挑战。需要填补这一研究空白。

Method: 使用多智能体生成流水线构建基准测试，确保视觉多样性和控制语言偏差。基准包含三个不同领域（日常场景、工程故障排除等）的1,457个任务，涵盖超过20,000张图像，模拟真实世界决策过程。

Result: 创建了VisChainBench基准测试，包含大规模任务集和图像数据，所有基准数据和构建代码已通过Hugging Face平台公开可用。

Conclusion: VisChainBench填补了现有基准测试的空白，能够严格评估LVLMs在顺序、相互依赖任务中进行多步视觉推理的能力，为研究多图像、多轮场景下的视觉推理提供了重要工具。

Abstract: Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench

</details>


### [52] [Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding](https://arxiv.org/abs/2512.06769)
*Hang Yin,Xiaomin He,PeiWen Yuan,Yiwei Li,Jiayi Shi,Wenxiao Fan,Shaoxiong Feng,Kan Li*

Main category: cs.CV

TL;DR: 提出Stitch and Tell方法，通过拼接图像并生成空间感知的文本描述，无需额外标注即可提升视觉语言模型的空间理解能力，减少空间幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型存在空间幻觉问题，即对图像中物体相对位置描述错误。作者认为这主要源于图像和文本之间的不对称特性，需要增强模型的空间理解能力。

Method: 提出Stitch and Tell方法：1）沿空间轴拼接图像创建拼接图像；2）基于拼接图像的布局生成空间感知的标题或问答对；3）无需额外标注、无需昂贵的高级模型或人工参与；4）可即插即用地注入结构化空间监督到训练数据中。

Result: 在LLaVA-v1.5-7B、LLaVA-Qwen2-1.5B和HALVA-7B三种架构上评估，使用两个训练数据集和八个基准测试。结果显示：空间理解任务显著提升（MME_Position +5.50%，Spatial-MM +4.19%），同时保持或提升通用视觉语言基准性能（COCO-QA +1.02%，MMBench +4.76%）。

Conclusion: 将空间感知结构显式注入训练数据是减少空间幻觉、提升空间理解能力的有效方法，同时能保持通用视觉语言能力。该方法简单、无需标注、可即插即用。

Abstract: Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.

</details>


### [53] [MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding](https://arxiv.org/abs/2512.06581)
*Yuhao Su,Anwesa Choudhuri,Zhongpai Gao,Benjamin Planche,Van Nguyen Nguyen,Meng Zheng,Yuhan Shen,Arun Innanje,Terrence Chen,Ehsan Elhamifar,Ziyan Wu*

Main category: cs.CV

TL;DR: 本文提出了MedVidBench医学视频理解基准和MedGRPO强化学习框架，解决了医学视频理解中空间精度、时序推理和临床语义的挑战，通过跨数据集奖励归一化和医学LLM评判器显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在医学视频理解方面存在困难，特别是在空间精度、时序推理和临床语义方面。现有方法难以处理医学视频的多层次任务（视频级、片段级、帧级），且标准强化学习在跨数据集训练时因奖励尺度不平衡而失败。

Method: 1. 构建MedVidBench基准：包含531,850个视频-指令对，涵盖8个医学来源，通过专家引导提示和双模型验证的质量保证流程；2. 提出MedGRPO强化学习框架：包含跨数据集奖励归一化（将各数据集的中位性能映射到共同奖励值）和医学LLM评判器（通过比较相似性评分在五个临床维度评估字幕质量）。

Result: 在MedVidBench上监督微调Qwen2.5-VL-7B模型在所有任务上显著优于GPT-4.1和Gemini-2.5-Flash；MedGRPO框架进一步在定位和字幕任务上超越了SFT基线，证明了基准的有效性和训练方法的鲁棒性。

Conclusion: 该研究为医学领域的视觉语言模型建立了基础性基准和稳健的训练方法，通过MedVidBench基准和MedGRPO强化学习框架，有效解决了医学视频理解中的关键挑战，推动了医学AI的发展。

Abstract: Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.

</details>


### [54] [Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics](https://arxiv.org/abs/2512.06612)
*Kazuya Nishimura,Haruka Hirose,Ryoma Bise,Kaito Shiku,Yasuhiro Kojima*

Main category: cs.CV

TL;DR: 提出STRank损失函数，通过相对表达模式而非绝对表达值来估计基因表达，以应对RNA测序中的随机噪声和批次效应。


<details>
  <summary>Details</summary>
Motivation: 从病理图像估计基因表达可降低RNA测序成本，但传统点对点损失函数难以准确估计绝对表达值，因为测序技术复杂性和细胞内在变异性导致观测数据包含随机噪声和批次效应。

Method: 提出学习相对表达模式而非绝对水平的新目标，假设基因相对表达水平在独立实验中呈现一致模式。基于此假设，提出STRank损失函数，对噪声和批次效应具有鲁棒性。

Result: 在合成数据集和真实数据集上的实验证明了该方法的有效性。

Conclusion: 通过关注相对表达模式而非绝对表达值，STRank损失函数能够更有效地从病理图像估计基因表达，减少噪声和批次效应的影响。

Abstract: Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.

</details>


### [55] [RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2512.06811)
*Xiang Lin,Weixin Li,Shu Guo,Lihong Wang,Di Huang*

Main category: cs.CV

TL;DR: RMAdapter：一种基于重构的多模态适配器，通过双分支架构平衡任务特定适应与通用知识保留，在少样本场景下显著提升视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型在少样本场景下的微调面临平衡任务特定适应与泛化能力的挑战。当前研究主要关注基于提示的方法，而基于适配器的方法研究不足且性能存在明显差距。

Method: 提出重构多模态适配器（RMAdapter），采用双分支架构：1）适应分支通过参数高效微调注入任务特定知识；2）重构分支通过将潜在空间特征重构回原始特征空间来保留通用知识。采用局部重构损失计算和共享投影模块保持轻量化，并加入一致性约束来调节判别性与泛化性的平衡。

Result: 在三个代表性任务上全面评估：新类别泛化、新目标数据集泛化、领域泛化。在不依赖数据增强或重复提示设计的情况下，RMAdapter在所有评估指标上均优于最先进方法。

Conclusion: RMAdapter通过双分支架构有效平衡了任务特定知识与通用知识，在保持轻量化的同时显著提升了视觉语言模型在少样本场景下的性能，为多模态适配器研究提供了新方向。

Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.

</details>


### [56] [Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior](https://arxiv.org/abs/2512.06866)
*Yulin Li,Haokun Gui,Ziyang Fan,Junjie Wang,Bin Kang,Bin Chen,Zhuotao Tian*

Main category: cs.CV

TL;DR: DyToK是一种无需训练的VLLM动态token压缩方法，利用VLLM注意力机制中的关键帧先验，动态调整每帧token保留比例，在保持准确性的同时实现4.3倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在处理长视频时面临二次计算复杂度增长的问题，传统关键帧采样方法在特征编码前引入额外计算成本，且二元帧选择范式效果欠佳。

Method: DyToK通过分析发现VLLM注意力层自然编码了查询条件的关键帧先验，利用这一特性动态调整每帧token保留比例，优先保留语义丰富的帧，抑制冗余信息。

Result: DyToK在效率-准确性权衡方面达到最先进水平，与现有压缩方法（如VisionZip和FastV）兼容，在LLaVA-OneVision和Qwen2.5-VL等多个VLLM上实现4.3倍推理加速同时保持准确性。

Conclusion: DyToK提供了一种无需训练的即插即用动态token压缩范式，有效解决了VLLM处理长视频时的效率瓶颈问题，具有广泛的应用兼容性。

Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .

</details>


### [57] [JoPano: Unified Panorama Generation via Joint Modeling](https://arxiv.org/abs/2512.06885)
*Wancheng Feng,Chen An,Zhenliang He,Meina Kan,Shiguang Shan,Lukun Wang*

Main category: cs.CV

TL;DR: JoPano提出了一种基于DiT的统一全景图生成方法，通过联合面适配器和条件切换机制，将文本到全景图和视角到全景图两个核心任务统一在一个模型中，解决了现有方法视觉质量受限和任务独立建模的问题。


<details>
  <summary>Details</summary>
Motivation: 现有全景图生成方法面临两大挑战：1）基于U-Net的架构限制了生成全景图的视觉质量；2）通常将文本到全景图和视角到全景图两个核心任务独立处理，导致建模冗余和效率低下。

Method: 提出基于DiT的联合面全景图生成方法，包含三个关键技术：1）基于立方体贴图表示的联合面适配器，将预训练DiT的生成能力迁移到全景图领域；2）泊松融合减少立方体面边界的不一致性；3）条件切换机制统一文本到全景图和视角到全景图任务。

Result: JoPano在文本到全景图和视角到全景图生成任务上都能生成高质量全景图，在FID、CLIP-FID、IS和CLIP-Score等指标上达到最先进性能，并提出了Seam-SSIM和Seam-Sobel指标定量评估接缝一致性。

Conclusion: JoPano通过统一架构解决了全景图生成中的视觉质量和任务冗余问题，基于DiT的联合建模方法为全景图生成提供了新的有效解决方案。

Abstract: Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.

</details>


### [58] [Personalized Image Descriptions from Attention Sequences](https://arxiv.org/abs/2512.06662)
*Ruoyu Xue,Hieu Le,Jingyi Xu,Sounak Mondal,Abe Leite,Gregory Zelinsky,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: DEPER模型通过同时建模语言风格和个性化视觉关注模式来生成更符合个人特点的图像描述，相比仅关注语言风格的方法平均提升24%


<details>
  <summary>Details</summary>
Motivation: 现有个性化图像描述模型只关注语言风格，忽略了不同人观看图像时的视觉关注模式差异。人们观看同一图像时会关注不同区域、对象和细节，并以不同顺序和语言风格进行描述，导致描述存在显著差异。

Method: 提出DEPER方法，学习同时捕捉语言风格和观看行为的主体嵌入表示，通过辅助注意力预测任务进行引导。使用轻量级适配器将这些嵌入与冻结的视觉语言模型对齐，实现少样本个性化而无需重新训练。

Result: 在四个涵盖不同观看任务和描述类型的数据集上，DEPER平均提升24%，表明建模个性化注意力能产生更符合人类特点且质量更高的描述。

Conclusion: 理解人们如何观看图像有助于预测他们会说什么；建模人类感知多样性可以提升多模态系统的性能和与人类的对齐程度。

Abstract: People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.

</details>


### [59] [NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification](https://arxiv.org/abs/2512.06921)
*Ziyang Song,Zelin Zang,Xiaofan Ye,Boqiang Xu,Long Bai,Jinlin Wu,Hongliang Ren,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: 该研究提出了首个针对神经外科领域的多模态解剖理解基准测试NeuroABench，包含9小时标注视频，评估68个解剖结构识别能力，发现当前最佳MLLM模型准确率仅40.87%，显著低于神经外科学员平均表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM研究主要关注手术流程理解，忽视了临床实践中至关重要的解剖学理解能力。神经外科医生依赖精确的解剖知识来解读手术视频，但缺乏专门评估解剖理解能力的基准测试。

Method: 开发了NeuroABench基准测试，包含9小时标注的神经外科视频，涵盖89个不同手术。采用新型多模态标注流程和多轮审查机制，评估68个临床解剖结构的识别能力。测试了10多个SOTA MLLM模型，并与4名神经外科学员进行对比实验。

Result: 最佳MLLM模型在解剖识别任务中仅达到40.87%准确率。神经外科学员测试中，最佳学员准确率56%，最低28%，平均46.5%。最佳MLLM表现与最低分学员相当，但显著低于学员平均水平。

Conclusion: MLLM在解剖理解方面取得进展，但与人类水平仍有显著差距。NeuroABench为评估和改进MLLM的解剖理解能力提供了标准化框架，有助于推动手术教育和辅助系统的发展。

Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.

</details>


### [60] [CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks](https://arxiv.org/abs/2512.06663)
*Yu Qi,Yumeng Zhang,Chenting Gong,Xiao Tan,Weiming Zhang,Wei Zhang,Jingdong Wang*

Main category: cs.CV

TL;DR: CoT4Det将感知任务重构为分类、计数和定位三个可解释步骤，显著提升大视觉语言模型在目标检测等感知任务上的性能


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在通用视觉问答任务上表现出色，但在感知任务（如目标检测、语义分割）上性能远低于任务专用专家模型，特别是在密集场景和小物体召回方面存在明显不足

Method: 提出Chain-of-Thought for Detection (CoT4Det)策略，将感知任务重构为三个更符合大视觉语言模型推理能力的可解释步骤：分类、计数和定位

Result: 在Qwen2.5-VL-7B-Instruct模型上，CoT4Det将COCO2017 val的mAP从19.0%提升到33.0%，在RefCOCO系列上超越基线2%，在Flickr30k entities上提升19%

Conclusion: CoT4Det方法显著提升了大视觉语言模型在感知任务上的性能，同时不损害其通用视觉语言能力，为将大视觉语言模型应用于感知任务提供了有效解决方案

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.

</details>


### [61] [1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://arxiv.org/abs/2512.06673)
*Shida Gao,Feng Xue,Xiangfeng Wang,Anlong Ming,Teng Long,Yihua Shao,Haozhe Wang,Zhaowen Lin,Wei Wang,Nicu Sebe*

Main category: cs.CV

TL;DR: DEViL提出了一种结合视频大语言模型和开放词汇检测器的方法，通过参考语义令牌实现端到端学习，解决时空定位中自回归空间解码导致的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在时空定位任务中采用自回归方式生成边界框作为文本标记，这导致输出序列过长，空间误差随时间累积，定位结果在视频中逐渐漂移。

Method: DEViL将视频大语言模型与开放词汇检测器耦合，通过参考语义令牌将用户查询提炼为丰富的语义表示，该令牌既作为控制信号又替代检测器的文本嵌入。同时提出管道挖掘时间正则化，确保检测器生成时间一致的目标对象查询。

Result: 实验表明DEViL在各种细粒度视频理解任务上表现优异，特别是在时空视觉定位和基于视觉问答的定位任务上取得显著效果。

Conclusion: DEViL通过结合视频大语言模型和开放词汇检测器，有效解决了时空定位中的误差累积问题，实现了更好的时空关联和定位精度。

Abstract: Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.

</details>


### [62] [MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.06810)
*Yueqian Wang,Songxiang Liu,Disong Wang,Nuo Xu,Guanglu Wan,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种用于视频多模态大语言模型的主动交互方法，通过文本到文本的方式让模型在视频播放过程中自主决定何时回应，无需手动调整阈值或精确标注回复时间。


<details>
  <summary>Details</summary>
Motivation: 现有视频MLLM系统多为回合制，只能在用户发言后回复，而实时应用需要模型在视频播放过程中主动决定何时回应。这是一个有前景但具有挑战性的方向。

Method: 提出基于文本到文本的主动交互方法，模型根据对话历史和当前视频帧的视觉上下文自主决定是回应还是保持沉默。采用多回合强化学习训练方法，鼓励及时准确的回应，无需精确的回复时间标注。在包含52k视频和两种对话类型的数据集上通过SFT和RL训练MMDuet2模型。

Result: MMDuet2在回应时机和质量上优于现有主动视频MLLM基线，在ProactiveVideoQA基准测试中达到最先进性能。

Conclusion: 提出的方法成功实现了视频多模态大语言模型的主动交互能力，通过多回合强化学习训练解决了传统方法需要手动调整阈值和精确标注回复时间的问题，为实时视频交互应用提供了有效解决方案。

Abstract: Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.

</details>


### [63] [RunawayEvil: Jailbreaking the Image-to-Video Generative Models](https://arxiv.org/abs/2512.06674)
*Songping Wang,Rufan Qian,Yueming Lyu,Qinglong Liu,Linzhuang Zou,Jie Qin,Songhua Liu,Caifeng Shan*

Main category: cs.CV

TL;DR: RunawayEvil：首个针对图像到视频生成模型的多模态越狱攻击框架，采用"策略-战术-行动"范式，具备动态演化能力，通过强化学习和LLM实现自我进化的攻击策略，在商业I2V模型上达到最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 图像到视频生成系统虽然提供了重要的创意控制能力，但其安全性特别是对越狱攻击的脆弱性尚未得到充分研究。当前缺乏针对多模态I2V模型的越狱攻击框架，需要填补这一研究空白。

Method: 提出RunawayEvil框架，基于"策略-战术-行动"范式，包含三个核心组件：1) 策略感知命令单元：通过强化学习驱动的策略定制和基于LLM的策略探索实现攻击策略的自我演化；2) 多模态战术规划单元：根据选定策略生成协调的文本越狱指令和图像篡改指南；3) 战术行动单元：执行和评估多模态协调攻击。这种自演化架构使框架能够持续适应和强化攻击策略而无需人工干预。

Result: 在Open-Sora 2.0和CogVideoX等商业I2V模型上进行广泛实验，RunawayEvil实现了最先进的攻击成功率。在COCO2017数据集上，RunawayEvil比现有方法高出58.5%到79%。

Conclusion: 该工作为I2V模型的漏洞分析提供了关键工具，为构建更鲁棒的视频生成系统奠定了基础。RunawayEvil框架展示了多模态系统面临的新型安全挑战，强调了在开发创意AI工具时需要考虑安全防护的重要性。

Abstract: Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a "Strategy-Tactic-Action" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.

</details>


### [64] [Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues](https://arxiv.org/abs/2512.07034)
*Tuan-Anh Vu,Hai Nguyen-Truong,Ziqiang Zheng,Binh-Son Hua,Qing Guo,Ivor Tsang,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: TransCues是一个用于透明物体分割的框架，通过边界特征增强和反射特征增强模块，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 玻璃等透明物体由于透明性和反射特性，现有分割方法难以将其与不透明材料区分。人类感知依赖边界和反射物体特征来识别透明物体，但现有文献未能充分捕捉这两种特性。

Method: 提出TransCues框架，采用金字塔式transformer编码器-解码器架构，包含边界特征增强模块和反射特征增强模块，以相互促进的方式整合这两种视觉线索。

Result: 在多个基准数据集上大幅超越现有方法：Trans10K-v2 (+4.2% mIoU)、MSD (+5.6% mIoU)、RGBD-Mirror (+10.1% mIoU)、TROSD (+13.1% mIoU)、Stanford2D3D (+8.3% mIoU)。

Conclusion: 通过整合边界和反射特征增强模块，TransCues能有效分割透明物体，在多个数据集上表现出卓越性能，验证了该方法对玻璃物体的有效性。

Abstract: Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.

</details>


### [65] [EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy](https://arxiv.org/abs/2512.06684)
*Yumeng He,Zanwei Zhou,Yekun Zheng,Chen Liang,Yunbo Wang,Xiaokang Yang*

Main category: cs.CV

TL;DR: EMGauss：基于高斯溅射的3D重建框架，将切片到3D重建重新定义为3D动态场景渲染问题，解决各向异性结构重建难题


<details>
  <summary>Details</summary>
Motivation: 体积电子显微镜（vEM）在纳米尺度3D成像中存在采集权衡，导致各向异性体积和有限的轴向分辨率。现有深度学习方法依赖横向先验来恢复各向同性，但对于形态各向异性结构效果不佳。

Method: 将切片到3D重建重新定义为基于高斯溅射的3D动态场景渲染问题，将轴向切片进展建模为2D高斯点云的时间演化。引入教师-学生引导机制，利用未观测切片的高置信度预测作为伪监督信号。

Result: 相比基于扩散和GAN的重建方法，EMGauss显著提高了插值质量，实现了连续切片合成，且无需大规模预训练。在数据稀疏情况下仍保持高保真度。

Conclusion: EMGauss为vEM提供了一种规避各向同性方法固有局限性的通用框架，不仅适用于vEM，还可能为跨不同成像领域的切片到3D重建提供通用解决方案。

Abstract: Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.

</details>


### [66] [DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation](https://arxiv.org/abs/2512.07051)
*Adnan Munir,Shujaat Khan*

Main category: cs.CV

TL;DR: DAUNet是一种轻量级UNet变体，结合可变形卷积V2和无参数注意力SimAM，在保持模型复杂度不变的情况下提升医学图像分割性能


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在自动化诊断和治疗规划系统中至关重要，需要开发既能处理几何变化又能保持高效参数利用的轻量级模型

Method: 提出DAUNet，在瓶颈层使用动态可变形卷积处理几何变化，在解码器和跳跃连接中使用SimAM注意力模块进行显著性感知的特征融合

Result: 在两个挑战性数据集（FH-PS-AoP超声和FUMPE CT肺栓塞检测）上，DAUNet在Dice分数、HD95和ASD指标上优于最先进模型，同时保持优越的参数效率

Conclusion: DAUNet对缺失上下文和低对比度区域的鲁棒性使其适合在实时和资源受限的临床环境中部署，可变形卷积和SimAM注意力的组合有效提升了分割性能

Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.

</details>


### [67] [Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation](https://arxiv.org/abs/2512.06689)
*Jisoo Park,Seonghak Lee,Guisik Kim,Taewoo Kim,Junseok Kwon*

Main category: cs.CV

TL;DR: UniVoiceLite是一个轻量级无监督的视听框架，统一了语音增强和语音分离任务，利用唇部运动和面部身份线索，无需配对噪声-干净数据。


<details>
  <summary>Details</summary>
Motivation: 现实世界音频通常同时包含背景噪声和重叠说话人，传统方法将语音增强和语音分离视为独立任务，现有集成方法通常复杂、参数量大且依赖监督训练，限制了可扩展性和泛化能力。

Method: 提出UniVoiceLite框架，利用唇部运动和面部身份线索引导语音提取，采用Wasserstein距离正则化稳定潜在空间，无需配对噪声-干净数据，实现轻量级无监督的统一解决方案。

Result: 实验结果表明，UniVoiceLite在噪声和多说话人场景下均表现出色，实现了效率与鲁棒泛化能力的结合。

Conclusion: UniVoiceLite成功统一了语音增强和语音分离任务，提供了一个轻量级、无监督的解决方案，在保持高效的同时实现了良好的泛化性能。

Abstract: Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.

</details>


### [68] [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/abs/2512.07141)
*Fenghua Weng,Chaochao Lu,Xia Hu,Wenqi Shao,Wenjie Wang*

Main category: cs.CV

TL;DR: 提出Think-Reflect-Revise (TRR)三阶段训练框架，通过策略引导的自我反思增强大型视觉语言模型的安全对齐能力，显著提升安全性能


<details>
  <summary>Details</summary>
Motivation: 现有单次"思考-回答"范式虽然提高了安全意识和可解释性，但仍容易受到上下文或视觉越狱攻击，因为单次推理可能忽略自身输出中的显式有害内容

Method: 提出TRR三阶段框架：1)构建包含5000个示例的ReSafe数据集，遵循思考-反思-修订过程；2)使用ReSafe数据集微调目标模型以初始化反思行为；3)通过强化学习强化策略引导的反思

Result: TRR显著提升了LVLMs的安全性能，在Qwen2.5-VL-7B上将总体安全响应率从42.8%提高到87.7%，同时在MMMU和MMStar等通用基准测试中保持稳定性能

Conclusion: 通过利用首次推理中揭示的恶意内容进行反思，可以实现真正的自我修正并防止不安全生成，TRR框架有效增强了LVLMs的安全对齐能力

Abstract: As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.

</details>


### [69] [FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation](https://arxiv.org/abs/2512.06738)
*M Yashwanth,Sampath Koti,Arunabh Singh,Shyam Marjit,Anirban Chakraborty*

Main category: cs.CV

TL;DR: FedSCAl是一个联邦学习框架，通过服务器-客户端对齐机制解决联邦无源域自适应问题，在存在显著客户端间域差异的情况下提升伪标签准确性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦无源域自适应（FFreeDA）问题，其中客户端持有未标记数据且存在显著的客户端间域差异。该设置限制了联邦学习框架只能使用预训练的服务器模型，无法访问源数据集，而源域与客户端域分布不同，导致现有方法在极端数据异构性下产生不可靠的伪标签。

Method: 提出FedSCAl框架，采用服务器-客户端对齐（SCAl）机制，通过对齐客户端和服务器模型的预测来正则化客户端更新。该机制减轻了客户端漂移，提高了伪标签准确性。

Result: 在基准视觉数据集上的广泛实验表明，FedSCAl在FFreeDA设置下的分类任务中始终优于最先进的联邦学习方法。

Conclusion: FedSCAl通过服务器-客户端对齐机制有效解决了联邦无源域自适应中的客户端漂移问题，提高了伪标签可靠性，在异构域场景下表现出优越性能。

Abstract: We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.

</details>


### [70] [Generating Storytelling Images with Rich Chains-of-Reasoning](https://arxiv.org/abs/2512.07198)
*Xiujie Song,Qi Jia,Shota Watanabe,Xiaoyi Pang,Ruijie Chen,Mengyue Wu,Kenny Q. Zhu*

Main category: cs.CV

TL;DR: 本文提出了Storytelling Image Generation任务，开发了StorytellingPainter两阶段生成管道，结合LLMs的推理能力和T2I模型的视觉合成能力来创建具有丰富语义连接的故事性图像。


<details>
  <summary>Details</summary>
Motivation: 故事性图像通过呈现丰富、逻辑连接的视觉线索来传达引人入胜的故事，具有广泛的应用价值。但由于其复杂的语义特性，这类图像难以创建且相对稀缺，因此需要探索如何利用生成式AI模型来创建这类图像。

Method: 提出了StorytellingPainter两阶段管道：第一阶段利用LLMs进行创造性推理生成故事描述，第二阶段使用T2I模型将故事描述合成为视觉图像。同时开发了包含语义复杂性评估器、KNN多样性评估器和故事-图像对齐评估器的专用评估框架。针对开源与专有LLMs的性能差距，探索了定制化训练策略，开发了Mini-Storytellers系列轻量级模型。

Result: 实验结果表明，所提出的方法是可行且有效的。通过结合LLMs的推理能力和T2I模型的视觉合成能力，能够成功生成具有丰富语义连接的故事性图像。

Conclusion: 本文成功定义了故事性图像生成任务，并开发了有效的生成管道和评估框架。通过利用生成式AI模型的能力，解决了故事性图像创建困难的问题，为这一领域的研究和应用奠定了基础。

Abstract: An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.

</details>


### [71] [A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning](https://arxiv.org/abs/2512.07136)
*Siyang Jiang,Mu Yuan,Xiang Ji,Bufang Yang,Zeyu Liu,Lilin Xu,Yang Li,Yuting He,Liran Dong,Wenrui Lu,Zhenyu Yan,Xiaofan Jiang,Wei Gao,Hongkai Chen,Guoliang Xing*

Main category: cs.CV

TL;DR: CUHK-X是一个用于人类动作识别、理解和推理的大规模多模态数据集，包含58,445个样本、40种动作，提供数据标签和文本描述两种标注，旨在解决现有LVLMs在非RGB模态上的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在处理深度、IMU、毫米波等非RGB模态时存在困难，主要原因是缺乏大规模的数据-描述配对资源。现有的人类动作识别数据集通常只提供粗糙的数据标签标注，无法捕捉动作的细粒度动态特征，限制了人类动作理解和推理任务的发展。

Method: 提出了CUHK-X数据集，包含58,445个样本，涵盖30名参与者在两个室内环境中执行的40种动作。为了改善描述的一致性，采用基于提示的场景创建方法，利用LLMs生成逻辑连贯的活动序列，然后进行人工验证。数据集包含三个基准测试和六个评估任务。

Result: 实验结果显示，在CUHK-X数据集上的平均准确率为：人类动作识别76.52%，人类动作理解40.76%，人类动作推理70.25%。该数据集旨在支持数据密集型学习方法在多模态人类活动分析中的应用和发展。

Conclusion: CUHK-X是一个全面的大规模多模态数据集，为人类动作识别、理解和推理任务提供了丰富的资源和基准测试，有助于推动社区在鲁棒的多模态人类活动分析方面的研究和应用。

Abstract: Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.

</details>


### [72] [Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models](https://arxiv.org/abs/2512.07564)
*Kassoum Sanogo,Renzo Ardiccioni*

Main category: cs.CV

TL;DR: 提出一种无需训练的自校正框架，通过不确定性引导的视觉重注意机制减少视觉语言模型的幻觉问题，在多个基准测试中显著降低幻觉率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型经常生成看似合理但实际错误的幻觉内容，现有方法需要重新训练或微调模型，成本较高且不灵活。需要一种无需训练的方法来纠正这些幻觉。

Method: 提出训练免费的自校正框架，结合多维不确定性量化（令牌熵、注意力分散、语义一致性、声明置信度）和注意力引导的裁剪机制，对未充分探索的图像区域进行重新关注，完全使用冻结的预训练模型，无需梯度更新。

Result: 在POPE和MMHAL BENCH基准测试中使用Qwen2.5-VL-7B架构验证，方法将幻觉率降低了9.8个百分点，在对抗性分割上物体存在准确性提高了4.7个百分点。定性分析证实不确定性引导的重注意成功将校正基于视觉证据。

Conclusion: 提出的训练免费自校正框架有效减少视觉语言模型的幻觉问题，通过不确定性引导的视觉重注意机制提高模型的可信度，为可信赖的多模态系统研究提供了新方法。

Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.

</details>


### [73] [UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement](https://arxiv.org/abs/2512.06750)
*Weiqi Li,Xuanyu Zhang,Bin Chen,Jingfen Xie,Yan Wang,Kexin Zhang,Junlin Li,Li Zhang,Jian Zhang,Shijie Zhao*

Main category: cs.CV

TL;DR: UARE是首个统一图像质量评估、修复和增强的视觉语言模型，通过多任务协同训练让质量评估指导修复过程


<details>
  <summary>Details</summary>
Motivation: 虽然图像质量评估和修复在概念上紧密相关，但现有工作大多将它们分开处理。统一多模态理解-生成模型的进展表明更强的理解能力可以提升生成性能，这促使研究如何用质量评估指导修复的统一模型

Method: 基于预训练的统一理解和生成模型，采用两阶段训练框架：1）渐进式从单一类型失真扩展到高阶混合退化；2）通过交错文本-图像数据进行统一微调，将质量评估信号与修复目标对齐

Result: 在图像质量评估、修复和增强任务上的广泛实验证明了UARE的有效性

Conclusion: UARE是首个统一处理图像质量评估、修复和增强的视觉语言模型，通过多任务协同训练实现了质量评估对修复过程的指导作用

Abstract: Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.

</details>


### [74] [START: Spatial and Textual Learning for Chart Understanding](https://arxiv.org/abs/2512.07186)
*Zhuoming Liu,Xiaofeng Gao,Feiyang Niu,Qiaozi Gao,Liu Liu,Robinson Piramuthu*

Main category: cs.CV

TL;DR: START是一个用于图表理解的多模态大语言模型，通过空间和文本学习增强对图表视觉布局和底层数据表示的联合理解，在多个基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 图表理解对于多模态大语言模型在实际场景中的应用至关重要。与自然图像不同，图表同时包含结构化视觉布局（空间属性）和底层数据表示（文本属性），理解这两者对于精确的细粒度图表推理是必要的。现有方法难以同时处理这两个方面。

Method: 提出START方法，包含两个核心组件：(1) 图表元素定位，增强对图表视觉布局的理解；(2) 图表到代码生成，强化对底层数据细节的掌握。通过创新的数据生成流程创建START-Dataset，首先利用MLLM将真实图表图像转换为可执行图表代码，恢复底层数据表示同时保持真实图表视觉分布，然后使用LLM演化代码以确定捕获图表视觉结构的元素位置。

Result: 提出Chart Spatial understanding Benchmark (CS-Bench)来评估模型理解图表空间结构的能力。START在不同模型规模和基准测试上都比基础模型有持续提升，并明显超越了现有最先进方法。

Conclusion: 通过空间和文本学习，START能够同时理解图表的视觉布局和底层数据表示，在图表理解任务上取得了显著进展。代码、数据和模型将公开提供。

Abstract: Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.

</details>


### [75] [VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation](https://arxiv.org/abs/2512.07215)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: 该论文比较了基于CLIP和DINOv2的视觉基础模型在手持物体抓取场景中的3D姿态估计性能，发现CLIP在语义理解方面表现优异，而DINOv2在几何特征提取方面更胜一筹。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型（VFMs）和视觉语言模型（VLMs）通过提供丰富的语义和几何表示，已经彻底改变了计算机视觉领域。本研究旨在全面比较基于CLIP和DINOv2的方法在手持物体抓取场景中的3D姿态估计性能，为机器人操作和抓取应用提供模型选择指导。

Method: 在手持物体抓取场景中，对基于CLIP和DINOv2的方法进行全面的视觉比较。评估两种模型在6D物体姿态估计任务上的表现，通过基准数据集上的大量实验分析它们的性能差异。

Result: 实验结果表明两种模型具有互补优势：基于CLIP的方法通过语言基础在语义理解方面表现优异，实现了更好的语义一致性；而基于DINOv2的方法在密集几何特征提取方面表现优越，具有增强的几何精度和竞争性性能。

Conclusion: 该分析为机器人操作、抓取和拾取应用中选择合适的视觉模型提供了重要见解，强调了根据具体任务需求（语义理解vs几何精度）选择CLIP或DINOv2的重要性。

Abstract: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.

</details>


### [76] [Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos](https://arxiv.org/abs/2512.06783)
*Tobias Leuthold,Michele Xiloyannis,Yves Zimmermann*

Main category: cs.CV

TL;DR: 提出一种实时后处理算法，融合BlazePose的3D和2D估计，通过加权优化结合骨骼长度约束和生物力学模型，提升姿态估计的解剖学一致性


<details>
  <summary>Details</summary>
Motivation: 现有实时姿态估计模型（如BlazePose）缺乏解剖学约束，在物理治疗等自动教练应用中存在改进空间，需要更准确、符合生物力学的姿态估计

Method: 采用加权优化后处理算法，融合BlazePose的3D和2D估计，惩罚与预期骨骼长度和生物力学模型的偏差，使用卡尔曼滤波器根据个体解剖结构精炼骨骼长度估计

Result: 在Physio2.2M数据集上，3D MPJPE降低10.2%，身体段间角度误差减少16.6%，相比BlazePose 3D估计有明显提升

Conclusion: 该方法提供了一种稳健、解剖学一致且计算高效的姿态估计方案，适用于消费级设备的自动物理治疗、医疗保健和运动教练应用，后端运行仅使用匿名数据

Abstract: Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.

</details>


### [77] [Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models](https://arxiv.org/abs/2512.07234)
*Biao Chen,Lin Zuo,Mengmeng Jing,Kunbin He,Yuchen Wang*

Main category: cs.CV

TL;DR: 提出Dropout Prompt Learning方法，通过考虑模态内上下文和模态间对齐的token重要性评估，为文本和视觉分支的token应用灵活dropout概率，并结合残差熵正则化提升视觉语言模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Dropout作为广泛使用的正则化技术能提升模型泛化能力，但传统dropout方法在视觉语言模型中的应用不够灵活。需要一种能同时考虑模态内上下文和模态间对齐的token重要性评估方法，以提升模型在低样本学习、长尾分类和分布外泛化等挑战性场景下的鲁棒性。

Method: 提出Dropout Prompt Learning：1）在文本和视觉分支的token上应用dropout，基于模态内上下文和模态间对齐评估每个token的重要性，为不同token分配灵活的dropout概率；2）提出残差熵正则化，在保持通用知识传递的语义对齐同时，鼓励dropout引入的多样化表示。

Result: 在15个基准测试上验证了方法的有效性，特别是在低样本学习、长尾分类和分布外泛化等挑战性场景中表现优异。在基础到新颖泛化任务上，性能超越正则化方法KgCoOp 5.10%和PromptSRC 2.13%。

Conclusion: Dropout Prompt Learning通过灵活的token级dropout策略和残差熵正则化，有效提升了视觉语言模型的鲁棒性和泛化能力，在多种挑战性场景中展现出优越性能。

Abstract: Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.

</details>


### [78] [VDOT: Efficient Unified Video Creation via Optimal Transport Distillation](https://arxiv.org/abs/2512.06802)
*Yutong Wang,Haiyu Zhang,Tianfan Xue,Yu Qiao,Yaohui Wang,Chang Xu,Xinyuan Chen*

Main category: cs.CV

TL;DR: VDOT是一个高效统一的视频生成模型，通过分布匹配蒸馏和最优传输技术优化，仅需4步推理就能达到传统方法100步的效果，同时支持多种条件生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型要么只能处理少数特定条件，要么因推理过程复杂导致生成时间过长，难以在实际应用中部署。需要开发一个既高效又能统一处理多种条件的视频生成模型。

Method: 采用分布匹配蒸馏范式，用最优传输距离替代KL散度来优化真实与生成分数分布的差异，避免梯度崩溃问题。同时集成判别器感知真实视频数据，并构建自动化视频标注过滤管道支持多任务训练。

Result: 实验表明，仅需4步推理的VDOT模型在性能上能够超越或匹配其他需要100步去噪的基线方法，同时保持了高效的生成速度。

Conclusion: VDOT通过最优传输技术和分布匹配蒸馏，成功实现了高效统一的视频生成，仅需极少推理步骤就能达到高质量生成效果，为实际应用提供了可行的解决方案。

Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.

</details>


### [79] [DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement](https://arxiv.org/abs/2512.07253)
*Handing Xu,Zhenguo Nie,Tairan Peng,Huimin Pan,Xin-Jun Liu*

Main category: cs.CV

TL;DR: 提出一种基于退化感知的实时内窥镜视频增强框架，通过跨帧传播退化表示实现高质量实时增强


<details>
  <summary>Details</summary>
Motivation: 内窥镜手术依赖术中视频，但视频常因光照不均、组织散射、遮挡和运动模糊而质量下降，现有深度学习方法计算量过大难以实时应用

Method: 使用对比学习提取图像退化表示，引入融合机制用退化表示调制图像特征指导单帧增强模型，通过退化与恢复图像间的循环一致性约束训练

Result: 实验表明该框架在性能与效率之间达到优越平衡，优于多种最先进方法

Conclusion: 退化感知建模对实时内窥镜视频增强有效，隐式学习和传播退化表示为临床应用提供了实用途径

Abstract: Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.

</details>


### [80] [MeshSplatting: Differentiable Rendering with Opaque Meshes](https://arxiv.org/abs/2512.06818)
*Jan Held,Sanghyun Son,Renaud Vandeghen,Daniel Rebain,Matheus Gadelha,Yi Zhou,Anthony Cioppa,Ming C. Lin,Marc Van Droogenbroeck,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: MeshSplatting是一种基于网格的重建方法，通过可微分渲染联合优化几何和外观，将神经渲染与交互式3D图形桥接，实现实时场景交互。


<details>
  <summary>Details</summary>
Motivation: 现有的基于基元的溅射方法（如3D高斯溅射）虽然实现了实时渲染的新视角合成，但其基于点的表示与AR/VR和游戏引擎中基于网格的流水线不兼容，需要一种能够与现有3D引擎无缝集成的网格表示方法。

Method: MeshSplatting通过可微分渲染联合优化几何和外观，使用受限Delaunay三角剖分强制连接性，并细化表面一致性，创建端到端平滑、视觉高质量的网格。

Result: 在Mip-NeRF360数据集上，MeshSplatting比当前最先进的基于网格的新视角合成方法MiLo提升了+0.69 dB的PSNR，同时训练速度快2倍，内存使用减少2倍。

Conclusion: MeshSplatting成功桥接了神经渲染和交互式3D图形，为实时场景交互提供了无缝的网格表示，能够在实时3D引擎中高效渲染。

Abstract: Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.

</details>


### [81] [ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation](https://arxiv.org/abs/2512.07328)
*Ziyang Mai,Yu-Wing Tai*

Main category: cs.CV

TL;DR: ContextAnyone是一个上下文感知的扩散框架，通过单张参考图像实现角色一致的文本到视频生成，解决了现有方法在保持发型、服装、体型等上下文特征一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频生成方法在保持角色身份一致性方面存在局限，特别是无法有效保留发型、服装、体型等重要的上下文视觉线索，这影响了视频的视觉连贯性。

Method: 提出ContextAnyone框架，通过联合重建参考图像和生成新视频帧，使模型能充分感知参考信息。采用Emphasize-Attention模块选择性增强参考感知特征，防止身份漂移；使用双指导损失结合扩散和参考重建目标；提出Gap-RoPE位置嵌入分离参考和视频token以稳定时序建模。

Result: 实验表明ContextAnyone在身份一致性和视觉质量方面优于现有的参考到视频方法，能够在不同动作和场景中生成连贯且保持上下文特征的角色视频。

Conclusion: ContextAnyone通过上下文感知的扩散框架有效解决了角色一致性的视频生成问题，在保持角色身份完整性的同时实现了高质量的文本到视频生成。

Abstract: Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.

</details>


### [82] [SparseCoop: Cooperative Perception with Kinematic-Grounded Queries](https://arxiv.org/abs/2512.06838)
*Jiahao Wang,Zhongwei Jiang,Wenchao Sun,Jiaru Zhong,Haibao Yu,Yuner Zhang,Chenyang Lu,Chuang Zhang,Lei He,Shaobing Xu,Jianqiang Wang*

Main category: cs.CV

TL;DR: SparseCoop是一个完全稀疏的协同感知框架，用于3D检测和跟踪，完全摒弃了中间BEV表示，通过实例查询、粗到精聚合和协同实例去噪实现高效协同感知。


<details>
  <summary>Details</summary>
Motivation: 当前协同感知方法存在通信成本高、灵活性差、对齐不精确等问题。基于密集BEV特征共享的方法面临二次增长的通信成本，且缺乏跨异步或不同视角的精确对齐能力。而稀疏查询方法则存在几何表示不足、融合策略次优和训练不稳定等问题。

Method: 提出SparseCoop框架，包含三个创新：1) 基于运动学的实例查询，使用包含3D几何和速度的显式状态向量进行精确时空对齐；2) 粗到精聚合模块实现鲁棒融合；3) 协同实例去噪任务加速和稳定训练。完全摒弃中间BEV表示。

Result: 在V2X-Seq和Griffin数据集上的实验表明，SparseCoop达到了最先进的性能，同时具有卓越的计算效率、低传输成本和强大的通信延迟鲁棒性。

Conclusion: SparseCoop通过完全稀疏的协同感知框架，解决了当前方法在通信成本、对齐精度和训练稳定性方面的限制，为自动驾驶协同感知提供了高效、鲁棒的解决方案。

Abstract: Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.

</details>


### [83] [DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection](https://arxiv.org/abs/2512.07351)
*Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Reem E. Mohamed,Md Rafiqul Islam,Asif Karim,Sami Azam*

Main category: cs.CV

TL;DR: DeepAgent：基于多智能体协作的深度伪造检测框架，通过视觉和音频模态的互补分析，结合随机森林元分类器提升检测性能


<details>
  <summary>Details</summary>
Motivation: 合成媒体特别是深度伪造的日益普及给数字内容验证带来挑战。现有方法通常将音频和视觉信息集成在单一模型中，容易受到模态不匹配、噪声和操纵的影响。需要一种更鲁棒的检测框架来应对这些局限性。

Method: 提出DeepAgent多智能体协作框架，包含两个互补智能体：Agent-1使用简化的AlexNet-based CNN检测深度伪造操作痕迹；Agent-2通过结合声学特征、Whisper音频转录和EasyOCR帧读取序列来检测视听不一致性。两个智能体的决策通过随机森林元分类器融合，利用各自学习的不同决策边界提升最终性能。

Result: 在三个基准数据集上评估：Agent-1在Celeb-DF和FakeAVCeleb组合数据集上达到94.35%测试准确率；Agent-2在FakeAVCeleb上达到93.69%准确率；元分类器在FakeAVCeleb上达到81.56%准确率。跨数据集验证中，在DeepFakeTIMIT上元分类器达到97.49%最终准确率，显示出强大的跨数据集泛化能力。

Conclusion: 基于层次结构的融合通过缓解单个模态的弱点增强了鲁棒性，证明了多智能体方法在处理深度伪造中多样化操纵类型的有效性。该方法为深度伪造检测提供了更可靠和适应性强的解决方案。

Abstract: The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.

</details>


### [84] [CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles](https://arxiv.org/abs/2512.06840)
*Satoshi Hashimoto,Tatsuya Konishi,Tomoya Kaichi,Kazunori Matsumoto,Mori Kurokawa*

Main category: cs.CV

TL;DR: 本文提出了一种结合持续学习和弱监督视频异常检测的新方法CADE，通过双生成器和多判别器集成来解决领域偏移和遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的弱监督视频异常检测方法主要针对静态数据集，忽视了数据领域可能变化的问题。当数据领域发生变化时，需要持续学习的视角，否则仅用新数据训练会导致对先前数据的性能下降（遗忘）。

Method: 提出了CADE方法：1）使用双生成器解决数据不平衡和标签不确定性问题；2）提出多判别器集成来捕捉因遗忘而遗漏的过去场景中的异常模式。

Result: 在常见的多场景视频异常检测数据集（如上海科技和夏洛特异常数据集）上，CADE显著优于现有的视频异常检测方法。

Conclusion: CADE是首个结合持续学习和弱监督视频异常检测视角的工作，通过双生成器和多判别器集成有效解决了领域偏移和遗忘问题，在多场景异常检测中表现出色。

Abstract: Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.

</details>


### [85] [Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2512.07360)
*Qiming Huang,Hao Ai,Jianbo Jiao*

Main category: cs.CV

TL;DR: 提出了一种结构感知的特征校正方法，通过构建区域邻接图来捕捉局部结构关系，从而改善CLIP特征在开放词汇语义分割中的局部判别能力，减少噪声并提升一致性。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在图像-文本对预训练中主要关注全局语义对齐，导致在细粒度视觉区域与文本关联时性能不佳，产生噪声和不一致的预测。这源于对比训练范式产生的分散偏差，仅使用CLIP特征难以缓解。

Method: 提出结构感知特征校正方法，基于图像的低级特征（如颜色和纹理）构建区域邻接图（RAG）来捕捉局部结构关系，利用该图通过增强局部判别能力来精炼CLIP特征。

Result: 大量实验表明，该方法能有效抑制分割噪声，改善区域级一致性，并在多个开放词汇分割基准上实现了强劲性能。

Conclusion: 通过引入基于图像本身的结构先验，提出的方法成功解决了CLIP特征在开放词汇语义分割中的局部判别不足问题，显著提升了分割质量。

Abstract: Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.

</details>


### [86] [Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2512.06845)
*Satoshi Hashimoto,Hitoshi Nishimura,Yanan Wang,Mori Kurokawa*

Main category: cs.CV

TL;DR: PA-VAD：一种无需真实异常视频的生成驱动视频异常检测方法，通过合成伪异常视频进行训练，在标准弱监督设置下达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 实际部署视频异常检测面临真实异常视频稀缺和收集成本高的问题，需要一种无需真实异常视频的训练方法

Method: 1) 合成阶段：使用CLIP选择类别相关初始图像，用视觉语言模型优化文本提示，调用视频扩散模型生成伪异常视频；2) 训练阶段：通过域对齐正则化模块缓解合成异常中的过度时空幅度问题，结合域对齐和内存使用感知更新

Result: 在ShanghaiTech上达到98.2%，在UCF-Crime上达到82.5%，分别比最强真实异常方法高+0.6%，比UVAD SOTA方法高+1.9%

Conclusion: 无需收集真实异常即可获得高精度异常检测，为可扩展部署提供了实用路径

Abstract: Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.

</details>


### [87] [Data-driven Exploration of Mobility Interaction Patterns](https://arxiv.org/abs/2512.07415)
*Gabriele Galatolo,Mirco Nanni*

Main category: cs.CV

TL;DR: 本文提出了一种基于数据挖掘的个体移动行为分析方法，通过从数据中直接发现个体间相互作用的证据和模式，而不是从预设行为模型出发。


<details>
  <summary>Details</summary>
Motivation: 理解个体移动行为及其对外部环境的反应是模拟人类物理动态的关键。现有解决方案通常从预设的行为模型出发，而本文旨在直接从数据中挖掘个体间相互作用的证据和模式，为改进现有模拟模型提供新见解。

Method: 采用数据挖掘视角，从数据中搜索可能反映个体间相互作用的移动事件，并在此基础上寻找复杂、持久的事件模式和随时间演化的配置。在两个真实案例（汽车和行人）上实例化该方法。

Result: 进行了全面的实验评估，包括性能、参数敏感性和样本结果解释。该方法能够发现个体间移动相互作用的机制，为改进现有模拟模型提供潜在帮助。

Conclusion: 提出的数据驱动方法能够从移动数据中直接发现个体间相互作用的模式和机制，为理解人类动态和改善人群模拟、应急管理等应用提供了新的视角和工具。

Abstract: Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.

</details>


### [88] [Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT](https://arxiv.org/abs/2512.06849)
*Matan Atad,Alexander W. Marka,Lisa Steinhelfer,Anna Curto-Vilalta,Yannik Leonhardt,Sarah C. Foreman,Anna-Sophia Walburga Dietrich,Robert Graf,Alexandra S. Gersing,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke,Hendrik Möller*

Main category: cs.CV

TL;DR: 提出一种弱监督方法，仅使用椎体级别的健康/恶性标签（无需病灶掩码），通过扩散自编码器和像素级差异图生成候选病灶区域，再通过Hide-and-Seek Attribution技术筛选出真正反映恶性的区域，实现椎体转移瘤的准确分割。


<details>
  <summary>Details</summary>
Motivation: CT中椎体转移瘤的准确分割在临床上很重要但难以规模化，因为体素级标注稀缺，且溶骨性和成骨性病变常与良性退行性改变相似。

Method: 结合扩散自编码器（DAE）生成椎体的分类器引导健康编辑，通过像素级差异图提出候选病灶区域。引入Hide-and-Seek Attribution技术：依次揭示每个候选区域同时隐藏其他区域，将编辑后的图像通过DAE投影回数据流形，使用潜在空间分类器量化该组件的孤立恶性贡献，高分区域形成最终的溶骨性或成骨性分割。

Result: 在保留的放射科医生标注上，尽管没有掩码监督，仍实现了强大的成骨性/溶骨性性能（F1: 0.91/0.85; Dice: 0.87/0.78），超过基线方法（F1: 0.79/0.67; Dice: 0.74/0.55）。

Conclusion: 椎体级别标签可以转化为可靠的病灶掩码，表明生成编辑结合选择性遮挡支持CT中准确的弱监督分割。

Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.

</details>


### [89] [When normalization hallucinates: unseen risks in AI-powered whole slide image processing](https://arxiv.org/abs/2512.07426)
*Karel Moens,Matthew B. Blaschko,Tinne Tuytelaars,Bart Diricx,Jonas De Vylder,Mustafa Yousif*

Main category: cs.CV

TL;DR: WSI归一化模型在真实临床数据上会产生难以察觉的幻觉伪影，传统评估方法无法检测，需要新的检测指标和更严格的验证流程。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的全切片图像归一化方法倾向于输出平均化结果，可能掩盖诊断重要特征，更严重的是会产生视觉难以察觉的幻觉伪影，对下游分析构成严重威胁，而现有评估方法往往忽视这一问题。

Method: 提出一种新颖的图像比较度量方法，专门用于自动检测归一化输出中的幻觉伪影。使用该度量方法系统评估多个在真实临床数据上重新训练的知名归一化方法。

Result: 在真实临床数据上重新训练的模型显示出令人担忧的幻觉频率，传统指标无法捕捉到显著的模型不一致性和失败情况。新提出的度量方法能够有效揭示这些问题。

Conclusion: 幻觉风险是真实存在且被低估的，需要开发更鲁棒、可解释的归一化技术，并在临床部署中实施更严格的验证协议。

Abstract: Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.

</details>


### [90] [Omni-Referring Image Segmentation](https://arxiv.org/abs/2512.06862)
*Qiancheng Zheng,Yunhang Shen,Gen Luo,Baiyang Song,Xing Sun,Xiaoshuai Sun,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出Omni-Referring Image Segmentation (OmniRIS)新任务，支持文本指令和带掩码/框/涂鸦的参考图像作为全模态提示，实现高度泛化的图像分割


<details>
  <summary>Details</summary>
Motivation: 现有单模态条件分割任务（如RIS和视觉RIS）存在局限性，需要更通用的分割方法，能够同时利用文本和视觉模态的优势

Method: 提出OmniRIS任务框架，支持文本指令和多种视觉提示（掩码、边界框、涂鸦）作为全模态输入；构建OmniRef数据集（30,956张图像，186,939个全模态提示）；开发OmniSegNet基线模型处理全模态提示编码等关键挑战

Result: 实验验证了OmniSegNet能够有效遵循全模态指令，并展示了OmniRIS在高度泛化图像分割方面的优越性

Conclusion: OmniRIS通过支持文本和视觉全模态提示，实现了更通用、更灵活的图像分割，为高度泛化的图像分割研究提供了新方向

Abstract: In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.

</details>


### [91] [Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training](https://arxiv.org/abs/2512.06864)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS：一种通过质量引导自训练的无监督视频实例分割框架，无需人工标注即可实现合成到真实视频的渐进适应，在YouTubeVIS-2019上达到52.6 AP50，超越之前最佳方法4.4%。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割面临像素级掩码和时间一致性标注的双重挑战。现有无监督方法如VideoCutLER虽然通过合成数据消除了光流依赖，但仍受限于合成到真实的域差距。

Method: 提出AutoQ-VIS框架，通过质量引导的自训练建立伪标签生成和自动质量评估的闭环系统，实现从合成到真实视频的渐进适应。

Result: 在YouTubeVIS-2019验证集上达到52.6 AP50，超越之前最佳方法VideoCutLER 4.4%，且无需任何人工标注。

Conclusion: 质量感知的自训练方法对于无监督视频实例分割是可行的，AutoQ-VIS通过闭环系统有效解决了合成到真实的域差距问题。

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.

</details>


### [92] [Spatial Retrieval Augmented Autonomous Driving](https://arxiv.org/abs/2512.06865)
*Xiaosong Jia,Chenhe Zhang,Yule Jiang,Songbur Wong,Zhiyuan Zhang,Chen Chen,Shaofeng Zhang,Xuanhe Zhou,Xue Yang,Junchi Yan,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出空间检索范式，通过引入离线检索的地理图像作为额外输入，增强自动驾驶系统的环境感知能力，克服传统车载传感器的视野限制和极端条件问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统依赖车载传感器进行环境感知，但受限于感知视野、遮挡以及黑暗、雨天等极端条件。相比之下，人类驾驶员能够在能见度差的情况下回忆道路结构。为了赋予模型这种"回忆"能力，需要新的感知范式。

Method: 提出空间检索范式，从离线缓存（如Google Maps或存储的自动驾驶数据集）中检索地理图像作为额外输入。扩展nuScenes数据集，通过Google Maps API检索地理图像并与自车轨迹对齐。在五个核心自动驾驶任务上建立基线。

Result: 扩展模态能够提升某些任务的性能。将开源数据集构建代码、数据和基准，供进一步研究这一新的自动驾驶范式。

Conclusion: 空间检索范式为自动驾驶系统提供了"回忆"能力，通过引入离线地理图像作为额外输入，能够克服传统车载传感器的限制，是一种即插即用的扩展方案。

Abstract: Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.
  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.

</details>


### [93] [Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective](https://arxiv.org/abs/2512.06870)
*Wangkai Li,Rui Sun,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: ECOCSeg提出了一种基于纠错输出码的语义分割新方法，通过将类别分解为属性并处理部分错误比特，提高了伪标签学习的稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在语义分割的伪标签学习中，传统方法使用one-hot编码容易产生错误伪标签，这些错误会在训练过程中被放大，影响模型性能。

Method: 提出ECOCSeg框架：1）引入基于ECOC的分类器，将类别分解为属性并处理部分不准确的比特；2）开发比特级标签去噪机制，为未标记图像生成更高质量的伪标签。

Result: ECOCSeg在多个UDA和SSL基准测试中，与现有方法结合后均表现出显著改进，适用于不同的分割架构。

Conclusion: ECOCSeg通过纠错输出码提供了一种细粒度的类别编码方法，有效解决了伪标签学习中的错误传播问题，提高了分割模型的稳定性和泛化能力。

Abstract: Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.

</details>


### [94] [SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification](https://arxiv.org/abs/2512.06877)
*Mohammed Q. Alkhatib,Ali Jamali,Swalpa Kumar Roy*

Main category: cs.CV

TL;DR: 提出基于卷积混合器范式的轻量级遥感场景分类模型，通过多尺度深度卷积和逐点操作交替进行空间与通道混合，在AID和EuroSAT数据集上取得良好精度与效率平衡。


<details>
  <summary>Details</summary>
Motivation: 遥感场景分类对地球观测至关重要，但现有CNN和ViT模型在空间分辨率、视角、方向和背景条件变化下泛化能力有限，需要更高效且鲁棒的解决方案。

Method: 采用卷积混合器架构，通过多尺度深度卷积进行空间混合，通过逐点操作进行通道混合，交替提取局部和上下文信息，保持参数和计算量较低。

Result: 在AID数据集上获得74.7%总体精度、74.57%平均精度和73.79 Kappa值；在EuroSAT数据集上获得93.90%总体精度、93.93%平均精度和93.22 Kappa值，优于现有CNN和transformer模型。

Conclusion: 提出的轻量级卷积混合器模型在遥感场景分类任务中实现了精度与效率的良好平衡，为实际应用提供了有效解决方案。

Abstract: Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer

</details>


### [95] [Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion](https://arxiv.org/abs/2512.06882)
*Yu Zhu,Naoya Chiba,Koichi Hashimoto*

Main category: cs.CV

TL;DR: 提出分层图像引导的3D分割框架，通过实例级到部件级的渐进式分割，解决工业场景中遮挡和尺度差异问题，无需昂贵标注且保持语义一致性。


<details>
  <summary>Details</summary>
Motivation: 工业环境中密集布局和多尺度物体导致可靠3D分割困难：严重遮挡削弱几何边界，大尺度差异使端到端模型难以同时捕捉粗粒度和细粒度细节。现有方法要么需要昂贵标注，要么存在跨视图语义不一致问题。

Method: 分层图像引导3D分割框架：1) 实例分割：渲染俯视图，用YOLO-World提示SAM生成掩码，投影回3D点云；2) 部件级分割：对每个实例渲染多视图图像，在各视图应用相同2D分割和反投影，通过贝叶斯更新融合确保跨视图语义一致性。

Result: 在真实工厂数据上实验表明，方法能有效处理遮挡和结构复杂性，获得一致高的每类mIoU分数。在公开数据集上的额外评估证实了框架的泛化能力，突出了其鲁棒性、标注效率和适应多样化3D环境的能力。

Conclusion: 提出的分层图像引导3D分割框架通过渐进式从实例到部件的分割策略，解决了工业场景中的遮挡和尺度差异挑战，实现了高效、鲁棒且语义一致的3D分割，无需昂贵标注。

Abstract: Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.

</details>


### [96] [Balanced Learning for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2512.06886)
*Wangkai Li,Rui Sun,Bohao Liao,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: BLDA提出了一种平衡学习方法，通过分析预测logits分布来识别过预测和欠预测类别，使用共享锚分布对齐不同类别的logits分布，并在自训练过程中加入logits校正项，以解决UDA语义分割中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在无监督域适应语义分割中，自训练方法由于固有的类别不平衡和域间数据/标签空间分布偏移，难以平衡学习各个类别。现有方法缺乏直接评估和缓解类别偏差的有效机制。

Method: 1) 通过分析预测logits分布识别过预测和欠预测类别；2) 使用共享锚分布进行后处理对齐不同类别的logits分布；3) 在线估计logits分布并在损失函数中加入logits校正项；4) 利用累积密度作为域共享结构知识连接源域和目标域。

Result: 在两个标准UDA语义分割基准测试上的大量实验表明，BLDA在集成到多种现有方法中时能持续提升性能，特别是对于欠预测类别。

Conclusion: BLDA提供了一种无需先验分布知识就能直接评估和缓解类别偏差的有效方法，通过平衡学习策略显著改善了UDA语义分割中类别不平衡问题。

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.

</details>


### [97] [Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation](https://arxiv.org/abs/2512.06888)
*Liyang Song,Hardik Bishnoi,Sai Kumar Reddy Manne,Sarah Ostadabbas,Briana J. Taylor,Michael Wan*

Main category: cs.CV

TL;DR: 该研究开发了首个可复现的婴儿呼吸监测系统，包含400个视频的数据集和基于计算机视觉的呼吸估计算法，填补了婴儿呼吸监测领域的空白。


<details>
  <summary>Details</summary>
Motivation: 婴儿呼吸异常与神经发育障碍和婴儿猝死综合征相关，但现有呼吸监测技术主要针对成人，缺乏专门针对婴儿的公开视频数据集和可复现算法。

Method: 1. 创建了包含400个视频的婴儿呼吸数据集（AIR-400），其中275个为新采集的标注视频；2. 开发了基于婴儿特定感兴趣区域检测和时空神经处理的算法，并利用光流输入增强性能；3. 建立了可复现的婴儿呼吸估计算法流程。

Result: 1. 建立了首个公开的婴儿呼吸视频数据集；2. 开发了首个可复现的婴儿呼吸估计算法；3. 为基于视觉的婴儿呼吸估计建立了基准测试标准；4. 所有数据集、代码和训练模型都已公开。

Conclusion: 该研究填补了婴儿呼吸监测领域的空白，为早期检测和治疗呼吸异常提供了技术基础，有助于预防神经发育障碍和婴儿猝死综合征。

Abstract: The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.

</details>


### [98] [Scaling Zero-Shot Reference-to-Video Generation](https://arxiv.org/abs/2512.06905)
*Zijian Zhou,Shikun Liu,Haozhe Liu,Haonan Qiu,Zhaochong An,Weiming Ren,Zhiheng Liu,Xiaoke Huang,Kam Woh Ng,Tian Xie,Xiao Han,Yuren Cong,Hang Li,Chuyan Zhu,Aditya Patel,Tao Xiang,Sen He*

Main category: cs.CV

TL;DR: Saber是一个无需参考图像-视频-文本三元组数据的零样本参考到视频生成框架，通过掩码训练策略和注意力模型设计实现身份一致性和参考感知表示。


<details>
  <summary>Details</summary>
Motivation: 当前参考到视频生成方法依赖昂贵的显式参考图像-视频-文本三元组数据，这种数据构建成本高且难以扩展，限制了方法的可扩展性。

Method: 提出Saber框架，仅使用视频-文本对进行训练，采用掩码训练策略和定制的基于注意力的模型设计，学习身份一致和参考感知的表示，并集成掩码增强技术减少复制粘贴伪影。

Result: Saber在OpenS2V-Eval基准测试中表现出色，优于使用R2V数据训练的方法，并展现出对不同数量参考图像的强大泛化能力。

Conclusion: Saber通过零样本框架解决了参考到视频生成的数据可扩展性问题，无需昂贵的显式三元组数据，实现了高质量的视频生成和良好的泛化性能。

Abstract: Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.

</details>


### [99] [An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research](https://arxiv.org/abs/2512.07652)
*Hamad Almazrouei,Mariam Al Nasseri,Maha Alzaabi*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的自主水下航行器系统，通过集成YOLOv12 Nano、CNN、PCA、K-Means++和LLM技术，实现水下物体自动检测、分析和报告生成，显著提升海洋探索效率。


<details>
  <summary>Details</summary>
Motivation: 传统海洋探索面临极端条件、能见度低、成本高等挑战，导致大量海洋区域未被探索。需要开发自动化系统来克服这些限制，降低人类潜水风险，提高任务效率。

Method: 系统集成了YOLOv12 Nano进行实时物体检测，ResNet50 CNN进行特征提取，PCA进行降维，K-Means++进行聚类分析，GPT-4o Mini LLM生成结构化报告。在包含55,000多张图像的DeepFish和OzFish数据集上进行训练评估。

Result: 系统检测性能达到mAP@0.5为0.512，精确率0.535，召回率0.438。PCA降维保留了98%的方差，K-Means聚类成功按视觉特征分组物体，LLM能有效生成包含位置数据的检测摘要。

Conclusion: 该集成方法显著降低了人类潜水风险，提高了任务效率，增强了水下数据分析的速度和深度，为在挑战性海洋环境中进行更有效的科学研究开辟了新途径。

Abstract: Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.

</details>


### [100] [DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations](https://arxiv.org/abs/2512.07674)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: DIST-CLIP是一个用于MRI图像标准化的统一框架，通过解耦解剖内容和图像对比度，使用CLIP编码器提取对比度表示，并利用自适应风格转移模块实现灵活的图像标准化。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像分析中具有巨大潜力，但临床泛化能力受到数据异质性的严重限制。特别是在磁共振成像中，扫描仪硬件差异、采集协议多样性和序列参数变化引入了显著的域偏移，掩盖了潜在的生物信号。现有的图像标准化方法存在局限性：基于图像的方法需要目标图像，而基于文本的方法依赖过于简化的标签，无法捕捉复杂的采集细节，且通常局限于变异性有限的数据集。

Method: 提出DIST-CLIP（基于CLIP指导的解耦风格转移）框架，明确解耦解剖内容和图像对比度。使用预训练的CLIP编码器提取对比度表示，然后通过新颖的自适应风格转移模块将这些对比度嵌入整合到解剖内容中。该框架可以灵活地使用目标图像或DICOM元数据进行指导。

Result: 在多样化的真实临床数据集上训练和评估DIST-CLIP，结果显示在风格转换保真度和解剖结构保持方面，相比最先进的方法有显著改进。该框架为风格转移和MRI数据标准化提供了一个灵活的解决方案。

Conclusion: DIST-CLIP通过解耦解剖内容和图像对比度，结合CLIP编码器和自适应风格转移，提供了一个统一且灵活的MRI标准化框架，能够有效处理真实临床环境中的异质性，显著提升了风格转换的保真度和解剖结构的保持能力。

Abstract: Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.

</details>


### [101] [Selective Masking based Self-Supervised Learning for Image Semantic Segmentation](https://arxiv.org/abs/2512.06981)
*Yuemin Wang,Ian Stavness*

Main category: cs.CV

TL;DR: 提出了一种用于语义分割的自监督学习方法，通过选择性掩码图像重建作为预训练任务，相比随机掩码方法在多个数据集上取得了更好的分割精度。


<details>
  <summary>Details</summary>
Motivation: 大多数掩码图像建模预训练方法使用随机掩码增强，但这种方法可能不是最优的。本文旨在开发一种更有效的选择性掩码方法，利用已训练模型的知识来改进预训练效果。

Method: 提出选择性掩码图像重建方法，通过迭代步骤选择重建损失最高的图像块进行掩码，而不是随机掩码。这种方法利用已训练模型的知识来指导掩码选择。

Result: 在两个通用数据集（Pascal VOC和Cityscapes）和两个杂草分割数据集（Nassar 2020和Sugarbeets 2016）上，选择性掩码方法比传统随机掩码方法和监督ImageNet预训练在下游分割精度上分别提高了2.9%和2.5%。同时显著提高了最低性能类别的准确性。

Conclusion: 选择性掩码图像重建方法为改进端到端语义分割工作流程提供了有效实用的解决方案，特别适用于需要有限模型容量以满足推理速度和计算资源要求的场景。使用相同的预训练和下游数据集能在低预算自监督预训练中获得最佳结果。

Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.

</details>


### [102] [Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment](https://arxiv.org/abs/2512.07702)
*Sangha Park,Eunji Kim,Yeongtak Oh,Jooyoung Choi,Sungroh Yoon*

Main category: cs.CV

TL;DR: NPC通过自动识别和应用负面提示来抑制生成图像中的非预期内容，从而提升文本到图像生成的精确对齐效果。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像生成取得了显著进展，但对于具有丰富组合结构或想象性元素的提示，实现精确的文本-图像对齐仍然具有挑战性。现有方法在处理复杂提示时往往会产生与文本描述不符的非预期内容。

Method: 提出NPC（Negative Prompting for Image Correction）自动化流程，通过分析交叉注意力模式来解释目标负面提示（与对齐错误直接相关）和非目标负面提示（与提示无关但在生成图像中存在的标记）如何增强对齐。采用验证器-字幕器-提议器框架生成候选负面提示，并使用显著文本空间评分进行排序，实现无需额外图像合成的有效选择。

Result: 在GenEval++和Imagine-Bench基准测试中，NPC显著优于强基线方法：在GenEval++上达到0.571 vs 0.371，在Imagine-Bench上获得最佳整体性能。

Conclusion: 通过指导模型不生成什么内容，NPC为扩散模型中的文本-图像对齐提供了一种原则性、完全自动化的方法，能够有效提升复杂提示下的生成质量。

Abstract: Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.

</details>


### [103] [Evaluating and Preserving High-level Fidelity in Super-Resolution](https://arxiv.org/abs/2512.07037)
*Josep M. Rocafort,Shaolin Su,Javier Vazquez-Corral,Alexandra Gomez-Villa*

Main category: cs.CV

TL;DR: 该论文提出衡量超分辨率模型的高层语义保真度作为补充评价标准，构建首个带保真度标注的数据集，分析现有指标相关性，并展示通过保真度反馈微调可同时提升语义保真度和感知质量。


<details>
  <summary>Details</summary>
Motivation: 当前超分辨率模型虽然能生成视觉质量高的图像，但有时会产生幻觉效应改变图像内容。这种高层语义变化容易被人类识别，但现有低层图像质量指标未能很好衡量。需要建立高层保真度测量作为补充标准，以评估生成式超分辨率模型的可靠性。

Method: 1) 构建首个带保真度标注的数据集，评估SOTA超分辨率模型的高层保真度表现；2) 分析现有图像质量指标与保真度测量的相关性；3) 展示基础模型能更好地处理高层语义任务；4) 基于保真度反馈微调超分辨率模型。

Result: 1) 建立了高层保真度测量标准；2) 创建了首个带保真度标注的数据集；3) 发现现有图像质量指标与高层保真度相关性不足；4) 基础模型能更好处理高层语义任务；5) 通过保真度反馈微调可同时提升语义保真度和感知质量。

Conclusion: 高层保真度测量是评估生成式超分辨率模型可靠性的重要补充标准。提出的保真度标准在模型评估和优化中具有潜在价值，通过保真度反馈微调可同时改善语义保真度和感知质量。

Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.

</details>


### [104] [SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination](https://arxiv.org/abs/2512.07730)
*Sangha Park,Seungryong Yoo,Jisoo Mok,Sungroh Yoon*

Main category: cs.CV

TL;DR: SAVE框架通过稀疏自编码器潜在特征引导来减少多模态大语言模型的对象幻觉问题


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然取得了显著进展，但仍然容易受到语言先验和视觉信息丢失导致的物体幻觉问题影响

Method: 提出SAVE框架，使用稀疏自编码器潜在特征引导模型。通过二元物体存在问答探针识别最能反映模型视觉信息处理的SAE特征（称为视觉理解特征），然后沿着这些特征引导模型

Result: 在标准基准测试中优于最先进的无训练方法，在CHAIR_S上提升10个百分点，在POPE和MMHal-Bench上获得一致增益。跨多个模型和层的广泛评估证实了方法的鲁棒性和泛化性

Conclusion: SAVE通过稀疏自编码器特征引导有效减少多模态大语言模型的物体幻觉，增强视觉基础理解，抑制不确定物体标记生成并增加对图像标记的关注

Abstract: Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\%p improvement in CHAIR\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.

</details>


### [105] [RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting](https://arxiv.org/abs/2512.07052)
*Hoang-Nhat Tran,Francesco Di Sario,Gabriele Spadaro,Giuseppe Valenzise,Enzo Tartaglione*

Main category: cs.CV

TL;DR: 提出了一种灵活的3D高斯泼溅压缩方案，支持在预定义边界内任意速率插值，无需重新训练即可适应不同带宽和设备限制


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽然能实现实时逼真渲染，但存在内存需求大、训练成本高的问题。现有压缩方法只能在固定速率下工作，无法适应变化的带宽和设备约束

Method: 提出灵活的3DGS压缩方案，支持在预定义边界内任意速率插值，计算轻量且无需为不同速率重新训练

Result: 该方法实现了高效高质量压缩，同时提供动态速率控制，在广泛的运行点上保持渲染质量

Conclusion: 该方法适合实际沉浸式应用部署，代码将在工作被接受后开源提供

Abstract: Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.

</details>


### [106] [WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling](https://arxiv.org/abs/2512.07821)
*Shaoheng Fang,Hanwen Jiang,Yunpeng Bai,Niloy J. Mitra,Qixing Huang*

Main category: cs.CV

TL;DR: WorldReel是一个4D视频生成器，通过联合生成RGB帧和4D场景表示（点云、相机轨迹、密集光流），实现时空一致性，在动态场景和移动相机下保持几何一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成器虽然视觉效果逼真，但在3D一致性方面存在根本性不足。为了解决动态内容和相机移动下的时空一致性问题，需要开发能够原生保持4D一致性的视频生成方法。

Method: 提出WorldReel系统，联合生成RGB帧和4D场景表示（点云、相机轨迹、密集光流）。采用合成数据和真实数据混合训练策略：合成数据提供精确的4D监督（几何、运动、相机），真实视频提供视觉多样性和真实感。

Result: WorldReel在动态场景和移动相机下的视频生成中达到了新的最先进水平，显著改进了几何一致性、运动连贯性指标，并减少了视角-时间伪影。能够泛化到真实世界视频，同时保持强大的几何保真度。

Conclusion: WorldReel将视频生成推向4D一致的世界建模，使智能体能够通过单一稳定的时空表示进行渲染、交互和场景推理，为构建一致的时空场景表示迈出了重要一步。

Abstract: Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.

</details>


### [107] [One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation](https://arxiv.org/abs/2512.07829)
*Yuan Gao,Chen Chen,Tianrong Chen,Jiatao Gu*

Main category: cs.CV

TL;DR: FAE提出了一种简单有效的框架，将预训练的视觉表征适配到适合生成的低维潜在空间中，仅需单个注意力层即可实现，同时保留重建和理解所需的信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将理解导向的预训练视觉表征适配到生成友好的潜在空间时面临挑战，因为理解特征需要高维潜在空间来捕捉多样化假设，而生成模型需要低维潜在空间来忠实保留注入的噪声，这种不匹配导致先前工作需要复杂的架构和目标。

Method: FAE框架通过耦合两个独立的深度解码器来适配预训练视觉表征：一个解码器训练用于重建原始特征空间，第二个解码器将重建特征作为输入进行图像生成。该框架通用性强，可与多种自监督编码器（如DINO、SigLIP）结合，并适用于扩散模型和归一化流两种生成模型家族。

Result: 在类别条件和文本到图像基准测试中，FAE表现出色。在ImageNet 256x256上，带CFG的扩散模型达到接近SOTA的FID 1.29（800轮）和1.70（80轮）；不带CFG时达到SOTA的FID 1.48（800轮）和2.08（80轮），展示了高质量和快速学习能力。

Conclusion: FAE提供了一种简单而有效的解决方案，成功地将预训练视觉表征适配到生成友好的低维潜在空间中，仅需最小架构修改即可实现高质量图像生成，同时保持理解和重建能力。

Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.

</details>


### [108] [Relational Visual Similarity](https://arxiv.org/abs/2512.07833)
*Thao Nguyen,Sicheng Mo,Krishna Kumar Singh,Yilin Wang,Jing Shi,Nicholas Kolkin,Eli Shechtman,Yong Jae Lee,Yuheng Li*

Main category: cs.CV

TL;DR: 该论文提出了关系相似性的概念，开发了首个能够测量图像间关系相似性的模型，揭示了现有视觉相似性度量方法在捕捉人类关系感知能力方面的重大缺陷。


<details>
  <summary>Details</summary>
Motivation: 人类不仅能感知属性相似性（如颜色、形状），还能感知关系相似性（如地球和桃子的分层结构相似）。然而，当前广泛使用的视觉相似性度量方法（如LPIPS、CLIP、DINO）都只关注感知属性相似性，无法捕捉人类感知到的丰富且常常令人惊讶的关系相似性。这揭示了视觉计算领域的一个重要空白。

Method: 1. 首先将关系图像相似性形式化为可测量的问题：当两幅图像的内部关系或视觉元素之间的功能对应时，即使它们的视觉属性不同，它们也具有关系相似性。
2. 构建了一个包含11.4万张图像-标题的数据集，其中标题经过匿名化处理，描述场景的底层关系逻辑而非表面内容。
3. 使用这个数据集对视觉-语言模型进行微调，以测量图像之间的关系相似性。

Result: 开发出了首个能够测量图像关系相似性的模型，该模型能够根据图像的底层关系结构而非可见外观来连接图像。研究表明，虽然关系相似性在现实世界中有许多应用，但现有的图像相似性模型都无法捕捉到它。

Conclusion: 关系相似性是视觉计算中一个被忽视但至关重要的维度。该研究开发了首个能够测量关系相似性的模型，填补了现有视觉相似性度量方法的重大空白，为理解人类如何感知和识别图像间的深层关系结构迈出了重要一步。

Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.

</details>


### [109] [Context-measure: Contextualizing Metric for Camouflage](https://arxiv.org/abs/2512.07076)
*Chen-Yang Wang,Gepeng Ji,Song Shao,Ming-Ming Cheng,Deng-Ping Fan*

Main category: cs.CV

TL;DR: 提出Context-measure：一种基于概率像素感知相关框架的上下文化评估范式，用于评估伪装物体分割，比现有上下文无关指标更可靠


<details>
  <summary>Details</summary>
Motivation: 当前伪装场景的评估指标忽视了上下文依赖这一关键因素，这些指标原本是为评估一般或显著物体设计的，假设空间上下文不相关，无法准确反映伪装场景的特性

Method: 提出Context-measure评估范式，基于概率像素感知相关框架，通过整合空间依赖性和像素级伪装量化，使评估更符合人类感知

Result: 在三个具有挑战性的伪装物体分割数据集上的广泛实验表明，Context-measure比现有的上下文无关指标提供更可靠的评估结果

Conclusion: Context-measure可以为涉及伪装模式的各种计算机视觉应用（如农业、工业和医疗场景）提供基础评估基准

Abstract: Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.

</details>


### [110] [COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision](https://arxiv.org/abs/2512.07107)
*Jaeyoon Lee,Hojoon Jung,Sungtae Hwang,Jihyong Oh,Jongwon Choi*

Main category: cs.CV

TL;DR: COREA是一个统一框架，联合学习可重照明的3D高斯和SDF，实现精确几何重建和真实重照明，通过3D到3D对齐策略解决现有方法几何粗糙和BRDF-光照分解不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射方法虽然扩展到网格重建和基于物理的渲染，但其几何仍从2D渲染学习，导致表面粗糙且BRDF-光照分解不可靠。需要直接在3D空间中学习几何信号以获得更精确的几何重建和稳定的光照分解。

Method: 提出粗到细的双向3D到3D对齐策略：深度提供粗对齐，深度梯度和法线细化精细结构；引入密度控制机制稳定高斯增长；联合学习可重照明3D高斯和SDF表示。

Result: 在标准基准测试中，COREA在新视角合成、网格重建和基于物理的渲染方面均取得优越性能，实现了几何保真度和内存效率的平衡。

Conclusion: COREA是首个统一框架，通过3D到3D对齐直接学习几何信号，实现了精确几何重建和真实重照明，解决了现有方法在几何精度和BRDF-光照分解方面的局限性。

Abstract: We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.

</details>


### [111] [Training-free Clothing Region of Interest Self-correction for Virtual Try-On](https://arxiv.org/abs/2512.07126)
*Shengjie Lu,Zhibin Wan,Jiejie Liu,Quan Zhang,Mingjie Sun*

Main category: cs.CV

TL;DR: 本文提出了一种改进的虚拟试穿方法，通过能量函数约束注意力机制，使生成结果更符合目标服装细节，并设计了新的评估指标VTID，在多个数据集上超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法在生成服装结果与目标服装之间存在模式、纹理和边界方面的差异，且现有评估指标仅关注图像真实感而忽略与目标元素的对齐。

Method: 提出使用能量函数约束生成过程中提取的注意力图，使注意力更集中于服装感兴趣区域，从而影响生成结果更符合目标服装细节。同时设计了新的评估指标VTID。

Result: 在VITON-HD和DressCode数据集上，LPIPS、FID、KID和VTID指标分别提升1.4%、2.3%、12.3%和5.8%。在服装更换重识别任务中，LTCC、PRCC、VC-Clothes数据集的Rank-1指标分别提升2.5%、1.1%和1.6%。

Conclusion: 提出的注意力约束方法和VTID评估指标有效提升了虚拟试穿的质量和评估全面性，在下游任务中也表现出良好的泛化能力。

Abstract: VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.

</details>


### [112] [MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP](https://arxiv.org/abs/2512.07128)
*Chau Truong,Hieu Ta Quang,Dung D. Le*

Main category: cs.CV

TL;DR: MulCLIP是一个端到端多级对齐框架，通过全局对比对齐、局部特征校准和子标题聚合补丁对齐，解决了CLIP模型在处理长文本描述时的局限性，在保持部署效率的同时提升了细粒度理解能力。


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉语言模型在短文本对齐上表现良好，但面对长文本详细描述时效果不佳。现有方法使用区域建议信息来映射视觉区域和长文本句子，但部署成本较高。需要一种既能处理长文本结构，又能保持部署效率的解决方案。

Method: 1. 保持图像与摘要及长标题的全局对比对齐，扩展位置嵌入以支持更长文本序列；2. 提出局部校准特征上的令牌重建对齐，增强单词和图像补丁之间的语义连接；3. 提出子标题聚合补丁对齐，自动提取和聚合每个子标题的上下文丰富补丁。

Result: 在多个基准测试中，MulCLIP方法持续提升了下游任务性能。消融研究证实其多尺度对齐是驱动比区域建议辅助方法更好细粒度能力的关键因素，特别适合多样化的实际应用场景。

Conclusion: MulCLIP通过创新的多级对齐框架，有效解决了CLIP模型处理长文本描述的局限性，在保持部署效率的同时显著提升了细粒度理解能力，为实际应用提供了更优的解决方案。

Abstract: Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.

</details>


### [113] [CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics](https://arxiv.org/abs/2512.07155)
*Dahyeon Kye,Jeahun Sung,MinKyu Jeon,Jihyong Oh*

Main category: cs.CV

TL;DR: CHIMERA是一个零样本扩散模型框架，通过缓存反转引导的去噪过程实现图像变形，解决了现有方法在语义对齐和平滑过渡方面的不足。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然具有强大的生成能力，但在实现平滑且语义一致的图像变形方面仍面临挑战。现有方法由于缺乏自适应结构和语义对齐，往往产生突兀的过渡或过饱和的外观。

Method: 提出CHIMERA框架，将变形问题构建为缓存反转引导的去噪过程。采用自适应缓存注入（ACI）在DDIM反转期间缓存输入特征并在去噪时自适应重新注入，实现空间和语义对齐；使用语义锚点提示（SAP）通过视觉语言模型生成共享锚点提示作为语义桥梁；引入全局-局部一致性评分（GLCS）作为变形导向的评估指标。

Result: 大量实验和用户研究表明，CHIMERA相比现有方法实现了更平滑、语义更一致的过渡，在图像变形任务上建立了新的最先进水平。

Conclusion: CHIMERA通过创新的缓存反转引导去噪框架，结合自适应特征注入和语义锚点提示，成功解决了图像变形中的语义对齐和平滑过渡问题，为扩散模型在图像变形任务中的应用提供了有效解决方案。

Abstract: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.

</details>


### [114] [TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration](https://arxiv.org/abs/2512.07171)
*Shravan Venkatraman,Rakesh Raj Madavan,Pavan Kumar S,Muthu Subash Kavitha*

Main category: cs.CV

TL;DR: TIDE是一个两阶段水下图像复原框架，通过专门的先验分解来显式建模退化特征并进行针对性复原，将水下退化分解为四个关键因素并设计专门的复原专家。


<details>
  <summary>Details</summary>
Motivation: 水下图像复原对海洋应用至关重要，但现有方法通常在整个图像上应用统一的复原策略，难以处理空间变化且同时发生的多种退化问题。

Method: TIDE采用两阶段逆退化估计框架：第一阶段将水下退化分解为颜色失真、雾霾、细节损失和噪声四个关键因素，并设计专门的复原专家生成多个假设；第二阶段基于局部退化模式自适应融合这些假设，并通过渐进细化阶段校正残留伪影。

Result: 在标准基准测试和挑战性的浑浊水域条件下，TIDE在基于参考的保真度指标上取得竞争性性能，同时在非参考感知质量指标上优于现有方法，在颜色校正和对比度增强方面有显著改进。

Conclusion: TIDE通过显式建模退化特征和专门的先验分解，有效解决了水下图像复原中空间变化的多重退化问题，为复杂水下环境提供了高质量的图像复原解决方案。

Abstract: Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\underline{t}$wo stage $\underline{i}$nverse $\underline{d}$egradation $\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.

</details>


### [115] [SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting](https://arxiv.org/abs/2512.07197)
*Seokhyun Youn,Soohyun Lee,Geonho Kim,Weeyoung Kwon,Sung-Ho Bae,Jihyong Oh*

Main category: cs.CV

TL;DR: 这篇综述首次系统性地总结了高效3D和4D高斯泼溅技术，将其分为参数压缩和结构压缩两大方向，并分析了方法趋势、数据集、评估指标和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术虽然能实现实时高保真3D重建和新视角合成，但其存储和渲染数百万高斯分布需要巨大的内存和计算资源，特别是在4D动态场景中问题更加严重。因此需要开发高效的高斯泼溅技术来减少冗余同时保持重建质量。

Method: 将现有高效3D和4D高斯泼溅方法系统性地分为两大方向：1) 参数压缩 - 减少每个高斯分布的参数数量；2) 结构压缩 - 减少高斯分布的总数量。对每个类别总结了核心思想和方法趋势。

Result: 提供了该领域的统一概述，涵盖了广泛使用的数据集、评估指标和代表性基准比较，为研究人员提供了全面的技术路线图。

Conclusion: 讨论了当前局限性，并展望了未来研究方向，旨在实现可扩展、紧凑且实时的静态和动态3D场景表示的高斯泼溅技术。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.

</details>


### [116] [MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning](https://arxiv.org/abs/2512.07203)
*Xuhui Zheng,Kang An,Ziliang Wang,Yuhang Wang,Faqiang Qian,Yichao Wu*

Main category: cs.CV

TL;DR: MMRPT是一个基于掩码多模态强化学习的预训练框架，通过强化学习奖励视觉基础而非文本模仿，增强多模态大模型的视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态预训练受限于图像-文本对的描述性偏差，导致模型更倾向于依赖表面语言线索而非真正的视觉理解。需要一种能强化视觉推理能力的预训练方法。

Method: 提出MMRPT框架：1)通过视觉token注意力估计句子级视觉依赖性，掩码高度依赖视觉的文本片段；2)引入强化学习到预训练中，使用语义-视觉奖励引导模型进行视觉基础的推理重建；3)模型通过强化学习信号奖励视觉基础而非文本模仿。

Result: 实验显示：1)在多种基准测试中获得一致的零样本性能提升；2)在有监督微调下显著提高了鲁棒性；3)证明基于强化学习的掩码推理为多模态模型提供了更可靠和泛化的预训练目标。

Conclusion: MMRPT通过将强化学习直接整合到多模态大模型预训练中，成功强化了视觉推理能力，为解决多模态预训练中的描述性偏差问题提供了有效方案。

Abstract: Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.

</details>


### [117] [AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT](https://arxiv.org/abs/2512.07206)
*Boyang Pan,Zeyu Zhang,Hongyu Meng,Bin Cui,Yingying Zhang,Wenli Hou,Junhao Li,Langdi Zhong,Xiaoxiao Chen,Xiaoyu Xu,Changjin Zuo,Chao Cheng,Nan-Jie Gong*

Main category: cs.CV

TL;DR: 开发了名为AutoLugano的深度学习系统，能够从FDG-PET/CT扫描中自动进行淋巴瘤分类，包括病灶分割、解剖定位和Lugano分期


<details>
  <summary>Details</summary>
Motivation: 开发一个全自动的端到端系统，从基线FDG-PET/CT扫描中自动完成淋巴瘤分类，包括病灶分割、解剖定位和Lugano分期，以辅助临床决策

Method: 系统包含三个顺序模块：1）基于3D nnU-Net的解剖感知病灶分割；2）使用TotalSegmentator工具包进行基于图谱的解剖定位；3）将受累区域空间分布转换为Lugano分期和治疗分组的自动化分期模块

Result: 在外部验证集上，区域受累检测准确率88.31%，敏感性74.47%，特异性94.21%，F1分数80.80%；治疗分层（局限期vs.进展期）准确率85.07%，特异性90.48%，敏感性82.61%

Conclusion: AutoLugano是首个全自动端到端系统，能将单次FDG-PET/CT扫描转换为完整的Lugano分期，在初始分期、治疗分层和临床决策支持方面具有强大潜力

Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.

</details>


### [118] [ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery](https://arxiv.org/abs/2512.07229)
*Fang Zhou,Zhiqiang Chen,Martin Pavlovski,Yizhong Zhang*

Main category: cs.CV

TL;DR: ReLKD是一个端到端框架，通过利用隐式的类间关系来增强广义类别发现中新颖类别的分类性能，包含三个关键模块：目标粒度模块、粗粒度模块和蒸馏模块。


<details>
  <summary>Details</summary>
Motivation: 广义类别发现(GCD)面临在只有已知类别标签的情况下对包含已知和未知类别的未标记数据进行分类的挑战。先前的研究通常独立处理每个类别，忽略了固有的类间关系。在现实场景中直接获取这些类间关系具有显著挑战性。

Method: ReLKD包含三个关键模块：1)目标粒度模块用于学习判别性表示；2)粗粒度模块用于捕获层次化的类间关系；3)蒸馏模块用于将知识从粗粒度模块转移到目标粒度模块，以优化表示学习。

Result: 在四个数据集上的大量实验证明了ReLKD的有效性，特别是在标记数据有限的场景下表现优异。

Conclusion: ReLKD通过有效利用隐式类间关系并将其知识转移到目标粒度表示学习中，显著提升了广义类别发现中新颖类别的分类性能。

Abstract: Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.

</details>


### [119] [STRinGS: Selective Text Refinement in Gaussian Splatting](https://arxiv.org/abs/2512.07230)
*Abhinav Raundhal,Gaurav Behera,P J Narayanan,Ravi Kiran Sarvadevabhatla,Makarand Tapaswi*

Main category: cs.CV

TL;DR: STRinGS是一个针对3D高斯泼溅(3DGS)的文本感知选择性优化框架，通过分别处理文本和非文本区域，显著提升文本可读性，并引入OCR字符错误率作为评估指标。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的文本作为标志、标签或指令承载重要上下文信息，但现有3D表示方法如3DGS难以保留细粒度文本细节，文本重建中的微小错误会导致显著的语义损失。

Method: 提出STRinGS框架，将文本和非文本区域分开处理：先优化文本区域，然后与非文本区域合并进行全场景优化，确保文本区域保持清晰可读。

Result: STRinGS在仅7K次迭代下，相比3DGS在文本可读性上获得63.6%的相对改进，能够处理具有挑战性的文本配置，生成清晰可读的文本。

Conclusion: STRinGS方法和配套数据集STRinGS-360共同推动了文本丰富环境中3D场景理解的边界，为更鲁棒的文本感知重建方法铺平了道路。

Abstract: Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.

</details>


### [120] [Unified Camera Positional Encoding for Controlled Video Generation](https://arxiv.org/abs/2512.07237)
*Cheng Zhang,Boying Li,Meng Wei,Yan-Pei Cao,Camilo Cruz Gambardella,Dinh Phung,Jianfei Cai*

Main category: cs.CV

TL;DR: 本文提出了UCPE（统一相机位置编码），一种几何一致的相机表示方法，通过相对光线编码和绝对方向编码，在视频生成中实现了卓越的相机控制能力，仅需增加不到1%的可训练参数。


<details>
  <summary>Details</summary>
Motivation: 现有相机编码方法通常基于简化的针孔模型假设，限制了在真实世界多样化相机内参和镜头畸变情况下的泛化能力。需要一种统一的相机表示方法来更好地支持3D感知、视频生成和自动驾驶等应用中的相机几何理解。

Method: 提出了相对光线编码（Relative Ray Encoding）来统一表示完整的相机信息（6自由度位姿、内参和镜头畸变），并识别出俯仰角和滚转角作为绝对方向编码的有效组件。这些设计组合成UCPE，通过轻量级空间注意力适配器集成到预训练的视频扩散Transformer中。

Result: UCPE在相机可控的视频生成任务中实现了最先进的相机控制能力和视觉保真度。构建了覆盖广泛相机运动和镜头类型的大型视频数据集进行系统训练和评估。实验验证了UCPE的有效性。

Conclusion: UCPE作为一种通用的相机表示方法，在相机可控视频生成中表现出色，并具有在多视角、视频和3D任务中作为Transformer通用相机表示的潜力。

Abstract: Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.

</details>


### [121] [Zero-Shot Textual Explanations via Translating Decision-Critical Features](https://arxiv.org/abs/2512.07245)
*Toshinori Yamauchi,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.CV

TL;DR: TEXTER是一种新的零样本图像分类器解释方法，通过隔离决策关键特征并映射到CLIP特征空间来生成更忠实、可解释的文本解释。


<details>
  <summary>Details</summary>
Motivation: 现有零样本解释方法仅对齐全局图像特征与语言，描述可见内容而非驱动预测的关键因素。大型视觉语言模型虽能生成描述，但非针对分类器特定推理设计。

Method: TEXTER首先识别对预测有贡献的神经元并强调这些神经元编码的决策关键特征，然后将这些强调的特征映射到CLIP特征空间以检索反映模型推理的文本解释。稀疏自编码器进一步提升Transformer架构的可解释性。

Result: 大量实验表明，TEXTER比现有方法生成更忠实和可解释的解释。

Conclusion: 通过隔离决策关键特征再进行对齐，TEXTER克服了现有方法的局限性，能生成更准确反映分类器推理过程的文本解释。

Abstract: Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.

</details>


### [122] [AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing](https://arxiv.org/abs/2512.07247)
*Ziming Hong,Tianyu Huang,Runnan Chen,Shanshan Ye,Mingming Gong,Bo Han,Tongliang Liu*

Main category: cs.CV

TL;DR: AdLift是首个针对3D高斯泼溅（3DGS）的编辑保护方法，通过将严格有界的2D对抗扰动提升到3D高斯表示中，防止任意视角和维度的指令驱动编辑。


<details>
  <summary>Details</summary>
Motivation: 扩散模型驱动的3DGS编辑技术虽然推动了3D内容创作，但也使3D资产面临未经授权编辑和恶意篡改的风险。现有针对2D图像的对抗扰动保护方法难以直接应用于3DGS，主要面临视角泛化保护以及保护能力与不可见性平衡两大挑战。

Method: 提出AdLift方法：1）将严格有界的2D对抗扰动提升到3D高斯表示的保护机制中；2）使用定制的Lifted PGD进行渐进优化，在训练视角上进行梯度截断和图像到高斯拟合的交替优化；3）通过梯度截断限制编辑模型的梯度传播，应用投影梯度严格约束图像级扰动；4）将扰动通过图像到高斯拟合操作反向传播到保护高斯参数。

Result: 实验结果表明，AdLift能有效保护3DGS资产免受最先进的指令驱动2D图像和3DGS编辑攻击，在不同视角下提供一致的对抗性保护性能，并能泛化到新视角。

Conclusion: AdLift是首个针对3DGS的编辑保护框架，成功解决了视角泛化保护和保护能力-不可见性平衡的挑战，为3DGS资产提供了有效的安全保障。

Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.

</details>


### [123] [A graph generation pipeline for critical infrastructures based on heuristics, images and depth data](https://arxiv.org/abs/2512.07269)
*Mike Diessner,Yannick Tarant*

Main category: cs.CV

TL;DR: 提出基于摄影测量的图生成管道，使用立体相机获取RGB图像和深度数据，通过深度学习进行目标检测和实例分割，结合启发式规则推断关系，为关键基础设施创建虚拟表示


<details>
  <summary>Details</summary>
Motivation: 传统基于激光扫描的3D点云方法成本高且需要专业知识，需要更经济高效的方法为关键基础设施创建虚拟表示以进行仿真和数字孪生

Method: 基于摄影测量的图生成管道：使用立体相机获取RGB图像和深度数据，通过深度学习进行目标检测和实例分割，结合用户定义的启发式规则推断对象间关系

Result: 在两个液压系统上的实验结果表明，该方法生成的图接近真实情况，具有灵活性可针对特定应用定制，透明度高适合关键基础设施的高风险决策

Conclusion: 提出的基于摄影测量的图生成管道是一种经济高效的替代方案，能够为关键基础设施创建准确的虚拟表示，适用于数字孪生和仿真应用

Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.

</details>


### [124] [RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2512.07273)
*Zhi Rao,Yucheng Zhou,Benjia Zhou,Yiqing Huang,Sergio Escalera,Jun Wan*

Main category: cs.CV

TL;DR: 提出RVLF三阶段框架，通过融合骨架运动与视觉特征解决手语表征不足问题，并引入GRPO优化策略改善句子级语义对齐，在无注释手语翻译任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前无注释手语翻译面临两个关键挑战：1）手语表征不足，难以捕捉细微视觉线索；2）基于LLM的方法存在句子级语义错位问题，限制了翻译质量。

Method: 提出三阶段强化视觉-语言框架RVLF：1）构建专门的手语大视觉语言模型，融合骨架运动线索和DINOv2提取的视觉特征；2）通过指令调优获得SLT-SFT基线模型；3）引入GRPO优化策略，结合BLEU和ROUGE奖励函数微调模型，得到SLT-GRPO模型。

Result: 在CSL-Daily、PHOENIX-2014T、How2Sign和OpenASL数据集上，BLEU-4分数分别提升+5.1、+1.11、+1.4和+1.61，无需外部大规模手语数据集预训练，显著改善翻译质量和语义一致性。

Conclusion: RVLF框架有效解决了手语表征不足和句子级语义错位问题，首次将GRPO引入手语翻译，通过强化学习优化显著提升无注释手语翻译性能，为手语翻译研究提供了新思路。

Abstract: Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.

</details>


### [125] [Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery](https://arxiv.org/abs/2512.07276)
*Mai Tsujimoto,Junjue Wang,Weihao Xuan,Naoto Yokoya*

Main category: cs.CV

TL;DR: Geo3DVQA是一个用于评估视觉语言模型在3D地理空间推理能力的基准测试，使用RGB遥感图像，包含11万个问题-答案对，涵盖16个任务类别和3个复杂度级别。现有VLMs在此任务上表现不佳，领域特定微调能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D地理空间分析方法依赖昂贵的专业传感器（如LiDAR和多光谱），限制了全球可访问性。现有方法难以整合多个3D线索、处理多样化查询并提供可解释的推理。需要开发基于RGB图像的3D地理空间推理方法。

Method: 提出Geo3DVQA基准测试，使用RGB遥感图像评估视觉语言模型的高度感知3D地理空间推理能力。基准包含11万个精心策划的问题-答案对，涵盖16个任务类别，分为三个复杂度级别：单特征推理、多特征推理和应用级空间分析。

Result: 评估了10个最先进的视觉语言模型，结果显示RGB到3D推理具有挑战性：GPT-4o准确率28.6%，Gemini-2.5-Flash准确率33.0%。领域特定微调的Qwen2.5-VL-7B模型达到49.6%准确率，提升了24.8个百分点。

Conclusion: Geo3DVQA揭示了当前VLMs在3D地理空间推理方面的局限性，同时证明了领域适应的有效性。该基准为可扩展、可访问和全面的3D地理空间分析设立了新的挑战前沿。

Abstract: Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.

</details>


### [126] [Generalized Referring Expression Segmentation on Aerial Photos](https://arxiv.org/abs/2512.07338)
*Luís Marnoto,Alexandre Bernardino,Bruno Martins*

Main category: cs.CV

TL;DR: Aerial-D是一个新的大规模航空图像指代表达分割数据集，包含37,288张图像和1,522,523个指代表达，涵盖259,709个标注目标，覆盖21个类别。该数据集通过全自动流水线构建，结合了基于规则的表达生成和LLM增强，并模拟了历史成像条件。使用RSRefSeg架构训练模型，在当代基准测试中表现优异，同时在历史航空照片的退化条件下保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 航空图像（如无人机拍摄的现代航空照片、历史航空档案、高分辨率卫星图像等）在指代表达分割任务中面临独特挑战：空间分辨率差异大、色彩使用不一致、目标可能只有几个像素、场景中物体密度高且存在部分遮挡。现有数据集无法充分应对这些挑战，因此需要专门针对航空图像的大规模指代表达分割数据集。

Method: 1. 构建Aerial-D数据集：包含37,288张图像、1,522,523个指代表达、259,709个标注目标，覆盖21个类别（从车辆、基础设施到土地覆盖类型）。2. 采用全自动流水线：结合系统性基于规则的表达生成和大型语言模型（LLM）增强程序，丰富语言多样性和视觉细节关注。3. 使用过滤器模拟历史成像条件。4. 采用RSRefSeg架构，在Aerial-D和先前航空数据集上联合训练模型，实现文本驱动的统一实例和语义分割。

Result: 联合训练在当代基准测试中取得了有竞争力的性能，同时在单色、棕褐色和颗粒状退化条件下（这些是档案航空摄影中常见的退化）保持了强大的准确性。数据集、训练模型和完整软件流水线已公开提供。

Conclusion: Aerial-D是一个大规模、多样化的航空图像指代表达分割数据集，通过全自动流水线构建并包含LLM增强。基于该数据集训练的模型不仅在当代航空图像上表现良好，还能有效处理历史航空照片的各种退化条件，为航空图像理解提供了重要资源。

Abstract: Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .

</details>


### [127] [Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting](https://arxiv.org/abs/2512.07345)
*Shilong Jin,Haoran Duan,Litao Hua,Wentao Huang,Yuan Zhou*

Main category: cs.CV

TL;DR: TD-Attn是一个解决T2I扩散模型中先验视角偏置问题的框架，通过3D感知注意力引导和分层注意力调制来提升多视角一致性，可作为通用插件增强3D任务效果。


<details>
  <summary>Details</summary>
Motivation: 现有从文本到图像扩散模型蒸馏的3D任务存在先验视角偏置问题，导致不同视角间外观冲突，这是因为主题词在交叉注意力计算中优先激活先验视角特征，而忽略目标视角条件。

Method: 提出TD-Attn框架：1) 3D-AAG模块构建视角一致的3D注意力高斯分布，强制空间一致性；2) HAM模块使用语义引导树和语义响应分析器定位和调制对视角条件敏感的CA层，支持构建更一致的3D注意力高斯。

Result: 实验表明TD-Attn可作为通用插件，显著提升3D任务中的多视角一致性，并支持可控精确的3D编辑。

Conclusion: TD-Attn通过数学分析揭示了T2I模型先验视角偏置的根源，并提出有效解决方案，为不依赖大量3D训练数据的3D任务提供了重要技术突破。

Abstract: Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.

</details>


### [128] [MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition](https://arxiv.org/abs/2512.07348)
*Xinyu Wei,Kangrui Cen,Hongyang Wei,Zhen Guo,Bairui Li,Zeqing Wang,Jinrui Zhang,Lei Zhang*

Main category: cs.CV

TL;DR: 该论文针对多图像组合(MICo)任务，构建了MICo-150K数据集、MICo-Bench基准测试和Qwen-MICo基线模型，解决了多参考图像生成中缺乏高质量训练数据的问题。


<details>
  <summary>Details</summary>
Motivation: 多图像组合任务在可控图像生成中面临挑战，主要障碍是缺乏高质量的训练数据。现有方法在处理多个参考输入时难以保持图像的一致性和连贯性。

Method: 1) 系统研究MICo任务，分为7类代表性任务；2) 收集高质量源图像并构建多样化MICo提示；3) 利用专有模型合成大量平衡的复合图像；4) 人工筛选和精炼，构建MICo-150K数据集；5) 创建分解与重组子集；6) 构建MICo-Bench基准测试；7) 提出Weighted-Ref-VIEScore评估指标；8) 在MICo-150K上微调多个模型。

Result: MICo-150K数据集有效提升了模型的多图像组合能力，基线模型Qwen-MICo在3图像组合任务中达到Qwen-Image-2509水平，同时支持任意多图像输入。数据集、基准测试和基线模型为MICo研究提供了宝贵资源。

Conclusion: 该研究通过构建大规模高质量数据集、评估基准和基线模型，为多图像组合任务提供了系统解决方案，显著提升了模型处理多参考输入的能力，为后续研究奠定了基础。

Abstract: In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.

</details>


### [129] [Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects](https://arxiv.org/abs/2512.07381)
*Shuohan Tao,Boyao Zhou,Hanzhang Tu,Yuwang Wang,Yebin Liu*

Main category: cs.CV

TL;DR: Tessellation GS：一种基于网格面的结构化2D高斯泼溅方法，用于从单相机重建动态场景，通过约束2D高斯到局部区域并使用层次神经特征推断属性，显著提升了稀疏视角和动态场景的重建性能。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯泼溅（GS）方法在视角外推方面表现不佳，特别是在稀疏视角和动态场景重建中容易过拟合，难以从单静态相机重建一般动态物体。

Method: 提出Tessellation GS方法：1）将2D高斯约束在网格面的局部区域；2）通过网格面上的层次神经特征推断高斯属性；3）使用细节感知损失函数驱动的自适应面细分策略指导高斯细分；4）利用重建基础模型的先验初始化高斯变形。

Result: 在表观和网格重建任务上，相比之前SOTA方法，LPIPS降低29.1%，Chamfer距离减少49.2%，显著提升了重建质量。

Conclusion: Tessellation GS通过结构化约束和自适应细分策略，有效解决了传统GS在动态场景重建中的过拟合问题，实现了从单相机对一般动态物体的鲁棒重建。

Abstract: 3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.

</details>


### [130] [LogicCBMs: Logic-Enhanced Concept-Based Learning](https://arxiv.org/abs/2512.07383)
*Deepika SN Vemuri,Gautham Bellamkonda,Aditya Pola,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 论文提出LogicCBM，通过可微逻辑运算增强概念瓶颈模型，超越简单的线性概念组合，提高模型表达能力和准确性


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型（CBMs）虽然提供语义抽象和可解释性，但仅通过线性组合概念来生成预测，这种线性组合存在固有局限性，限制了模型的表达能力和对概念间复杂关系的捕捉

Method: 提出LogicCBM，引入精心设计的逻辑模块，通过可微逻辑运算连接从CBMs学习到的概念，使模型能够超越简单的加权概念组合，利用各种逻辑运算生成最终预测，同时保持端到端可学习性

Result: 在知名基准测试和合成数据集上的实证研究表明，LogicCBM模型具有更好的准确性，能够执行有效的干预，并且保持高度可解释性

Conclusion: 通过命题逻辑增强概念学习模型是有效的，LogicCBM不仅能够捕捉概念间关系，还能提高模型在逻辑运算方面的表达能力，为概念瓶颈模型提供了更强大的扩展

Abstract: Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.

</details>


### [131] [How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline](https://arxiv.org/abs/2512.07385)
*Chunhui Zhang,Li Liu,Zhipeng Zhang,Yong Wang,Hao Wen,Xi Zhou,Shiming Ge,Yanfeng Wang*

Main category: cs.CV

TL;DR: 提出新的无人机反无人机跟踪任务UAV-Anti-UAV，构建百万级数据集，并提出基于Mamba的MambaSTS方法进行空间-时间-语义集成学习


<details>
  <summary>Details</summary>
Motivation: 当前反无人机研究主要关注固定地面摄像头采集的RGB、红外或RGB-IR视频，缺乏从移动无人机平台跟踪目标无人机的研究。无人机反无人机跟踪面临双重动态干扰的挑战，需要新的解决方案。

Method: 提出MambaSTS方法：使用Mamba模型学习全局语义特征，Transformer学习空间特征，利用状态空间模型在长序列建模的优势，通过时间令牌传播机制建立视频级长期上下文。

Result: 构建了包含1,810个视频的百万级数据集，每个视频都有人工标注的边界框、语言提示和15个跟踪属性。在50种现代深度跟踪算法上的实验表明，UAV-Anti-UAV领域仍有很大改进空间。

Conclusion: 提出了新的无人机反无人机跟踪任务和数据集，展示了该任务的挑战性，提出的MambaSTS方法为这一领域提供了有效的基线解决方案，为未来研究奠定了基础。

Abstract: Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.

</details>


### [132] [Reconstructing Objects along Hand Interaction Timelines in Egocentric Video](https://arxiv.org/abs/2512.07394)
*Zhifan Zhu,Siddhant Bansal,Shashank Tripathi,Dima Damen*

Main category: cs.CV

TL;DR: 论文提出ROHIT任务，通过手交互时间线(HIT)重建物体姿态，利用约束优化传播(COP)框架在稳定抓握场景中提升重建效果


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在动态手物交互场景中准确重建物体姿态，特别是在手稳定抓握物体时，物体相对于手的姿态保持恒定，这为无3D真值标注提供了机会

Method: 定义手交互时间线(HIT)描述物体从静止到被抓握、使用、释放的过程；提出约束优化传播(COP)框架，利用稳定抓握期间的姿态约束在时间线上传播物体姿态

Result: 在HOT3D和EPIC-Kitchens数据集上评估，COP框架将稳定抓握重建提升6.2-11.3%，HIT重建提升达24.5%，仅使用2D投影误差评估而无3D真值

Conclusion: ROHIT任务和COP框架有效解决了手物交互场景中的物体重建问题，利用稳定抓握的约束实现了无3D真值的姿态传播和优化

Abstract: We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.

</details>


### [133] [InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs](https://arxiv.org/abs/2512.07410)
*Bin Li,Ruichi Zhang,Han Liang,Jingyan Zhang,Juze Zhang,Xin Chen,Lan Xu,Jingyi Yu,Jingya Wang*

Main category: cs.CV

TL;DR: InterAgent是首个端到端的文本驱动物理多智能体人形控制框架，通过自回归扩散变换器和多流块设计，实现物理合理、语义准确的多智能体交互行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要局限于单智能体场景，忽视了多智能体交互中必需的物理合理互动，需要开发能够模拟人类复杂社交协调行为的框架。

Method: 提出自回归扩散变换器，采用多流块设计分离本体感知、外部感知和动作以避免跨模态干扰；引入交互图外部感知表示捕捉细粒度关节间空间依赖；设计稀疏边基注意力机制动态剪枝冗余连接并强调关键智能体间空间关系。

Result: InterAgent在多个基准测试中一致优于现有方法，达到最先进性能，能够仅从文本提示生成连贯、物理合理且语义准确的多智能体行为。

Conclusion: InterAgent成功填补了多智能体人形控制领域的空白，为文本驱动的物理合理多智能体交互提供了首个端到端解决方案，推动了该领域的研究发展。

Abstract: Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.

</details>


### [134] [Unified Video Editing with Temporal Reasoner](https://arxiv.org/abs/2512.07469)
*Xiangpeng Yang,Ji Xie,Yiyuan Yang,Yan Huang,Min Xu,Qiang Wu*

Main category: cs.CV

TL;DR: VideoCoF提出了一种基于帧链推理的视频编辑方法，通过预测编辑区域潜在表示作为显式推理步骤，实现无需掩码的精确指令到区域对齐和细粒度编辑。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法面临关键权衡：专家模型依赖任务特定先验（如掩码）难以统一；统一的时间上下文学习模型无需掩码但缺乏显式空间线索，导致指令到区域映射不精确和定位能力弱。

Method: 提出VideoCoF框架，采用"观察-推理-编辑"流程，强制视频扩散模型先预测推理标记（编辑区域潜在表示），再生成目标视频标记。引入RoPE对齐策略利用推理标记确保运动对齐并支持超出训练时长的长度外推。

Result: 仅使用5万个视频对的最小数据成本，在VideoCoF-Bench上达到最先进性能，验证了方法的效率和有效性。

Conclusion: VideoCoF通过显式推理步骤解决了视频编辑中精确性与统一性的冲突，无需用户提供掩码即可实现精确的指令到区域对齐和细粒度编辑，同时支持运动对齐和长度外推。

Abstract: Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.

</details>


### [135] [Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance](https://arxiv.org/abs/2512.07480)
*Naifu Xue,Zhaoyang Jia,Jiahao Li,Bin Li,Zihan Zheng,Yuan Zhang,Yan Lu*

Main category: cs.CV

TL;DR: S2VC是一种基于单步扩散的视频编解码器，通过条件编码框架和高效单步扩散生成器，在低码率下实现高质量重建，同时降低采样复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统和神经视频编解码器在低码率下提升感知质量仍然具有挑战性。现有方法要么受限于生成能力导致伪影，要么依赖预训练扩散模型但采样复杂度高。需要一种既能提高感知质量又能降低计算成本的方法。

Method: 提出S2VC单步扩散视频编解码器：1) 结合条件编码框架与高效单步扩散生成器；2) 引入上下文语义指导，从缓冲特征中提取帧自适应语义，替代文本描述；3) 在扩散U-Net中加入时间一致性指导，确保帧间时序连贯性。

Result: 实验表明S2VC在感知质量上达到最先进水平，相比之前的感知方法平均节省52.73%的码率，证明了单步扩散在高效高质量视频压缩中的潜力。

Conclusion: S2VC通过单步扩散方法成功解决了低码率视频压缩中感知质量与计算效率的平衡问题，为高效高质量视频压缩提供了有前景的解决方案。

Abstract: While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.

</details>


### [136] [MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer](https://arxiv.org/abs/2512.07500)
*Penghui Liu,Jiangshan Wang,Yutong Shen,Shanhui Mo,Chenyang Qi,Yue Ma*

Main category: cs.CV

TL;DR: MultiMotion是一个用于多对象视频运动转移的统一框架，通过Mask-aware Attention Motion Flow和RectPC求解器解决DiT架构中的运动纠缠问题，并在首个DiT多对象运动转移基准数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器（DiT）架构在多对象视频运动转移中面临运动纠缠和缺乏对象级控制的挑战，需要一种能够明确解耦和控制多个对象运动特征的方法。

Method: 提出了MultiMotion框架，核心创新包括：1) Mask-aware Attention Motion Flow (AMF)，利用SAM2掩码在DiT流程中显式解耦和控制多个对象的运动特征；2) RectPC，一种高阶预测器-校正器求解器，用于高效准确采样，特别适用于多实体生成。

Result: 构建了首个专门用于DiT多对象运动转移的基准数据集，MultiMotion在该数据集上实现了精确、语义对齐且时间一致的多对象运动转移，保持了DiT的高质量和可扩展性。

Conclusion: MultiMotion通过AMF和RectPC成功解决了DiT在多对象视频运动转移中的核心挑战，为多对象运动控制提供了有效的统一框架，代码已在补充材料中提供。

Abstract: Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.

</details>


### [137] [SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation](https://arxiv.org/abs/2512.07503)
*Yao Teng,Zhihuan Jiang,Han Shi,Xian Liu,Xuefei Ning,Guohao Dai,Yu Wang,Zhenguo Li,Xihui Liu*

Main category: cs.CV

TL;DR: SJD++是一种无需训练的概率并行解码算法，通过多令牌预测和草稿重用机制，将自回归文本到图像生成的推理延迟降低2-3倍，步骤压缩2-7倍，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 大型自回归模型虽然能生成高质量、高分辨率图像，但推理速度慢，因为需要数百到数千次顺序前向传递进行下一个令牌预测。需要加速自回归文本到图像生成过程。

Method: 提出Speculative Jacobi Decoding++ (SJD++)算法，结合Jacobi解码的迭代多令牌预测机制和推测采样的概率草稿验证机制，并重用高置信度草稿令牌而非全部重新采样。

Result: 在多个代表性自回归文本到图像生成模型上实验，SJD++实现了2-3倍的推理延迟降低和2-7倍的步骤压缩，同时保持视觉质量无可见退化。

Conclusion: SJD++是一种有效的训练免费加速方法，显著提升自回归图像生成效率，为实际应用提供了可行的解决方案。

Abstract: Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\times$ to $3\times$ inference latency reduction and $2\times$ to $7\times$ step compression, while preserving visual quality with no observable degradation.

</details>


### [138] [ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points](https://arxiv.org/abs/2512.07504)
*Ryota Okumura,Kaede Shiohara,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: ControlVP：通过用户引导修正文本生成图像中消失点不一致问题的框架，提升几何一致性


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型（如Stable Diffusion）虽然视觉质量出色，但经常存在几何不一致问题，特别是消失点不一致，导致平行线投影在2D空间中无法正确收敛，破坏了场景的结构真实感，尤其在建筑场景中更为明显

Method: 提出ControlVP框架，扩展预训练扩散模型，通过建筑轮廓提供结构引导，并引入几何约束来明确促进图像边缘与透视线索的对齐

Result: 该方法增强了全局几何一致性，同时保持了与基线相当的视觉保真度，特别适用于需要准确空间结构的应用，如图像到3D重建

Conclusion: ControlVP有效解决了文本生成图像中的消失点不一致问题，提升了结构真实感，为需要精确几何结构的应用提供了有价值的工具

Abstract: Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .

</details>


### [139] [MeshRipple: Structured Autoregressive Generation of Artist-Meshes](https://arxiv.org/abs/2512.07514)
*Junkai Lin,Hang Long,Huipeng Guo,Jielei Zhang,JiaYi Yang,Tianle Guo,Yang Yang,Jianwen Li,Wenxiao Zhang,Matthias Nießner,Wei Yang*

Main category: cs.CV

TL;DR: MeshRipple通过前沿感知的BFS标记化、扩展预测策略和稀疏注意力全局内存，解决了自回归网格生成中长距离几何依赖断裂的问题，生成具有高表面保真度和拓扑完整性的网格。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归网格生成器由于内存限制，将面序列化为片段进行训练，并使用滑动窗口推理，但这种不匹配破坏了长距离几何依赖关系，导致生成网格中出现孔洞和碎片化组件。

Method: MeshRipple包含三个关键创新：1）前沿感知的BFS标记化，使生成顺序与表面拓扑对齐；2）扩展预测策略，保持连贯、连接的表面增长；3）稀疏注意力全局内存，提供有效无界的感受野来解决长距离拓扑依赖。

Result: MeshRipple能够生成具有高表面保真度和拓扑完整性的网格，在性能上超越了近期强大的基线方法。

Conclusion: MeshRipple通过其集成设计解决了自回归网格生成中的关键限制，实现了连贯的网格生成，保持了长距离几何依赖关系，显著提升了生成质量。

Abstract: Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.

</details>


### [140] [All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs](https://arxiv.org/abs/2512.07580)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Longzhen Yang,Yihang Liu,Chengmei Yang,Ying Wen,Xianfeng Tang,Hui Liu,Yuyin Zhou,Lianghua He*

Main category: cs.CV

TL;DR: 本文发现视觉大语言模型深层视觉令牌信息消失现象，提出"信息地平线"概念，证明深层随机剪枝优于现有方法，结合随机剪枝的DivPrune在Qwen-2.5-VL-7B上剪枝50%令牌仍保持96.9%性能。


<details>
  <summary>Details</summary>
Motivation: 视觉大语言模型依赖数百个视觉令牌表示图像，计算成本高。现有训练无关剪枝方法在深层（如20层后）表现不佳，作者假设这是由于"令牌信息消失"现象导致。

Method: 提出量化令牌信息含量的方法：通过移除令牌后模型输出概率的变化来衡量。分析发现"信息地平线"现象，基于此提出在深层使用简单随机剪枝，并将随机剪枝与现有方法结合。

Result: 发现三个关键现象：1）视觉令牌信息随层深逐渐均匀化并在中间层消失；2）信息地平线位置因任务而异（OCR比VQA更深）；3）地平线与模型能力相关。结合随机剪枝的DivPrune在Qwen-2.5-VL-7B上剪枝50%令牌仍保持96.9%性能。

Conclusion: 深层视觉令牌信息消失导致现有剪枝方法失效，简单随机剪枝在深层更有效。结合随机剪枝能提升现有方法性能，为视觉大语言模型的高效推理提供了新思路。

Abstract: Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by "vanishing token information", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as "information horizon", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.

</details>


### [141] [LongCat-Image Technical Report](https://arxiv.org/abs/2512.07584)
*Meituan LongCat Team,Hanghang Ma,Haoxian Tan,Jiale Huang,Junqiang Wu,Jun-Yan He,Lishuai Gao,Songlin Xiao,Xiaoming Wei,Xiaoqi Ma,Xunliang Cai,Yayong Guan,Jie Hu*

Main category: cs.CV

TL;DR: LongCat-Image是一个开创性的开源双语图像生成基础模型，在文本渲染、真实感、部署效率和开发者可访问性方面解决当前主流模型的挑战，通过精心设计的数据策略和紧凑架构实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前主流图像生成模型在多语言文本渲染、真实感、部署效率和开发者可访问性方面的核心挑战，特别是中文文本渲染的行业标准问题。

Method: 1) 在预训练、中期训练和SFT阶段采用严格的数据筛选策略；2) 在强化学习阶段协调使用精心设计的奖励模型；3) 采用紧凑的6B参数扩散模型架构，远小于常见的20B+ MoE架构；4) 建立完整的开源生态系统。

Result: 1) 在文本渲染能力和真实感方面达到新的SOTA水平；2) 在中文字符渲染方面建立新的行业标准，支持复杂罕见字符，在覆盖率和准确性上超越开源和商业方案；3) 实现高效部署，VRAM使用最小化，推理速度快；4) 在图像编辑任务上也达到SOTA结果。

Conclusion: LongCat-Image通过创新的数据策略和紧凑架构，在多语言图像生成特别是中文文本渲染方面取得突破性进展，其完整的开源生态系统将为开发者和研究者提供强大支持，推动视觉内容创作的前沿发展。

Abstract: We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.

</details>


### [142] [More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery](https://arxiv.org/abs/2512.07596)
*Wenzhen Dong,Jieming Yu,Yiming Huang,Hongqiu Wang,Lei Zhu,Albert C. S. Chung,Hongliang Ren,Long Bai*

Main category: cs.CV

TL;DR: SAM 3在机器人辅助手术中的实证评估：相比SAM和SAM 2，在零样本分割、3D感知和视频跟踪方面有显著提升，但语言提示在手术领域表现欠佳，复杂动态场景仍有局限。


<details>
  <summary>Details</summary>
Motivation: 评估SAM 3在机器人辅助手术中的性能，特别是其新引入的语言提示分割和增强的3D感知能力，探索其在动态手术场景中的实际应用效果。

Method: 在MICCAI EndoVis 2017和2018基准上进行综合测试，评估SAM 3的零样本分割（点、边界框、语言提示）和视频跟踪能力；同时在SCARED、StereoMIS和EndoNeRF数据集上测试其单目深度估计和3D重建能力。

Result: SAM 3在空间提示下的图像和视频分割明显优于SAM和SAM 2；在单目深度估计和3D器械重建方面表现出色；但语言提示在手术领域表现不理想，复杂动态手术场景仍存在局限性。

Conclusion: SAM 3在机器人辅助手术中展现出显著进步，特别是在零样本分割和3D感知方面，但语言提示需要领域特定训练，复杂动态场景的处理能力仍需改进。

Abstract: The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.

</details>


### [143] [Online Segment Any 3D Thing as Instance Tracking](https://arxiv.org/abs/2512.07599)
*Hanshi Wang,Zijian Cai,Jin Gao,Yiwei Zhang,Weiming Hu,Ke Wang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: AutoSeg3D将在线3D分割重构为实例跟踪问题，通过对象查询实现时空信息传播，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于查询的3D分割方法忽视了时间维度这一关键因素，而感知本质上是动态过程。机器人视角变化常导致物体部分可见，需要超越瞬时不完整视图的全面物体理解。

Method: 1) 将在线3D分割重构为实例跟踪问题；2) 利用对象查询进行时间信息传播：长期实例关联促进特征和物体身份一致性，短期实例更新丰富即时观测；3) 引入空间一致性学习缓解VFMs的碎片化问题。

Result: 在ScanNet200上超越ESAM 2.8 AP，在ScanNet、SceneNN和3RScan数据集上均取得一致性能提升，建立了新的SOTA。

Conclusion: 通过将3D分割重构为实例跟踪问题，利用稀疏对象查询实现时空信息交换和一致性学习，不仅增强了空间理解，还避免了密集时间点云交互的计算负担，显著提升了具身智能体的环境感知能力。

Abstract: Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.

</details>


### [144] [Decomposition Sampling for Efficient Region Annotations in Active Learning](https://arxiv.org/abs/2512.07606)
*Jingna Qiu,Frauke Wilm,Mathias Öttl,Jonas Utz,Maja Schlereth,Moritz Schillinger,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: DECOMP是一种新的主动学习采样策略，专门针对密集预测任务，通过将图像分解为类别特定组件并采样每个类别的区域，提高标注多样性，特别关注困难类别。


<details>
  <summary>Details</summary>
Motivation: 密集预测任务（如医学影像分割）的标注成本高、时间密集。现有方法存在计算内存成本高、区域选择不相关、过度依赖不确定性采样等问题，需要更高效的主动学习策略。

Method: DECOMP使用伪标签将图像分解为类别特定组件，从每个类别中采样区域，并结合类别预测置信度指导采样过程，确保困难类别获得更多标注。

Result: 在ROI分类、2D分割和3D分割任务中，DECOMP始终优于基线方法，能更好地采样少数类别区域并提升这些困难类别的性能。

Conclusion: DECOMP通过分解采样策略有效解决了密集预测主动学习中的关键问题，提高了标注效率，特别是在处理困难类别方面表现优异。

Abstract: Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.

</details>


### [145] [MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation](https://arxiv.org/abs/2512.07628)
*Zhiqi Li,Wenhuan Li,Tengfei Wang,Zhenwei Wang,Junta Wu,Haoyuan Wang,Yunhan Yang,Zehuan Huang,Yang Li,Peidong Liu,Chunchao Guo*

Main category: cs.CV

TL;DR: MoCA提出了一种可扩展的组合式3D生成模型，通过重要性组件路由和未选组件压缩技术，解决了现有方法因全局注意力计算复杂度随组件数量增加而急剧上升的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于部分的3D生成方法在增加组件数量时，由于二次方全局注意力成本导致可扩展性差，需要一种更高效的组合式3D生成方法。

Method: MoCA采用两种关键设计：1) 基于重要性的组件路由，选择top-k相关组件进行稀疏全局注意力计算；2) 不重要组件压缩，在降低全局注意力计算复杂度的同时保留未选组件的上下文先验。

Result: 实验表明MoCA在组合式物体和场景生成任务上均优于基线方法，实现了高效、细粒度的组合式3D资产创建。

Conclusion: MoCA通过创新的注意力机制设计，解决了组合式3D生成中的可扩展性问题，为大规模组件组合的3D生成提供了有效解决方案。

Abstract: Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA

</details>


### [146] [Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method](https://arxiv.org/abs/2512.07651)
*Yuanye Liu,Hanxiao Zhang,Nannan Shi,Yuxin Shi,Arif Mahmood,Murtaza Taj,Xiahai Zhuang*

Main category: cs.CV

TL;DR: LiQA数据集为肝脏纤维化分期提供多中心MRI基准，包含440例患者数据，支持分割和分期任务，挑战赛最佳方法采用半监督学习和多视图共识策略。


<details>
  <summary>Details</summary>
Motivation: 肝脏纤维化是全球重大健康负担，需要准确分期以进行有效临床管理。现有算法在复杂真实世界条件下（如域偏移、模态缺失、空间错位）的鲁棒性不足。

Method: 建立LiQA数据集（440例患者多期相多中心MRI），挑战赛最佳方法采用：1）半监督学习框架结合外部数据进行鲁棒分割；2）多视图共识方法结合CAM正则化进行分期。

Result: 评估表明，利用多源数据和解剖约束能显著增强模型在临床环境中的鲁棒性。该数据集为肝脏分割和纤维化分期算法提供了标准化基准。

Conclusion: LiQA数据集和挑战赛方法为肝脏纤维化分期提供了有效的基准和解决方案，多源数据整合和解剖约束能提升模型在复杂临床场景中的性能。

Abstract: Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.

</details>


### [147] [Optimization-Guided Diffusion for Interactive Scene Generation](https://arxiv.org/abs/2512.07661)
*Shiaho Li,Naisheng Ye,Tianyu Li,Kashyap Chitta,Tuo An,Peng Su,Boyang Wang,Haiou Liu,Chen Lv,Hongyang Li*

Main category: cs.CV

TL;DR: OMEGA是一个优化引导、无需训练的场景生成框架，通过约束优化在扩散采样过程中增强物理合理性和行为一致性，特别适用于生成安全关键的对抗场景。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶评估需要真实多样的多智能体驾驶场景，但现有驾驶数据集中安全关键事件稀缺且代表性不足。现有数据驱动场景生成模型缺乏可控性或产生违反物理/社会约束的样本，限制了其实用性。

Method: 提出OMEGA框架：1）在扩散模型的反向扩散步骤中通过约束优化重新锚定，引导生成物理合理且行为一致的轨迹；2）将自我车辆-攻击者交互建模为分布空间的博弈论优化，近似纳什均衡以生成真实的安全关键对抗场景。

Result: 在nuPlan和Waymo数据集上的实验表明：OMEGA将物理和行为有效场景的比例从32.35%提升到72.27%（自由探索能力），从11%提升到80%（可控性生成）；能生成5倍多的近碰撞帧（碰撞时间小于3秒）同时保持场景真实性。

Conclusion: OMEGA通过优化引导的扩散采样显著提升了场景生成的现实性、一致性和可控性，为自动驾驶系统评估提供了更有效的安全关键场景生成方法。

Abstract: Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.

</details>


### [148] [EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset](https://arxiv.org/abs/2512.07668)
*Ronan John,Aditya Kesari,Vincenzo DiMatteo,Kristin Dana*

Main category: cs.CV

TL;DR: 提出了EgoCampus数据集和EgoCampusNet方法，用于预测户外校园环境中行人导航时的视觉注意力


<details>
  <summary>Details</summary>
Motivation: 现有大多数自我中心数据集专注于室内任务或缺少眼动追踪数据，缺乏对户外导航环境中行人视觉注意力的研究

Method: 使用Meta的Project Aria眼镜收集数据，包含眼动追踪、RGB相机、惯性传感器和GPS；开发了EgoCampusNet方法来预测导航行人的眼动注视点

Result: 创建了EgoCampus数据集，包含25条独特户外路径、6公里距离、80多名不同行人的眼动标注视频；提供了研究真实世界注意力的新资源

Conclusion: 该工作为研究真实世界注意力提供了新数据集和方法，为未来导航眼动预测模型研究奠定了基础资源

Abstract: We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .

</details>


### [149] [sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only](https://arxiv.org/abs/2512.07698)
*Arslan Artykov,Corentin Sautier,Vincent Lepetit*

Main category: cs.CV

TL;DR: 首个从自由移动单目视频中联合预测部件分割和关节参数的数据驱动方法，仅用合成数据训练就能泛化到真实物体


<details>
  <summary>Details</summary>
Motivation: 理解铰接物体对机器人和数字孪生至关重要，但现有方法主要依赖多视角系统、物体扫描或静态相机，缺乏从自由移动单目视频中恢复部件分割和关节参数的解决方案

Method: 提出数据驱动方法，从自由移动单目视频中联合预测部件分割和关节参数，仅使用合成数据进行训练

Result: 方法在仅使用合成数据训练的情况下，展现出对真实世界物体的强大泛化能力，为铰接物体理解提供了可扩展的实用解决方案

Conclusion: 该方法可直接处理随意录制的视频，适合动态环境中的实时应用，为铰接物体理解提供了新的实用途径

Abstract: Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/

</details>


### [150] [UnCageNet: Tracking and Pose Estimation of Caged Animal](https://arxiv.org/abs/2512.07712)
*Sayak Dutta,Harish Katti,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: 提出三阶段预处理流程，通过笼子分割、修复和评估，解决动物追踪和姿态估计在笼子遮挡下的性能下降问题


<details>
  <summary>Details</summary>
Motivation: 现有动物追踪和姿态估计系统（如STEP、ViTPose）在处理带有笼子结构和系统性遮挡的图像视频时性能大幅下降，需要解决笼子遮挡对算法性能的影响

Method: 三阶段预处理流程：1) 使用Gabor增强的ResNet-UNet架构进行笼子分割，配备72个方向可调滤波器；2) 使用CRFill进行内容感知的笼子修复；3) 在修复后的无笼帧上进行姿态估计和追踪评估

Result: 实验验证表明，通过该流程去除笼子遮挡后，姿态估计和追踪性能可达到与无遮挡环境相当的水平，关键点检测精度和轨迹一致性均有显著提升

Conclusion: 提出的三阶段预处理流程能有效解决笼子遮挡对动物追踪和姿态估计系统的影响，使算法在复杂遮挡环境下仍能保持良好性能

Abstract: Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.

</details>


### [151] [ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation](https://arxiv.org/abs/2512.07720)
*Fan Yang,Heyuan Li,Peihao Li,Weihao Yuan,Lingteng Qiu,Chaoyue Song,Cheng Chen,Yisheng He,Shifeng Zhang,Xiaoguang Han,Steven Hoi,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出一种结合3D重建模型和视频扩散模型的方法，从单张输入图像生成高质量的上半身3D虚拟形象，解决现有方法在纹理模糊、运动僵硬和结构不稳定等问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D虚拟形象生成方法存在两方面的局限性：基于大重建模型的方法虽然快速且能产生稳定身体结构，但存在纹理模糊和运动僵硬的问题；而生成式视频模型虽然能合成逼真动态结果，但经常出现身体结构错误和身份漂移等不稳定行为。需要结合两者的优势来解决这些问题。

Method: 提出一个新颖框架，使用3D重建模型提供稳健的结构和外观先验，然后引导一个实时自回归视频扩散模型进行渲染。这种方法结合了几何稳定性和生成能力，能够合成高频、逼真的细节和流畅动态。

Result: 实验表明，该方法显著减少了伪影，在视觉质量上相比领先方法有实质性提升。能够有效减少纹理模糊和运动僵硬，同时防止视频生成方法中常见的结构不一致问题。

Conclusion: 通过将3D重建的几何稳定性与视频模型的生成能力相结合，该方法能够生成具有逼真外观和动态、时间一致运动的高保真数字虚拟形象，为游戏和虚拟现实等实时应用提供了稳健高效的解决方案。

Abstract: Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa

</details>


### [152] [SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery](https://arxiv.org/abs/2512.07733)
*Meng Cao,Xingyu Li,Xue Liu,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: SpatialDreamer是一个强化学习框架，通过主动探索、视觉想象和基于证据的推理来解决MLLMs在复杂空间推理任务中的局限性，使用GeoPO方法解决长序列推理任务的奖励监督问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在需要心理模拟的复杂空间推理任务上表现有限，主要依赖被动观察空间数据，缺乏主动的心理意象过程。

Method: 提出SpatialDreamer强化学习框架，采用主动探索、世界模型的视觉想象和基于证据的推理闭环过程。为解决长序列推理任务的细粒度奖励监督问题，提出几何策略优化（GeoPO），包含树结构采样和具有几何一致性约束的步骤级奖励估计。

Result: 在多个具有挑战性的基准测试中取得了极具竞争力的结果，标志着MLLMs在类人主动空间心理模拟方面的关键进展。

Conclusion: SpatialDreamer通过强化学习和主动心理模拟机制，显著提升了多模态大语言模型在复杂空间推理任务上的性能。

Abstract: Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.

</details>


### [153] [HLTCOE Evaluation Team at TREC 2025: VQA Track](https://arxiv.org/abs/2512.07738)
*Dengjia Zhang,Charles Weng,Katherine Guerrerio,Yi Lu,Kenton Murray,Alexander Martin,Reno Kriz,Benjamin Van Durme*

Main category: cs.CV

TL;DR: HLTCOE团队在TREC VQA的答案生成任务中，提出了一个列表式学习框架，通过重排序候选答案来提高语义精度和排序一致性。


<details>
  <summary>Details</summary>
Motivation: 旨在改进视频问答任务中答案生成的语义精度和排序一致性，特别是在需要时间推理和语义消歧的问题上。

Method: 采用两阶段方法：1) 基础多模态模型生成多个候选答案；2) 使用新颖的Masked Pointer Cross-Entropy Loss with Rank Weights训练的重排序模型对候选答案进行排序。

Result: 实验显示该方法在准确性和排序稳定性方面取得一致提升，特别是在需要时间推理和语义消歧的问题上表现更好。

Conclusion: 通过将生成式建模与判别式排序相结合，该方法能够产生连贯、细粒度的答案列表，实现了稳定且可解释的列表式优化。

Abstract: The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.

</details>


### [154] [Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation](https://arxiv.org/abs/2512.07747)
*Shihao Zhao,Yitong Chen,Zeyinzi Jiang,Bojia Zi,Shaozhe Hao,Yu Liu,Chaojie Mao,Kwan-Yee K. Wong*

Main category: cs.CV

TL;DR: Unison是一个低成本的统一多模态理解与生成模型，采用两阶段方案，仅需50万训练样本和50GPU小时，能自动解析用户意图和任务参数，覆盖多种理解与生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态方法存在两大问题：自回归方法需要大量计算资源，两阶段方法任务覆盖有限且生成质量差。两者都缺乏自动解析输入元信息的能力，需要手动配置参数。

Method: 采用两阶段方案，连接预训练的理解和生成模型进行对齐微调。模型能自动解析用户意图、确定任务类型、提取所需元信息，实现无需人工干预的全自动化多模态任务处理。

Result: 在仅50万训练样本和50GPU小时的极低成本下，模型能准确自动识别任务和提取相关参数，在多种理解和生成任务上取得优异性能。

Conclusion: Unison证明了在低成本设置下实现高质量统一多模态理解与生成的可行性，通过自动任务解析和参数提取实现了全自动化的多模态任务处理。

Abstract: Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.

</details>


### [155] [UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction](https://arxiv.org/abs/2512.07756)
*Mayank Anand,Ujair Alam,Surya Prakash,Priya Shukla,Gora Chand Nandi,Domenec Puig*

Main category: cs.CV

TL;DR: UltrasODM是一个双流框架，通过每帧不确定性校准、显著性诊断和可操作提示来辅助超声医师采集图像，减少重建误差，提高临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床超声采集高度依赖操作者，快速的探头运动和亮度波动常导致重建误差，降低信任度和临床效用。需要一种能辅助超声医师、提高重建可靠性的系统。

Method: 提出UltrasODM双流框架：1)对比排序模块按运动相似性分组帧；2)光流流与Dual-Mamba时序模块融合，用于稳健的6-DoF姿态估计；3)人机交互层结合贝叶斯不确定性、临床校准阈值和显著性图。当不确定性超过阈值时，系统发出非侵入性警报，建议纠正措施。

Result: 在临床自由手超声数据集上评估，相比UltrasOM，UltrasODM将漂移减少15.2%，距离误差减少12.1%，Hausdorff距离减少10.1%，同时产生每帧不确定性和显著性输出。

Conclusion: UltrasODM通过强调透明度和临床医师反馈，提高了重建可靠性，支持更安全、更可信的临床工作流程。代码已公开。

Abstract: Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.

</details>


### [156] [Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2512.07760)
*Menglin Wang,Xiaojin Gong,Jiachen Li,Genlin Ji*

Main category: cs.CV

TL;DR: 本文提出了一种无监督可见光-红外行人重识别方法，通过模态感知Jaccard距离缓解模态差异带来的距离偏差，并结合"分割-对比"策略学习模态不变特征表示，在基准数据集上取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 无监督可见光-红外行人重识别面临的主要挑战是由于可见光和红外模态之间的显著差异，难以估计可靠的跨模态关联。现有方法通常采用最优传输来关联模态内聚类，但容易传播局部聚类错误，且忽略了全局实例级关系。

Method: 1. 提出模态感知Jaccard距离来缓解由模态差异引起的距离偏差，通过全局聚类估计更可靠的跨模态关联；2. 设计"分割-对比"策略获取模态特定的全局原型，在全局关联指导下显式对齐这些原型，实现模态不变且ID可区分的表示学习。

Result: 该方法在基准VI-ReID数据集上取得了最先进的性能，显著优于现有方法，验证了其有效性。

Conclusion: 通过挖掘和关注可见光-红外模态偏差，从偏差缓解的全局关联和模态不变表示学习两个方面解决跨模态学习问题，提出的方法虽然概念简单但效果显著。

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.

</details>


### [157] [Distribution Matching Variational AutoEncoder](https://arxiv.org/abs/2512.07778)
*Sen Ye,Jianning Pei,Mengde Xu,Shuyang Gu,Chunyu Wang,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: DMVAE通过分布匹配约束显式对齐编码器潜在分布与任意参考分布，超越传统VAE的高斯先验，发现SSL衍生分布能平衡重建保真度和建模效率


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型（如VAE和基础模型对齐编码器）隐式约束潜在空间而未显式塑造其分布，不清楚哪种分布最适合建模。需要明确研究哪种潜在分布结构更有利于建模。

Method: 提出Distribution-Matching VAE (DMVAE)，通过分布匹配约束显式对齐编码器的潜在分布与任意参考分布。这超越了传统VAE的高斯先验，可以对齐自监督特征、扩散噪声或其他先验分布。

Result: 发现SSL衍生分布在重建保真度和建模效率之间提供了良好平衡，在ImageNet上仅用64个训练周期就达到gFID=3.2。分布级对齐比固定先验更关键。

Conclusion: 选择合适的潜在分布结构（通过分布级对齐实现），而不是依赖固定先验，是弥合易于建模的潜在空间和高保真图像合成之间差距的关键。

Abstract: Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \textbf{Distribution-Matching VAE} (\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.

</details>


### [158] [OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory](https://arxiv.org/abs/2512.07802)
*Zhaochong An,Menglin Jia,Haonan Qiu,Zijian Zhou,Xiaoke Huang,Zhiheng Liu,Weiming Ren,Kumara Kahatapitiya,Ding Liu,Sen He,Chenyang Zhang,Tao Xiang,Fanny Yang,Serge Belongie,Tian Xie*

Main category: cs.CV

TL;DR: OneStory提出了一种新的多镜头视频生成方法，通过将多镜头视频生成重构为"下一镜头生成"任务，使用全局紧凑的跨镜头上下文建模来生成连贯的长篇叙事视频。


<details>
  <summary>Details</summary>
Motivation: 现有多镜头视频生成方法难以有效建模长距离跨镜头上下文，因为它们依赖有限的时间窗口或单关键帧条件，导致在复杂叙事下性能下降。需要一种能够全局建模跨镜头上下文的方法来生成一致且可扩展的叙事视频。

Method: 1) 将多镜头视频生成重构为下一镜头生成任务，实现自回归镜头合成；2) 引入帧选择模块，基于先前镜头的信息帧构建语义相关的全局记忆；3) 设计自适应条件器，执行重要性引导的补丁化以生成紧凑上下文进行直接条件化；4) 构建高质量多镜头数据集并设计有效的训练策略。

Result: 在自建的60K数据集上微调预训练图像到视频模型后，OneStory在文本和图像条件设置下，在多样复杂场景中实现了最先进的叙事连贯性，能够生成可控且沉浸式的长篇视频叙事。

Conclusion: OneStory通过全局紧凑的跨镜头上下文建模，有效解决了多镜头视频生成中的长距离依赖问题，实现了连贯且可扩展的叙事视频生成，为长篇视频创作提供了新的解决方案。

Abstract: Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.

</details>


### [159] [Multi-view Pyramid Transformer: Look Coarser to See Broader](https://arxiv.org/abs/2512.07806)
*Gyeongjin Kang,Seungkwon Yang,Seungtae Nam,Younggeun Lee,Jungwoo Kim,Eunbyung Park*

Main category: cs.CV

TL;DR: MVP是一种可扩展的多视角Transformer架构，能够从数十到数百张图像中单次前向传播重建大型3D场景，通过局部到全局的层次结构和精细到粗糙的表示聚合实现高效高质量重建。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景重建方法在处理大量输入图像和大型复杂场景时面临计算效率和可扩展性挑战，需要一种能够同时处理数十到数百张图像并高效重建大型3D场景的架构。

Method: 提出Multi-view Pyramid Transformer (MVP)，基于两个核心设计原则：1) 局部到全局的视角间层次结构，从局部视图逐步扩展到组视图再到完整场景；2) 精细到粗糙的视角内层次结构，从详细空间表示逐步聚合成紧凑的信息密集token。结合3D高斯泼溅作为底层3D表示。

Result: 在多样化数据集上验证，当与3D高斯泼溅结合时，实现了最先进的泛化重建质量，同时保持高效率和可扩展性，能够适应广泛的视角配置。

Conclusion: MVP通过双层次结构设计实现了计算效率与表示丰富性的平衡，为大规模3D场景重建提供了一种高效可扩展的解决方案，在保持高质量重建的同时显著提升了处理大量输入图像的能力。

Abstract: We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details," MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.

</details>


### [160] [Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes](https://arxiv.org/abs/2512.07807)
*Shai Krakovsky,Gal Fiebelman,Sagie Benaim,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 提出了一种在3D高斯表示中嵌入语言场的新方法，通过极低维语义瓶颈特征和多分辨率哈希编码器解决现有方法在语义特征对齐和效率方面的问题，在HolyScenes数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将语言场嵌入3D表示可以实现更丰富的空间环境语义理解，支持自然语言查询和编辑场景，但现有特征蒸馏方法在处理大规模互联网数据时面临语义特征错位和效率低下的挑战。

Method: 1) 在3D高斯表示中引入极低维语义瓶颈特征，通过渲染和多分辨率基于特征的哈希编码器处理；2) 提出衰减下采样器模块和多种正则化方法解决2D特征语义错位问题。

Result: 在HolyScenes数据集上的评估显示，该方法在性能和效率方面均超越了现有方法，显著提升了运行时效率和GPU内存使用效率。

Conclusion: 该方法有效解决了大规模场景中语言场嵌入的语义对齐和效率问题，为实现更直观的人机交互和丰富的空间语义理解提供了可行方案。

Abstract: Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.

</details>


### [161] [OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing](https://arxiv.org/abs/2512.07826)
*Haoyang He,Jie Wang,Jiangning Zhang,Zhucun Xue,Xingyuan Bu,Qiangpeng Yang,Shilei Wen,Lei Xie*

Main category: cs.CV

TL;DR: 该论文提出了OpenVE-3M，一个用于指令式视频编辑的大规模高质量开源数据集，包含空间对齐和非空间对齐两类编辑类型，并通过精心设计的数据流水线生成。同时构建了OpenVE-Bench基准测试，并训练了OpenVE-Edit模型，在基准测试上达到了新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前指令式图像编辑数据集的质量和多样性不断提升，但大规模、高质量的指令式视频编辑数据集仍然稀缺。为了填补这一空白，需要构建一个开源、大规模、高质量的指令式视频编辑数据集。

Method: 1. 构建OpenVE-3M数据集：包含空间对齐编辑（全局风格、背景更换、局部更改、局部移除、局部添加、字幕编辑）和非空间对齐编辑（摄像机多镜头编辑和创意编辑）两大类。2. 通过精心设计的数据流水线生成数据并进行严格的质量过滤。3. 构建OpenVE-Bench基准测试：包含431个视频编辑对，涵盖多样化的编辑任务，采用三个与人类判断高度一致的关键指标。4. 训练OpenVE-Edit模型：一个50亿参数的模型，在OpenVE-3M数据集上进行训练。

Result: 1. OpenVE-3M在规模、编辑类型多样性、指令长度和整体质量方面超越了现有的开源数据集。2. OpenVE-Edit模型在OpenVE-Bench基准测试上达到了新的最优性能，超越了包括140亿参数基线模型在内的所有先前开源模型。3. 该模型展示了显著的效率和有效性。

Conclusion: 该研究通过构建大规模高质量的指令式视频编辑数据集OpenVE-3M、统一的基准测试OpenVE-Bench以及高效的OpenVE-Edit模型，为指令式视频编辑领域提供了重要的资源和工具，填补了该领域的数据集空白，并建立了新的性能标准。

Abstract: The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.

</details>


### [162] [UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation](https://arxiv.org/abs/2512.07831)
*Jiehui Huang,Yuechen Zhang,Xu He,Yuan Gao,Zhi Cen,Bin Xia,Yan Zhou,Xin Tao,Pengfei Wan,Jiaya Jia*

Main category: cs.CV

TL;DR: UnityVideo是一个统一的多模态视频生成框架，通过联合学习分割掩码、人体骨架、DensePose、光流和深度图等多种模态，实现世界感知的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型受限于单模态条件约束，缺乏跨模态交互和模态多样性，限制了其对世界的整体理解能力。

Method: 提出两个核心组件：1）动态加噪统一异构训练范式；2）带上下文学习器的模态切换器，通过模块化参数和上下文学习实现统一处理。构建了130万样本的大规模统一数据集。

Result: 通过联合优化，UnityVideo加速了收敛过程，显著提升了零样本泛化能力，在未见数据上表现出色，实现了更优的视频质量、一致性和物理世界约束对齐。

Conclusion: UnityVideo通过统一的多模态学习框架，解决了视频生成中的模态限制问题，为世界感知的视频生成提供了有效解决方案。

Abstract: Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo

</details>


### [163] [Voxify3D: Pixel Art Meets Volumetric Rendering](https://arxiv.org/abs/2512.07834)
*Yi-Chuan Huang,Jiewen Chan,Hao-Jen Chien,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Voxify3D是一个两阶段可微分框架，通过正交像素艺术监督、基于补丁的CLIP对齐和调色板约束的Gumbel-Softmax量化，实现从3D网格到体素艺术的自动化生成，解决了几何抽象、语义保持和离散颜色一致性的冲突要求。


<details>
  <summary>Details</summary>
Motivation: 体素艺术在游戏和数字媒体中广泛应用，但从3D网格自动生成面临挑战：现有方法要么过度简化几何结构，要么无法实现像素级精确、调色板约束的体素艺术美学。需要解决几何抽象、语义保持和离散颜色一致性之间的冲突要求。

Method: 提出Voxify3D两阶段可微分框架：1) 正交像素艺术监督消除透视畸变，实现体素-像素精确对齐；2) 基于补丁的CLIP对齐在不同离散化级别保持语义；3) 调色板约束的Gumbel-Softmax量化，在离散颜色空间进行可微分优化，支持可控调色板策略。

Result: 实验显示优越性能：CLIP-IQA得分37.12，用户偏好率77.90%，在多样化角色上表现良好，支持可控抽象（2-8种颜色，20x-50x分辨率）。

Conclusion: Voxify3D通过协同整合三个关键组件，解决了极端离散化下的语义保持、通过体积渲染实现像素艺术美学以及端到端离散优化等基本挑战，实现了高质量的体素艺术生成。

Abstract: Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [164] [Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals](https://arxiv.org/abs/2512.05998)
*Michael Todasco*

Main category: cs.AI

TL;DR: 本研究测试了将LLM评估任务设计为投注游戏（使用虚构的LLM货币）是否能提高预测准确性并产生校准的信心信号。实验发现投注机制能产生可读的信心信号，大额投注准确率高达99%，小额投注仅74%。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在评估其他模型时，其判断通常缺乏信心表示。本研究旨在探索通过金融框架（投注游戏）是否能改善预测准确性并产生校准的信心信号。

Method: 研究设计了100个数学和逻辑问题，6个基线模型（3个当前代，3个前代）回答问题。3个预测模型在两种条件下预测每个问题-基线对的正确性：控制条件（简单正确/错误预测）和激励条件（预测加上1-100,000 LLMCoin的投注）。共进行了5,400次预测。

Result: 激励条件下的预测准确率略高（81.5% vs. 79.1%），学习速度显著更快（第1轮到第4轮改进12.0% vs. 2.9%）。投注金额与信心相关：40,000+硬币的大额投注准确率约99%，而<1,000硬币的小额投注准确率仅约74%。

Conclusion: 投注机制创造了从二元输出中缺失的可读信心信号，表明简单的金融框架可能帮助LLM成为风险感知的预测者，使其内部信念变得可见和可用。该协议为未来的元评估系统和LLM间预测市场奠定了基础。

Abstract: Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. "Whale" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.

</details>


### [165] [Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach](https://arxiv.org/abs/2512.06161)
*Gondy Leroy,Prakash Bisht,Sai Madhuri Kandula,Nell Maltman,Sydney Rice*

Main category: cs.AI

TL;DR: 本研究提出了一种基于BioBERT的透明可解释机器学习方法，用于分析临床文本并自动化自闭症谱系障碍诊断，通过混合数据集训练策略获得了97%敏感性和98%特异性的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍诊断需求日益增长，但现有机器学习模型多为黑箱且通常基于单一数据集训练，限制了其泛化能力和临床可信度。

Method: 使用BioBERT语言模型分析非结构化临床文本，训练模型标记行为描述并映射到诊断标准，然后分配最终标签（ASD或非ASD）。评估了迁移学习能力，比较了顺序训练和混合训练策略，并与黑箱方法进行对比。

Result: 透明模型表现稳健，混合数据训练策略获得最佳结果（97%敏感性，98%特异性）。顺序训练导致性能略有下降。黑箱模型在顺序或混合训练下表现较差（90%敏感性，96%特异性）。透明方法整体优于黑箱方法。

Conclusion: 透明可解释的机器学习方法在自闭症诊断中优于黑箱方法，混合数据集训练可获得更好性能。这为神经发育诊断中更可信、可泛化和临床可操作的AI工具奠定了基础。

Abstract: Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.

</details>


### [166] [ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment](https://arxiv.org/abs/2512.06196)
*Charlie Masters,Marta Grześkiewicz,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: ARCANE框架通过将AI对齐问题转化为多智能体协作问题，使用自然语言评分标准动态表示利益相关者偏好，实现可解释、无需重新训练即可在交互时调整的对齐方法。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体越来越多地部署到长期任务中，保持其与利益相关者偏好的一致性变得至关重要。需要可解释的奖励模型让利益相关者能够理解和审核模型目标，并且能够在交互时引导智能体，无需重新训练即可纳入偏好变化。

Method: 提出ARCANE框架，将对齐问题构建为多智能体协作问题，动态地将利益相关者偏好表示为自然语言评分标准（可验证的加权标准集合）。受效用理论启发，将评分标准学习构建为重构问题，应用正则化的组序列策略优化(GSPO)程序来平衡可解释性、忠实性和计算效率。

Result: 使用从GDPVal基准派生的219个标记评分标准语料库进行评估，在需要多步推理和工具使用的挑战性任务上测试ARCANE。学习的评分标准产生紧凑、易读的评估，并支持可配置的权衡（如正确性与简洁性）而无需重新训练。

Conclusion: 基于评分标准的奖励模型为复杂、长期AI系统提供了一条有前景的可解释、测试时自适应对齐路径，能够实现无需重新训练的动态偏好调整。

Abstract: As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.

</details>


### [167] [On measuring grounding and generalizing grounding problems](https://arxiv.org/abs/2512.06205)
*Daniel Quigley,Eric Maynard*

Main category: cs.AI

TL;DR: 论文将符号接地问题从二元判断重构为包含真实性、保持性、忠实性、鲁棒性、组合性等需求的审计框架，应用于四种接地模式，通过三个案例研究分析不同系统的接地能力。


<details>
  <summary>Details</summary>
Motivation: 解决符号接地问题——即符号如何与现实世界实体建立联系，而不是仅仅作为形式系统中的形状。将哲学上关于表征的探讨操作化，为哲学家、计算机科学家、语言学家和数学家提供一个共同语言和技术框架来系统研究接地和意义。

Method: 提出一个基于评估元组（上下文、意义类型、威胁模型、参考分布）的审计框架，包含五个核心需求：真实性（机制在智能体内部且通过学习/进化获得）、保持性（原子意义保持完整）、忠实性（相关性和因果性）、鲁棒性（在声明扰动下优雅降级）、组合性（整体由部分系统构建）。将此框架应用于四种接地模式（符号、指称、向量、关系）和三个案例研究。

Result: 模型论语义学实现精确组合但缺乏因果保证；大语言模型在语言任务上显示相关拟合和局部鲁棒性，但在没有接地交互的世界任务上缺乏成功选择；人类语言通过进化和发育获得满足强真实性需求。框架为不同学科提供了系统研究接地问题的共同工具。

Conclusion: 通过将符号接地问题操作化为一个包含多个维度的审计框架，为跨学科研究意义和表征提供了系统方法，揭示了不同系统在接地能力上的优势和局限，促进了哲学、计算机科学、语言学和数学之间的对话。

Abstract: The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.

</details>


### [168] [AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems](https://arxiv.org/abs/2512.06240)
*Chuanhao Nie,Yunbo Liu,Chao Wang*

Main category: cs.AI

TL;DR: 该论文综述了人工智能在反洗钱工作流中的应用，提出了基于图检索增强生成的AI驱动KYC方案，实验证明该架构能提高检测准确性、降低误报率，并增强KYC流程的效率和透明度。


<details>
  <summary>Details</summary>
Motivation: 洗钱和金融欺诈每年造成数万亿美元损失，威胁全球金融稳定，传统监管手段面临挑战。需要现代化技术来提高反洗钱检测准确性、降低误报率，并减轻人工调查负担。

Method: 论文首先综述AI在反洗钱中的应用，然后提出AI驱动的KYC应用方案，该方案将基于图的检索增强生成（RAG Graph）与生成模型结合，用于增强KYC流程的效率、透明度和决策支持。

Result: 实验结果显示，RAG-Graph架构在不同评估场景下表现出高忠实度和强答案相关性，显著提升了KYC客户尽职调查/增强尽职调查工作流的效率和透明度，有助于实现更可持续、资源优化的合规实践。

Conclusion: AI技术能够现代化反洗钱工作流，提高检测准确性和效率。未来研究方向包括：用于隐私保护协作的联邦学习、公平可解释AI、自适应防御的强化学习，以及人机协同可视化系统，确保下一代反洗钱架构的透明性、可问责性和鲁棒性。

Abstract: Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.

</details>


### [169] [How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion](https://arxiv.org/abs/2512.06296)
*Sooho Moon,Yunyong Ko*

Main category: cs.AI

TL;DR: 该论文提出了PROBE评估框架，通过考虑预测锐度和流行度偏差鲁棒性来改进知识图谱补全的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全（KGC）评估指标存在两个关键问题：1）忽略了预测锐度（对单个预测的严格程度评估）；2）缺乏对流行度偏差的鲁棒性（预测低流行度实体的能力）。这导致现有指标可能高估或低估KGC模型的准确性。

Method: 提出了PROBE评估框架，包含两个核心组件：1）秩变换器（RT）- 根据所需的预测锐度水平估计每个预测的得分；2）秩聚合器（RA）- 以流行度感知的方式聚合所有得分。

Result: 在真实世界知识图谱上的实验表明，现有指标倾向于高估或低估KGC模型的准确性，而PROBE能够提供对KGC模型的全面理解和可靠的评估结果。

Conclusion: PROBE框架通过同时考虑预测锐度和流行度偏差鲁棒性，为知识图谱补全提供了更全面和可靠的评估方法，有助于更好地理解和比较不同KGC模型的性能。

Abstract: Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.

</details>


### [170] [DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2512.06337)
*Xuan Xie,Xuan Wang,Wenjie Wang*

Main category: cs.AI

TL;DR: DaGRPO通过引入序列级梯度矫正和离策略数据增强，解决了GRPO在长链推理训练中的不稳定性和样本效率低的问题，在数学推理和OOD泛化基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: GRPO虽然能有效激发大语言模型的长链推理能力，但存在训练不稳定和样本效率低的问题。研究发现根本原因在于策略内样本缺乏区分度：对于常规查询，高度同质的样本导致破坏性梯度冲突；对于困难查询，有效正样本稀缺导致优化无效。

Method: 提出DaGRPO方法，包含两个核心机制：1) 序列级梯度矫正：使用细粒度评分动态屏蔽低区分度的样本对，从源头消除梯度冲突；2) 离策略数据增强：引入高质量锚点样本，为困难任务恢复训练信号。

Result: 在9个数学推理和OOD泛化基准上的实验表明，DaGRPO显著超越现有SFT、GRPO和混合基线，达到新的SOTA性能（如在数学基准上平均准确率提升+4.7%）。深入分析证实DaGRPO有效缓解梯度爆炸并加速长链推理能力的涌现。

Conclusion: DaGRPO通过解决GRPO的区分度不足问题，显著提升了长链推理训练的效果和效率，为大语言模型的推理能力优化提供了有效解决方案。

Abstract: The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.

</details>


### [171] [Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression](https://arxiv.org/abs/2512.06393)
*Qiming Bao,Xiaoxuan Fu*

Main category: cs.AI

TL;DR: LLMs在逻辑推理中对语义保持的等价变换表现稳定，但对缺失规则和矛盾证据极度脆弱


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在许多自然语言任务上表现出色，但其在逻辑推理中对结构化扰动的泛化能力尚未得到充分理解。研究者希望探究LLMs在面对逻辑结构变化时的推理可靠性。

Method: 提出了一个受控评估框架，通过四种针对性压力测试来探测推理可靠性：1) 规则删除（删除冗余或必要规则）；2) 矛盾证据注入；3) 逻辑保持的重写（使用六种等价定律）；4) 多定律等价堆叠（同时应用2-5个逻辑变换）。在BERT、Qwen2和LLaMA类模型上进行实验。

Result: 所有模型在基础任务上都达到完美准确率，对冗余规则删除和所有等价重写（单定律或多定律）都能完全泛化。但在必要规则删除时准确率急剧下降到25%，在明确矛盾存在时完全崩溃（0%准确率）。

Conclusion: LLMs对语义保持的逻辑变换具有稳定的不变性，但对缺失或冲突证据仍然极其脆弱。该框架为隔离此类推理失败模式提供了清晰的诊断工具，突显了当前LLMs在逻辑泛化能力上的持续差距。

Abstract: Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.
  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.

</details>


### [172] [UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems](https://arxiv.org/abs/2512.06406)
*Xianzong Wu,Xiaohong Li,Lili Quan,Qiang Hu*

Main category: cs.AI

TL;DR: UncertaintyZoo是一个统一的不确定性量化工具包，集成了29种UQ方法，涵盖五大类别，为LLMs的置信度评估提供标准化接口。


<details>
  <summary>Details</summary>
Motivation: LLMs在现实应用中存在错误预测风险，特别是在安全关键场景中可能导致损失。虽然已有多种不确定性量化方法，但缺乏集成工具，阻碍了UQ方法的实际应用和未来研究。

Method: 开发UncertaintyZoo工具包，集成29种不确定性量化方法，涵盖五大类别，提供标准化接口。在代码漏洞检测任务上对CodeBERT和ChatGLM3模型评估现有UQ方法的有效性。

Result: UncertaintyZoo能够有效揭示预测不确定性，工具已开源并提供演示视频。

Conclusion: UncertaintyZoo填补了LLMs不确定性量化工具集的空白，为实际应用和未来研究提供了统一平台。

Abstract: Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.

</details>


### [173] [Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City](https://arxiv.org/abs/2512.06431)
*Mohamed Shamroukh,Mohamed Alkhuzamy Aziz*

Main category: cs.AI

TL;DR: 该研究为埃及基纳市开发了定制化的城市规划模型，使用Python编程和Voronoi图算法评估公共服务设施覆盖率，发现平均覆盖率为81.3%，并识别了空间分布不均的问题。


<details>
  <summary>Details</summary>
Motivation: 埃及国家公共服务规划标准往往无法适应地方独特特征，需要开发能够反映地方特色的定制化规划模型。

Method: 采用混合方法（描述性、分析性和实验性），使用Python编程开发基于Voronoi图的智能空间分析算法，创建城市特定的规划标准并评估公共服务设施当前覆盖率。

Result: 应用模型显示平均服务覆盖率为81.3%，救护车站效率最高（99.8%），公园和开放空间覆盖率最低（10%）。空间分析显示市中心服务密度高（>45个/km²），郊区显著降低（<5个/km²），Hajer Qena区未服务区域最多，第一区服务覆盖率最高。

Conclusion: 该研究成功开发了本地化规划标准模型和自动化评估算法，为埃及城市提供了可复制的数据驱动城市规划框架。

Abstract: National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.

</details>


### [174] [The Effect of Belief Boxes and Open-mindedness on Persuasion](https://arxiv.org/abs/2512.06573)
*Onur Bilgin,Abdullah As Sami,Sriram Sai Vujjini,John Licato*

Main category: cs.AI

TL;DR: 研究探索了LLM智能体信念陈述（信念盒）如何影响其行为和说服力，以及开放心态指令的作用，发现信念陈述影响智能体对相反观点的抵抗力和说服力，开放心态指令影响信念改变的倾向性。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统在推理和决策应用中日益普及，需要让基于LLM的智能体具备类似命题信念的能力。研究旨在探索智能体信念陈述如何实际影响其行为和对信念的态度，以及这些信念陈述是否显著影响智能体在多智能体场景中的说服能力。

Method: 通过一系列实验探索信念盒技术，包括在提示空间中维护信念陈述（信念盒），研究信念陈述及其强度如何影响智能体对相反观点的抵抗力和说服力，同时研究开放心态指令如何影响智能体的行为。

Result: 研究发现：1）指示智能体保持开放心态会影响其信念改变的倾向性；2）纳入信念陈述及其强度会影响智能体对相反观点的抵抗力和说服力；3）在辩论中被相反观点包围（同伴压力场景）时，信念陈述影响信念改变的可能性。

Conclusion: 结果证明了信念盒技术在推理和决策任务中的可行性和有效性，为LLM智能体在多智能体系统中实现类似命题信念的能力提供了实证支持。

Abstract: As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.

</details>


### [175] [FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection](https://arxiv.org/abs/2512.06629)
*Xiao-li Xia,Hou-biao Li*

Main category: cs.AI

TL;DR: FlatFormer提出了一种基于"信息注入而非结构堆叠"设计范式的轻量级知识追踪模型，通过混合输入编码和预计算幂律偏置机制，在保持高性能的同时大幅降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 知识追踪模型面临"性能-复杂度陷阱"：捕捉复杂认知动态（如学习会话和记忆衰减）通常需要深度层次架构，这导致实时部署的计算成本过高。需要一种既能保持高性能又能降低复杂度的解决方案。

Method: 提出FlatFormer架构，采用标准平面Transformer，通过两种轻量级注入机制增强：(1) 混合输入编码策略，结合可学习会话标识符和固定正弦步长嵌入；(2) 将预计算的幂律偏置直接集成到注意力对数中，显式建模遗忘曲线。

Result: 在四个大规模数据集（如EdNet、Junyi）上的实验表明，FlatFormer达到最先进性能。在EdNet数据集上，相比最强层次基线（HiTSKT），绝对AUC提升8.3%，参数使用量不到15%，推理速度约快3倍。

Conclusion: 高认知保真度不需要架构复杂性，通过信息注入而非结构堆叠的设计范式可以在保持高性能的同时显著降低计算成本，为实时知识追踪部署提供了可行方案。

Abstract: Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.

</details>


### [176] [LightSearcher: Efficient DeepSearch via Experiential Memory](https://arxiv.org/abs/2512.06653)
*Hengzhi Lan,Yue Yu,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Ting Bai*

Main category: cs.AI

TL;DR: LightSearcher是一个高效的强化学习框架，通过结合文本经验记忆和自适应奖励机制，在保持准确性的同时显著减少DeepSearch范式中的工具调用次数和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的RL驱动的DeepSearch系统存在准确性-效率的跷跷板权衡：频繁调用搜索工具可以提高事实准确性，但会导致不必要的计算开销和效率下降。需要解决这一固有矛盾。

Method: 提出LightSearcher框架，包含两个核心组件：1) 通过对比学习推理轨迹生成可解释的成功推理模式摘要的文本经验记忆；2) 仅在正确答案场景下惩罚冗余工具调用的自适应奖励塑造机制。

Result: 在四个多跳QA基准测试中，LightSearcher在保持与SOTA基线ReSearch相当的准确性的同时，将搜索工具调用减少39.6%，推理时间减少48.6%，token消耗减少21.2%。

Conclusion: LightSearcher通过创新的经验记忆和奖励机制设计，有效平衡了DeepSearch范式中的准确性-效率权衡，实现了更高效的深度推理系统。

Abstract: DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.

</details>


### [177] [Academic journals' AI policies fail to curb the surge in AI-assisted academic writing](https://arxiv.org/abs/2512.06705)
*Yongyuan He,Yi Bu*

Main category: cs.AI

TL;DR: 研究分析了5114种期刊和520万篇论文，发现尽管70%期刊制定了AI使用政策，但AI写作工具使用率在各学科均大幅增长，且政策有无对使用率无显著影响。非英语国家、物理科学和高开放获取期刊增长最快。关键发现是透明度差距：2023年后的7.5万篇论文中仅76篇（0.1%）明确披露了AI使用。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在学术写作中的快速应用，期刊和出版商纷纷出台相关政策，但这些政策的实际效果尚不明确。本研究旨在评估AI使用指南在现实世界中的真实影响。

Method: 研究分析了5,114种期刊和超过520万篇论文，通过大规模数据分析评估AI政策的效果。特别对164,000篇科学出版物进行了全文分析，重点关注2023年后发表的75,000篇论文中AI使用披露情况。

Result: 1. 尽管70%的期刊采用了AI政策（主要是要求披露），但研究人员使用AI写作工具的比例在各学科均大幅增长；2. 有政策和无政策期刊之间的AI使用率无显著差异；3. 非英语国家、物理科学和高开放获取期刊的AI使用增长最快；4. 透明度差距显著：2023年后发表的75,000篇论文中，只有76篇（0.1%）明确披露了AI使用。

Conclusion: 当前的AI政策在很大程度上未能促进透明度或限制AI的采用。研究呼吁重新评估伦理框架，以促进科学中负责任的AI整合。

Abstract: The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.

</details>


### [178] [Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning](https://arxiv.org/abs/2512.06835)
*Tingyu Li,Zheng Sun,Jingxuan Wei,Siyuan Li,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: DoGe框架通过双解耦方法解决视觉语言模型在专业领域强化学习中的数据稀缺和奖励破解问题，将学习过程分解为思考者和解决者两个组件，并采用两阶段训练和课程学习策略。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在专业领域（如化学、地球科学、多模态数学）进行强化学习时面临高质量多模态数据稀缺的问题。现有方法如合成数据和自奖励机制存在分布有限和对齐困难的问题，导致奖励破解现象——模型利用高奖励模式，导致策略熵崩溃和训练不稳定。

Method: 提出DoGe（解耦以泛化）双解耦框架：1）将学习过程解耦为思考者和解决者两个组件，引导模型首先从上下文学习而非直接解决问题；2）采用两阶段强化学习后训练方法，从自由探索上下文到实际解决任务；3）构建演化课程学习管道，包括扩展的本土领域知识语料库和迭代演化的种子问题池。

Result: 实验表明，该方法在各种基准测试中持续优于基线方法，为实现自演化的大型视觉语言模型提供了可扩展的途径。

Conclusion: DoGe框架通过解耦学习过程和增加训练数据多样性，有效解决了视觉语言模型在专业领域强化学习中的数据稀缺和奖励破解问题，为实现自演化的大型视觉语言模型提供了可行的解决方案。

Abstract: Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.

</details>


### [179] [JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models](https://arxiv.org/abs/2512.06859)
*Ce Chi,Xing Wang,Zhendong Wang,Xiaofan Liu,Ce Li,Zhiyan Song,Chen Zhao,Kexin Yang,Boshen Shi,Jingjing Yang,Chao Deng,Junlan Feng*

Main category: cs.AI

TL;DR: JT-DA-8B是一个专门用于复杂表格推理任务的8B参数大语言模型，通过构建包含34个表格推理任务的多样化训练语料，结合SFT和RL优化，在多种表格推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前在表格推理场景中缺乏高质量监督数据，需要专门针对复杂表格推理任务的大语言模型来处理现实世界中的多样化表格分析需求。

Method: 1) 构建包含29个公开表格QA数据集和300万张表格的多样化训练语料；2) 提出自动流水线生成多步分析任务；3) 基于开源的JT-Coder-8B模型，采用LLM评分和工作流对齐过滤来提炼高质量表格中心数据；4) 结合监督微调(SFT)和强化学习(RL)优化模型；5) 提出四阶段表格推理工作流：表格预处理、表格感知、工具集成推理和提示工程。

Result: JT-DA-8B在各种表格推理任务中实现了强大的性能表现，证明了数据中心的生成方法和工作流驱动的优化策略的有效性。

Conclusion: 通过构建全面的训练语料、采用数据中心的生成方法和工作流驱动的优化策略，JT-DA-8B成功解决了复杂表格推理任务，为现实世界表格分析提供了有效的解决方案。

Abstract: In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.

</details>


### [180] [Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?](https://arxiv.org/abs/2512.06867)
*John Licato,Stephen Steinle,Brayden Hollis*

Main category: cs.AI

TL;DR: 研究探讨了在大型语言模型中使用角色提示是否能在对抗性战略环境中产生可测量的行为差异，通过PERIL世界征服棋盘游戏测试发现，某些与战略思维相关的角色能提升游戏表现，但需要中介机制将角色转化为启发式值。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型中的角色提示似乎能触发不同的文本生成风格，但尚不清楚这些差异是否能转化为可测量的行为差异，特别是在对抗性战略环境中。研究者希望了解角色提示是否影响战略决策表现。

Method: 使用PERIL（世界征服棋盘游戏）作为测试平台，比较角色衍生的启发式策略与手动选择的策略。引入基于探索性因子分析的结构化翻译过程作为中介，将LLM生成的清单响应映射为启发式值。

Result: 研究发现某些与战略思维相关的角色确实能提高游戏表现，但前提是使用中介机制将角色转化为启发式值。与直接推断的启发式相比，该方法提高了启发式的可靠性和表面效度。

Conclusion: 该研究增进了对角色提示如何影响基于LLM的决策的理解，并提出了一种将心理测量学原理应用于LLM的启发式生成方法，为研究角色类型对决策的影响提供了更好的工具。

Abstract: Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.

</details>


### [181] [On Memory: A comparison of memory mechanisms in world models](https://arxiv.org/abs/2512.06983)
*Eli J. Laird,Corey Clark*

Main category: cs.AI

TL;DR: 本文研究了基于Transformer的世界模型的有效记忆跨度，分析了多种记忆增强机制，提出了记忆编码与记忆注入的分类法，并通过状态回忆任务评估了不同机制在扩展世界模型记忆和实现轨迹闭环方面的效果。


<details>
  <summary>Details</summary>
Motivation: 世界模型使智能体能够在想象环境中进行规划，但基于Transformer的世界模型在长时程规划中存在有效记忆跨度有限的限制，这导致长轨迹生成中的感知漂移，阻碍了在想象轨迹中实现闭环的能力。

Method: 研究分析了多种记忆增强机制，提出了区分记忆编码和记忆注入机制的分类法，从残差流动态的角度理解这些机制扩展世界模型记忆的作用。通过状态回忆评估任务，测量了每种机制的记忆回忆能力并分析了各自的权衡。

Result: 研究发现记忆机制能够提高视觉Transformer的有效记忆跨度，并为在世界模型想象中完成轨迹闭环提供了路径。不同记忆机制在记忆回忆方面表现出各自的优势和权衡。

Conclusion: 记忆增强机制对于扩展基于Transformer的世界模型的记忆跨度至关重要，能够缓解长轨迹生成中的感知漂移问题，为实现想象轨迹中的闭环提供了可行的技术路径。

Abstract: World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.

</details>


### [182] [ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes](https://arxiv.org/abs/2512.07081)
*Rongjia Zhou,Chengzhuo Li,Carl Yang,Jiaying Lu*

Main category: cs.AI

TL;DR: ClinNoteAgents是一个基于LLM的多智能体框架，用于从临床自由文本笔记中提取心衰再入院风险因素并进行预测，减少对结构化数据和人工标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 心衰是美国老年人再入院的主要原因之一。临床笔记包含丰富的患者信息，但在心衰再入院风险分析中未被充分利用。传统模型依赖专家规则、医学术语库和本体来解释临床笔记，但这些笔记通常包含拼写错误、缩写和领域特定术语。

Method: 提出ClinNoteAgents，一个基于大语言模型的多智能体框架，将自由文本临床笔记转化为：(1) 用于关联分析的结构化临床和社会风险因素表示；(2) 用于心衰30天再入院预测的临床医生风格抽象。

Result: 在2,065名患者（再入院率35.16%）的3,544份笔记上评估，在从自由文本提取风险因素、识别关键贡献因素和预测再入院风险方面表现出色。

Conclusion: 通过减少对结构化字段的依赖并最小化人工标注和模型训练，ClinNoteAgents为数据有限的医疗系统提供了一种可扩展且可解释的基于笔记的心衰再入院风险建模方法。

Abstract: Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.

</details>


### [183] [A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy](https://arxiv.org/abs/2512.07109)
*Miguel Ingram,Arthur Joseph Merritt*

Main category: cs.AI

TL;DR: 本文提出了首个针对400个任务的9类别分类法，通过CNN验证其视觉一致性，并揭示了Transformer架构在任务合成上的局限性，发现了神经亲和力天花板效应。


<details>
  <summary>Details</summary>
Motivation: 响应Hodel等人对任务相关性正式定义的呼吁，建立系统化的任务分类体系，以诊断当前AI模型在抽象推理任务上的性能瓶颈。

Method: 1. 开发基于规则代码分析的9类别任务分类法（400个任务，97.5%准确率）；2. 使用原始网格像素训练CNN验证分类法的视觉一致性；3. 在302个任务上微调170万参数Transformer，分析局部模式与全局合成的性能差距。

Result: 1. 分类法在S3数据集上达到95.24%准确率，整体36.25%（3.3倍随机概率）；2. 发现35.3%任务对Transformer具有低神经亲和力；3. 揭示组合性差距：69.5%任务局部准确率>80%但全局准确率<10%；4. 在独立研究中验证预测能力：低亲和力任务51.9% vs 高亲和力77.7%（p<0.001）。

Conclusion: 当前AI进展受限于架构适应性而非训练数据，需要开发具有亲和力对齐模块的混合架构。分类法为精确诊断任务难度和模型局限性提供了有效工具。

Abstract: Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,

</details>


### [184] [ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation](https://arxiv.org/abs/2512.07178)
*Latifa Dwiyanti,Sergio Ryan Wibisono,Hidetaka Nambo*

Main category: cs.AI

TL;DR: 提出一个Python包，将SHAP与大型语言模型（GPT）集成，为特征重要性解释生成上下文文本描述，提升非技术用户的理解性。


<details>
  <summary>Details</summary>
Motivation: SHAP虽然能有效可视化特征重要性，但缺乏对非技术用户有意义的上下文解释。在高风险领域部署机器学习模型时，可解释性尤为重要。

Method: 开发Python包，将SHAP与OpenAI的GPT集成，通过用户定义的参数（特征别名、描述、背景信息）生成上下文文本解释。

Result: 在医疗相关案例研究中应用该包，通过李克特量表和后续访谈的用户评估显示，生成的解释比纯可视化输出更易理解和上下文更合适。

Conclusion: 可视化与上下文文本结合可能支持更用户友好和可信赖的模型解释，尽管结果还是初步的。

Abstract: Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.

</details>


### [185] [PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations](https://arxiv.org/abs/2512.07179)
*Wonbeen Lee,Channyoung Lee,Junho Sohn,Hansam Cho*

Main category: cs.AI

TL;DR: 提出PICKT模型解决知识追踪中的冷启动问题，通过知识图谱处理多种输入数据格式，在真实环境中验证了性能与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型存在输入数据格式受限、新学生/新问题冷启动问题、以及实际服务环境稳定性不足等局限性，需要更实用的解决方案。

Method: 提出PICKT模型，利用知识图谱结构化概念间关系，考虑题目和概念文本信息，有效处理多种输入数据格式，解决冷启动问题。

Result: 实验证明模型在真实操作环境中表现优异，在新学生注册和新题目添加两个核心冷启动挑战上显著优于现有模型，验证了稳定性和实用性。

Conclusion: PICKT模型为下一代智能辅导系统的实际部署提供了重要的理论与技术基础，增强了在真实产品环境中的适用性。

Abstract: With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.

</details>


### [186] [M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling](https://arxiv.org/abs/2512.07314)
*Yuxiao Luo,Songming Zhang,Sijie Ruan,Siran Chen,Kang Liu,Yang Xu,Yu Zheng,Ling Yin*

Main category: cs.AI

TL;DR: M-STAR是一个多尺度时空自回归框架，通过从粗到细的预测过程生成长期轨迹，在保真度和生成速度上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归和扩散模型的方法在生成长周期轨迹（如周轨迹）时效率低下，且缺乏显式的时空多尺度建模能力

Method: 提出M-STAR框架，包含多尺度时空标记器编码分层移动模式，以及基于Transformer的解码器进行下一尺度自回归预测

Result: 在两个真实数据集上的实验表明，M-STAR在轨迹保真度上优于现有方法，并显著提高了生成速度

Conclusion: M-STAR通过多尺度时空建模有效解决了长期轨迹生成的效率和保真度问题，为人类移动建模提供了新方法

Abstract: Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.

</details>


### [187] [A Geometric Unification of Concept Learning with Concept Cones](https://arxiv.org/abs/2512.07355)
*Alexandre Rocchi--Henry,Thomas Fel,Gianni Franchi*

Main category: cs.AI

TL;DR: 论文通过几何框架统一了概念瓶颈模型(CBMs)和稀疏自编码器(SAEs)，提出两者都学习激活空间中的线性方向形成概念锥，区别仅在于如何选择这个锥。基于此提出了评估SAEs学习概念与CBM概念对齐程度的量化指标。


<details>
  <summary>Details</summary>
Motivation: 两种可解释性传统——监督式的概念瓶颈模型和无监督的稀疏自编码器——虽然并行发展但很少对话。CBMs通过监督定义概念，SAEs通过稀疏编码发现概念，需要建立两者之间的理论桥梁。

Method: 提出几何框架：两种方法都学习激活空间中的线性方向，其非负组合形成概念锥。基于此框架，将CBMs作为人类定义的参考几何，评估SAEs学习的概念锥与CBM概念锥的包含关系，提出量化对齐指标。

Result: 发现了稀疏度和扩展因子的"最佳点"，能最大化与CBM概念的几何和语义对齐。提供了评估SAEs进展和发现概念与人类概念对齐程度的原理性指标。

Conclusion: 通过共享的几何框架统一了监督和无监督的概念发现方法，为衡量SAEs学习的概念与人类概念的对齐程度提供了理论依据和量化工具。

Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.

</details>


### [188] [LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services](https://arxiv.org/abs/2512.07436)
*Hang He,Chuhuai Yue,Chengqi Dong,Mingxue Tian,Zhenfeng Liu,Jiajun Chai,Xiaohan Wang,Yufei Zhang,Qun Liao,Guojun Yin,Wei Lin,Chengcheng Wan,Haiying Sun,Ting Su*

Main category: cs.AI

TL;DR: 该论文介绍了LocalSearchBench，这是首个针对本地生活服务的智能搜索基准测试，包含超过15万条高质量数据条目和300个多跳问答任务，用于评估大型推理模型在复杂本地生活场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型的研究主要集中在通用信息检索领域，很少关注具有独特挑战的垂直领域。本地生活服务领域的查询通常具有模糊性，需要在商家和产品之间进行多跳推理，这些挑战尚未得到充分解决。

Method: 构建了LocalSearchBench基准测试，包含来自不同城市和业务类型的超过150,000条高质量条目，并基于真实用户查询构建了300个多跳问答任务。同时开发了LocalPlayground统一环境，集成了多种工具供智能体交互。

Result: 实验表明，即使是当前最先进的大型推理模型在LocalSearchBench上也表现不佳：最佳模型（DeepSeek-V3.1）仅达到34.34%的正确率，大多数模型在完整性（平均77.33%）和忠实性（平均61.99%）方面存在问题。

Conclusion: 该研究强调了在本地生活服务领域需要专门的基准测试和领域特定的智能体训练，以应对该领域的独特挑战。

Abstract: Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.

</details>


### [189] [Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement](https://arxiv.org/abs/2512.07611)
*Yongsheng Lian*

Main category: cs.AI

TL;DR: 本研究系统比较了三种强化学习算法（PPO、GRPO、DAPO）在提升大语言模型复杂推理能力方面的效果，通过控制性迁移学习评估发现RL训练模型在所有任务上都优于基础模型，但改进程度因基准而异。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索不同强化学习算法如何有效提升大语言模型的复杂推理能力，并为RL-based LLM训练提供实用的参数配置指导。

Method: 采用控制性迁移学习评估方法：首先在专门的Countdown Game上对模型进行微调，然后在通用推理基准测试套件上进行评估。对GRPO和DAPO中的组大小、KL惩罚系数等参数进行系统分析。

Result: 所有RL训练模型在各项任务上都优于对应的基础模型；增加GRPO和DAPO中的组大小能带来更稳定的训练动态和更高准确率；KL惩罚系数的影响是非单调的；DAPO中的动态采样组件并未提升性能，禁用DS时DAPO表现最佳。

Conclusion: 强化学习能有效提升大语言模型的推理能力，但不同算法和参数配置对性能影响显著。研究为RL-based LLM训练提供了实用的参数配置指导，特别是关于组大小优化和动态采样组件的有效性评估。

Abstract: This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.

</details>


### [190] [Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE](https://arxiv.org/abs/2512.07710)
*Anxiang Zeng,Haibo Zhang,Hailing Zhang,Kaixiang Mo,Liang Yao,Ling Hu,Long Zhang,Shuman Liu,Shuyi Xie,Yanshi Li,Yizhang Chen,Yuepeng Sheng,Yuwei Huang,Zhaochen Xu,Zhiqiang Zhou,Ziqin Liew*

Main category: cs.AI

TL;DR: 提出了CompassMax-V3-Thinking，一个千亿规模的MoE推理模型，采用基于"每个提示都必须重要"原则的新RL框架，解决了大规模RL训练中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 将RL扩展到千亿规模时暴露出关键效率问题：零方差提示浪费rollout资源、长时域重要性采样不稳定、标准奖励模型导致优势反转，以及rollout处理的系统性瓶颈。

Method: 提出四项统一创新：1) 多阶段零方差消除，过滤非信息性提示并稳定基于组的策略优化；2) ESPO熵自适应优化方法，平衡token级和序列级重要性采样；3) 路由器重放策略，对齐训练时MoE路由器决策与推理时行为；4) 高吞吐RL系统，采用FP8精度rollout、重叠奖励计算和长度感知调度。

Result: 这些贡献形成了统一的管道，使千亿规模MoE模型的RL训练变得稳定高效。最终模型在内部和公开评估中都表现出色。

Conclusion: 通过解决大规模RL训练中的关键效率问题，成功构建了稳定高效的千亿规模MoE推理模型训练框架，为大规模语言模型的强化学习训练提供了有效解决方案。

Abstract: We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.

</details>


### [191] [RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2512.07761)
*Xiqiao Xiong,Ouxiang Li,Zhuo Liu,Moxin Li,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化学习的多轮越狱攻击方法，通过优化最终输出的有害性作为奖励，并引入过程奖励来提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到越狱攻击的威胁，现有方法通常依赖单轮优化，不足以学习长期攻击策略。需要开发能够通过多轮交互有效攻击黑盒模型的方法。

Method: 将问题构建为多轮强化学习任务，直接优化最终输出的有害性作为结果奖励。提出两种启发式过程奖励：1) 控制中间输出的有害性以避免触发黑盒模型的拒绝机制；2) 保持中间输出的语义相关性以避免偏离主题。

Result: 在多个基准测试上的实验结果表明，该方法在多个模型上持续提高了攻击成功率，证明了方法的有效性。

Conclusion: 通过强化学习框架和多轮优化策略，能够有效提升对黑盒语言模型的越狱攻击成功率，为模型安全部署提供了重要参考。

Abstract: Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.

</details>


### [192] [ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning](https://arxiv.org/abs/2512.07795)
*Nearchos Potamitis,Lars Klein,Akhil Arora*

Main category: cs.AI

TL;DR: ReasonBENCH：首个量化LLM推理不稳定性的基准，通过多轮评估协议提供统计可靠的性能与成本指标，揭示当前推理方法普遍存在高不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理评估主要报告单次运行准确率，忽略了随机解码带来的内在不确定性，导致无法可靠评估方法性能的稳定性、可复现性和成本一致性。

Method: 开发ReasonBENCH基准，包含：(1)标准化推理框架、模型和任务的模块化评估库；(2)报告质量和成本统计可靠指标的多轮评估协议；(3)鼓励方差感知报告的公开排行榜。

Result: 跨多个领域任务发现，绝大多数推理策略和模型表现出高不稳定性。即使平均性能相似的策略，置信区间宽度可达四倍差异；性能最佳的方法通常成本更高且更不稳定。

Conclusion: 推理不稳定性损害了跨运行的可复现性和报告性能的可靠性。可复现性是可靠LLM推理的关键维度，ReasonBENCH为未来推理方法和不确定性量化技术奠定了基础。

Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .

</details>


### [193] [Large Causal Models from Large Language Models](https://arxiv.org/abs/2512.07796)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: DEMOCRITUS是一个利用大语言模型构建大规模因果模型的新范式，通过提取多领域文本中的因果关系，将其整合成统一的因果知识图谱。


<details>
  <summary>Details</summary>
Motivation: 传统因果推理方法局限于特定领域和假设驱动，依赖实验产生的数值数据。本文旨在利用大语言模型的潜力，从广泛文本中提取多领域因果知识，构建跨越不同领域的大规模因果模型。

Method: 使用高质量LLM提出主题、生成因果问题、从多领域文本中提取因果陈述。通过六个模块的流水线将碎片化、可能冲突的因果主张转化为关系因果三元组，并嵌入大规模因果模型中。采用新的范畴机器学习方法整合这些因果知识。

Result: DEMOCRITUS系统已在考古学、生物学、气候变化、经济学、医学和技术等多个领域成功应用，能够构建跨越不同领域的大规模因果模型。分析了系统的计算成本分布，确定了扩展到更大模型的当前瓶颈。

Conclusion: DEMOCRITUS展示了利用大语言模型构建大规模因果模型的新范式潜力，能够整合多领域因果知识。系统目前存在局限性，但为扩展其能力指明了方向。

Abstract: We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [194] [Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://arxiv.org/abs/2512.06097)
*Emre Umucu,Guillermina Solis,Leon Garza,Emilia Rivas,Beatrice Lee,Anantaa Kotal,Aritran Piplai*

Main category: cs.CL

TL;DR: 该论文提出了一个基于直接偏好优化（DPO）的框架，用于改进大型语言模型在医疗护理对话中的事实准确性、语义连贯性和同理心等人类中心特质，解决了现有LLM在医疗应用中存在的事实不可靠和缺乏同理心沟通的问题。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型在医疗护理应用中存在两个关键缺陷：事实不可靠性和缺乏同理心沟通。这些缺陷在敏感医疗环境中尤其危险，因为非专业护理人员和患者需要医学相关指导和情感支持。

Method: 采用基于直接偏好优化（DPO）的对齐框架，使用成对偏好数据对领域适应的LLM进行微调。偏好响应体现支持性和可访问的沟通风格，而被拒绝的响应则代表规定性或过于技术化的语调。

Result: 经验评估显示，DPO调优模型在语义对齐、事实准确性和人类中心评估分数方面优于基线和商业替代方案（如谷歌医疗对话系统）。改进表明基于偏好的对齐为开发可信赖、有同理心且临床知情的AI助手提供了可扩展且透明的途径。

Conclusion: 偏好对齐为开发可信赖、有同理心且临床知情的AI助手提供了可扩展且透明的途径，能够显著改善护理人员和医疗保健沟通的质量。

Abstract: General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design

</details>


### [195] [Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yoloxóchtil Mixtec ASR](https://arxiv.org/abs/2512.06169)
*Chris Crawford*

Main category: cs.CL

TL;DR: 本文研究使用形态学感知的分词器来辅助和简化Yoloxóchitl Mixtec音频语料的层间注释标注，结合ASR和文本序列到序列工具，旨在提高效率并减少人工标注工作量。


<details>
  <summary>Details</summary>
Motivation: 传统ASR系统在处理具有非连接性形态的语言（如Yoloxóchitl Mixtec）时面临挑战，特别是音调形态的保留问题。需要开发专门的分词器来更好地处理这类语言的独特形态特征，提高标注效率。

Method: 提出了两种新颖的非线性分词方案：1）Segment and Melody分词器，仅提取音调而不预测分割；2）Sequence of Processes分词器，预测单词分割，使端到端ASR系统能一次性生成分割和未分割的转录。与传统BPE和Unigram模型进行比较。

Result: 新颖分词器与BPE和Unigram模型具有竞争力，Segment-and-Melody模型在词错误率方面优于传统分词器，但在字符错误率方面未达到相同水平。形态学和信息论指标分析显示与下游性能存在预测相关性。

Conclusion: 专门为非连接性形态语言设计的非线性分词器在ASR任务中与传统BPE和Unigram模型具有竞争力，为处理复杂形态语言提供了新思路。需要进一步研究这些分词器在下游处理任务中的适用性。

Abstract: This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yoloxóchitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.

</details>


### [196] [Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots](https://arxiv.org/abs/2512.06193)
*Jihyung Park,Saleh Afroogh,Junfeng Jiao*

Main category: cs.CL

TL;DR: GAUGE是一个轻量级的logit-based框架，用于实时检测对话中的隐性情感升级，弥补传统毒性过滤器无法识别情感强化或漂移导致隐性伤害的不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为情感伴侣使用时，即使没有明显毒性，重复的情感强化或情感漂移也可能导致隐性伤害，而现有基于外部分类器或临床标准的防护机制难以实时捕捉对话中的细微动态变化。

Method: 提出GAUGE框架，基于logit测量大语言模型输出如何概率性地改变对话的情感状态，实现轻量级的实时隐性对话升级检测。

Result: 论文提出了GAUGE框架，但摘要中未展示具体实验结果，需要阅读全文了解该框架在检测隐性情感升级方面的实际效果。

Conclusion: GAUGE为解决大语言模型作为情感伴侣时产生的隐性伤害问题提供了一种实时、轻量级的检测方法，能够捕捉传统毒性过滤器无法识别的对话情感升级。

Abstract: Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.

</details>


### [197] [Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge](https://arxiv.org/abs/2512.06228)
*Xuanxin Wu,Yuki Arase,Masaaki Nagata*

Main category: cs.CL

TL;DR: 利用LLM-as-a-Judge自动构建策略对齐的训练数据，无需人工标注或平行语料，实现可适应不同简化策略的文本简化系统


<details>
  <summary>Details</summary>
Motivation: 文本简化需要根据不同应用采用不同的简化策略（如仅替换复杂词汇或整体重写），但实现策略驱动的控制仍是一个开放挑战。现有方法依赖昂贵的人工标注或平行语料。

Method: 提出利用大型语言模型作为评判者（LLM-as-a-Judge）自动构建策略对齐的训练数据，完全无需人工标注或平行语料。该方法能够构建适应不同简化策略的简化系统。

Result: 即使是小规模开源LLM（如Phi-3-mini-3.8B）在词汇导向的简化任务上超越了GPT-4o，在整体重写任务上达到可比性能。自动指标和人工评估均验证了方法的有效性，且在不同模型家族和规模上均表现出一致的改进。

Conclusion: 提出的LLM-as-a-Judge方法能够自动构建策略对齐的训练数据，实现可适应不同简化策略的高效文本简化系统，且小模型也能超越大模型在特定任务上的表现，证明了方法的鲁棒性和有效性。

Abstract: Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.

</details>


### [198] [LOCUS: A System and Method for Low-Cost Customization for Universal Specialization](https://arxiv.org/abs/2512.06239)
*Dhanasekar Sundararaman,Keying Li,Wayne Xiong,Aashna Garg*

Main category: cs.CL

TL;DR: LOCUS是一个低成本的NLP模型定制流程，通过少量标注数据实现高效模型构建，结合检索、合成数据生成和参数高效调优，在NER和文本分类任务上超越GPT-4o等基线，同时大幅降低成本和模型大小。


<details>
  <summary>Details</summary>
Motivation: 传统NLP模型定制需要大量标注数据和计算资源，成本高昂。需要一种能够利用少量标注数据就能构建高性能专用模型的方法，同时大幅降低计算成本和模型规模。

Method: LOCUS采用三步流程：1）从大规模存储库中检索相关数据；2）通过上下文数据生成合成额外训练样本；3）使用全参数或低秩适配（LoRA）进行参数高效微调。专注于命名实体识别和文本分类任务。

Result: LOCUS在多个基准测试中持续超越强基线（包括GPT-4o），内存优化模型保留了99%的全微调精度，同时仅使用5%的内存占用。在多个基准测试中击败GPT-4o，而参数数量不到其1%。

Conclusion: LOCUS证明了通过少量标注数据、智能检索和合成数据生成，结合参数高效微调技术，可以构建高性能、低成本的专用NLP模型，为实际应用提供了经济高效的解决方案。

Abstract: We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.

</details>


### [199] [Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup](https://arxiv.org/abs/2512.06256)
*Aniruddha Maiti,Satya Nimmagadda,Kartha Veerya Jammuladinne,Niladri Sengupta,Ananya Jana*

Main category: cs.CL

TL;DR: 两个大语言模型在多智能体设置中相互对话，初始种子句后模型轮流生成响应，观察发现对话开始连贯但后期陷入重复循环，出现收敛现象。


<details>
  <summary>Details</summary>
Motivation: 研究两个大语言模型在没有外部输入的多智能体设置中相互对话时会发生什么，探索它们长期交互的动态行为。

Method: 使用Mistral Nemo Base 2407和Llama 2 13B hf模型，从短种子句开始，让两个模型轮流读取对方输出并生成响应，持续固定步数，应用词汇和嵌入指标测量对话偏离初始种子的程度以及两个模型输出的相似性。

Result: 大多数对话开始连贯但后期陷入重复，出现短短语在多个轮次中重复，一旦开始重复，两个模型倾向于产生相似输出而非引入新方向，导致相同或相似文本的循环，即使模型规模大、独立训练且无提示指令，仍出现收敛现象。

Conclusion: 两个大语言模型在多智能体对话设置中会自发收敛到重复模式，表现为对话后期陷入循环重复，这种收敛现象揭示了模型在长期交互中的行为特性。

Abstract: In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.

</details>


### [200] [Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models](https://arxiv.org/abs/2512.06266)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Wei Ruan,Xiaoqi Liu,Xiaoxue Cheng,Xiyun Xu,Yang Song,Yanzipeng Gao,Yiming Jia,Yun Xing,Yuntao Wen,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.CL

TL;DR: Nanbeige4-3B是一个高性能的小规模语言模型，通过创新的预训练调度器、后训练数据质量提升机制、双重偏好蒸馏和多阶段强化学习，在3B参数规模下达到了超越同类模型甚至媲美更大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 推动小规模语言模型的性能边界，证明通过精心设计的训练策略和数据优化，小模型也能达到甚至超越更大模型的性能水平。

Method: 1. 预训练阶段：使用细粒度热身-稳定-衰减调度器，分阶段优化数据混合；2. 后训练阶段：结合审议生成优化和思维链重构机制提升SFT数据质量；3. 使用旗舰推理模型通过双重偏好蒸馏方法蒸馏Nanbeige4-3B；4. 应用多阶段强化学习，利用可验证奖励和偏好建模增强推理和对齐能力。

Result: 在广泛基准测试中，Nanbeige4-3B不仅显著优于同等参数规模的模型，还能与更大规模的模型竞争，展现了小规模模型的高性能潜力。

Conclusion: 通过创新的训练策略和数据优化方法，成功扩展了小规模语言模型的缩放定律边界，证明了精心设计的小模型可以达到与更大模型相媲美的性能水平。

Abstract: We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.

</details>


### [201] [Modeling Contextual Passage Utility for Multihop Question Answering](https://arxiv.org/abs/2512.06464)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级方法，通过建模上下文相关的篇章效用（考虑篇章间依赖关系）来改进多跳问答中的篇章重排序，相比基于相关性的方法能提升问答性能。


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要从多个文本篇章中识别和综合信息。现有方法主要关注识别相关篇章，但进一步评估篇章效用有助于去除冗余篇章，减少噪声和答案生成中的不准确性。现有效用预测方法独立建模篇章效用，忽略了多跳推理的关键方面：篇章效用是上下文相关的，受其与其他篇章关系的影响（是否提供补充信息或与其他篇章形成关键链接）。

Method: 提出了一种轻量级方法来建模上下文相关的篇章效用，考虑篇章间依赖关系。微调一个小型基于Transformer的模型来预测多跳问答中的篇章效用分数。利用高级推理模型的推理轨迹来捕捉回答问题时使用篇章的顺序，并获取合成训练数据。

Result: 通过全面实验证明，基于效用的检索篇章评分相比基于相关性的重排序方法，能带来改进的重排序效果和下游问答性能提升。

Conclusion: 建模上下文相关的篇章效用（考虑篇章间依赖关系）对于多跳问答是有效的，相比传统基于相关性的方法能更好地识别和利用篇章间的互补和链接关系，从而提升问答系统的整体性能。

Abstract: Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.

</details>


### [202] [Knowing What's Missing: Assessing Information Sufficiency in Question Answering](https://arxiv.org/abs/2512.06476)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: 提出Identify-then-Verify框架，通过先识别缺失信息再验证的方法，更可靠地判断上下文是否足够回答问题


<details>
  <summary>Details</summary>
Motivation: 现有简单提示策略在处理需要推理的问题时经常失败，无法可靠判断上下文是否包含足够信息来回答问题

Method: 提出结构化Identify-then-Verify框架：首先生成多个关于缺失信息的假设并建立语义共识，然后进行关键验证步骤，强制模型重新检查源文本确认信息是否真正缺失

Result: 在多样化的多跳和事实性QA数据集上评估，结果显示该方法通过引导模型证明其关于缺失信息的判断，能产生更准确的充分性判断并清晰表达信息缺口

Conclusion: 通过先推理缺失信息再验证的方法，能够更可靠地评估上下文充分性，提高问答系统的可靠性

Abstract: Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.

</details>


### [203] [Classifying German Language Proficiency Levels Using Large Language Models](https://arxiv.org/abs/2512.06483)
*Elias-Leander Ahlers,Witold Brunsmann,Malte Schilling*

Main category: cs.CL

TL;DR: 本文研究了使用大语言模型自动将德语文本按CEFR语言能力等级分类的方法，通过构建多样化数据集并评估多种技术策略，实现了比现有方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: 语言能力评估对教育至关重要，能够根据学习者需求提供定制化教学。本文旨在探索使用大语言模型自动将德语文本按CEFR框架分类到不同能力等级的方法。

Method: 构建了包含多个现有CEFR标注语料库和合成数据的多样化数据集，评估了三种方法：提示工程策略、微调LLaMA-3-8B-Instruct模型，以及基于LLM内部神经状态的探测分类方法。

Result: 结果显示，这些方法相比先前方法取得了持续的性能提升，表明大语言模型在可靠且可扩展的CEFR分类方面具有潜力。

Conclusion: 大语言模型在德语文本CEFR等级自动分类方面表现出良好潜力，能够实现可靠且可扩展的语言能力评估，为教育领域的个性化教学提供了技术支持。

Abstract: Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.

</details>


### [204] [ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models](https://arxiv.org/abs/2512.06515)
*Somnath Banerjee,Sayan Layek,Sayantan Adak,Mykola Pechenizkiy,Animesh Mukherjee,Rima Hazra*

Main category: cs.CL

TL;DR: ProSocialAlign是一个测试时参数高效框架，通过词典约束生成和方向性调节，在保持基础模型不变的情况下实现安全、共情、价值对齐的响应生成。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型安全范式在情感激烈或高风险场景中存在不足：仅拒绝可能疏远用户，而简单顺从可能放大风险。需要一种能在保持安全的同时提供共情和人性化回应的解决方案。

Method: 提出测试时参数高效框架，将安全形式化为词典约束生成：首先应用硬约束消除有害延续，然后在安全集合内优化亲社会质量。结合(i)方向性调节（在参数空间减去学习的"伤害向量"），(ii)偏好感知自回归奖励建模（跨属性联合训练，带梯度冲突解决），实现细粒度、用户可控的解码。

Result: 在五个安全基准测试中表现出最先进的性能，减少不安全泄漏并提升与人类价值观的对齐度，在多个评估指标上获得显著提升。

Conclusion: ProSocialAlign为在推理时生成上下文敏感、安全且与人类对齐的响应提供了稳健和模块化的基础。

Abstract: Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned "harm vector" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.

</details>


### [205] [Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract](https://arxiv.org/abs/2512.06586)
*Mikhail Zimin,Milyausha Shamsutdinova,Georgii Andriushchenko*

Main category: cs.CL

TL;DR: AlignRuScore：将AlignScore指标适配到俄语，用于评估俄语文本的事实一致性，填补了俄语事实一致性评估工具的空白。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对俄语文本的事实一致性评估工具，现有工具主要面向英语语料。为了填补这一空白，需要开发适用于俄语的事实一致性评估方法。

Method: 通过微调基于RuBERT的对齐模型，使用特定任务的分类和回归头，在俄语和翻译的英语数据集上进行训练，将AlignScore指标适配到俄语。

Result: 结果表明，统一的对齐指标可以成功移植到俄语，为稳健的多语言事实一致性评估奠定了基础。发布了翻译语料库、模型检查点和代码。

Conclusion: AlignRuScore成功将事实一致性评估工具扩展到俄语，为多语言自然语言处理应用提供了重要的评估资源，支持进一步的研究工作。

Abstract: Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.

</details>


### [206] [The Online Discourse of Virtual Reality and Anxiety](https://arxiv.org/abs/2512.06656)
*Kwabena Yamoah,Cass Dykeman*

Main category: cs.CL

TL;DR: 该研究使用语料库语言学方法分析在线讨论中虚拟现实（VR）与焦虑相关的话题，发现VR、Oculus和头显是最常讨论的词汇，并识别了与VR设计、体验和开发相关的介词搭配模式。


<details>
  <summary>Details</summary>
Motivation: VR在治疗广泛性焦虑障碍或社交焦虑等临床问题中显示出潜力，为患者福祉和护理创造了新途径。了解用户对该技术的在线讨论可能进一步支持其疗效。

Method: 采用语料库语言学方法，使用Sketch Engine软件识别在线讨论中频繁使用的词汇及其搭配关系，基于English Trends语料库进行分析。

Result: 在VR与焦虑子语料库中，VR、Oculus和头显是最常讨论的词汇，这些结果指向虚拟系统的发展以及使虚拟环境观看和交互成为可能的物理设备。此外还识别了"of virtual reality"（设计）、"in virtual reality"（体验）和"for virtual reality"（开发）等介词搭配模式。

Conclusion: 研究结果为VR与焦虑在一般话语中的讨论提供了新视角，并为通过发展和可访问性支持咨询需求的未来机会提供了途径。

Abstract: VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR

</details>


### [207] [CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis](https://arxiv.org/abs/2512.06679)
*Smitha Muthya Sudheendra,Mani Deep Cherukuri,Jaideep Srivastava*

Main category: cs.CL

TL;DR: CMV-Fuse是一个跨模态视图融合框架，通过结合抽象意义表示、成分句法、依存句法和语义注意力四种语言视角，模拟人类语言处理过程，提升方面级情感分析性能。


<details>
  <summary>Details</summary>
Motivation: 当前方面级情感分析系统通常利用孤立的语言视角，忽略了人类自然利用的结构表征之间的复杂相互作用。自然语言理解本质上依赖于从表层句法到深层语义和世界知识的多个互补视角的整合。

Method: 提出CMV-Fuse框架，系统整合四种语言视角：抽象意义表示、成分句法、依存句法和语义注意力，并增强外部知识集成。通过局部句法、中间语义和全局知识三个层次的分层门控注意力融合，捕捉细粒度结构模式和广泛上下文理解。采用新颖的结构感知多视图对比学习机制确保互补表征的一致性。

Result: 在标准基准测试上的广泛实验表明，CMV-Fuse相比强基线有显著改进，分析揭示了每种语言视角如何对更稳健的情感分析做出贡献。

Conclusion: 通过模拟人类语言处理的多视角融合方法，CMV-Fuse能够更有效地整合互补的语言信息，提升方面级情感分析的性能和鲁棒性。

Abstract: Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.

</details>


### [208] [Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis](https://arxiv.org/abs/2512.06681)
*Amartya Hatua*

Main category: cs.CL

TL;DR: GPT-2的情感计算机制研究：早期层负责词汇情感检测，但上下文整合发生在晚期层而非中期层，且采用统一的非模块化机制


<details>
  <summary>Details</summary>
Motivation: 研究GPT-2中情感信息处理的机制，特别是验证关于两阶段情感架构的假设：早期词汇检测和中期上下文整合

Method: 使用系统性激活修补技术，在GPT-2的所有12个层中进行因果分析，测试三种上下文整合假设

Result: 早期层（0-3）确实作为词汇情感检测器，编码稳定的位置特定极性信号；但所有三种中期层上下文整合假设都被证伪，上下文整合主要发生在晚期层（8-11），通过统一的非模块化机制

Conclusion: GPT-2的情感计算与预测的层次模式不同，上下文整合发生在晚期层，这突显了对大型语言模型中上下文整合机制进行进一步实证表征的必要性

Abstract: We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.

</details>


### [209] [Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation](https://arxiv.org/abs/2512.06690)
*Chengbing Wang,Yang Zhang,Wenjie Wang,Xiaoyan Zhao,Fuli Feng,Xiangnan He,Tat-Seng Chua*

Main category: cs.CL

TL;DR: FlyThinker提出了一种"边思考边生成"的高效个性化长文本生成框架，通过并行推理模型生成潜在token级推理来动态指导响应生成，解决了传统方法在长文本生成中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前偏好对齐方法主要针对群体级偏好，忽视了个人用户需求。现有个性化方法如提示定制或微调难以推理隐含偏好，而"先思考后生成"方法在长文本生成中面临挑战：静态一次性推理需要为完整响应生成捕获所有相关信息，学习困难且难以适应内容演化。

Method: FlyThinker采用"边思考边生成"框架，使用独立的推理模型并行生成潜在token级推理，这些推理被融合到生成模型中动态指导响应生成。推理模型仅依赖先前响应而非自身先前输出，保持了不同位置间的训练并行性，所有推理token可在单次前向传递中生成。

Result: 在真实世界基准测试上的广泛实验表明，FlyThinker在保持训练和推理效率的同时，实现了更好的个性化生成效果。

Conclusion: FlyThinker通过并行推理和生成的设计，有效解决了长文本个性化生成中的效率和适应性问题，为个性化语言模型提供了更实用的解决方案。

Abstract: Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent "think-then-generate" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient "think-while-generating" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.

</details>


### [210] [Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models](https://arxiv.org/abs/2512.06711)
*Yulin Huang,Yaxuan Luan,Jinxu Guo,Xiangchen Song,Yuchen Liu*

Main category: cs.CL

TL;DR: 提出了一种参数高效的指令微调方法，通过差分隐私噪声分配与梯度裁剪的协同优化框架，在保护隐私的同时提升训练效率


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型指令微调中的隐私保护和效率问题，传统方法在隐私预算消耗和训练稳定性方面存在不足

Method: 保持主干模型冻结，通过低维投影子空间更新参数，在梯度计算中引入裁剪和自适应噪声分配，结合梯度约束、噪声分配和参数投影的统一框架

Result: 在准确性、隐私预算和参数效率方面优于基线模型，在多样化和不确定数据条件下保持稳定性能

Conclusion: 丰富了差分隐私与参数高效微调的理论整合，展示了在指令任务中的实际适应性，为复杂指令环境下的安全训练提供了可行解决方案

Abstract: This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.

</details>


### [211] ["The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ](https://arxiv.org/abs/2512.06732)
*Aarushi Wagh,Saniya Srivastava*

Main category: cs.CL

TL;DR: ImplicitBBQ基准测试扩展了BBQ基准，通过隐式线索评估大语言模型中的偏见，发现GPT-4o在隐式偏见测试中表现显著下降，揭示了现有显式基准无法检测的隐性偏见。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型偏见的基准主要依赖显式线索（如直接声明宗教、种族、性别等受保护属性），但现实世界互动中偏见往往通过姓名、文化线索或特征等隐式方式体现。这种关键疏忽在公平性评估中造成了显著的盲点。

Method: 作者引入了ImplicitBBQ基准，扩展了问答偏见基准（BBQ），在6个类别中加入了隐式线索的受保护属性。通过该基准评估了GPT-4o模型，比较了其在隐式和显式BBQ提示下的表现差异。

Result: GPT-4o在ImplicitBBQ上的表现与显式BBQ提示相比存在令人担忧的差异："性取向"子类别的准确率下降高达7%，大多数其他类别也出现了一致的下降趋势。这表明当前大语言模型包含现有显式基准无法检测的隐性偏见。

Conclusion: ImplicitBBQ为自然语言处理领域的细致公平性评估提供了重要工具，揭示了仅依赖显式偏见评估的局限性，强调需要更全面的评估方法来检测和缓解语言模型中的隐性偏见。

Abstract: Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the "sexual orientation" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.

</details>


### [212] [A Patient-Doctor-NLP-System to contest inequality for less privileged](https://arxiv.org/abs/2512.06734)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

TL;DR: PDFTEMRA是一个紧凑的Transformer架构，通过模型蒸馏、频域调制、集成学习和随机激活模式等技术降低计算成本，同时保持语言理解性能，适用于资源受限的医疗NLP应用，特别是为印地语使用者和视障用户提供医疗辅助。


<details>
  <summary>Details</summary>
Motivation: 尽管迁移学习加速了大型语言模型的发展，但在资源受限的真实医疗场景中训练和部署这些大型模型仍然具有挑战性。本研究旨在解决视障用户和低资源语言（如印地语）使用者在农村医疗环境中获得有限支持的问题。

Method: 提出了PDFTEMRA（Performant Distilled Frequency Transformer Ensemble Model with Random Activations），这是一个紧凑的基于Transformer的架构，集成了模型蒸馏、频域调制、集成学习和随机激活模式，以降低计算成本同时保持语言理解性能。

Result: 在针对印地语和可访问性场景定制的医疗问答和咨询数据集上进行训练和评估，结果显示PDFTEMRA在显著降低计算需求的同时实现了可比较的性能表现。

Conclusion: PDFTEMRA适用于可访问、包容性、低资源的医疗NLP应用，表明其在资源受限环境中具有实际部署的潜力。

Abstract: Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.

</details>


### [213] [One Word Is Not Enough: Simple Prompts Improve Word Embeddings](https://arxiv.org/abs/2512.06744)
*Rajeev Ranjan*

Main category: cs.CL

TL;DR: 研究发现，通过在单词前添加语义提示（如"meaning: {word}"）可以显著提升文本嵌入模型在单词相似度任务上的表现，无需训练即可超越传统静态嵌入方法。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入模型主要针对句子级应用进行评估，但它们在孤立单词上的表现尚未得到充分理解。本研究旨在探索如何通过简单提示提升文本嵌入模型在单词相似度任务上的性能。

Method: 在7个文本嵌入模型（包括OpenAI、Cohere、Voyage AI等公司的模型）上测试了3个标准单词相似度基准（SimLex-999、WordSim-353、MEN-3000）。通过在单词前添加语义提示（如"meaning: {word}"或"Represent the semantic concept: {word}"）来改进嵌入效果，这是一种零样本技术，无需额外训练。

Result: 语义提示显著提升了单词相似度相关性：在SimLex-999上最多提升+0.29；某些模型在原始单词上完全失败（相关性=0），但通过提示恢复（提升+0.73）。最佳结果：Cohere模型在SimLex-999上达到0.692相关性，OpenAI模型在WordSim-353上达到0.811，在MEN-3000上达到0.855。这些结果超越了传统静态嵌入方法如Word2Vec（0.40）和LexVec（0.48），建立了纯嵌入方法的新SOTA。

Conclusion: 简单的语义提示技术可以显著提升文本嵌入模型在单词相似度任务上的表现，无需训练即可超越传统静态嵌入方法，为单词级语义理解提供了有效的零样本解决方案。

Abstract: Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like "meaning: {word}" or "Represent the semantic concept: {word}" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.

</details>


### [214] [Becoming Experienced Judges: Selective Test-Time Learning for Evaluators](https://arxiv.org/abs/2512.06751)
*Seungyeon Jwa,Daechul Ahn,Reokyoung Kim,Dongyeop Kang,Jonghyun Choi*

Main category: cs.CL

TL;DR: 提出LWE框架，让LLM评估器在推理过程中通过自我反馈改进元提示，无需训练集，并在不一致案例上选择性更新以提升效率和效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估器存在两个问题：1) 独立处理每个案例，无法积累经验；2) 使用固定提示，缺乏样本特定的评估标准。需要让评估器在推理过程中持续改进。

Method: 提出LWE框架：维护一个演化的元提示，生成样本特定的评估指令，并通过自我反馈进行改进。进一步提出Selective LWE，只在自我不一致的案例上更新元提示，提高计算效率。

Result: 在两个成对比较基准测试中，Selective LWE优于强基线方法，证明评估器可以通过简单的选择性更新在顺序测试中持续改进，从最困难的案例中学习最多。

Conclusion: LWE框架使LLM评估器能够在推理过程中通过自我反馈持续改进，选择性更新策略在保持顺序学习优势的同时显著提高成本效益，为自动评估系统提供了新的改进途径。

Abstract: Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.

</details>


### [215] [From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs](https://arxiv.org/abs/2512.06776)
*Yuchuan Tian,Yuchen Liang,Jiacheng Sun,Shuo Zhang,Guangwen Yang,Yingte Shu,Sibo Fang,Tianyu Guo,Kai Han,Chao Xu,Hanting Chen,Xinghao Chen,Yunhe Wang*

Main category: cs.CL

TL;DR: 该论文提出NBDiff方法，将自回归语言模型高效适配为块扩散语言模型，解决了传统方法中的因果性与双向性不匹配问题，在7B参数规模上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 自回归解码的串行性导致吞吐量瓶颈，而从头训练扩散语言模型成本高昂且浪费现有自回归模型的知识。现有适配方法未能解决自回归因果性与块扩散双向性之间的根本性不匹配问题。

Method: 将自回归视为块大小为1的块扩散模型，设计上下文因果注意力掩码（上下文因果，仅在活动块内双向），采用高效并行适配流程，使用辅助自回归损失最大化数据利用并保留预训练知识，逐步增加生成块大小。

Result: NBDiff-7B（基础和指令版本）继承了长上下文建模和推理能力，在7B类扩散语言模型中达到最先进性能，在通用知识、数学和代码基准测试中相比强基线有显著提升。

Conclusion: 原则性的自回归到块扩散适配是一种有效且计算高效的方法，可作为从头训练扩散语言模型的替代方案。

Abstract: Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.

</details>


### [216] [LLM4SFC: Sequential Function Chart Generation via Large Language Models](https://arxiv.org/abs/2512.06787)
*Ofek Glick,Vladimir Tchuiev,Marah Ghoummaid,Michal Moshkovitz,Dotan Di-Castro*

Main category: cs.CL

TL;DR: LLM4SFC框架首次实现从自然语言描述生成可执行的顺序功能图(SFC)，解决了图形化PLC编程语言生成的挑战，在真实工业数据集上达到75%-94%的成功率。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型已用于生成文本化PLC编程语言（如结构化文本），但IEC 61131-3标准的图形化语言（如顺序功能图）仍未被充分探索。生成SFC面临图形特性和嵌入式ST代码的挑战，传统方法常产生不可执行或与工业工具链不兼容的代码。

Method: LLM4SFC框架包含三个核心组件：1) 精简结构化表示，捕捉SFC拓扑结构和嵌入式ST代码，减少文本冗余；2) 微调和少样本检索增强生成(RAG)，使模型符合SFC编程规范；3) 结构化生成方法，实时修剪非法令牌，确保符合SFC文本格式。

Result: 在自动化制造项目的真实SFC数据集上评估，使用开源和专有大语言模型，LLM4SFC可靠地生成语法有效的SFC程序，成功桥接图形和文本PLC语言，生成成功率达到75%-94%。

Conclusion: LLM4SFC是首个从自然语言生成可执行SFC的框架，有效解决了图形化PLC编程语言生成的挑战，为自动化工业编程铺平了道路。

Abstract: While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.

</details>


### [217] [Large Language Model-Based Generation of Discharge Summaries](https://arxiv.org/abs/2512.06812)
*Tiago Rodrigues,Carla Teixeira Lopes*

Main category: cs.CL

TL;DR: 本文研究了使用五种大型语言模型（包括开源和专有模型）自动生成出院小结的任务，发现专有模型（特别是Gemini）在单样本提示下表现最佳，而开源模型虽然也有潜力但存在幻觉和重复信息的问题。


<details>
  <summary>Details</summary>
Motivation: 出院小结是医疗专业人员编写的详细记录患者就诊情况的文档，包含对患者护理至关重要的丰富信息。自动化生成出院小结可以显著减少医疗专业人员的工作量，减少错误，并确保关键患者信息易于获取和可操作。

Method: 本研究探索了五种大型语言模型在该任务上的应用，包括开源模型（Mistral、Llama 2）和专有系统（GPT-3、GPT-4、Gemini 1.5 Pro），利用MIMIC-III的摘要和笔记数据。评估方法包括精确匹配、软重叠和无需参考指标的评估。

Result: 结果显示专有模型，特别是使用单样本提示的Gemini，表现优于其他模型，生成的摘要与黄金标准摘要的相似度最高。开源模型虽然也有潜力（特别是微调后的Mistral），但在性能上落后，经常出现幻觉和重复信息的问题。临床专家的人工评估证实了专有模型生成的摘要具有实际效用。

Conclusion: 尽管存在幻觉和缺失信息等挑战，研究结果表明大型语言模型，特别是专有模型，只要确保数据隐私，就是自动生成出院小结的有希望的候选方案。

Abstract: Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.

</details>


### [218] [CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation](https://arxiv.org/abs/2512.06814)
*Dibyanayan Bandyopadhyay,Soham Bhattacharjee,Mohammed Hasanuzzaman,Asif Ekbal*

Main category: cs.CL

TL;DR: CAuSE是一个通过因果抽象生成忠实自然语言解释的框架，适用于预训练多模态分类器，通过交换干预训练，在多个数据集和模型上表现出色。


<details>
  <summary>Details</summary>
Motivation: 多模态分类器通常被视为黑盒模型，现有解释方法中自然语言解释（NLEs）最直观易用，但需要确保这些解释能忠实反映分类器的内部决策行为（忠实性）。

Method: 提出CAuSE框架，通过交换干预训练，为预训练多模态分类器生成忠实的自然语言解释，形成底层分类器的因果抽象。

Result: CAuSE在多个数据集和模型上表现出良好的泛化能力，在重新设计的因果忠实性度量标准上优于其他方法，定性分析也支持其优势。

Conclusion: CAuSE是一个有效的框架，能够为多模态分类器生成忠实的自然语言解释，并通过详细的错误分析指出了其失败案例，代码已开源。

Abstract: Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE

</details>


### [219] [AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices](https://arxiv.org/abs/2512.06848)
*Sepyan Purnama Kristanto,Lutfi Hakim,Hermansyah*

Main category: cs.CL

TL;DR: AquaFusionNet：一个轻量级跨模态框架，统一显微镜成像和物理化学传感器数据，用于实时监测小型饮用水系统中的微生物污染，在边缘设备上实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入地区的小型饮用水系统中微生物污染波动迅速，现有监测工具只能捕捉部分信息。显微镜成像提供微生物级可见性，物理化学传感器显示短期水质变化，但操作员需要分别解读这些数据流，导致实时决策不可靠。

Method: 提出AquaFusionNet框架，通过专门为低功耗硬件设计的门控交叉注意力机制，学习微生物外观与传感器动态之间的统计依赖关系。使用AquaMicro12K数据集（包含12,846张饮用水环境标注的显微图像）进行训练。

Result: 在印度尼西亚东爪哇7个设施部署6个月，处理184万帧图像，污染事件检测达到94.8% mAP@0.5，异常预测准确率96.3%，在Jetson Nano上功耗仅4.8W。相比代表性轻量级检测器，在相同或更低功耗下提供更高精度。

Conclusion: 跨模态耦合减少了单模态检测器常见的故障模式，特别是在污垢、浊度峰值和不一致照明条件下。所有模型、数据和硬件设计已开源，促进分散式水安全基础设施的复制和适应。

Abstract: Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.

</details>


### [220] [Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs](https://arxiv.org/abs/2512.06869)
*Wanyang Hong,Zhaoning Zhang,Yi Chen,Libo Zhang,Baihui Liu,Linbo Qiao,Zhiliang Tian,Dongsheng Li*

Main category: cs.CL

TL;DR: Rhea框架通过角色感知启发式情景注意力机制，将对话历史解耦为指令记忆和情景记忆两个独立模块，有效缓解大语言模型在多轮对话中的累积上下文衰减问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在单轮任务中表现出色，但在多轮对话中性能会逐渐下降。研究者将这种现象定义为"累积上下文衰减" - 由注意力污染、稀释和漂移导致的上下文完整性渐进式退化。

Method: 提出Rhea（角色感知启发式情景注意力）框架，将对话历史解耦为两个功能独立的记忆模块：1）指令记忆（IM）：通过结构优先级机制持久存储高保真全局约束；2）情景记忆（EM）：通过非对称噪声控制和启发式上下文检索动态管理用户-模型交互。在推理过程中，Rhea应用优先级注意力构建高信噪比上下文。

Result: 在多个多轮对话基准测试（包括MT-Eval和Long-MT-Bench+）上，Rhea缓解了性能衰减，在10分制上提高了1.04分（相对于强基线有16%的相对增益）。此外，Rhea在长时程交互中保持了接近完美的指令保真度（IAR > 8.1）。

Conclusion: Rhea为构建更精确、指令一致的对话式大语言模型提供了一个原则性和有效的框架，能够显著改善多轮对话中的上下文管理问题。

Abstract: Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.

</details>


### [221] [An Analysis of Large Language Models for Simulating User Responses in Surveys](https://arxiv.org/abs/2512.06874)
*Ziyun Yu,Yiru Zhou,Chen Zhao,Hongyi Wen*

Main category: cs.CL

TL;DR: 本文研究使用大语言模型模拟用户意见的局限性，发现即使采用CLAIMSIM方法增强多样性，LLMs仍难以准确模拟不同人口统计特征用户的真实观点。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（特别是经过RLHF训练的）存在偏向主流观点的偏见，这引发了对它们能否代表不同人口统计和文化背景用户的担忧。研究者希望探索LLMs在模拟跨领域调查问卷回答方面的能力。

Method: 研究采用直接提示和思维链提示两种方法，并提出CLAIMSIM方法（观点多样化方法），该方法从LLMs的参数知识中提取观点作为上下文输入。在调查问卷回答任务上进行实验评估。

Result: 实验表明，虽然CLAIMSIM能产生更多样化的回答，但两种方法都难以准确模拟用户。分析发现两个关键局限：1）LLMs倾向于在不同人口统计特征间保持固定观点，生成单一视角的声明；2）当面对相互冲突的观点时，LLMs难以推理人口统计特征间的细微差异，限制了它们针对特定用户特征调整回答的能力。

Conclusion: 当前的大语言模型在模拟多样化用户观点方面存在显著局限性，特别是在处理人口统计特征的细微差异和生成多视角观点方面。需要进一步研究来改进LLMs在模拟真实用户意见方面的能力。

Abstract: Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.

</details>


### [222] [Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles](https://arxiv.org/abs/2512.06919)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla*

Main category: cs.CL

TL;DR: 开发自动化方法从PRO-CTCAE库中选择最小但全面的症状子集，基于历史安全数据和MedDRA语义分析，平衡信号覆盖与患者负担


<details>
  <summary>Details</summary>
Motivation: PRO-CTCAE系统包含大量症状项，传统基于预期毒性谱的选择方法存在挑战：选择过多项会增加患者负担降低依从性，选择过少可能遗漏重要安全信号，需要更客观、可重复的方法来平衡覆盖范围与患者负担

Method: 1) 将PRO-CTCAE症状项映射到对应的MedDRA首选术语；2) 使用Safeterm高维语义空间编码临床和上下文多样性；3) 基于历史不良事件数据计算每个候选项的相关性和发生率；4) 应用谱分析到组合效用和多样性矩阵，识别正交医学概念集；5) 按重要性排序症状并基于解释信息建议截断点

Result: 该方法已作为Safeterm试验安全应用的一部分实现，通过模拟和实际肿瘤学案例研究进行评估，证明能够有效平衡信号覆盖与患者负担

Conclusion: 这种自动化方法通过利用MedDRA语义和历史数据，为PRO-CTCAE设计提供了客观、可重复的解决方案，能够平衡安全信号覆盖与患者报告负担，简化临床试验设计流程

Abstract: The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.

</details>


### [223] [Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI](https://arxiv.org/abs/2512.06922)
*George Mikros*

Main category: cs.CL

TL;DR: LLMs在司法语言学中既是分析工具又是挑战来源：可用于大规模语料分析和作者识别，但会通过风格模仿、作者混淆和生成合成文本破坏语言特征的独特性。当前AI文本检测方法存在局限性，司法语言学需要方法重构以保持科学可信度和法律可采纳性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对司法语言学构成双重挑战：一方面可作为强大的分析工具，另一方面通过风格模仿、作者混淆和合成文本生成破坏了语言特征独特性这一基本假设。当前AI文本检测技术存在高误报率（特别是对非母语英语写作者）和对抗策略脆弱性等问题，这些不确定性在法律可采纳性标准下引发担忧。

Method: 文章分析了LLMs在司法语言学中的双重角色，评估了当前AI文本检测技术（基于分类器、风格计量学和水印方法）的局限性，并提出了方法重构方案。研究基于现有风格计量学研究，探讨LLMs模拟人类写作风格的能力及其可检测差异。

Result: 研究发现LLMs能够近似表面风格特征，但与人类写作者存在可检测差异。当前AI文本检测技术面临重大限制：对非母语英语写作者的高误报率，以及对同形异义替换等对抗策略的脆弱性。这些不确定性在法律可采纳性标准下存在问题。

Conclusion: 司法语言学需要进行方法重构以保持科学可信度和法律可采纳性。建议的调整包括：混合人机工作流程、超越二元分类的可解释检测范式，以及测量不同人群错误和偏见的验证机制。该领域的核心见解（语言揭示其生产者信息）仍然有效，但必须适应日益复杂的人类和机器作者链。

Abstract: Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.

</details>


### [224] [XAM: Interactive Explainability for Authorship Attribution Models](https://arxiv.org/abs/2512.06924)
*Milad Alshomary,Anisha Bhatnagar,Peter Zeng,Smaranda Muresan,Owen Rambow,Kathleen McKeown*

Main category: cs.CL

TL;DR: IXAM是一个交互式可解释性框架，用于探索作者归属模型的嵌入空间，并通过多粒度写作风格特征解释模型预测


<details>
  <summary>Details</summary>
Motivation: 现有的作者归属模型缺乏交互式可解释性工具，用户难以理解模型如何基于写作风格做出预测决策

Method: 开发交互式框架，允许用户探索模型嵌入空间，构建多粒度写作风格特征作为解释，并与预定义风格解释进行对比

Result: 通过用户评估证明，该框架相比预定义风格解释具有更高价值，能更好地帮助用户理解模型预测机制

Conclusion: IXAM框架为作者归属模型提供了有效的交互式可解释性工具，增强了用户对模型决策过程的理解和信任

Abstract: We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.

</details>


### [225] [Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation](https://arxiv.org/abs/2512.06938)
*Ivanhoé Botcazou,Tassadit Amghar,Sylvain Lamprier,Frédéric Saubion*

Main category: cs.CL

TL;DR: 本文提出Progress Ratio Embeddings(PRE)方法，通过连续的三角函数信号实现稳定的文本生成长度控制，相比现有方法在超出训练分布时表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 现代神经语言模型在文本生成方面取得了高精度，但对生成长度的精确控制仍然不足。现有基于反向位置嵌入(RPE)的方法在超出训练分布时存在不稳定性问题。

Method: 提出Progress Ratio Embeddings(PRE)方法，使用连续的三角函数"不耐烦信号"嵌入，与标准Transformer架构无缝集成，提供稳定的长度控制而不降低文本质量。

Result: PRE方法在两个广泛使用的新闻摘要基准测试中验证了其有效性，能够稳定控制生成长度，且在标准评估指标下不降低文本准确性，还能很好地泛化到未见过的目标长度。

Conclusion: PRE方法为文本生成提供了稳健的长度控制机制，解决了现有方法在超出训练分布时的稳定性问题，同时保持了生成质量。

Abstract: Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.

</details>


### [226] [Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models](https://arxiv.org/abs/2512.06991)
*Jing Jie Tan,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum,Anissa Mokraoui,Shih-Yu Lo*

Main category: cs.CL

TL;DR: PICEPR是一种基于心理学内容嵌入的人格识别新算法，通过"系列提示"方法结合内容和嵌入两条管道，利用模块化解码器LLM进行内容生成和人格特征提取，在人格识别任务上实现了5-15%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种自然语言处理任务中表现出色，但在人格识别领域仍有改进空间。研究者希望开发一种专门针对人格识别的算法，结合心理学知识和LLM的能力，提升人格特征提取和识别的准确性。

Method: 提出PICEPR算法，采用"Prompting-in-a-Series"方法，包含两条管道：(a)内容管道：利用模块化解码器LLM总结或生成内容；(b)嵌入管道：作为人格特征提取器和人格丰富内容生成器。同时对比了闭源模型(gpt4o、gemini)和开源模型(mistral)的生成质量。

Result: PICEPR算法在人格识别任务上实现了新的最先进性能，相比现有方法有5-15%的改进。通过实验验证了算法的合理性，并比较了不同模型生成内容的质量。

Conclusion: PICEPR算法成功地将心理学知识与LLM能力相结合，通过创新的"系列提示"方法显著提升了人格识别的性能，为LLM在人格分析领域的应用提供了新的思路和工具。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel "Prompting-in-a-Series" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \textit{gpt4o} from OpenAI and \textit{gemini} from Google, along with open-source models like \textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.

</details>


### [227] [FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations](https://arxiv.org/abs/2512.07015)
*Mayank Ravishankara*

Main category: cs.CL

TL;DR: FVA-RAG框架通过从归纳验证转向演绎证伪，使用对抗性检索策略生成"杀死查询"来寻找矛盾证据，有效减少检索奉承导致的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统存在"检索奉承"漏洞：当查询基于错误前提或常见误解时，向量检索器倾向于获取符合用户偏见的文档而非客观事实，导致模型"带引用的幻觉"。

Method: 提出FVA-RAG框架，将检索范式从归纳验证（寻求支持）转向演绎证伪（寻求反驳）。部署独立的对抗性检索策略，主动生成"杀死查询"来寻找矛盾证据，并引入双重验证机制，明确权衡草稿答案与"反上下文"。

Result: 在常见误解数据集上的初步实验表明，FVA-RAG相比标准RAG基线显著提高了对奉承幻觉的鲁棒性，有效充当事实生成的推理时"红队"。

Conclusion: FVA-RAG通过证伪导向的检索和对抗性验证，为减少RAG系统中的检索奉承和幻觉问题提供了有效解决方案。

Abstract: Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.

</details>


### [228] [Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models](https://arxiv.org/abs/2512.07059)
*Richard Young*

Main category: cs.CL

TL;DR: 使用TEMPEST多轮攻击框架评估10个前沿大语言模型的安全性，发现当前对齐技术对自适应多轮攻击存在根本性脆弱性，模型规模不能预测对抗鲁棒性，而思维模式可作为有效的安全增强手段。


<details>
  <summary>Details</summary>
Motivation: 尽管在安全对齐方面投入了大量资源，但大型语言模型对复杂多轮对抗攻击的脆弱性仍然缺乏充分研究，且不清楚模型规模或推理模式是否会影响鲁棒性。

Method: 采用TEMPEST多轮攻击框架评估来自8个供应商的10个前沿模型，针对1000种有害行为生成超过97,000个API查询，通过独立安全分类器进行自动化评估。

Result: 结果显示脆弱性存在显著差异：6个模型的攻击成功率(ASR)达到96%-100%，4个模型表现出有意义的抵抗能力(ASR 42%-78%)；在相同架构上启用扩展推理可将ASR从97%降至42%。

Conclusion: 当前对齐技术对自适应多轮攻击存在根本性脆弱性，模型规模不能预测对抗鲁棒性，而审慎推理（思维模式）是一个有前景的防御方向。

Abstract: Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.

</details>


### [229] [Do Large Language Models Truly Understand Cross-cultural Differences?](https://arxiv.org/abs/2512.07075)
*Shiwei Guo,Sihang Jiang,Qianxi He,Yanghua Xiao,Jiaqing Liang,Bi Yude,Minggui He,Shimin Tao,Li Zhang*

Main category: cs.CL

TL;DR: SAGE是一个基于场景的基准测试，通过跨文化核心概念对齐和生成式任务设计来评估大语言模型的跨文化理解和推理能力，包含4530个测试项，覆盖9个维度和15个现实场景。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLMs跨文化理解能力的基准存在三个关键限制：缺乏上下文场景、跨文化概念映射不足、深层文化推理能力有限。需要一个新的基准来全面评估LLMs是否真正具备跨文化理解能力。

Method: 基于文化理论将跨文化能力分为9个维度，筛选210个核心概念，在15个具体现实场景中构建4530个测试项，遵循既定的项目设计原则，支持数据集持续扩展。

Result: SAGE数据集证实可迁移到其他语言，揭示了模型在维度和场景上的弱点，暴露了跨文化推理的系统性限制。LLMs距离真正的细致跨文化理解仍有差距。

Conclusion: 虽然已有进展，但大语言模型要达到真正细致的跨文化理解仍有距离。SAGE基准为评估和提升LLMs的跨文化能力提供了系统框架。

Abstract: In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.

</details>


### [230] [Leveraging KV Similarity for Online Structured Pruning in LLMs](https://arxiv.org/abs/2512.07090)
*Jungmin Lee,Gwangeun Byeon,Yulhwa Kim,Seokin Hong*

Main category: cs.CL

TL;DR: Token Filtering是一种轻量级在线结构化剪枝技术，通过测量token冗余度并跳过冗余注意力计算来加速LLM推理，无需校准数据。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法依赖离线校准数据，在不同输入间泛化能力差，导致不稳定。需要一种无需校准数据、在推理过程中直接做出剪枝决策的方法。

Method: 提出Token Filtering技术：1) 通过联合键值相似度测量token冗余度；2) 设计方差感知融合策略，自适应加权不同注意力头的键值相似度；3) 在推理过程中跳过冗余注意力计算，减少计算成本。

Result: 在LLaMA-2 (7B/13B)、LLaMA-3 (8B)和Mistral (7B)上的实验表明，Token Filtering在常识推理基准上保持准确性，在MMLU等挑战性任务上即使剪枝50%仍保持强性能，优于现有结构化剪枝方法。

Conclusion: Token Filtering提供了一种无需校准数据的稳定在线剪枝方法，通过测量token冗余度和自适应融合策略，在保持模型性能的同时显著减少推理成本。

Abstract: Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.

</details>


### [231] [DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning](https://arxiv.org/abs/2512.07132)
*Nithin Sivakumaran,Justin Chih-Yao Chen,David Wan,Yue Zhang,Jaehong Yoon,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: DART是一个多智能体框架，通过视觉智能体之间的辩论分歧来识别有用的视觉工具，利用工具信息促进讨论并选择最佳答案，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 专业视觉工具可以为大语言模型或视觉语言模型提供专家知识，但确定何时调用哪些工具具有挑战性。现有方法在工具调用时机和选择上存在困难。

Method: 提出DART多智能体框架：1）利用多个辩论视觉智能体之间的分歧识别有用的视觉工具；2）工具引入新信息并提供工具对齐的同意分数；3）使用聚合智能体基于智能体输出和工具信息选择最佳答案。

Result: 在四个不同基准测试中，DART优于多智能体辩论和单智能体工具调用框架：在A-OKVQA上比次优基线（带法官模型的多智能体辩论）提升3.4%，在MMMU上提升2.4%。在M3D医疗数据集上比其他基线提升1.3%。文本重叠度分析显示DART讨论更丰富，工具调用分布显示多样化工具被可靠使用。

Conclusion: DART通过利用多智能体辩论分歧来指导工具调用，有效解决了视觉工具选择问题，在多个领域表现出优越性能，并能良好适应新工具。

Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.

</details>


### [232] [GUMBridge: a Corpus for Varieties of Bridging Anaphora](https://arxiv.org/abs/2512.07134)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: GUMBridge是一个新的英语桥接指代资源，涵盖16种不同文体，提供桥接现象广泛覆盖和细粒度子类型分类标注，评估显示桥接消解和子类型分类对当代LLMs仍是挑战性任务。


<details>
  <summary>Details</summary>
Motivation: 现有英语桥接指代资源大多规模小、现象覆盖有限、文体覆盖不足，需要更全面、多样化的桥接指代标注资源来支持自然语言处理研究。

Method: 构建GUMBridge资源，包含16种不同文体的英语文本，提供桥接现象的广泛覆盖和细粒度子类型分类标注，并进行标注质量评估。

Result: 创建了GUMBridge资源，涵盖16种文体，提供了桥接现象的全面覆盖和详细子类型分类。使用开源和闭源LLMs在三个任务上的基线性能评估显示，桥接消解和子类型分类对当代LLMs仍是困难任务。

Conclusion: GUMBridge为桥接指代研究提供了更全面、多样化的资源，填补了现有资源的不足，同时验证了桥接消解和分类任务对当前LLMs的挑战性，为未来研究提供了重要基准。

Abstract: Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in "There is 'a house'. 'The door' is red," where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.

</details>


### [233] [MASim: Multilingual Agent-Based Simulation for Social Science](https://arxiv.org/abs/2512.07195)
*Xuan Zhang,Wenxuan Zhang,Anxu Wang,See-Kiong Ng,Yang Deng*

Main category: cs.CL

TL;DR: MASim是首个支持多语言智能体交互的模拟框架，用于研究跨语言社会行为，包含公共舆论建模和媒体影响力分析功能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体角色扮演模拟大多是单语言的，无法模拟现实社会中重要的跨语言互动，限制了计算社会科学的研究范围。

Method: 开发了MASim多语言智能体模拟框架，支持具有不同社会语言特征的生成式智能体进行多轮交互。构建了MAPS基准测试，结合全球人口分布的调查问题和人口统计角色。

Result: 实验显示MASim能够复现社会文化现象，校准、敏感性、一致性和文化案例研究表明该框架有效，突出了多语言模拟对于可扩展、可控计算社会科学的重要性。

Conclusion: MASim填补了多语言智能体模拟的空白，为研究跨语言社会互动提供了有效工具，推动了计算社会科学的发展。

Abstract: Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.

</details>


### [234] [NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models](https://arxiv.org/abs/2512.07218)
*Feng Liang,Weixin Zeng,Runhao Zhao,Xiang Zhao*

Main category: cs.CL

TL;DR: NeSTR是一个神经符号时序推理框架，通过结合符号表示和混合反思推理来增强大语言模型的时序敏感性，在零样本设置下显著提升时序推理性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在时序推理方面存在挑战，特别是处理复杂时序约束时。现有方法要么符号方法未能充分利用LLM的推理能力，要么反思方法缺乏结构化时序表示导致不一致或幻觉推理。即使有时序上下文可用，LLM仍可能误解或误用时序信息。

Method: 提出神经符号时序推理（NeSTR）框架，集成结构化符号表示与混合反思推理。通过符号编码保留显式时序关系，通过验证强制执行逻辑一致性，使用溯因反思纠正错误推理。

Result: 在多样化的时序问答基准测试中，NeSTR实现了优越的零样本性能，无需任何微调就能持续改进时序推理，展示了神经符号集成在增强大语言模型时序理解方面的优势。

Conclusion: NeSTR框架通过神经符号集成有效解决了LLM在时序推理方面的局限性，为增强大语言模型的时序敏感性提供了一种有前景的方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.

</details>


### [235] [Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection](https://arxiv.org/abs/2512.07246)
*Mengqi Wang,Jianwei Wang,Qing Liu,Xiwei Xu,Zhenchang Xing,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 提出TreeED和ForestED框架，使用LLM诱导决策树进行错误检测，提升可解释性和鲁棒性，相比最佳基线平均F1分数提高16.1%


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的错误检测方法存在两个主要问题：1) 依赖黑盒决策过程，缺乏可解释性；2) 对提示词敏感，输出不一致，缺乏鲁棒性。需要既能利用LLM知识又能提供透明决策过程的解决方案。

Method: 提出LLM-as-an-inducer框架：1) TreeED使用LLM根据数据上下文、决策树规范和输出要求诱导决策树骨架，包含规则节点（简单验证）、GNN节点（复杂模式）和叶子节点（最终决策）；2) ForestED通过不确定性采样获取多个行子集，为每个子集构建决策树，使用基于期望最大化的算法联合估计树可靠性和优化共识预测。

Result: 实验表明该方法准确、可解释且鲁棒，平均F1分数比最佳基线提高16.1%。

Conclusion: 提出的LLM-as-an-inducer框架通过诱导决策树解决了现有LLM-as-a-labeler方法的可解释性和鲁棒性问题，为表格数据错误检测提供了更可靠的解决方案。

Abstract: Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.

</details>


### [236] [TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation](https://arxiv.org/abs/2512.07265)
*Bhavana Akkiraju,Srihari Bandarupalli,Swathi Sambangi,Vasavi Ravuri,R Vijaya Saraswathi,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 该研究为泰卢固语-英语语音翻译创建了高质量基准，比较了级联与端到端架构，发现端到端系统在低资源场景下具有竞争力，并评估了多种自动评估指标的有效性。


<details>
  <summary>Details</summary>
Motivation: 泰卢固语作为拥有超过8000万使用者的形态丰富语言，其语音翻译研究严重不足。研究旨在填补这一空白，为泰卢固语-英语语音翻译建立高质量基准。

Method: 使用46小时手动验证的CSTD语料库数据（30h/8h/8h训练/开发/测试分割），系统比较级联架构（IndicWhisper + IndicMT）与端到端架构（微调SeamlessM4T模型），并评估BLEU、METEOR、ChrF++、ROUGE-L、TER和BERTScore等指标与人工评估的一致性。

Result: 级联架构因使用大量泰卢固语特定训练数据而表现最佳，但微调的端到端模型使用显著更少的泰卢固语数据仍表现出色。研究发现传统指标比BERTScore能更好地区分泰卢固语-英语翻译质量。

Conclusion: 研究表明，通过仔细的超参数调整和足够的平行数据（可能少于100小时），端到端系统在低资源场景下可以达到与级联方法相当的性能。研究提供了可复现的基准、端到端系统在低资源场景下竞争力的实证证据，以及形态复杂语言对自动评估的实用指导。

Abstract: Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.

</details>


### [237] [Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data](https://arxiv.org/abs/2512.07277)
*Srihari Bandarupalli,Bhavana Akkiraju,Charan Devarakonda,Vamsiraghusimha Narsinga,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 通过跨语言持续预训练方法，使用无标注语音数据和形态感知分词，为低资源波斯-阿拉伯语系语言开发了高效的自动语音识别模型，在参数量减少5倍的情况下达到可比性能。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的自动语音识别面临标注数据稀缺和计算资源不足的双重限制，现有大模型方法不适用于资源有限的语言环境。

Method: 构建3000小时多语言无标注语料库，采用跨语言持续预训练策略，结合形态感知分词技术，开发了300M参数的模型。

Result: 模型在波斯语上超越Whisper Large v3（1.5B参数），在阿拉伯语和乌尔都语上取得竞争性结果，参数量减少5倍且标注数据需求大幅降低。

Conclusion: ASR质量并非主要依赖模型规模，数据相关性和策略性预训练对低资源场景更为关键，为边缘化语言提供了不依赖大规模计算基础设施的实用路径。

Abstract: Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.

</details>


### [238] [Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models](https://arxiv.org/abs/2512.07288)
*Tomoki Doi,Masaru Isonuma,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 该研究探讨了如何通过训练提升大语言模型自我解释的忠实度，并验证了这种改进在不同解释风格间的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然能根据用户指令生成不同风格的自我解释，但这些解释往往缺乏忠实度（即不能真实反映模型的决策过程）。现有研究对如何提升忠实度以及不同解释风格间的改进是否具有泛化性这两个问题尚未深入探索。

Method: 研究使用三个分类任务和三种解释风格。首先通过特征归因方法构建可能具有忠实度的单词约束解释（伪忠实自我解释），然后对指令调优模型进行持续学习训练。

Result: 实验表明：1）训练能提升所有分类任务和解释风格的自我解释忠实度；2）这种改进在多词设置和未见任务中显示出泛化迹象；3）三种风格间存在一致的跨风格泛化，表明训练可能促进忠实自我解释能力的广泛提升。

Conclusion: 通过基于伪忠实自我解释的持续学习训练，可以有效提升大语言模型自我解释的忠实度，且这种改进在不同解释风格间具有泛化性，为开发更可靠的模型解释方法提供了新途径。

Abstract: Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.

</details>


### [239] [Multilingual corpora for the study of new concepts in the social sciences and humanities:](https://arxiv.org/abs/2512.07367)
*Revekka Kyriakoglou,Anna Pappa*

Main category: cs.CL

TL;DR: 本文提出了一种构建多语言语料库的混合方法，用于支持人文社会科学中新兴概念的研究，以"非技术创新"为例。该方法结合公司网站文本和年度报告，通过自动化处理流程创建可用于机器学习的标注数据集。


<details>
  <summary>Details</summary>
Motivation: 为支持人文社会科学中新兴概念的研究，需要构建专门的多语言语料库。传统方法难以捕捉新兴概念的词汇变异性，且缺乏适合自然语言处理应用的结构化数据资源。

Method: 采用混合方法构建语料库：1) 从公司网站自动提取并清洗法英双语文本；2) 收集年度报告并按文档标准自动筛选。处理流程包括语言检测、内容过滤、相关片段提取和结构化元数据标注。从初始语料库创建英文数据集，为专家词典中的每个术语提取包含前后各两句的上下文块，并标注相关主题类别。

Result: 构建了一个可重复且可扩展的资源，既可用于分析新兴概念周围的词汇变异性，也可生成专门用于自然语言处理应用的数据集。创建了适合监督分类任务的标注数据集，每个术语出现都带有上下文和主题类别标注。

Conclusion: 该方法为研究人文社会科学中的新兴概念提供了有效的语料库构建框架，通过混合数据源和自动化处理流程，创建了既适合概念分析又适合机器学习应用的多功能资源。

Abstract: This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.

</details>


### [240] [Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning](https://arxiv.org/abs/2512.07454)
*Amir Mohammad Akhlaghi,Amirhossein Shabani,Mostafa Abdolmaleki,Saeed Reza Kheradpisheh*

Main category: cs.CL

TL;DR: 波斯-Phi是一个3.8B参数模型，通过创新的课程学习流程将英语单语模型Phi-3 Mini有效适配到波斯语，为低资源语言提供高效的多语言扩展方案


<details>
  <summary>Details</summary>
Motivation: 当前AI民主化受到训练低资源语言大语言模型所需巨大计算成本的阻碍，需要为波斯语等资源有限语言开发高效的多语言能力扩展方案

Method: 采用资源高效的课程学习流程：首先使用双语叙事（Tiny Stories）进行"预热"阶段以对齐嵌入，然后通过参数高效微调（PEFT）进行持续预训练和指令调优

Result: 波斯-Phi在HuggingFace的Open Persian LLM Leaderboard上取得了有竞争力的结果，证明了紧凑模型也能具备强大的多语言能力

Conclusion: 该研究提供了一个经过验证的可扩展框架，能够以最小硬件资源将最先进的大语言模型扩展到资源不足的语言，挑战了强大多语言能力需要大规模模型或多语言基线的假设

Abstract: The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique "warm-up" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.

</details>


### [241] [Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning](https://arxiv.org/abs/2512.07461)
*Tong Wu,Yang Liu,Jun Bai,Zixia Jia,Shuyi Zhang,Ziyong Lin,Yanting Wang,Song-Chun Zhu,Zilong Zheng*

Main category: cs.CL

TL;DR: NPR是一个无需教师指导的框架，让大语言模型通过自我进化获得真正的并行推理能力，实现从顺序模拟到原生并行认知的转变，在8个推理基准上性能提升达24.5%，推理速度提升4.6倍。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型推理通常是顺序模拟的，缺乏真正的并行认知能力。研究者希望开发一个无需外部监督的框架，让LLMs能够自我进化出真正的并行推理能力，实现更高效、可扩展的智能体推理。

Method: NPR框架包含三个关键创新：1）自我蒸馏的渐进训练范式，从"冷启动"格式发现过渡到严格的拓扑约束；2）并行感知策略优化算法，直接在执行图中优化分支策略；3）重构SGLang内存管理和流程控制的NPR引擎，支持稳定的大规模并行强化学习训练。

Result: 在8个推理基准测试中，基于Qwen3-4B训练的NPR实现了最高24.5%的性能提升和最高4.6倍的推理加速。与之前基线经常回退到自回归解码不同，NPR展示了100%真正的并行执行。

Conclusion: NPR建立了一个新的标准，实现了自我进化、高效且可扩展的智能体推理，为大语言模型从顺序模拟转向原生并行认知提供了有效框架。

Abstract: We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.

</details>


### [242] [SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG](https://arxiv.org/abs/2512.07515)
*Pengqian Lu,Jie Lu,Anjin Liu,Guangquan Zhang*

Main category: cs.CL

TL;DR: SPAD是一种新的RAG幻觉检测方法，通过将token概率分解为7个来源并聚合POS标签来识别异常，实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有方法将RAG中的幻觉归因于内部知识（FFN）与检索上下文之间的二元冲突，这种视角不完整，忽略了生成过程中其他组件的影响，如用户查询、先前生成的token、当前token本身以及最后的LayerNorm调整

Method: SPAD方法首先将每个token的概率数学归因到7个不同来源：查询、RAG、过去token、当前token、FFN、最终LayerNorm和初始嵌入，量化每个来源对当前token生成的贡献；然后通过POS标签聚合这些分数，量化不同组件如何驱动特定语言类别；通过识别异常（如名词依赖最终LayerNorm）来有效检测幻觉

Result: 大量实验表明，SPAD实现了最先进的性能

Conclusion: SPAD通过更全面的概率归因框架，超越了传统的二元冲突视角，能够更有效地检测RAG中的幻觉

Abstract: Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance

</details>


### [243] [LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings](https://arxiv.org/abs/2512.07522)
*Sebastian Sztwiertnia,Felix Friedrich,Kristian Kersting,Patrick Schramowski,Björn Deiseroth*

Main category: cs.CL

TL;DR: LIME是一种通过元数据增强语言模型预训练的方法，使用语法、语义和上下文属性的元数据来丰富词元嵌入，显著提升预训练效率和语言建模能力。


<details>
  <summary>Details</summary>
Motivation: 当前仅解码器语言模型的预训练依赖大量高质量数据，但这类数据的可用性已接近极限。虽然元数据常用于创建和整理数据集，但其作为直接训练信号的潜力尚未充分探索。

Method: 提出LIME方法，将捕获语法、语义和上下文属性的元数据嵌入到词元嵌入中。还开发了LIME+1变体，使用移位元数据来指导词元生成。

Result: LIME使模型对训练数据分布的适应速度提升高达56%，仅增加0.01%的参数且计算开销可忽略。改善了分词效果，显著增强了语言建模能力和生成任务性能，这些优势在500M到2B规模的模型中均存在。LIME+1将推理性能提升高达38%，算术准确率提升高达35%。

Conclusion: 元数据作为直接训练信号具有巨大潜力，LIME方法能显著提升预训练效率和语言模型性能，同时LIME+1变体还能指导生成过程，为语言模型预训练提供了新的有效途径。

Abstract: Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.

</details>


### [244] [Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs](https://arxiv.org/abs/2512.07525)
*Xiaoran Liu,Yuerong Song,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Zhaoxiang Liu,Shiguo Lian,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 该论文提出了一种改进的RoPE位置编码方法，通过利用复数点积的虚部信息来增强长上下文依赖建模能力。


<details>
  <summary>Details</summary>
Motivation: 标准RoPE实现只使用复数点积的实部计算注意力分数，丢弃了包含重要相位信息的虚部，这可能导致长上下文依赖建模中关系细节的损失。

Method: 提出扩展方法重新整合被丢弃的虚部，利用完整的复数表示创建双组件注意力分数，增强位置信息的保留。

Result: 在长上下文语言建模基准测试中，该方法相比标准RoPE持续提升性能，且随着上下文长度增加，改进效果更加显著。

Conclusion: 通过利用复数点积的完整信息，该方法能够更好地建模长上下文依赖关系，为位置编码提供了更有效的解决方案。

Abstract: Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.

</details>


### [245] [SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents](https://arxiv.org/abs/2512.07538)
*Michelle Wastl,Jannis Vamvas,Rico Sennrich*

Main category: cs.CL

TL;DR: 该论文提出了SwissGov-RSD数据集，这是首个用于跨语言文档级语义差异识别的自然数据集，包含224个多语言平行文档，并评估了多种LLM和编码器模型在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 识别跨文档（尤其是不同语言间）的语义差异对于文本生成评估和多语言内容对齐至关重要，但这一任务本身很少受到关注，缺乏合适的自然数据集。

Method: 构建了SwissGov-RSD数据集，包含英语-德语、英语-法语、英语-意大利语的224个多平行文档，由人工进行词级差异标注。评估了开源和闭源大语言模型以及编码器模型在不同微调设置下的表现。

Result: 当前自动方法在该新基准测试上表现较差，远低于它们在单语、句子级和合成基准测试上的性能，揭示了LLM和编码器模型在该任务上的显著差距。

Conclusion: 跨语言文档级语义差异识别是一个具有挑战性的任务，现有模型表现不佳，需要进一步研究。作者公开了代码和数据集以促进该领域发展。

Abstract: Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.

</details>


### [246] [Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation](https://arxiv.org/abs/2512.07540)
*Boxuan Lyu,Haiyue Song,Hidetaka Kamigaito,Chenchen Ding,Hideki Tanaka,Masao Utiyama,Kotaro Funakoshi,Manabu Okumura*

Main category: cs.CL

TL;DR: 本文提出在生成式错误跨度检测任务中应用最小贝叶斯风险解码，以解决最大后验概率解码中模型概率与人工标注相似度不匹配的问题，并通过蒸馏技术降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的生成式错误跨度检测方法通常使用最大后验概率解码，假设模型估计的概率与人工标注的相似度完美相关。然而，作者观察到与人工标注不相似的标注可能获得比人工标注更高的模型似然，这表明MAP解码存在局限性。

Method: 作者将最小贝叶斯风险解码应用于生成式错误跨度检测模型，使用句子级和跨度级相似度度量作为效用函数，根据候选假设与人工标注的近似相似度进行选择。此外，为了降低MBR解码的计算成本，作者应用了MBR蒸馏技术，使标准贪婪模型能够匹配MBR解码性能。

Result: 实验结果表明，MBR解码在系统级、句子级和跨度级均优于MAP基线。通过MBR蒸馏，标准贪婪模型能够匹配MBR解码性能，有效消除了推理时的延迟瓶颈。

Conclusion: MBR解码能够有效解决生成式错误跨度检测中模型概率与人工标注相似度不匹配的问题，显著提升性能。通过蒸馏技术可以降低计算成本，使该方法更具实用性。

Abstract: Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.

</details>


### [247] [Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects](https://arxiv.org/abs/2512.07543)
*Frederic Blum*

Main category: cs.CL

TL;DR: 该研究重新检验了基本词汇中语音特征统计过表征现象，发现先前大多数关于语音象征模式的研究结果在控制语言间的谱系和地域依赖关系后不再稳健，仅有少数模式保持稳定。


<details>
  <summary>Details</summary>
Motivation: 先前关于语言基本词汇中语音象征模式的研究可能存在样本偏差、模型缺陷，且未充分控制语言间的谱系和地域依赖关系，导致结果稳健性存疑。

Method: 使用Lexibank数据库中2864种语言的数据（原研究为245种），在原始模型基础上增加空间和谱系依赖关系的统计控制，重新检验语音象征模式。

Result: 大多数先前观察到的语音象征模式在控制谱系和地域因素后不再稳健，许多模式完全消失；仅有少数模式在新样本中保持高度稳定。

Conclusion: 语音象征现象在基本词汇中的分布可能比先前认为的更有限，研究强调了在语言普遍性主张中需要进行多层次稳健性检验的重要性。

Abstract: The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.

</details>


### [248] [MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue](https://arxiv.org/abs/2512.07544)
*Kyungro Lee,Dongha Choi,Hyunju Lee*

Main category: cs.CL

TL;DR: MoCoRP框架通过显式建模人设句子与回复之间的NLI关系，提升基于人设对话的一致性和质量


<details>
  <summary>Details</summary>
Motivation: 现有基于人设的对话数据集缺乏人设句子与回复之间的显式关系，导致模型难以有效捕捉人设信息，影响对话的一致性和质量

Method: 提出MoCoRP框架，利用NLI专家显式提取人设句子与回复之间的NLI关系，使模型能够有效将适当的人设信息融入回复；将该框架应用于BART等预训练模型，并通过对齐调优扩展到现代大语言模型

Result: 在ConvAI2和MPChat公开数据集上，MoCoRP超越了现有基线，在人设一致性和上下文感知对话生成方面表现优异；不仅在定量指标上表现出色，在定性方面也有显著提升

Conclusion: 显式建模人设-回复关系在基于人设的对话中是有效的，MoCoRP框架能够生成更具一致性和吸引力的对话

Abstract: As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.

</details>


### [249] [A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification](https://arxiv.org/abs/2512.07571)
*Nicolas Calbucura,Valentin Barriere*

Main category: cs.CL

TL;DR: 提出一种简单方法，通过特征选择将语音信息有效集成到文本预训练大语言模型中，用于特定分类任务，在论辩谬误检测任务上达到SOTA


<details>
  <summary>Details</summary>
Motivation: 解决音频序列长度远大于文本序列导致的融合困难问题，以及现有语音分词器输出长序列难以低成本集成到大语言模型中的挑战

Method: 使用基于lasso的特征选择方法，在多模态词袋表示中保留最重要的音频token，通过自监督语言建模目标使语言模型适应这些token，然后在下游任务上进行微调

Result: 相比单模态模型、更大的SpeechLM模型或通过学习表示集成音频的方法，性能均有提升；在论辩谬误检测和分类任务上达到state-of-the-art结果

Conclusion: 提出的简单方法能有效增强文本预训练大语言模型，即使随机选择音频token也能提升单模态模型性能，为多模态融合提供了有效解决方案

Abstract: This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).

</details>


### [250] [Complementary Learning Approach for Text Classification using Large Language Models](https://arxiv.org/abs/2512.07583)
*Navid Asgari,Benjamin M. Cole*

Main category: cs.CL

TL;DR: 提出了一种利用大语言模型进行成本效益研究的结构化方法，结合人类与机器的优势，通过思维链和少样本学习来改善人机协作


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在研究中应用成本较高且存在固有弱点，需要开发一种既能发挥学者和机器各自优势，又能弥补各自不足的成本效益方法

Method: 采用结构化方法论，结合思维链和少样本学习提示技术，将定性研究中合作团队的最佳实践扩展到定量研究的人机协作中，让人类能够运用溯因推理和自然语言来审查机器和人类的工作

Result: 该方法能够有效管理LLMs的固有弱点，通过低成本技术实现高效研究，并成功应用于分析1990-2017年间1,934份制药联盟新闻稿中的人机评分差异

Conclusion: 提出了一种成本效益高的人机协作研究方法，通过结构化方法论使学者能够有效利用大语言模型，同时管理其固有弱点，为定量研究提供了新的人机协作范式

Abstract: In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).

</details>


### [251] [Metric-Fair Prompting: Treating Similar Samples Similarly](https://arxiv.org/abs/2512.07608)
*Jing Wang,Jie Shen,Xing Niu,Tong Zhang,Jeremy Weiss*

Main category: cs.CL

TL;DR: 提出Metric-Fair Prompting框架，通过度量公平性约束指导LLM在医学多选题中做出决策，利用相似问题联合处理提升个体公平性和准确性


<details>
  <summary>Details</summary>
Motivation: 在高风险的医学多选题回答中，需要确保语言模型决策的公平性，特别是保证相似问题得到相似处理（个体公平性），同时提升模型的准确性

Method: 1) 将每个(问题,选项)对视为二元实例；2) 使用NLP嵌入计算问题相似度；3) 在相似问题的联合对中而非孤立地解决问题；4) 设计提示强制全局决策协议：提取关键临床特征，将每个(问题,选项)映射到置信度分数f(x)，并施加Lipschitz式约束确保相似输入获得相似分数

Result: 在MedQA(US)基准测试中，Metric-Fair Prompting相比标准单项目提示方法提升了性能，表明公平性引导、置信度导向的推理能够增强LLM在高风险临床多选题上的准确性

Conclusion: 提出的度量公平性提示框架通过强制相似问题获得一致处理，不仅提升了语言模型决策的公平性，还意外地提高了在医学多选题上的准确性，为高风险应用中的LLM部署提供了新思路

Abstract: We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \((\text{question}, \text{option})\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.

</details>


### [252] [PCMind-2.1-Kaiyuan-2B Technical Report](https://arxiv.org/abs/2512.07612)
*Kairong Luo,Zhenbo Sun,Xinyu Shi,Shengqi Chen,Bowen Yu,Yunyi Chen,Chenyi Dang,Hengtao Tao,Hui Wang,Fangming Liu,Kaifeng Lyu,Wenguang Chen*

Main category: cs.CL

TL;DR: PCMind-2.1-Kaiyuan-2B是一个完全开源的20亿参数模型，旨在解决开源社区与行业之间的知识差距，通过创新的数据混合策略、选择性重复训练和多领域课程训练等方法，在资源受限条件下实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展导致开源社区与行业之间存在显著的知识差距，主要原因是行业依赖闭源的高质量数据和训练方法。为了弥合这一差距，需要开发完全开源的解决方案，使资源有限的团队也能进行高效训练。

Method: 1. 分位数数据基准测试方法：系统比较异构开源数据集，为数据混合策略提供见解；2. 战略选择性重复方案：在多阶段范式中有效利用稀疏的高质量数据；3. 多领域课程训练策略：按质量排序样本。此外还包括高度优化的数据预处理流程和针对FP16稳定性的架构修改。

Result: Kaiyuan-2B在性能上与最先进的完全开源模型竞争，证明了在资源受限预训练中的实用性和可扩展性。所有资产（包括模型权重、数据和代码）都在Apache 2.0许可下发布。

Conclusion: 该研究为资源受限的预训练提供了实用且可扩展的解决方案，通过创新的数据管理和训练策略，使开源社区能够开发出与行业模型竞争的高质量语言模型。

Abstract: The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.

</details>


### [253] [Bridging Code Graphs and Large Language Models for Better Code Understanding](https://arxiv.org/abs/2512.07666)
*Zeqi Chen,Zhaoyang Chu,Yi Gui,Feng Guo,Yao Wan,Chuan Shi*

Main category: cs.CL

TL;DR: CGBridge是一种即插即用方法，通过外部可训练的桥接模块将代码图信息注入大型语言模型，提升代码理解的结构语义能力，在代码摘要和翻译任务上显著优于原始模型和图增强提示方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码智能任务中表现出色，但依赖线性化token序列限制了其对程序结构语义的理解能力。现有方法要么受提示长度限制，要么需要特定任务架构调整，不兼容大规模指令跟随LLM。

Method: 提出CGBridge方法：1) 在27万代码图数据集上自监督预训练代码图编码器学习结构语义；2) 训练外部桥接模块通过跨模态注意力对齐代码、图和文本语义；3) 桥接模块生成结构感知提示注入冻结LLM，微调下游任务。

Result: 在代码摘要任务上相对提升16.19%和9.12%，在代码翻译任务上相对提升9.84%和38.87%。推理速度比LoRA调优模型快4倍以上，实现高效的结构感知代码理解。

Conclusion: CGBridge通过外部桥接模块有效增强LLM的结构语义理解能力，在保持与大规模指令跟随LLM兼容性的同时，显著提升代码智能任务性能，兼具效果和效率优势。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.

</details>


### [254] [HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs](https://arxiv.org/abs/2512.07687)
*Sujoy Nath,Arkaprabha Basu,Sharanya Dasgupta,Swagatam Das*

Main category: cs.CL

TL;DR: 本文提出HalluShift++方法，通过分析多模态大语言模型内部层动态中的可测量异常来检测幻觉问题，将幻觉检测从文本LLMs扩展到多模态场景。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉语言理解任务中表现出色，但经常产生与视觉内容事实不一致的幻觉描述，可能导致严重后果。当前方法主要依赖外部LLM评估器，但这些评估器本身也存在幻觉问题且存在领域适应挑战。

Method: 提出假设认为幻觉表现为MLLMs内部层动态中的可测量异常，而不仅仅是分布偏移。通过层间分析特定假设，HalluShift++将幻觉检测从文本LLMs扩展到多模态场景。

Result: HalluShift++能够有效检测多模态大语言模型中的幻觉问题，代码已开源在GitHub上。

Conclusion: 通过分析MLLMs内部层动态异常来检测幻觉是有效的，HalluShift++为多模态场景下的幻觉评估提供了新方法，超越了依赖外部LLM评估器的传统方法。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.

</details>


### [255] [Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map](https://arxiv.org/abs/2512.07694)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: SafeTerm是一个AI驱动的系统，通过多维向量空间嵌入医学查询术语和MedDRA首选术语，使用余弦相似度和极值聚类自动生成相关术语的排名列表，为药物安全审查中的MedDRA查询生成提供补充方法。


<details>
  <summary>Details</summary>
Motivation: 在药物上市前安全审查中，将相关不良事件术语分组到标准化MedDRA查询或FDA OCMQs对于信号检测至关重要。传统方法需要手动操作，效率低下且容易出错，因此需要开发自动化系统来改进这一过程。

Method: SafeTerm系统将医学查询术语和MedDRA首选术语嵌入多维向量空间，然后应用余弦相似度和极值聚类方法，根据相关性评分生成排名列表。系统使用多标准统计方法对术语进行排名。

Result: 在FDA OCMQ v3.0（104个查询）上的验证显示，在中等阈值下召回率>95%，较高阈值下精确度可达86%。最优阈值（~0.70-0.75）下召回率约50%，精确度约33%。窄术语子集表现相似但需要稍高的相似度阈值。

Conclusion: SafeTerm AI驱动系统为自动化MedDRA查询生成提供了可行的补充方法。建议初始使用~0.60的相似度阈值，对于精炼术语选择可增加阈值。

Abstract: In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.

</details>


### [256] [Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?](https://arxiv.org/abs/2512.07777)
*Karin de Langis,Püren Öncel,Ryan Peters,Andrew Elfenbein,Laura Kristen Allen,Andreas Schramm,Dongyeop Kang*

Main category: cs.CL

TL;DR: 研究发现LLMs能够通过内部表征识别不连贯叙事，但在生成评分回答时无法有效区分连贯与不连贯故事，表明LLMs对叙事连贯性的理解存在缺陷


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究大型语言模型（LLMs）能否可靠地区分连贯与不连贯的叙事，评估LLMs对故事连贯性的理解能力

Method: 使用配对叙事数据集进行研究，通过探测研究发现LLMs的内部表征能够识别不连贯叙事，但通过多种提示变体让LLMs生成评分回答来测试其区分能力

Result: LLMs的内部表征能可靠识别不连贯叙事，但生成的评分回答无法有效区分连贯与不连贯故事；LLMs对违反场景的不连贯性（如沙漠中的雨天）比对违反角色特质的不连贯性（如素食者点汉堡）更敏感；思维链推理无法消除这些缺陷

Conclusion: LLMs对叙事连贯性的理解不完整，可能更依赖原型世界知识而非基于意义的叙事连贯性构建，内部状态与行为之间存在差距

Abstract: Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.

</details>


### [257] [On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models](https://arxiv.org/abs/2512.07783)
*Charlie Zhang,Graham Neubig,Xiang Yue*

Main category: cs.CL

TL;DR: 本文通过完全受控的实验框架，揭示了预训练、中期训练和RL后训练在语言模型推理能力发展中的因果贡献，明确了RL仅在预训练留有足够空间且针对模型能力边界时才能产生真正能力提升。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习技术虽然提升了语言模型的推理能力，但难以确定这种提升是真正扩展了模型能力，还是仅仅利用了预训练中已有的知识。由于现代训练流程缺乏控制（预训练语料不透明、中期训练研究不足、RL目标与未知先验知识复杂交互），需要开发完全受控的实验框架来澄清这一问题。

Method: 开发了完全受控的实验框架，使用合成推理任务，包含明确的原子操作、可解析的逐步推理轨迹，并系统性地操纵训练分布。从两个维度评估模型：1) 外推泛化到更复杂的组合；2) 跨表面上下文的语境泛化。通过该框架分离预训练、中期训练和RL后训练的因果贡献。

Result: 研究发现：1) RL仅在预训练留有足够空间且针对模型能力边界（困难但尚未超出能力范围的任务）时才能产生真正的能力提升（pass@128）；2) 语境泛化需要最小但足够的预训练暴露，之后RL可以可靠地迁移；3) 在固定计算量下，中期训练相比仅使用RL能显著提升性能；4) 过程级奖励减少了奖励黑客行为并提高了推理保真度。

Conclusion: 研究结果阐明了预训练、中期训练和RL之间的相互作用，为理解和改进推理语言模型的训练策略提供了基础。特别强调了中期训练在训练流程中的核心但未被充分探索的作用，以及RL有效性的边界条件。

Abstract: Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.

</details>


### [258] [Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support](https://arxiv.org/abs/2512.07801)
*Raunak Jain,Mudita Khurana*

Main category: cs.CL

TL;DR: 论文提出"协作因果意义建构(CCS)"作为决策支持AI的研究议程，旨在解决当前AI助手与专家合作时出现的验证循环、过度依赖等问题，强调AI应成为认知工作的合作伙伴而非工具。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的AI助手在复杂高风险决策环境中表现不佳，人类-AI团队往往表现不如最佳个体，专家在验证循环和过度依赖之间摇摆，承诺的互补性未能实现。这不仅是准确性问题，更是AI辅助概念的根本缺陷。

Method: 提出协作因果意义建构(CCS)作为研究议程和组织框架，设计作为认知工作伙伴的系统：维护专家推理的演化模型，帮助表达和修订目标，共同构建和压力测试因果假设，从联合决策结果中学习，实现人类和AI的共同改进。

Result: 论文提出了CCS框架，并勾勒了相关挑战：使协作思维具有工具价值的训练生态、共同构建模型的表示和交互协议、以信任和互补性为中心的评估方法。

Conclusion: 这些方向可以将多智能体系统研究重新定位为参与协作意义建构的智能体，使其成为与人类伙伴共同思考的AI队友，而不仅仅是工具。

Abstract: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.

</details>


### [259] [Do Generalisation Results Generalise?](https://arxiv.org/abs/2512.07832)
*Matteo Boglioni,Andrea Sgobbi,Gabriel Tavernini,Francesco Rita,Marius Mosbach,Tiago Pimentel*

Main category: cs.CL

TL;DR: 该研究质疑仅用单一OOD数据集评估LLM泛化能力的有效性，提出通过分析多个OOD测试集间的偏相关性来更准确评估模型泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM泛化能力的方法通常只关注单一OOD数据集，但实际部署中模型会遇到更复杂多样的数据分布变化。需要更全面的评估方法来准确衡量模型的真实泛化能力。

Method: 在微调过程中评估模型在多个OOD测试集上的性能，然后计算这些测试集性能之间的偏相关性（控制域内性能的影响），以此评估泛化性能的相关性。

Result: 分析OLMo2和OPT模型发现，不同OOD测试集间的泛化结果没有统一的趋势：两个OOD测试集之间是否存在正相关或负相关，强烈依赖于所分析的具体模型选择。

Conclusion: LLM的OOD泛化性能评估需要更全面的方法，仅依赖单一OOD数据集可能无法准确反映模型的真实泛化能力，不同模型在不同OOD任务上的表现存在显著差异。

Abstract: A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.

</details>
