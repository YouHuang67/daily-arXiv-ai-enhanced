{"id": "2512.05998", "categories": ["cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05998", "abs": "https://arxiv.org/abs/2512.05998", "authors": ["Michael Todasco"], "title": "Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals", "comment": "25 pages, 8 tables, 2 figures. Pilot study. Data, prompts, and code available at https://osf.io/dc24t/", "summary": "Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. \"Whale\" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.", "AI": {"tldr": "\u672c\u7814\u7a76\u6d4b\u8bd5\u4e86\u5c06LLM\u8bc4\u4f30\u4efb\u52a1\u8bbe\u8ba1\u4e3a\u6295\u6ce8\u6e38\u620f\uff08\u4f7f\u7528\u865a\u6784\u7684LLM\u8d27\u5e01\uff09\u662f\u5426\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u4ea7\u751f\u6821\u51c6\u7684\u4fe1\u5fc3\u4fe1\u53f7\u3002\u5b9e\u9a8c\u53d1\u73b0\u6295\u6ce8\u673a\u5236\u80fd\u4ea7\u751f\u53ef\u8bfb\u7684\u4fe1\u5fc3\u4fe1\u53f7\uff0c\u5927\u989d\u6295\u6ce8\u51c6\u786e\u7387\u9ad8\u8fbe99%\uff0c\u5c0f\u989d\u6295\u6ce8\u4ec574%\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc4\u4f30\u5176\u4ed6\u6a21\u578b\u65f6\uff0c\u5176\u5224\u65ad\u901a\u5e38\u7f3a\u4e4f\u4fe1\u5fc3\u8868\u793a\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u901a\u8fc7\u91d1\u878d\u6846\u67b6\uff08\u6295\u6ce8\u6e38\u620f\uff09\u662f\u5426\u80fd\u6539\u5584\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u4ea7\u751f\u6821\u51c6\u7684\u4fe1\u5fc3\u4fe1\u53f7\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86100\u4e2a\u6570\u5b66\u548c\u903b\u8f91\u95ee\u9898\uff0c6\u4e2a\u57fa\u7ebf\u6a21\u578b\uff083\u4e2a\u5f53\u524d\u4ee3\uff0c3\u4e2a\u524d\u4ee3\uff09\u56de\u7b54\u95ee\u9898\u30023\u4e2a\u9884\u6d4b\u6a21\u578b\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u9884\u6d4b\u6bcf\u4e2a\u95ee\u9898-\u57fa\u7ebf\u5bf9\u7684\u6b63\u786e\u6027\uff1a\u63a7\u5236\u6761\u4ef6\uff08\u7b80\u5355\u6b63\u786e/\u9519\u8bef\u9884\u6d4b\uff09\u548c\u6fc0\u52b1\u6761\u4ef6\uff08\u9884\u6d4b\u52a0\u4e0a1-100,000 LLMCoin\u7684\u6295\u6ce8\uff09\u3002\u5171\u8fdb\u884c\u4e865,400\u6b21\u9884\u6d4b\u3002", "result": "\u6fc0\u52b1\u6761\u4ef6\u4e0b\u7684\u9884\u6d4b\u51c6\u786e\u7387\u7565\u9ad8\uff0881.5% vs. 79.1%\uff09\uff0c\u5b66\u4e60\u901f\u5ea6\u663e\u8457\u66f4\u5feb\uff08\u7b2c1\u8f6e\u5230\u7b2c4\u8f6e\u6539\u8fdb12.0% vs. 2.9%\uff09\u3002\u6295\u6ce8\u91d1\u989d\u4e0e\u4fe1\u5fc3\u76f8\u5173\uff1a40,000+\u786c\u5e01\u7684\u5927\u989d\u6295\u6ce8\u51c6\u786e\u7387\u7ea699%\uff0c\u800c<1,000\u786c\u5e01\u7684\u5c0f\u989d\u6295\u6ce8\u51c6\u786e\u7387\u4ec5\u7ea674%\u3002", "conclusion": "\u6295\u6ce8\u673a\u5236\u521b\u9020\u4e86\u4ece\u4e8c\u5143\u8f93\u51fa\u4e2d\u7f3a\u5931\u7684\u53ef\u8bfb\u4fe1\u5fc3\u4fe1\u53f7\uff0c\u8868\u660e\u7b80\u5355\u7684\u91d1\u878d\u6846\u67b6\u53ef\u80fd\u5e2e\u52a9LLM\u6210\u4e3a\u98ce\u9669\u611f\u77e5\u7684\u9884\u6d4b\u8005\uff0c\u4f7f\u5176\u5185\u90e8\u4fe1\u5ff5\u53d8\u5f97\u53ef\u89c1\u548c\u53ef\u7528\u3002\u8be5\u534f\u8bae\u4e3a\u672a\u6765\u7684\u5143\u8bc4\u4f30\u7cfb\u7edf\u548cLLM\u95f4\u9884\u6d4b\u5e02\u573a\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.06161", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06161", "abs": "https://arxiv.org/abs/2512.06161", "authors": ["Gondy Leroy", "Prakash Bisht", "Sai Madhuri Kandula", "Nell Maltman", "Sydney Rice"], "title": "Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach", "comment": "9 pages", "summary": "Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBioBERT\u7684\u900f\u660e\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u4e34\u5e8a\u6587\u672c\u5e76\u81ea\u52a8\u5316\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\u8bca\u65ad\uff0c\u901a\u8fc7\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u7b56\u7565\u83b7\u5f97\u4e8697%\u654f\u611f\u6027\u548c98%\u7279\u5f02\u6027\u7684\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\u8bca\u65ad\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u591a\u4e3a\u9ed1\u7bb1\u4e14\u901a\u5e38\u57fa\u4e8e\u5355\u4e00\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3002", "method": "\u4f7f\u7528BioBERT\u8bed\u8a00\u6a21\u578b\u5206\u6790\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6587\u672c\uff0c\u8bad\u7ec3\u6a21\u578b\u6807\u8bb0\u884c\u4e3a\u63cf\u8ff0\u5e76\u6620\u5c04\u5230\u8bca\u65ad\u6807\u51c6\uff0c\u7136\u540e\u5206\u914d\u6700\u7ec8\u6807\u7b7e\uff08ASD\u6216\u975eASD\uff09\u3002\u8bc4\u4f30\u4e86\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\uff0c\u6bd4\u8f83\u4e86\u987a\u5e8f\u8bad\u7ec3\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u4e0e\u9ed1\u7bb1\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u900f\u660e\u6a21\u578b\u8868\u73b0\u7a33\u5065\uff0c\u6df7\u5408\u6570\u636e\u8bad\u7ec3\u7b56\u7565\u83b7\u5f97\u6700\u4f73\u7ed3\u679c\uff0897%\u654f\u611f\u6027\uff0c98%\u7279\u5f02\u6027\uff09\u3002\u987a\u5e8f\u8bad\u7ec3\u5bfc\u81f4\u6027\u80fd\u7565\u6709\u4e0b\u964d\u3002\u9ed1\u7bb1\u6a21\u578b\u5728\u987a\u5e8f\u6216\u6df7\u5408\u8bad\u7ec3\u4e0b\u8868\u73b0\u8f83\u5dee\uff0890%\u654f\u611f\u6027\uff0c96%\u7279\u5f02\u6027\uff09\u3002\u900f\u660e\u65b9\u6cd5\u6574\u4f53\u4f18\u4e8e\u9ed1\u7bb1\u65b9\u6cd5\u3002", "conclusion": "\u900f\u660e\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u81ea\u95ed\u75c7\u8bca\u65ad\u4e2d\u4f18\u4e8e\u9ed1\u7bb1\u65b9\u6cd5\uff0c\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u53ef\u83b7\u5f97\u66f4\u597d\u6027\u80fd\u3002\u8fd9\u4e3a\u795e\u7ecf\u53d1\u80b2\u8bca\u65ad\u4e2d\u66f4\u53ef\u4fe1\u3001\u53ef\u6cdb\u5316\u548c\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684AI\u5de5\u5177\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.06196", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06196", "abs": "https://arxiv.org/abs/2512.06196", "authors": ["Charlie Masters", "Marta Grze\u015bkiewicz", "Stefano V. Albrecht"], "title": "ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment", "comment": "Accepted to the AAAI 2026 LLAMAS Workshop (Large Language Model Agents for Multi-Agent Systems)", "summary": "As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.", "AI": {"tldr": "ARCANE\u6846\u67b6\u901a\u8fc7\u5c06AI\u5bf9\u9f50\u95ee\u9898\u8f6c\u5316\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u95ee\u9898\uff0c\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u8bc4\u5206\u6807\u51c6\u52a8\u6001\u8868\u793a\u5229\u76ca\u76f8\u5173\u8005\u504f\u597d\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5728\u4ea4\u4e92\u65f6\u8c03\u6574\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5230\u957f\u671f\u4efb\u52a1\u4e2d\uff0c\u4fdd\u6301\u5176\u4e0e\u5229\u76ca\u76f8\u5173\u8005\u504f\u597d\u7684\u4e00\u81f4\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u6a21\u578b\u8ba9\u5229\u76ca\u76f8\u5173\u8005\u80fd\u591f\u7406\u89e3\u548c\u5ba1\u6838\u6a21\u578b\u76ee\u6807\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u4ea4\u4e92\u65f6\u5f15\u5bfc\u667a\u80fd\u4f53\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u7eb3\u5165\u504f\u597d\u53d8\u5316\u3002", "method": "\u63d0\u51faARCANE\u6846\u67b6\uff0c\u5c06\u5bf9\u9f50\u95ee\u9898\u6784\u5efa\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u95ee\u9898\uff0c\u52a8\u6001\u5730\u5c06\u5229\u76ca\u76f8\u5173\u8005\u504f\u597d\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u8bc4\u5206\u6807\u51c6\uff08\u53ef\u9a8c\u8bc1\u7684\u52a0\u6743\u6807\u51c6\u96c6\u5408\uff09\u3002\u53d7\u6548\u7528\u7406\u8bba\u542f\u53d1\uff0c\u5c06\u8bc4\u5206\u6807\u51c6\u5b66\u4e60\u6784\u5efa\u4e3a\u91cd\u6784\u95ee\u9898\uff0c\u5e94\u7528\u6b63\u5219\u5316\u7684\u7ec4\u5e8f\u5217\u7b56\u7565\u4f18\u5316(GSPO)\u7a0b\u5e8f\u6765\u5e73\u8861\u53ef\u89e3\u91ca\u6027\u3001\u5fe0\u5b9e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u4f7f\u7528\u4eceGDPVal\u57fa\u51c6\u6d3e\u751f\u7684219\u4e2a\u6807\u8bb0\u8bc4\u5206\u6807\u51c6\u8bed\u6599\u5e93\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u9700\u8981\u591a\u6b65\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u6d4b\u8bd5ARCANE\u3002\u5b66\u4e60\u7684\u8bc4\u5206\u6807\u51c6\u4ea7\u751f\u7d27\u51d1\u3001\u6613\u8bfb\u7684\u8bc4\u4f30\uff0c\u5e76\u652f\u6301\u53ef\u914d\u7f6e\u7684\u6743\u8861\uff08\u5982\u6b63\u786e\u6027\u4e0e\u7b80\u6d01\u6027\uff09\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u5956\u52b1\u6a21\u578b\u4e3a\u590d\u6742\u3001\u957f\u671fAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u53ef\u89e3\u91ca\u3001\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u5bf9\u9f50\u8def\u5f84\uff0c\u80fd\u591f\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u52a8\u6001\u504f\u597d\u8c03\u6574\u3002"}}
{"id": "2512.06097", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06097", "abs": "https://arxiv.org/abs/2512.06097", "authors": ["Emre Umucu", "Guillermina Solis", "Leon Garza", "Emilia Rivas", "Beatrice Lee", "Anantaa Kotal", "Aritran Piplai"], "title": "Empathy by Design: Aligning Large Language Models for Healthcare Dialogue", "comment": null, "summary": "General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u62a4\u7406\u5bf9\u8bdd\u4e2d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u540c\u7406\u5fc3\u7b49\u4eba\u7c7b\u4e2d\u5fc3\u7279\u8d28\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u5728\u533b\u7597\u5e94\u7528\u4e2d\u5b58\u5728\u7684\u4e8b\u5b9e\u4e0d\u53ef\u9760\u548c\u7f3a\u4e4f\u540c\u7406\u5fc3\u6c9f\u901a\u7684\u95ee\u9898\u3002", "motivation": "\u901a\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u62a4\u7406\u5e94\u7528\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a\u4e8b\u5b9e\u4e0d\u53ef\u9760\u6027\u548c\u7f3a\u4e4f\u540c\u7406\u5fc3\u6c9f\u901a\u3002\u8fd9\u4e9b\u7f3a\u9677\u5728\u654f\u611f\u533b\u7597\u73af\u5883\u4e2d\u5c24\u5176\u5371\u9669\uff0c\u56e0\u4e3a\u975e\u4e13\u4e1a\u62a4\u7406\u4eba\u5458\u548c\u60a3\u8005\u9700\u8981\u533b\u5b66\u76f8\u5173\u6307\u5bfc\u548c\u60c5\u611f\u652f\u6301\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u4f7f\u7528\u6210\u5bf9\u504f\u597d\u6570\u636e\u5bf9\u9886\u57df\u9002\u5e94\u7684LLM\u8fdb\u884c\u5fae\u8c03\u3002\u504f\u597d\u54cd\u5e94\u4f53\u73b0\u652f\u6301\u6027\u548c\u53ef\u8bbf\u95ee\u7684\u6c9f\u901a\u98ce\u683c\uff0c\u800c\u88ab\u62d2\u7edd\u7684\u54cd\u5e94\u5219\u4ee3\u8868\u89c4\u5b9a\u6027\u6216\u8fc7\u4e8e\u6280\u672f\u5316\u7684\u8bed\u8c03\u3002", "result": "\u7ecf\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cDPO\u8c03\u4f18\u6a21\u578b\u5728\u8bed\u4e49\u5bf9\u9f50\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u4eba\u7c7b\u4e2d\u5fc3\u8bc4\u4f30\u5206\u6570\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u548c\u5546\u4e1a\u66ff\u4ee3\u65b9\u6848\uff08\u5982\u8c37\u6b4c\u533b\u7597\u5bf9\u8bdd\u7cfb\u7edf\uff09\u3002\u6539\u8fdb\u8868\u660e\u57fa\u4e8e\u504f\u597d\u7684\u5bf9\u9f50\u4e3a\u5f00\u53d1\u53ef\u4fe1\u8d56\u3001\u6709\u540c\u7406\u5fc3\u4e14\u4e34\u5e8a\u77e5\u60c5\u7684AI\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u9014\u5f84\u3002", "conclusion": "\u504f\u597d\u5bf9\u9f50\u4e3a\u5f00\u53d1\u53ef\u4fe1\u8d56\u3001\u6709\u540c\u7406\u5fc3\u4e14\u4e34\u5e8a\u77e5\u60c5\u7684AI\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u9014\u5f84\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u62a4\u7406\u4eba\u5458\u548c\u533b\u7597\u4fdd\u5065\u6c9f\u901a\u7684\u8d28\u91cf\u3002"}}
{"id": "2512.06205", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06205", "abs": "https://arxiv.org/abs/2512.06205", "authors": ["Daniel Quigley", "Eric Maynard"], "title": "On measuring grounding and generalizing grounding problems", "comment": "36 pages, 85 sources", "summary": "The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.", "AI": {"tldr": "\u8bba\u6587\u5c06\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u4ece\u4e8c\u5143\u5224\u65ad\u91cd\u6784\u4e3a\u5305\u542b\u771f\u5b9e\u6027\u3001\u4fdd\u6301\u6027\u3001\u5fe0\u5b9e\u6027\u3001\u9c81\u68d2\u6027\u3001\u7ec4\u5408\u6027\u7b49\u9700\u6c42\u7684\u5ba1\u8ba1\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u56db\u79cd\u63a5\u5730\u6a21\u5f0f\uff0c\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u5206\u6790\u4e0d\u540c\u7cfb\u7edf\u7684\u63a5\u5730\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u2014\u2014\u5373\u7b26\u53f7\u5982\u4f55\u4e0e\u73b0\u5b9e\u4e16\u754c\u5b9e\u4f53\u5efa\u7acb\u8054\u7cfb\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u4f5c\u4e3a\u5f62\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5f62\u72b6\u3002\u5c06\u54f2\u5b66\u4e0a\u5173\u4e8e\u8868\u5f81\u7684\u63a2\u8ba8\u64cd\u4f5c\u5316\uff0c\u4e3a\u54f2\u5b66\u5bb6\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u3001\u8bed\u8a00\u5b66\u5bb6\u548c\u6570\u5b66\u5bb6\u63d0\u4f9b\u4e00\u4e2a\u5171\u540c\u8bed\u8a00\u548c\u6280\u672f\u6846\u67b6\u6765\u7cfb\u7edf\u7814\u7a76\u63a5\u5730\u548c\u610f\u4e49\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u8bc4\u4f30\u5143\u7ec4\uff08\u4e0a\u4e0b\u6587\u3001\u610f\u4e49\u7c7b\u578b\u3001\u5a01\u80c1\u6a21\u578b\u3001\u53c2\u8003\u5206\u5e03\uff09\u7684\u5ba1\u8ba1\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u6838\u5fc3\u9700\u6c42\uff1a\u771f\u5b9e\u6027\uff08\u673a\u5236\u5728\u667a\u80fd\u4f53\u5185\u90e8\u4e14\u901a\u8fc7\u5b66\u4e60/\u8fdb\u5316\u83b7\u5f97\uff09\u3001\u4fdd\u6301\u6027\uff08\u539f\u5b50\u610f\u4e49\u4fdd\u6301\u5b8c\u6574\uff09\u3001\u5fe0\u5b9e\u6027\uff08\u76f8\u5173\u6027\u548c\u56e0\u679c\u6027\uff09\u3001\u9c81\u68d2\u6027\uff08\u5728\u58f0\u660e\u6270\u52a8\u4e0b\u4f18\u96c5\u964d\u7ea7\uff09\u3001\u7ec4\u5408\u6027\uff08\u6574\u4f53\u7531\u90e8\u5206\u7cfb\u7edf\u6784\u5efa\uff09\u3002\u5c06\u6b64\u6846\u67b6\u5e94\u7528\u4e8e\u56db\u79cd\u63a5\u5730\u6a21\u5f0f\uff08\u7b26\u53f7\u3001\u6307\u79f0\u3001\u5411\u91cf\u3001\u5173\u7cfb\uff09\u548c\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u6a21\u578b\u8bba\u8bed\u4e49\u5b66\u5b9e\u73b0\u7cbe\u786e\u7ec4\u5408\u4f46\u7f3a\u4e4f\u56e0\u679c\u4fdd\u8bc1\uff1b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u4efb\u52a1\u4e0a\u663e\u793a\u76f8\u5173\u62df\u5408\u548c\u5c40\u90e8\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u6ca1\u6709\u63a5\u5730\u4ea4\u4e92\u7684\u4e16\u754c\u4efb\u52a1\u4e0a\u7f3a\u4e4f\u6210\u529f\u9009\u62e9\uff1b\u4eba\u7c7b\u8bed\u8a00\u901a\u8fc7\u8fdb\u5316\u548c\u53d1\u80b2\u83b7\u5f97\u6ee1\u8db3\u5f3a\u771f\u5b9e\u6027\u9700\u6c42\u3002\u6846\u67b6\u4e3a\u4e0d\u540c\u5b66\u79d1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7814\u7a76\u63a5\u5730\u95ee\u9898\u7684\u5171\u540c\u5de5\u5177\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u64cd\u4f5c\u5316\u4e3a\u4e00\u4e2a\u5305\u542b\u591a\u4e2a\u7ef4\u5ea6\u7684\u5ba1\u8ba1\u6846\u67b6\uff0c\u4e3a\u8de8\u5b66\u79d1\u7814\u7a76\u610f\u4e49\u548c\u8868\u5f81\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u7cfb\u7edf\u5728\u63a5\u5730\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\u548c\u5c40\u9650\uff0c\u4fc3\u8fdb\u4e86\u54f2\u5b66\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u8bed\u8a00\u5b66\u548c\u6570\u5b66\u4e4b\u95f4\u7684\u5bf9\u8bdd\u3002"}}
{"id": "2512.06169", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06169", "abs": "https://arxiv.org/abs/2512.06169", "authors": ["Chris Crawford"], "title": "Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yolox\u00f3chtil Mixtec ASR", "comment": "67 pages, 5 figures, 6 tables", "summary": "This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yolox\u00f3chitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528\u5f62\u6001\u5b66\u611f\u77e5\u7684\u5206\u8bcd\u5668\u6765\u8f85\u52a9\u548c\u7b80\u5316Yolox\u00f3chitl Mixtec\u97f3\u9891\u8bed\u6599\u7684\u5c42\u95f4\u6ce8\u91ca\u6807\u6ce8\uff0c\u7ed3\u5408ASR\u548c\u6587\u672c\u5e8f\u5217\u5230\u5e8f\u5217\u5de5\u5177\uff0c\u65e8\u5728\u63d0\u9ad8\u6548\u7387\u5e76\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\u3002", "motivation": "\u4f20\u7edfASR\u7cfb\u7edf\u5728\u5904\u7406\u5177\u6709\u975e\u8fde\u63a5\u6027\u5f62\u6001\u7684\u8bed\u8a00\uff08\u5982Yolox\u00f3chitl Mixtec\uff09\u65f6\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u97f3\u8c03\u5f62\u6001\u7684\u4fdd\u7559\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u5206\u8bcd\u5668\u6765\u66f4\u597d\u5730\u5904\u7406\u8fd9\u7c7b\u8bed\u8a00\u7684\u72ec\u7279\u5f62\u6001\u7279\u5f81\uff0c\u63d0\u9ad8\u6807\u6ce8\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u975e\u7ebf\u6027\u5206\u8bcd\u65b9\u6848\uff1a1\uff09Segment and Melody\u5206\u8bcd\u5668\uff0c\u4ec5\u63d0\u53d6\u97f3\u8c03\u800c\u4e0d\u9884\u6d4b\u5206\u5272\uff1b2\uff09Sequence of Processes\u5206\u8bcd\u5668\uff0c\u9884\u6d4b\u5355\u8bcd\u5206\u5272\uff0c\u4f7f\u7aef\u5230\u7aefASR\u7cfb\u7edf\u80fd\u4e00\u6b21\u6027\u751f\u6210\u5206\u5272\u548c\u672a\u5206\u5272\u7684\u8f6c\u5f55\u3002\u4e0e\u4f20\u7edfBPE\u548cUnigram\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u65b0\u9896\u5206\u8bcd\u5668\u4e0eBPE\u548cUnigram\u6a21\u578b\u5177\u6709\u7ade\u4e89\u529b\uff0cSegment-and-Melody\u6a21\u578b\u5728\u8bcd\u9519\u8bef\u7387\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u5206\u8bcd\u5668\uff0c\u4f46\u5728\u5b57\u7b26\u9519\u8bef\u7387\u65b9\u9762\u672a\u8fbe\u5230\u76f8\u540c\u6c34\u5e73\u3002\u5f62\u6001\u5b66\u548c\u4fe1\u606f\u8bba\u6307\u6807\u5206\u6790\u663e\u793a\u4e0e\u4e0b\u6e38\u6027\u80fd\u5b58\u5728\u9884\u6d4b\u76f8\u5173\u6027\u3002", "conclusion": "\u4e13\u95e8\u4e3a\u975e\u8fde\u63a5\u6027\u5f62\u6001\u8bed\u8a00\u8bbe\u8ba1\u7684\u975e\u7ebf\u6027\u5206\u8bcd\u5668\u5728ASR\u4efb\u52a1\u4e2d\u4e0e\u4f20\u7edfBPE\u548cUnigram\u6a21\u578b\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e3a\u5904\u7406\u590d\u6742\u5f62\u6001\u8bed\u8a00\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u8fd9\u4e9b\u5206\u8bcd\u5668\u5728\u4e0b\u6e38\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2512.05969", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05969", "abs": "https://arxiv.org/abs/2512.05969", "authors": ["Hokin Deng"], "title": "Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices", "comment": "See $\\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and $\\href{https://github.com/hokindeng/VMEvalKit}{code}$", "summary": "We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the \"Task Pair\" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u5177\u5907\u63a8\u7406\u80fd\u529b\uff0c\u5728\u8c61\u68cb\u3001\u8ff7\u5bab\u3001\u6570\u72ec\u7b49\u4efb\u52a1\u4e0a\u8fbe\u523060%\u6210\u529f\u7387\uff0c\u5e76\u5efa\u7acb\u4e86\"\u4efb\u52a1\u5bf9\"\u5b9e\u9a8c\u8303\u5f0f\u548c\u8bc4\u4f30\u6846\u67b6VMEvalKit\u3002", "motivation": "\u63a2\u7d22\u89c6\u9891\u751f\u6210\u6a21\u578b\u662f\u5426\u5177\u5907\u63a8\u7406\u80fd\u529b\uff0c\u5efa\u7acb\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u8303\u5f0f\u6765\u7cfb\u7edf\u6d4b\u8bd5\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\"\u4efb\u52a1\u5bf9\"\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u6784\u5efa\u5305\u542b39\u4e2a\u6a21\u578b\u7684\u4ee3\u7801\u6846\u67b6VMEvalKit\uff0c\u652f\u6301\u81ea\u52a8\u8bc4\u4f30\u5e76\u4e0e\u4eba\u5de5\u5224\u65ad\u5f3a\u76f8\u5173\u3002", "result": "Sora-2\u7b49\u9886\u5148\u6a21\u578b\u5728\u8c61\u68cb\u3001\u8ff7\u5bab\u3001\u6570\u72ec\u3001\u5fc3\u7406\u65cb\u8f6c\u3001\u745e\u6587\u77e9\u9635\u7b49\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u523060%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u89c6\u9891\u751f\u6210\u6a21\u578b\u5df2\u5177\u5907\u521d\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u5efa\u7acb\u7684\u8bc4\u4f30\u8303\u5f0f\u5177\u6709\u9ad8\u5ea6\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u89c6\u9891\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002"}}
{"id": "2512.06240", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06240", "abs": "https://arxiv.org/abs/2512.06240", "authors": ["Chuanhao Nie", "Yunbo Liu", "Chao Wang"], "title": "AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems", "comment": null, "summary": "Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u53cd\u6d17\u94b1\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684AI\u9a71\u52a8KYC\u65b9\u6848\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u67b6\u6784\u80fd\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u964d\u4f4e\u8bef\u62a5\u7387\uff0c\u5e76\u589e\u5f3aKYC\u6d41\u7a0b\u7684\u6548\u7387\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u6d17\u94b1\u548c\u91d1\u878d\u6b3a\u8bc8\u6bcf\u5e74\u9020\u6210\u6570\u4e07\u4ebf\u7f8e\u5143\u635f\u5931\uff0c\u5a01\u80c1\u5168\u7403\u91d1\u878d\u7a33\u5b9a\uff0c\u4f20\u7edf\u76d1\u7ba1\u624b\u6bb5\u9762\u4e34\u6311\u6218\u3002\u9700\u8981\u73b0\u4ee3\u5316\u6280\u672f\u6765\u63d0\u9ad8\u53cd\u6d17\u94b1\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u964d\u4f4e\u8bef\u62a5\u7387\uff0c\u5e76\u51cf\u8f7b\u4eba\u5de5\u8c03\u67e5\u8d1f\u62c5\u3002", "method": "\u8bba\u6587\u9996\u5148\u7efc\u8ff0AI\u5728\u53cd\u6d17\u94b1\u4e2d\u7684\u5e94\u7528\uff0c\u7136\u540e\u63d0\u51faAI\u9a71\u52a8\u7684KYC\u5e94\u7528\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5c06\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG Graph\uff09\u4e0e\u751f\u6210\u6a21\u578b\u7ed3\u5408\uff0c\u7528\u4e8e\u589e\u5f3aKYC\u6d41\u7a0b\u7684\u6548\u7387\u3001\u900f\u660e\u5ea6\u548c\u51b3\u7b56\u652f\u6301\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cRAG-Graph\u67b6\u6784\u5728\u4e0d\u540c\u8bc4\u4f30\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u9ad8\u5fe0\u5b9e\u5ea6\u548c\u5f3a\u7b54\u6848\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86KYC\u5ba2\u6237\u5c3d\u804c\u8c03\u67e5/\u589e\u5f3a\u5c3d\u804c\u8c03\u67e5\u5de5\u4f5c\u6d41\u7684\u6548\u7387\u548c\u900f\u660e\u5ea6\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u53ef\u6301\u7eed\u3001\u8d44\u6e90\u4f18\u5316\u7684\u5408\u89c4\u5b9e\u8df5\u3002", "conclusion": "AI\u6280\u672f\u80fd\u591f\u73b0\u4ee3\u5316\u53cd\u6d17\u94b1\u5de5\u4f5c\u6d41\uff0c\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\uff1a\u7528\u4e8e\u9690\u79c1\u4fdd\u62a4\u534f\u4f5c\u7684\u8054\u90a6\u5b66\u4e60\u3001\u516c\u5e73\u53ef\u89e3\u91caAI\u3001\u81ea\u9002\u5e94\u9632\u5fa1\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u53ca\u4eba\u673a\u534f\u540c\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u786e\u4fdd\u4e0b\u4e00\u4ee3\u53cd\u6d17\u94b1\u67b6\u6784\u7684\u900f\u660e\u6027\u3001\u53ef\u95ee\u8d23\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.06193", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06193", "abs": "https://arxiv.org/abs/2512.06193", "authors": ["Jihyung Park", "Saleh Afroogh", "Junfeng Jiao"], "title": "Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots", "comment": null, "summary": "Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \\textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.", "AI": {"tldr": "GAUGE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684logit-based\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u5bf9\u8bdd\u4e2d\u7684\u9690\u6027\u60c5\u611f\u5347\u7ea7\uff0c\u5f25\u8865\u4f20\u7edf\u6bd2\u6027\u8fc7\u6ee4\u5668\u65e0\u6cd5\u8bc6\u522b\u60c5\u611f\u5f3a\u5316\u6216\u6f02\u79fb\u5bfc\u81f4\u9690\u6027\u4f24\u5bb3\u7684\u4e0d\u8db3\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u60c5\u611f\u4f34\u4fa3\u4f7f\u7528\u65f6\uff0c\u5373\u4f7f\u6ca1\u6709\u660e\u663e\u6bd2\u6027\uff0c\u91cd\u590d\u7684\u60c5\u611f\u5f3a\u5316\u6216\u60c5\u611f\u6f02\u79fb\u4e5f\u53ef\u80fd\u5bfc\u81f4\u9690\u6027\u4f24\u5bb3\uff0c\u800c\u73b0\u6709\u57fa\u4e8e\u5916\u90e8\u5206\u7c7b\u5668\u6216\u4e34\u5e8a\u6807\u51c6\u7684\u9632\u62a4\u673a\u5236\u96be\u4ee5\u5b9e\u65f6\u6355\u6349\u5bf9\u8bdd\u4e2d\u7684\u7ec6\u5fae\u52a8\u6001\u53d8\u5316\u3002", "method": "\u63d0\u51faGAUGE\u6846\u67b6\uff0c\u57fa\u4e8elogit\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u5982\u4f55\u6982\u7387\u6027\u5730\u6539\u53d8\u5bf9\u8bdd\u7684\u60c5\u611f\u72b6\u6001\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u7684\u5b9e\u65f6\u9690\u6027\u5bf9\u8bdd\u5347\u7ea7\u68c0\u6d4b\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86GAUGE\u6846\u67b6\uff0c\u4f46\u6458\u8981\u4e2d\u672a\u5c55\u793a\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u9700\u8981\u9605\u8bfb\u5168\u6587\u4e86\u89e3\u8be5\u6846\u67b6\u5728\u68c0\u6d4b\u9690\u6027\u60c5\u611f\u5347\u7ea7\u65b9\u9762\u7684\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "GAUGE\u4e3a\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u60c5\u611f\u4f34\u4fa3\u65f6\u4ea7\u751f\u7684\u9690\u6027\u4f24\u5bb3\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u65f6\u3001\u8f7b\u91cf\u7ea7\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u6355\u6349\u4f20\u7edf\u6bd2\u6027\u8fc7\u6ee4\u5668\u65e0\u6cd5\u8bc6\u522b\u7684\u5bf9\u8bdd\u60c5\u611f\u5347\u7ea7\u3002"}}
{"id": "2512.05987", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05987", "abs": "https://arxiv.org/abs/2512.05987", "authors": ["Chenyue Yu", "Jianyu Yu"], "title": "Adaptive Dataset Quantization: A New Direction for Dataset Pruning", "comment": "Accepted by ICCPR 2025", "summary": "This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u8fb9\u7f18\u8bbe\u5907\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5b58\u50a8\u548c\u901a\u4fe1\u6210\u672c\u7684\u6570\u636e\u96c6\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u6837\u672c\u5185\u5197\u4f59\u800c\u975e\u4f20\u7edf\u65b9\u6cd5\u5173\u6ce8\u7684\u6837\u672c\u95f4\u5197\u4f59\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8bad\u7ec3\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u7684\u6570\u636e\u96c6\u538b\u7f29\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u6709\u9650\uff0c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u5b58\u50a8\u548c\u901a\u4fe1\u6210\u672c\u9ad8\u6602\u3002\u4f20\u7edf\u7684\u6570\u636e\u96c6\u526a\u679d\u548c\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6837\u672c\u95f4\u5197\u4f59\uff0c\u4f46\u5ffd\u7565\u4e86\u6837\u672c\u5185\u7684\u5197\u4f59\u4fe1\u606f\u3002\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u538b\u7f29\u6bcf\u4e2a\u6837\u672c\u5185\u90e8\u7684\u5197\u4f59\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u7279\u5f81\u3002", "method": "\u9996\u5148\u5bf9\u6bcf\u4e2a\u6837\u672c\u5e94\u7528\u7ebf\u6027\u5bf9\u79f0\u91cf\u5316\u4ee5\u83b7\u5f97\u521d\u59cb\u91cf\u5316\u8303\u56f4\u548c\u5c3a\u5ea6\u3002\u7136\u540e\u5f15\u5165\u81ea\u9002\u5e94\u91cf\u5316\u5206\u914d\u7b97\u6cd5\uff0c\u4e3a\u5177\u6709\u4e0d\u540c\u7cbe\u5ea6\u8981\u6c42\u7684\u6837\u672c\u5206\u914d\u4e0d\u540c\u7684\u91cf\u5316\u6bd4\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u603b\u538b\u7f29\u6bd4\u7387\u6052\u5b9a\u3002\u8be5\u65b9\u6cd5\u9996\u6b21\u4f7f\u7528\u6709\u9650\u6bd4\u7279\u8868\u793a\u6570\u636e\u96c6\u4ee5\u51cf\u5c11\u5b58\u50a8\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cImageNet-1K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5728\u76f8\u540c\u538b\u7f29\u6bd4\u7387\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6a21\u578b\u8bad\u7ec3\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u91cf\u5316\u548c\u6570\u636e\u96c6\u526a\u679d\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u91cf\u5316\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u6837\u672c\u5185\u5197\u4f59\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e2d\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u5b58\u50a8\u548c\u901a\u4fe1\u6210\u672c\u95ee\u9898\u3002\u81ea\u9002\u5e94\u91cf\u5316\u5206\u914d\u7b97\u6cd5\u80fd\u591f\u6839\u636e\u6837\u672c\u7cbe\u5ea6\u9700\u6c42\u7075\u6d3b\u5206\u914d\u91cf\u5316\u6bd4\u7387\uff0c\u5728\u4fdd\u6301\u603b\u538b\u7f29\u6bd4\u7684\u540c\u65f6\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2512.06296", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06296", "abs": "https://arxiv.org/abs/2512.06296", "authors": ["Sooho Moon", "Yunyong Ko"], "title": "How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion", "comment": "5 pages, 4 figures, 2 tables, ACM WSDM 2026", "summary": "Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86PROBE\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u9884\u6d4b\u9510\u5ea6\u548c\u6d41\u884c\u5ea6\u504f\u5dee\u9c81\u68d2\u6027\u6765\u6539\u8fdb\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff08KGC\uff09\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u5ffd\u7565\u4e86\u9884\u6d4b\u9510\u5ea6\uff08\u5bf9\u5355\u4e2a\u9884\u6d4b\u7684\u4e25\u683c\u7a0b\u5ea6\u8bc4\u4f30\uff09\uff1b2\uff09\u7f3a\u4e4f\u5bf9\u6d41\u884c\u5ea6\u504f\u5dee\u7684\u9c81\u68d2\u6027\uff08\u9884\u6d4b\u4f4e\u6d41\u884c\u5ea6\u5b9e\u4f53\u7684\u80fd\u529b\uff09\u3002\u8fd9\u5bfc\u81f4\u73b0\u6709\u6307\u6807\u53ef\u80fd\u9ad8\u4f30\u6216\u4f4e\u4f30KGC\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86PROBE\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u79e9\u53d8\u6362\u5668\uff08RT\uff09- \u6839\u636e\u6240\u9700\u7684\u9884\u6d4b\u9510\u5ea6\u6c34\u5e73\u4f30\u8ba1\u6bcf\u4e2a\u9884\u6d4b\u7684\u5f97\u5206\uff1b2\uff09\u79e9\u805a\u5408\u5668\uff08RA\uff09- \u4ee5\u6d41\u884c\u5ea6\u611f\u77e5\u7684\u65b9\u5f0f\u805a\u5408\u6240\u6709\u5f97\u5206\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u6307\u6807\u503e\u5411\u4e8e\u9ad8\u4f30\u6216\u4f4e\u4f30KGC\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u800cPROBE\u80fd\u591f\u63d0\u4f9b\u5bf9KGC\u6a21\u578b\u7684\u5168\u9762\u7406\u89e3\u548c\u53ef\u9760\u7684\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "PROBE\u6846\u67b6\u901a\u8fc7\u540c\u65f6\u8003\u8651\u9884\u6d4b\u9510\u5ea6\u548c\u6d41\u884c\u5ea6\u504f\u5dee\u9c81\u68d2\u6027\uff0c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u548c\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u548c\u6bd4\u8f83\u4e0d\u540cKGC\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2512.06337", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06337", "abs": "https://arxiv.org/abs/2512.06337", "authors": ["Xuan Xie", "Xuan Wang", "Wenjie Wang"], "title": "DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization", "comment": null, "summary": "The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.", "AI": {"tldr": "DaGRPO\u901a\u8fc7\u5f15\u5165\u5e8f\u5217\u7ea7\u68af\u5ea6\u77eb\u6b63\u548c\u79bb\u7b56\u7565\u6570\u636e\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86GRPO\u5728\u957f\u94fe\u63a8\u7406\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548cOOD\u6cdb\u5316\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "GRPO\u867d\u7136\u80fd\u6709\u6548\u6fc0\u53d1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u94fe\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\u6839\u672c\u539f\u56e0\u5728\u4e8e\u7b56\u7565\u5185\u6837\u672c\u7f3a\u4e4f\u533a\u5206\u5ea6\uff1a\u5bf9\u4e8e\u5e38\u89c4\u67e5\u8be2\uff0c\u9ad8\u5ea6\u540c\u8d28\u7684\u6837\u672c\u5bfc\u81f4\u7834\u574f\u6027\u68af\u5ea6\u51b2\u7a81\uff1b\u5bf9\u4e8e\u56f0\u96be\u67e5\u8be2\uff0c\u6709\u6548\u6b63\u6837\u672c\u7a00\u7f3a\u5bfc\u81f4\u4f18\u5316\u65e0\u6548\u3002", "method": "\u63d0\u51faDaGRPO\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1) \u5e8f\u5217\u7ea7\u68af\u5ea6\u77eb\u6b63\uff1a\u4f7f\u7528\u7ec6\u7c92\u5ea6\u8bc4\u5206\u52a8\u6001\u5c4f\u853d\u4f4e\u533a\u5206\u5ea6\u7684\u6837\u672c\u5bf9\uff0c\u4ece\u6e90\u5934\u6d88\u9664\u68af\u5ea6\u51b2\u7a81\uff1b2) \u79bb\u7b56\u7565\u6570\u636e\u589e\u5f3a\uff1a\u5f15\u5165\u9ad8\u8d28\u91cf\u951a\u70b9\u6837\u672c\uff0c\u4e3a\u56f0\u96be\u4efb\u52a1\u6062\u590d\u8bad\u7ec3\u4fe1\u53f7\u3002", "result": "\u57289\u4e2a\u6570\u5b66\u63a8\u7406\u548cOOD\u6cdb\u5316\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDaGRPO\u663e\u8457\u8d85\u8d8a\u73b0\u6709SFT\u3001GRPO\u548c\u6df7\u5408\u57fa\u7ebf\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\uff08\u5982\u5728\u6570\u5b66\u57fa\u51c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347+4.7%\uff09\u3002\u6df1\u5165\u5206\u6790\u8bc1\u5b9eDaGRPO\u6709\u6548\u7f13\u89e3\u68af\u5ea6\u7206\u70b8\u5e76\u52a0\u901f\u957f\u94fe\u63a8\u7406\u80fd\u529b\u7684\u6d8c\u73b0\u3002", "conclusion": "DaGRPO\u901a\u8fc7\u89e3\u51b3GRPO\u7684\u533a\u5206\u5ea6\u4e0d\u8db3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u94fe\u63a8\u7406\u8bad\u7ec3\u7684\u6548\u679c\u548c\u6548\u7387\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06228", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06228", "abs": "https://arxiv.org/abs/2512.06228", "authors": ["Xuanxin Wu", "Yuki Arase", "Masaaki Nagata"], "title": "Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge", "comment": null, "summary": "Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.", "AI": {"tldr": "\u5229\u7528LLM-as-a-Judge\u81ea\u52a8\u6784\u5efa\u7b56\u7565\u5bf9\u9f50\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u5e73\u884c\u8bed\u6599\uff0c\u5b9e\u73b0\u53ef\u9002\u5e94\u4e0d\u540c\u7b80\u5316\u7b56\u7565\u7684\u6587\u672c\u7b80\u5316\u7cfb\u7edf", "motivation": "\u6587\u672c\u7b80\u5316\u9700\u8981\u6839\u636e\u4e0d\u540c\u5e94\u7528\u91c7\u7528\u4e0d\u540c\u7684\u7b80\u5316\u7b56\u7565\uff08\u5982\u4ec5\u66ff\u6362\u590d\u6742\u8bcd\u6c47\u6216\u6574\u4f53\u91cd\u5199\uff09\uff0c\u4f46\u5b9e\u73b0\u7b56\u7565\u9a71\u52a8\u7684\u63a7\u5236\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6216\u5e73\u884c\u8bed\u6599\u3002", "method": "\u63d0\u51fa\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\uff08LLM-as-a-Judge\uff09\u81ea\u52a8\u6784\u5efa\u7b56\u7565\u5bf9\u9f50\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5b8c\u5168\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u5e73\u884c\u8bed\u6599\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6784\u5efa\u9002\u5e94\u4e0d\u540c\u7b80\u5316\u7b56\u7565\u7684\u7b80\u5316\u7cfb\u7edf\u3002", "result": "\u5373\u4f7f\u662f\u5c0f\u89c4\u6a21\u5f00\u6e90LLM\uff08\u5982Phi-3-mini-3.8B\uff09\u5728\u8bcd\u6c47\u5bfc\u5411\u7684\u7b80\u5316\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86GPT-4o\uff0c\u5728\u6574\u4f53\u91cd\u5199\u4efb\u52a1\u4e0a\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\u3002\u81ea\u52a8\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u5747\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u4e0a\u5747\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684LLM-as-a-Judge\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u6784\u5efa\u7b56\u7565\u5bf9\u9f50\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5b9e\u73b0\u53ef\u9002\u5e94\u4e0d\u540c\u7b80\u5316\u7b56\u7565\u7684\u9ad8\u6548\u6587\u672c\u7b80\u5316\u7cfb\u7edf\uff0c\u4e14\u5c0f\u6a21\u578b\u4e5f\u80fd\u8d85\u8d8a\u5927\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2512.05991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.05991", "abs": "https://arxiv.org/abs/2512.05991", "authors": ["Chang Liu", "Tianjiao Jing", "Chengcheng Ma", "Xuanqi Zhou", "Zhengxuan Lian", "Qin Jin", "Hongliang Yuan", "Shi-Sheng Huang"], "title": "EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head", "comment": null, "summary": "Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.", "AI": {"tldr": "EmoDiffTalk\uff1a\u9996\u4e2a\u652f\u6301\u8fde\u7eed\u591a\u6a21\u6001\u60c5\u611f\u7f16\u8f91\u76843D\u9ad8\u65af\u6cfc\u6e85\u8bf4\u8bdd\u5934\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u611f\u611f\u77e5\u9ad8\u65af\u6269\u6563\u548c\u6587\u672c\u5230\u52a8\u4f5c\u5355\u5143\u63a7\u5236\u5668\u5b9e\u73b0\u7cbe\u7ec6\u9762\u90e8\u52a8\u753b", "motivation": "\u73b0\u6709\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u771f\u5b9e\u611f3D\u8bf4\u8bdd\u5934\u5728\u60c5\u611f\u8868\u8fbe\u64cd\u63a7\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u591a\u6a21\u6001\u63a7\u5236\u8fdb\u884c\u7ec6\u7c92\u5ea6\u548c\u6269\u5c55\u6027\u52a8\u6001\u60c5\u611f\u7f16\u8f91\u65b9\u9762", "method": "\u63d0\u51fa\u60c5\u611f\u611f\u77e5\u9ad8\u65af\u6269\u6563\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a1\uff09\u52a8\u4f5c\u5355\u5143\u63d0\u793a\u7684\u9ad8\u65af\u6269\u6563\u8fc7\u7a0b\u7528\u4e8e\u7ec6\u7c92\u5ea6\u9762\u90e8\u52a8\u753b\uff1b2\uff09\u51c6\u786e\u7684\u6587\u672c\u5230\u52a8\u4f5c\u5355\u5143\u60c5\u611f\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u6587\u672c\u8f93\u5165\u63d0\u4f9b\u7cbe\u786e\u4e14\u6269\u5c55\u6027\u7684\u52a8\u6001\u60c5\u611f\u7f16\u8f91", "result": "\u5728EmoTalk3D\u548cRenderMe-360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEmoDiffTalk\u5728\u60c5\u611f\u7ec6\u5fae\u5ea6\u3001\u5507\u90e8\u540c\u6b65\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u4e3a\u9ad8\u8d28\u91cf\u3001\u6269\u6563\u9a71\u52a8\u3001\u591a\u6a21\u6001\u53ef\u7f16\u8f913D\u8bf4\u8bdd\u5934\u5408\u6210\u5efa\u7acb\u4e86\u539f\u5219\u6027\u9014\u5f84", "conclusion": "EmoDiffTalk\u662f\u9996\u6279\u652f\u6301\u5728\u57fa\u4e8e\u52a8\u4f5c\u5355\u5143\u7684\u8868\u60c5\u7a7a\u95f4\u5185\u8fdb\u884c\u8fde\u7eed\u591a\u6a21\u6001\u60c5\u611f\u7f16\u8f91\u76843D\u9ad8\u65af\u6cfc\u6e85\u8bf4\u8bdd\u5934\u751f\u6210\u6846\u67b6\u4e4b\u4e00\uff0c\u4e3a\u9ad8\u8d28\u91cf\u53ef\u7f16\u8f913D\u8bf4\u8bdd\u5934\u5408\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411"}}
{"id": "2512.06393", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.06393", "abs": "https://arxiv.org/abs/2512.06393", "authors": ["Qiming Bao", "Xiaoxuan Fu"], "title": "Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression", "comment": null, "summary": "Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.\n  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.", "AI": {"tldr": "LLMs\u5728\u903b\u8f91\u63a8\u7406\u4e2d\u5bf9\u8bed\u4e49\u4fdd\u6301\u7684\u7b49\u4ef7\u53d8\u6362\u8868\u73b0\u7a33\u5b9a\uff0c\u4f46\u5bf9\u7f3a\u5931\u89c4\u5219\u548c\u77db\u76fe\u8bc1\u636e\u6781\u5ea6\u8106\u5f31", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u903b\u8f91\u63a8\u7406\u4e2d\u5bf9\u7ed3\u6784\u5316\u6270\u52a8\u7684\u6cdb\u5316\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7a76LLMs\u5728\u9762\u5bf9\u903b\u8f91\u7ed3\u6784\u53d8\u5316\u65f6\u7684\u63a8\u7406\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53d7\u63a7\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u79cd\u9488\u5bf9\u6027\u538b\u529b\u6d4b\u8bd5\u6765\u63a2\u6d4b\u63a8\u7406\u53ef\u9760\u6027\uff1a1) \u89c4\u5219\u5220\u9664\uff08\u5220\u9664\u5197\u4f59\u6216\u5fc5\u8981\u89c4\u5219\uff09\uff1b2) \u77db\u76fe\u8bc1\u636e\u6ce8\u5165\uff1b3) \u903b\u8f91\u4fdd\u6301\u7684\u91cd\u5199\uff08\u4f7f\u7528\u516d\u79cd\u7b49\u4ef7\u5b9a\u5f8b\uff09\uff1b4) \u591a\u5b9a\u5f8b\u7b49\u4ef7\u5806\u53e0\uff08\u540c\u65f6\u5e94\u75282-5\u4e2a\u903b\u8f91\u53d8\u6362\uff09\u3002\u5728BERT\u3001Qwen2\u548cLLaMA\u7c7b\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u57fa\u7840\u4efb\u52a1\u4e0a\u90fd\u8fbe\u5230\u5b8c\u7f8e\u51c6\u786e\u7387\uff0c\u5bf9\u5197\u4f59\u89c4\u5219\u5220\u9664\u548c\u6240\u6709\u7b49\u4ef7\u91cd\u5199\uff08\u5355\u5b9a\u5f8b\u6216\u591a\u5b9a\u5f8b\uff09\u90fd\u80fd\u5b8c\u5168\u6cdb\u5316\u3002\u4f46\u5728\u5fc5\u8981\u89c4\u5219\u5220\u9664\u65f6\u51c6\u786e\u7387\u6025\u5267\u4e0b\u964d\u523025%\uff0c\u5728\u660e\u786e\u77db\u76fe\u5b58\u5728\u65f6\u5b8c\u5168\u5d29\u6e83\uff080%\u51c6\u786e\u7387\uff09\u3002", "conclusion": "LLMs\u5bf9\u8bed\u4e49\u4fdd\u6301\u7684\u903b\u8f91\u53d8\u6362\u5177\u6709\u7a33\u5b9a\u7684\u4e0d\u53d8\u6027\uff0c\u4f46\u5bf9\u7f3a\u5931\u6216\u51b2\u7a81\u8bc1\u636e\u4ecd\u7136\u6781\u5176\u8106\u5f31\u3002\u8be5\u6846\u67b6\u4e3a\u9694\u79bb\u6b64\u7c7b\u63a8\u7406\u5931\u8d25\u6a21\u5f0f\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u7a81\u663e\u4e86\u5f53\u524dLLMs\u5728\u903b\u8f91\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u6301\u7eed\u5dee\u8ddd\u3002"}}
{"id": "2512.06239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06239", "abs": "https://arxiv.org/abs/2512.06239", "authors": ["Dhanasekar Sundararaman", "Keying Li", "Wayne Xiong", "Aashna Garg"], "title": "LOCUS: A System and Method for Low-Cost Customization for Universal Specialization", "comment": null, "summary": "We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.", "AI": {"tldr": "LOCUS\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u7684NLP\u6a21\u578b\u5b9a\u5236\u6d41\u7a0b\uff0c\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u6784\u5efa\uff0c\u7ed3\u5408\u68c0\u7d22\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\uff0c\u5728NER\u548c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-4o\u7b49\u57fa\u7ebf\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6210\u672c\u548c\u6a21\u578b\u5927\u5c0f\u3002", "motivation": "\u4f20\u7edfNLP\u6a21\u578b\u5b9a\u5236\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5c31\u80fd\u6784\u5efa\u9ad8\u6027\u80fd\u4e13\u7528\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u6a21\u578b\u89c4\u6a21\u3002", "method": "LOCUS\u91c7\u7528\u4e09\u6b65\u6d41\u7a0b\uff1a1\uff09\u4ece\u5927\u89c4\u6a21\u5b58\u50a8\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u6570\u636e\uff1b2\uff09\u901a\u8fc7\u4e0a\u4e0b\u6587\u6570\u636e\u751f\u6210\u5408\u6210\u989d\u5916\u8bad\u7ec3\u6837\u672c\uff1b3\uff09\u4f7f\u7528\u5168\u53c2\u6570\u6216\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3002\u4e13\u6ce8\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u3002", "result": "LOCUS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff08\u5305\u62ecGPT-4o\uff09\uff0c\u5185\u5b58\u4f18\u5316\u6a21\u578b\u4fdd\u7559\u4e8699%\u7684\u5168\u5fae\u8c03\u7cbe\u5ea6\uff0c\u540c\u65f6\u4ec5\u4f7f\u75285%\u7684\u5185\u5b58\u5360\u7528\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51fb\u8d25GPT-4o\uff0c\u800c\u53c2\u6570\u6570\u91cf\u4e0d\u5230\u51761%\u3002", "conclusion": "LOCUS\u8bc1\u660e\u4e86\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u3001\u667a\u80fd\u68c0\u7d22\u548c\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u53ef\u4ee5\u6784\u5efa\u9ad8\u6027\u80fd\u3001\u4f4e\u6210\u672c\u7684\u4e13\u7528NLP\u6a21\u578b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.05993", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05993", "abs": "https://arxiv.org/abs/2512.05993", "authors": ["Ruchika Verma", "Shrishtee Kandoi", "Robina Afzal", "Shengjia Chen", "Jannes Jegminat", "Michael W. Karlovich", "Melissa Umphlett", "Timothy E. Richardson", "Kevin Clare", "Quazi Hossain", "Jorge Samanamud", "Phyllis L. Faust", "Elan D. Louis", "Ann C. McKee", "Thor D. Stein", "Jonathan D. Cherry", "Jesse Mez", "Anya C. McGoldrick", "Dalilah D. Quintana Mora", "Melissa J. Nirenberg", "Ruth H. Walker", "Yolfrankcis Mendez", "Susan Morgello", "Dennis W. Dickson", "Melissa E. Murray", "Carlos Cordon-Cardo", "Nadejda M. Tsankova", "Jamie M. Walker", "Diana K. Dangoor", "Stephanie McQuillan", "Emma L. Thorn", "Claudia De Sanctis", "Shuying Li", "Thomas J. Fuchs", "Kurt Farrell", "John F. Crary", "Gabriele Campanella"], "title": "Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology", "comment": null, "summary": "Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.", "AI": {"tldr": "NeuroFM\uff1a\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u795e\u7ecf\u75c5\u7406\u5b66\u9886\u57df\u8bad\u7ec3\u7684\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u5728\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u5206\u6790\u4efb\u52a1\u4e0a\u4f18\u4e8e\u901a\u7528\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b", "motivation": "\u73b0\u6709\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u5916\u79d1\u75c5\u7406\u6570\u636e\u8bad\u7ec3\uff0c\u8fd9\u4e9b\u6570\u636e\u8fc7\u5ea6\u4ee3\u8868\u975e\u795e\u7ecf\u7cfb\u7edf\u7ec4\u7ec7\u548c\u75be\u75c5\uff0c\u7f3a\u4e4f\u795e\u7ecf\u75c5\u7406\u5b66\u7279\u6709\u7684\u7ec6\u80de\u7c7b\u578b\u3001\u7ed3\u6784\u548c\u75c5\u7406\u7279\u5f81\uff0c\u5bfc\u81f4\u5728\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u5206\u6790\u4e0a\u5b58\u5728\u9886\u57df\u4e0d\u5339\u914d\u95ee\u9898", "method": "\u5f00\u53d1NeuroFM\uff0c\u4e13\u95e8\u5728\u6db5\u76d6\u591a\u79cd\u795e\u7ecf\u9000\u884c\u6027\u75c5\u7406\u7684\u8111\u7ec4\u7ec7\u5168\u5207\u7247\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u9488\u5bf9\u795e\u7ecf\u75c5\u7406\u5b66\u9886\u57df\u8fdb\u884c\u4e13\u4e1a\u5316\u5efa\u6a21", "result": "NeuroFM\u5728\u591a\u4e2a\u795e\u7ecf\u75c5\u7406\u5b66\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u5305\u62ec\u6df7\u5408\u6027\u75f4\u5446\u75be\u75c5\u5206\u7c7b\u3001\u6d77\u9a6c\u533a\u57df\u5206\u5272\u3001\u795e\u7ecf\u9000\u884c\u6027\u5171\u6d4e\u5931\u8c03\u8bc6\u522b\uff08\u6db5\u76d6\u5c0f\u8111\u6027\u7279\u53d1\u6027\u9707\u98a4\u548c\u810a\u9ad3\u5c0f\u8111\u6027\u5171\u6d4e\u5931\u8c03\u4e9a\u578b\uff09", "conclusion": "\u9886\u57df\u4e13\u4e1a\u5316\u57fa\u7840\u6a21\u578b\u5728\u795e\u7ecf\u75c5\u7406\u5b66\u7279\u5f81\u6355\u6349\u65b9\u9762\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0cNeuroFM\u4e3a\u6570\u5b57\u75c5\u7406\u5b66\u4e13\u4e1a\u9886\u57df\u7684\u7279\u5b9a\u6a21\u578b\u5f00\u53d1\u6811\u7acb\u4e86\u5148\u4f8b\uff0c\u80fd\u591f\u4e3a\u8111\u75be\u75c5\u8bca\u65ad\u548c\u7814\u7a76\u63d0\u4f9b\u66f4\u51c6\u786e\u53ef\u9760\u7684AI\u5206\u6790"}}
{"id": "2512.06256", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06256", "abs": "https://arxiv.org/abs/2512.06256", "authors": ["Aniruddha Maiti", "Satya Nimmagadda", "Kartha Veerya Jammuladinne", "Niladri Sengupta", "Ananya Jana"], "title": "Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup", "comment": "accepted to LLM 2025", "summary": "In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.", "AI": {"tldr": "\u4e24\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u76f8\u4e92\u5bf9\u8bdd\uff0c\u521d\u59cb\u79cd\u5b50\u53e5\u540e\u6a21\u578b\u8f6e\u6d41\u751f\u6210\u54cd\u5e94\uff0c\u89c2\u5bdf\u53d1\u73b0\u5bf9\u8bdd\u5f00\u59cb\u8fde\u8d2f\u4f46\u540e\u671f\u9677\u5165\u91cd\u590d\u5faa\u73af\uff0c\u51fa\u73b0\u6536\u655b\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u4e24\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6ca1\u6709\u5916\u90e8\u8f93\u5165\u7684\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u76f8\u4e92\u5bf9\u8bdd\u65f6\u4f1a\u53d1\u751f\u4ec0\u4e48\uff0c\u63a2\u7d22\u5b83\u4eec\u957f\u671f\u4ea4\u4e92\u7684\u52a8\u6001\u884c\u4e3a\u3002", "method": "\u4f7f\u7528Mistral Nemo Base 2407\u548cLlama 2 13B hf\u6a21\u578b\uff0c\u4ece\u77ed\u79cd\u5b50\u53e5\u5f00\u59cb\uff0c\u8ba9\u4e24\u4e2a\u6a21\u578b\u8f6e\u6d41\u8bfb\u53d6\u5bf9\u65b9\u8f93\u51fa\u5e76\u751f\u6210\u54cd\u5e94\uff0c\u6301\u7eed\u56fa\u5b9a\u6b65\u6570\uff0c\u5e94\u7528\u8bcd\u6c47\u548c\u5d4c\u5165\u6307\u6807\u6d4b\u91cf\u5bf9\u8bdd\u504f\u79bb\u521d\u59cb\u79cd\u5b50\u7684\u7a0b\u5ea6\u4ee5\u53ca\u4e24\u4e2a\u6a21\u578b\u8f93\u51fa\u7684\u76f8\u4f3c\u6027\u3002", "result": "\u5927\u591a\u6570\u5bf9\u8bdd\u5f00\u59cb\u8fde\u8d2f\u4f46\u540e\u671f\u9677\u5165\u91cd\u590d\uff0c\u51fa\u73b0\u77ed\u77ed\u8bed\u5728\u591a\u4e2a\u8f6e\u6b21\u4e2d\u91cd\u590d\uff0c\u4e00\u65e6\u5f00\u59cb\u91cd\u590d\uff0c\u4e24\u4e2a\u6a21\u578b\u503e\u5411\u4e8e\u4ea7\u751f\u76f8\u4f3c\u8f93\u51fa\u800c\u975e\u5f15\u5165\u65b0\u65b9\u5411\uff0c\u5bfc\u81f4\u76f8\u540c\u6216\u76f8\u4f3c\u6587\u672c\u7684\u5faa\u73af\uff0c\u5373\u4f7f\u6a21\u578b\u89c4\u6a21\u5927\u3001\u72ec\u7acb\u8bad\u7ec3\u4e14\u65e0\u63d0\u793a\u6307\u4ee4\uff0c\u4ecd\u51fa\u73b0\u6536\u655b\u73b0\u8c61\u3002", "conclusion": "\u4e24\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u8bbe\u7f6e\u4e2d\u4f1a\u81ea\u53d1\u6536\u655b\u5230\u91cd\u590d\u6a21\u5f0f\uff0c\u8868\u73b0\u4e3a\u5bf9\u8bdd\u540e\u671f\u9677\u5165\u5faa\u73af\u91cd\u590d\uff0c\u8fd9\u79cd\u6536\u655b\u73b0\u8c61\u63ed\u793a\u4e86\u6a21\u578b\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u884c\u4e3a\u7279\u6027\u3002"}}
{"id": "2512.05996", "categories": ["cs.CV", "cs.CY", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.05996", "abs": "https://arxiv.org/abs/2512.05996", "authors": ["Yi Liu", "Jingyu Song", "Vedanth Kallakuri", "Katherine A. Skinner"], "title": "FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting", "comment": "18 pages, under review", "summary": "Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.", "AI": {"tldr": "FishDetector-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f31\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u6c34\u4e0b\u9c7c\u7c7b\u68c0\u6d4b\u3001\u5206\u5272\u548c\u8ba1\u6570\uff0c\u5728DeepFish\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u6307\u6807\u3002", "motivation": "\u6c34\u4e0b\u9c7c\u7c7b\u56fe\u50cf\u5206\u6790\u5bf9\u751f\u6001\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u89c6\u89c9\u8d28\u91cf\u4e0b\u964d\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u800c\u9762\u4e34\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u5f31\u76d1\u7763\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684MLLM\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u65b0\u9896\u7684\u68c0\u6d4b\u5230\u8ba1\u6570\u63d0\u793a\uff0c\u786e\u4fdd\u7a7a\u95f4\u4e00\u81f4\u7684\u68c0\u6d4b\u548c\u8ba1\u6570\uff1b2\uff09\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\uff0c\u5229\u7528\u7a00\u758f\u70b9\u6807\u7b7e\u7684\u53ef\u6269\u5c55\u8303\u5f0f\u3002", "result": "\u5728DeepFish\u6570\u636e\u96c6\u4e0a\uff0cAP\u63d0\u534720%\uff0cmIoU\u63d0\u534710%\uff0cMAE\u964d\u4f4e30%\uff0cGAME\u964d\u4f4e35%\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5956\u52b1\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c\u4e14\u5728\u5176\u4ed6\u6c34\u4e0b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u8de8\u57df\u9c81\u68d2\u6027\u3002", "conclusion": "FishDetector-R1\u901a\u8fc7\u5f31\u76d1\u7763\u4e3a\u51c6\u786e\u7684\u6c34\u4e0b\u89c6\u89c9\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u751f\u6001\u76d1\u6d4b\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.06406", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06406", "abs": "https://arxiv.org/abs/2512.06406", "authors": ["Xianzong Wu", "Xiaohong Li", "Lili Quan", "Qiang Hu"], "title": "UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems", "comment": null, "summary": "Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.", "AI": {"tldr": "UncertaintyZoo\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5de5\u5177\u5305\uff0c\u96c6\u6210\u4e8629\u79cdUQ\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e94\u5927\u7c7b\u522b\uff0c\u4e3aLLMs\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u6807\u51c6\u5316\u63a5\u53e3\u3002", "motivation": "LLMs\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5b58\u5728\u9519\u8bef\u9884\u6d4b\u98ce\u9669\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u53ef\u80fd\u5bfc\u81f4\u635f\u5931\u3002\u867d\u7136\u5df2\u6709\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u96c6\u6210\u5de5\u5177\uff0c\u963b\u788d\u4e86UQ\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u548c\u672a\u6765\u7814\u7a76\u3002", "method": "\u5f00\u53d1UncertaintyZoo\u5de5\u5177\u5305\uff0c\u96c6\u621029\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e94\u5927\u7c7b\u522b\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u63a5\u53e3\u3002\u5728\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u4efb\u52a1\u4e0a\u5bf9CodeBERT\u548cChatGLM3\u6a21\u578b\u8bc4\u4f30\u73b0\u6709UQ\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "UncertaintyZoo\u80fd\u591f\u6709\u6548\u63ed\u793a\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5de5\u5177\u5df2\u5f00\u6e90\u5e76\u63d0\u4f9b\u6f14\u793a\u89c6\u9891\u3002", "conclusion": "UncertaintyZoo\u586b\u8865\u4e86LLMs\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5de5\u5177\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u548c\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u5e73\u53f0\u3002"}}
{"id": "2512.06266", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06266", "abs": "https://arxiv.org/abs/2512.06266", "authors": ["Chen Yang", "Guangyue Peng", "Jiaying Zhu", "Ran Le", "Ruixiang Feng", "Tao Zhang", "Wei Ruan", "Xiaoqi Liu", "Xiaoxue Cheng", "Xiyun Xu", "Yang Song", "Yanzipeng Gao", "Yiming Jia", "Yun Xing", "Yuntao Wen", "Zekai Wang", "Zhenwei An", "Zhicong Sun", "Zongchao Chen"], "title": "Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models", "comment": null, "summary": "We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.", "AI": {"tldr": "Nanbeige4-3B\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u9884\u8bad\u7ec3\u8c03\u5ea6\u5668\u3001\u540e\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u63d0\u5347\u673a\u5236\u3001\u53cc\u91cd\u504f\u597d\u84b8\u998f\u548c\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff0c\u57283B\u53c2\u6570\u89c4\u6a21\u4e0b\u8fbe\u5230\u4e86\u8d85\u8d8a\u540c\u7c7b\u6a21\u578b\u751a\u81f3\u5ab2\u7f8e\u66f4\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u63a8\u52a8\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u8fb9\u754c\uff0c\u8bc1\u660e\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u6570\u636e\u4f18\u5316\uff0c\u5c0f\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u7684\u6027\u80fd\u6c34\u5e73\u3002", "method": "1. \u9884\u8bad\u7ec3\u9636\u6bb5\uff1a\u4f7f\u7528\u7ec6\u7c92\u5ea6\u70ed\u8eab-\u7a33\u5b9a-\u8870\u51cf\u8c03\u5ea6\u5668\uff0c\u5206\u9636\u6bb5\u4f18\u5316\u6570\u636e\u6df7\u5408\uff1b2. \u540e\u8bad\u7ec3\u9636\u6bb5\uff1a\u7ed3\u5408\u5ba1\u8bae\u751f\u6210\u4f18\u5316\u548c\u601d\u7ef4\u94fe\u91cd\u6784\u673a\u5236\u63d0\u5347SFT\u6570\u636e\u8d28\u91cf\uff1b3. \u4f7f\u7528\u65d7\u8230\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u53cc\u91cd\u504f\u597d\u84b8\u998f\u65b9\u6cd5\u84b8\u998fNanbeige4-3B\uff1b4. \u5e94\u7528\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u548c\u504f\u597d\u5efa\u6a21\u589e\u5f3a\u63a8\u7406\u548c\u5bf9\u9f50\u80fd\u529b\u3002", "result": "\u5728\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNanbeige4-3B\u4e0d\u4ec5\u663e\u8457\u4f18\u4e8e\u540c\u7b49\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u8fd8\u80fd\u4e0e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u7ade\u4e89\uff0c\u5c55\u73b0\u4e86\u5c0f\u89c4\u6a21\u6a21\u578b\u7684\u9ad8\u6027\u80fd\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u6570\u636e\u4f18\u5316\u65b9\u6cd5\uff0c\u6210\u529f\u6269\u5c55\u4e86\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u7f29\u653e\u5b9a\u5f8b\u8fb9\u754c\uff0c\u8bc1\u660e\u4e86\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5c0f\u6a21\u578b\u53ef\u4ee5\u8fbe\u5230\u4e0e\u66f4\u5927\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u6c34\u5e73\u3002"}}
{"id": "2512.06431", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.06431", "abs": "https://arxiv.org/abs/2512.06431", "authors": ["Mohamed Shamroukh", "Mohamed Alkhuzamy Aziz"], "title": "Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City", "comment": null, "summary": "National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u57c3\u53ca\u57fa\u7eb3\u5e02\u5f00\u53d1\u4e86\u5b9a\u5236\u5316\u7684\u57ce\u5e02\u89c4\u5212\u6a21\u578b\uff0c\u4f7f\u7528Python\u7f16\u7a0b\u548cVoronoi\u56fe\u7b97\u6cd5\u8bc4\u4f30\u516c\u5171\u670d\u52a1\u8bbe\u65bd\u8986\u76d6\u7387\uff0c\u53d1\u73b0\u5e73\u5747\u8986\u76d6\u7387\u4e3a81.3%\uff0c\u5e76\u8bc6\u522b\u4e86\u7a7a\u95f4\u5206\u5e03\u4e0d\u5747\u7684\u95ee\u9898\u3002", "motivation": "\u57c3\u53ca\u56fd\u5bb6\u516c\u5171\u670d\u52a1\u89c4\u5212\u6807\u51c6\u5f80\u5f80\u65e0\u6cd5\u9002\u5e94\u5730\u65b9\u72ec\u7279\u7279\u5f81\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u53cd\u6620\u5730\u65b9\u7279\u8272\u7684\u5b9a\u5236\u5316\u89c4\u5212\u6a21\u578b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff08\u63cf\u8ff0\u6027\u3001\u5206\u6790\u6027\u548c\u5b9e\u9a8c\u6027\uff09\uff0c\u4f7f\u7528Python\u7f16\u7a0b\u5f00\u53d1\u57fa\u4e8eVoronoi\u56fe\u7684\u667a\u80fd\u7a7a\u95f4\u5206\u6790\u7b97\u6cd5\uff0c\u521b\u5efa\u57ce\u5e02\u7279\u5b9a\u7684\u89c4\u5212\u6807\u51c6\u5e76\u8bc4\u4f30\u516c\u5171\u670d\u52a1\u8bbe\u65bd\u5f53\u524d\u8986\u76d6\u7387\u3002", "result": "\u5e94\u7528\u6a21\u578b\u663e\u793a\u5e73\u5747\u670d\u52a1\u8986\u76d6\u7387\u4e3a81.3%\uff0c\u6551\u62a4\u8f66\u7ad9\u6548\u7387\u6700\u9ad8\uff0899.8%\uff09\uff0c\u516c\u56ed\u548c\u5f00\u653e\u7a7a\u95f4\u8986\u76d6\u7387\u6700\u4f4e\uff0810%\uff09\u3002\u7a7a\u95f4\u5206\u6790\u663e\u793a\u5e02\u4e2d\u5fc3\u670d\u52a1\u5bc6\u5ea6\u9ad8\uff08>45\u4e2a/km\u00b2\uff09\uff0c\u90ca\u533a\u663e\u8457\u964d\u4f4e\uff08<5\u4e2a/km\u00b2\uff09\uff0cHajer Qena\u533a\u672a\u670d\u52a1\u533a\u57df\u6700\u591a\uff0c\u7b2c\u4e00\u533a\u670d\u52a1\u8986\u76d6\u7387\u6700\u9ad8\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u672c\u5730\u5316\u89c4\u5212\u6807\u51c6\u6a21\u578b\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u7b97\u6cd5\uff0c\u4e3a\u57c3\u53ca\u57ce\u5e02\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u6570\u636e\u9a71\u52a8\u57ce\u5e02\u89c4\u5212\u6846\u67b6\u3002"}}
{"id": "2512.06464", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06464", "abs": "https://arxiv.org/abs/2512.06464", "authors": ["Akriti Jain", "Aparna Garimella"], "title": "Modeling Contextual Passage Utility for Multihop Question Answering", "comment": "Accepted at IJCNLP-AACL 2025", "summary": "Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7bc7\u7ae0\u6548\u7528\uff08\u8003\u8651\u7bc7\u7ae0\u95f4\u4f9d\u8d56\u5173\u7cfb\uff09\u6765\u6539\u8fdb\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u7bc7\u7ae0\u91cd\u6392\u5e8f\uff0c\u76f8\u6bd4\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u65b9\u6cd5\u80fd\u63d0\u5347\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u591a\u8df3\u95ee\u7b54\u9700\u8981\u4ece\u591a\u4e2a\u6587\u672c\u7bc7\u7ae0\u4e2d\u8bc6\u522b\u548c\u7efc\u5408\u4fe1\u606f\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bc6\u522b\u76f8\u5173\u7bc7\u7ae0\uff0c\u4f46\u8fdb\u4e00\u6b65\u8bc4\u4f30\u7bc7\u7ae0\u6548\u7528\u6709\u52a9\u4e8e\u53bb\u9664\u5197\u4f59\u7bc7\u7ae0\uff0c\u51cf\u5c11\u566a\u58f0\u548c\u7b54\u6848\u751f\u6210\u4e2d\u7684\u4e0d\u51c6\u786e\u6027\u3002\u73b0\u6709\u6548\u7528\u9884\u6d4b\u65b9\u6cd5\u72ec\u7acb\u5efa\u6a21\u7bc7\u7ae0\u6548\u7528\uff0c\u5ffd\u7565\u4e86\u591a\u8df3\u63a8\u7406\u7684\u5173\u952e\u65b9\u9762\uff1a\u7bc7\u7ae0\u6548\u7528\u662f\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\uff0c\u53d7\u5176\u4e0e\u5176\u4ed6\u7bc7\u7ae0\u5173\u7cfb\u7684\u5f71\u54cd\uff08\u662f\u5426\u63d0\u4f9b\u8865\u5145\u4fe1\u606f\u6216\u4e0e\u5176\u4ed6\u7bc7\u7ae0\u5f62\u6210\u5173\u952e\u94fe\u63a5\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u6765\u5efa\u6a21\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7bc7\u7ae0\u6548\u7528\uff0c\u8003\u8651\u7bc7\u7ae0\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002\u5fae\u8c03\u4e00\u4e2a\u5c0f\u578b\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u6765\u9884\u6d4b\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u7bc7\u7ae0\u6548\u7528\u5206\u6570\u3002\u5229\u7528\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u8f68\u8ff9\u6765\u6355\u6349\u56de\u7b54\u95ee\u9898\u65f6\u4f7f\u7528\u7bc7\u7ae0\u7684\u987a\u5e8f\uff0c\u5e76\u83b7\u53d6\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u901a\u8fc7\u5168\u9762\u5b9e\u9a8c\u8bc1\u660e\uff0c\u57fa\u4e8e\u6548\u7528\u7684\u68c0\u7d22\u7bc7\u7ae0\u8bc4\u5206\u76f8\u6bd4\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u80fd\u5e26\u6765\u6539\u8fdb\u7684\u91cd\u6392\u5e8f\u6548\u679c\u548c\u4e0b\u6e38\u95ee\u7b54\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5efa\u6a21\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7bc7\u7ae0\u6548\u7528\uff08\u8003\u8651\u7bc7\u7ae0\u95f4\u4f9d\u8d56\u5173\u7cfb\uff09\u5bf9\u4e8e\u591a\u8df3\u95ee\u7b54\u662f\u6709\u6548\u7684\uff0c\u76f8\u6bd4\u4f20\u7edf\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u8bc6\u522b\u548c\u5229\u7528\u7bc7\u7ae0\u95f4\u7684\u4e92\u8865\u548c\u94fe\u63a5\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u5347\u95ee\u7b54\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2512.06573", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.06573", "abs": "https://arxiv.org/abs/2512.06573", "authors": ["Onur Bilgin", "Abdullah As Sami", "Sriram Sai Vujjini", "John Licato"], "title": "The Effect of Belief Boxes and Open-mindedness on Persuasion", "comment": "Accepted at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026), Marbella, Spain", "summary": "As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86LLM\u667a\u80fd\u4f53\u4fe1\u5ff5\u9648\u8ff0\uff08\u4fe1\u5ff5\u76d2\uff09\u5982\u4f55\u5f71\u54cd\u5176\u884c\u4e3a\u548c\u8bf4\u670d\u529b\uff0c\u4ee5\u53ca\u5f00\u653e\u5fc3\u6001\u6307\u4ee4\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u4fe1\u5ff5\u9648\u8ff0\u5f71\u54cd\u667a\u80fd\u4f53\u5bf9\u76f8\u53cd\u89c2\u70b9\u7684\u62b5\u6297\u529b\u548c\u8bf4\u670d\u529b\uff0c\u5f00\u653e\u5fc3\u6001\u6307\u4ee4\u5f71\u54cd\u4fe1\u5ff5\u6539\u53d8\u7684\u503e\u5411\u6027\u3002", "motivation": "\u968f\u7740\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u63a8\u7406\u548c\u51b3\u7b56\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u9700\u8981\u8ba9\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5177\u5907\u7c7b\u4f3c\u547d\u9898\u4fe1\u5ff5\u7684\u80fd\u529b\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u667a\u80fd\u4f53\u4fe1\u5ff5\u9648\u8ff0\u5982\u4f55\u5b9e\u9645\u5f71\u54cd\u5176\u884c\u4e3a\u548c\u5bf9\u4fe1\u5ff5\u7684\u6001\u5ea6\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u4fe1\u5ff5\u9648\u8ff0\u662f\u5426\u663e\u8457\u5f71\u54cd\u667a\u80fd\u4f53\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u8bf4\u670d\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u63a2\u7d22\u4fe1\u5ff5\u76d2\u6280\u672f\uff0c\u5305\u62ec\u5728\u63d0\u793a\u7a7a\u95f4\u4e2d\u7ef4\u62a4\u4fe1\u5ff5\u9648\u8ff0\uff08\u4fe1\u5ff5\u76d2\uff09\uff0c\u7814\u7a76\u4fe1\u5ff5\u9648\u8ff0\u53ca\u5176\u5f3a\u5ea6\u5982\u4f55\u5f71\u54cd\u667a\u80fd\u4f53\u5bf9\u76f8\u53cd\u89c2\u70b9\u7684\u62b5\u6297\u529b\u548c\u8bf4\u670d\u529b\uff0c\u540c\u65f6\u7814\u7a76\u5f00\u653e\u5fc3\u6001\u6307\u4ee4\u5982\u4f55\u5f71\u54cd\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u6307\u793a\u667a\u80fd\u4f53\u4fdd\u6301\u5f00\u653e\u5fc3\u6001\u4f1a\u5f71\u54cd\u5176\u4fe1\u5ff5\u6539\u53d8\u7684\u503e\u5411\u6027\uff1b2\uff09\u7eb3\u5165\u4fe1\u5ff5\u9648\u8ff0\u53ca\u5176\u5f3a\u5ea6\u4f1a\u5f71\u54cd\u667a\u80fd\u4f53\u5bf9\u76f8\u53cd\u89c2\u70b9\u7684\u62b5\u6297\u529b\u548c\u8bf4\u670d\u529b\uff1b3\uff09\u5728\u8fa9\u8bba\u4e2d\u88ab\u76f8\u53cd\u89c2\u70b9\u5305\u56f4\uff08\u540c\u4f34\u538b\u529b\u573a\u666f\uff09\u65f6\uff0c\u4fe1\u5ff5\u9648\u8ff0\u5f71\u54cd\u4fe1\u5ff5\u6539\u53d8\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u7ed3\u679c\u8bc1\u660e\u4e86\u4fe1\u5ff5\u76d2\u6280\u672f\u5728\u63a8\u7406\u548c\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u7c7b\u4f3c\u547d\u9898\u4fe1\u5ff5\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2512.06476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06476", "abs": "https://arxiv.org/abs/2512.06476", "authors": ["Akriti Jain", "Aparna Garimella"], "title": "Knowing What's Missing: Assessing Information Sufficiency in Question Answering", "comment": null, "summary": "Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.", "AI": {"tldr": "\u63d0\u51faIdentify-then-Verify\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u8bc6\u522b\u7f3a\u5931\u4fe1\u606f\u518d\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u66f4\u53ef\u9760\u5730\u5224\u65ad\u4e0a\u4e0b\u6587\u662f\u5426\u8db3\u591f\u56de\u7b54\u95ee\u9898", "motivation": "\u73b0\u6709\u7b80\u5355\u63d0\u793a\u7b56\u7565\u5728\u5904\u7406\u9700\u8981\u63a8\u7406\u7684\u95ee\u9898\u65f6\u7ecf\u5e38\u5931\u8d25\uff0c\u65e0\u6cd5\u53ef\u9760\u5224\u65ad\u4e0a\u4e0b\u6587\u662f\u5426\u5305\u542b\u8db3\u591f\u4fe1\u606f\u6765\u56de\u7b54\u95ee\u9898", "method": "\u63d0\u51fa\u7ed3\u6784\u5316Identify-then-Verify\u6846\u67b6\uff1a\u9996\u5148\u751f\u6210\u591a\u4e2a\u5173\u4e8e\u7f3a\u5931\u4fe1\u606f\u7684\u5047\u8bbe\u5e76\u5efa\u7acb\u8bed\u4e49\u5171\u8bc6\uff0c\u7136\u540e\u8fdb\u884c\u5173\u952e\u9a8c\u8bc1\u6b65\u9aa4\uff0c\u5f3a\u5236\u6a21\u578b\u91cd\u65b0\u68c0\u67e5\u6e90\u6587\u672c\u786e\u8ba4\u4fe1\u606f\u662f\u5426\u771f\u6b63\u7f3a\u5931", "result": "\u5728\u591a\u6837\u5316\u7684\u591a\u8df3\u548c\u4e8b\u5b9e\u6027QA\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5bfc\u6a21\u578b\u8bc1\u660e\u5176\u5173\u4e8e\u7f3a\u5931\u4fe1\u606f\u7684\u5224\u65ad\uff0c\u80fd\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u5145\u5206\u6027\u5224\u65ad\u5e76\u6e05\u6670\u8868\u8fbe\u4fe1\u606f\u7f3a\u53e3", "conclusion": "\u901a\u8fc7\u5148\u63a8\u7406\u7f3a\u5931\u4fe1\u606f\u518d\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u53ef\u9760\u5730\u8bc4\u4f30\u4e0a\u4e0b\u6587\u5145\u5206\u6027\uff0c\u63d0\u9ad8\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u9760\u6027"}}
{"id": "2512.06629", "categories": ["cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.06629", "abs": "https://arxiv.org/abs/2512.06629", "authors": ["Xiao-li Xia", "Hou-biao Li"], "title": "FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection", "comment": "36 pages, 14 figures,Table 5", "summary": "Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.", "AI": {"tldr": "FlatFormer\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\"\u4fe1\u606f\u6ce8\u5165\u800c\u975e\u7ed3\u6784\u5806\u53e0\"\u8bbe\u8ba1\u8303\u5f0f\u7684\u8f7b\u91cf\u7ea7\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u8f93\u5165\u7f16\u7801\u548c\u9884\u8ba1\u7b97\u5e42\u5f8b\u504f\u7f6e\u673a\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u9762\u4e34\"\u6027\u80fd-\u590d\u6742\u5ea6\u9677\u9631\"\uff1a\u6355\u6349\u590d\u6742\u8ba4\u77e5\u52a8\u6001\uff08\u5982\u5b66\u4e60\u4f1a\u8bdd\u548c\u8bb0\u5fc6\u8870\u51cf\uff09\u901a\u5e38\u9700\u8981\u6df1\u5ea6\u5c42\u6b21\u67b6\u6784\uff0c\u8fd9\u5bfc\u81f4\u5b9e\u65f6\u90e8\u7f72\u7684\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u80fd\u964d\u4f4e\u590d\u6742\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFlatFormer\u67b6\u6784\uff0c\u91c7\u7528\u6807\u51c6\u5e73\u9762Transformer\uff0c\u901a\u8fc7\u4e24\u79cd\u8f7b\u91cf\u7ea7\u6ce8\u5165\u673a\u5236\u589e\u5f3a\uff1a(1) \u6df7\u5408\u8f93\u5165\u7f16\u7801\u7b56\u7565\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u4f1a\u8bdd\u6807\u8bc6\u7b26\u548c\u56fa\u5b9a\u6b63\u5f26\u6b65\u957f\u5d4c\u5165\uff1b(2) \u5c06\u9884\u8ba1\u7b97\u7684\u5e42\u5f8b\u504f\u7f6e\u76f4\u63a5\u96c6\u6210\u5230\u6ce8\u610f\u529b\u5bf9\u6570\u4e2d\uff0c\u663e\u5f0f\u5efa\u6a21\u9057\u5fd8\u66f2\u7ebf\u3002", "result": "\u5728\u56db\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08\u5982EdNet\u3001Junyi\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFlatFormer\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u5728EdNet\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u6700\u5f3a\u5c42\u6b21\u57fa\u7ebf\uff08HiTSKT\uff09\uff0c\u7edd\u5bf9AUC\u63d0\u53478.3%\uff0c\u53c2\u6570\u4f7f\u7528\u91cf\u4e0d\u523015%\uff0c\u63a8\u7406\u901f\u5ea6\u7ea6\u5feb3\u500d\u3002", "conclusion": "\u9ad8\u8ba4\u77e5\u4fdd\u771f\u5ea6\u4e0d\u9700\u8981\u67b6\u6784\u590d\u6742\u6027\uff0c\u901a\u8fc7\u4fe1\u606f\u6ce8\u5165\u800c\u975e\u7ed3\u6784\u5806\u53e0\u7684\u8bbe\u8ba1\u8303\u5f0f\u53ef\u4ee5\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u5b9e\u65f6\u77e5\u8bc6\u8ffd\u8e2a\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.06483", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06483", "abs": "https://arxiv.org/abs/2512.06483", "authors": ["Elias-Leander Ahlers", "Witold Brunsmann", "Malte Schilling"], "title": "Classifying German Language Proficiency Levels Using Large Language Models", "comment": "Accepted at 3rd International Conference on Foundation and Large Language Models (FLLM2025), Vienna (Austria)", "summary": "Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5c06\u5fb7\u8bed\u6587\u672c\u6309CEFR\u8bed\u8a00\u80fd\u529b\u7b49\u7ea7\u5206\u7c7b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u6837\u5316\u6570\u636e\u96c6\u5e76\u8bc4\u4f30\u591a\u79cd\u6280\u672f\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u8bed\u8a00\u80fd\u529b\u8bc4\u4f30\u5bf9\u6559\u80b2\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u6839\u636e\u5b66\u4e60\u8005\u9700\u6c42\u63d0\u4f9b\u5b9a\u5236\u5316\u6559\u5b66\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5c06\u5fb7\u8bed\u6587\u672c\u6309CEFR\u6846\u67b6\u5206\u7c7b\u5230\u4e0d\u540c\u80fd\u529b\u7b49\u7ea7\u7684\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u591a\u4e2a\u73b0\u6709CEFR\u6807\u6ce8\u8bed\u6599\u5e93\u548c\u5408\u6210\u6570\u636e\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u3001\u5fae\u8c03LLaMA-3-8B-Instruct\u6a21\u578b\uff0c\u4ee5\u53ca\u57fa\u4e8eLLM\u5185\u90e8\u795e\u7ecf\u72b6\u6001\u7684\u63a2\u6d4b\u5206\u7c7b\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684CEFR\u5206\u7c7b\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fb7\u8bed\u6587\u672cCEFR\u7b49\u7ea7\u81ea\u52a8\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u8bed\u8a00\u80fd\u529b\u8bc4\u4f30\uff0c\u4e3a\u6559\u80b2\u9886\u57df\u7684\u4e2a\u6027\u5316\u6559\u5b66\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2512.06012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06012", "abs": "https://arxiv.org/abs/2512.06012", "authors": ["Emmanuel Akeweje", "Conall Kirk", "Chi-Wai Chan", "Denis Dowling", "Mimi Zhang"], "title": "High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing", "comment": null, "summary": "Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u901a\u91cf\u6210\u50cf\u548c\u805a\u7c7b\u5206\u6790\u5927\u89c4\u6a21\u8868\u5f81\u91d1\u5c5e\u7c89\u672b\u5f62\u6001\uff0c\u4e3aSLM\u5de5\u827a\u63d0\u4f9b\u5b9e\u65f6\u539f\u6599\u76d1\u63a7", "motivation": "\u4f20\u7edf\u7c89\u672b\u8868\u5f81\u65b9\u6cd5\u901a\u91cf\u4f4e\u4e14\u5b9a\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u5de5\u4e1a\u89c4\u6a21\u6279\u6b21\u7684\u5f02\u8d28\u6027\uff0c\u800cSLM\u96f6\u4ef6\u8d28\u91cf\u4e25\u91cd\u4f9d\u8d56\u539f\u6599\u5f62\u6001", "method": "\u5f00\u53d1\u4e09\u79cd\u805a\u7c7b\u6d41\u7a0b\uff1a\u81ea\u7f16\u7801\u5668\u6d41\u7a0b\u3001\u5f62\u72b6\u63cf\u8ff0\u7b26\u6d41\u7a0b\u548c\u51fd\u6570\u6570\u636e\u6d41\u7a0b\uff0c\u5728\u7ea6126,000\u4e2a\u7c89\u672b\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u5085\u91cc\u53f6\u63cf\u8ff0\u7b26+k-means\u65b9\u6cd5", "result": "\u5085\u91cc\u53f6\u63cf\u8ff0\u7b26+k-means\u6d41\u7a0b\u6700\u6709\u6548\uff0c\u83b7\u5f97\u6700\u4f4e\u7684Davies-Bouldin\u6307\u6570\u548c\u6700\u9ad8\u7684Calinski-Harabasz\u5206\u6570\uff0c\u540c\u65f6\u5728\u6807\u51c6\u5de5\u4f5c\u7ad9\u4e0a\u6bcf\u4e2a\u9897\u7c92\u4fdd\u6301\u4e9a\u6beb\u79d2\u7ea7\u8fd0\u884c\u65f6\u95f4", "conclusion": "\u8be5\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u4e86\u7c89\u672b\u5f62\u6001\u7684\u5feb\u901f\u81ea\u52a8\u8bc4\u4f30\uff0c\u652f\u6301\u8ddf\u8e2a\u91cd\u590d\u4f7f\u7528\u5468\u671f\u4e2d\u7684\u5f62\u72b6\u6f14\u53d8\uff0c\u4e3aSLM\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5b9e\u65f6\u539f\u6599\u76d1\u63a7\u63d0\u4f9b\u4e86\u9014\u5f84"}}
{"id": "2512.06653", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06653", "abs": "https://arxiv.org/abs/2512.06653", "authors": ["Hengzhi Lan", "Yue Yu", "Li Qian", "Li Peng", "Jie Wu", "Wei Liu", "Jian Luan", "Ting Bai"], "title": "LightSearcher: Efficient DeepSearch via Experiential Memory", "comment": "10 pages, 5 figures", "summary": "DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.", "AI": {"tldr": "LightSearcher\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u7ecf\u9a8c\u8bb0\u5fc6\u548c\u81ea\u9002\u5e94\u5956\u52b1\u673a\u5236\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11DeepSearch\u8303\u5f0f\u4e2d\u7684\u5de5\u5177\u8c03\u7528\u6b21\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684RL\u9a71\u52a8\u7684DeepSearch\u7cfb\u7edf\u5b58\u5728\u51c6\u786e\u6027-\u6548\u7387\u7684\u8df7\u8df7\u677f\u6743\u8861\uff1a\u9891\u7e41\u8c03\u7528\u641c\u7d22\u5de5\u5177\u53ef\u4ee5\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46\u4f1a\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u6548\u7387\u4e0b\u964d\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u56fa\u6709\u77db\u76fe\u3002", "method": "\u63d0\u51faLightSearcher\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6210\u529f\u63a8\u7406\u6a21\u5f0f\u6458\u8981\u7684\u6587\u672c\u7ecf\u9a8c\u8bb0\u5fc6\uff1b2) \u4ec5\u5728\u6b63\u786e\u7b54\u6848\u573a\u666f\u4e0b\u60e9\u7f5a\u5197\u4f59\u5de5\u5177\u8c03\u7528\u7684\u81ea\u9002\u5e94\u5956\u52b1\u5851\u9020\u673a\u5236\u3002", "result": "\u5728\u56db\u4e2a\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLightSearcher\u5728\u4fdd\u6301\u4e0eSOTA\u57fa\u7ebfReSearch\u76f8\u5f53\u7684\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5c06\u641c\u7d22\u5de5\u5177\u8c03\u7528\u51cf\u5c1139.6%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1148.6%\uff0ctoken\u6d88\u8017\u51cf\u5c1121.2%\u3002", "conclusion": "LightSearcher\u901a\u8fc7\u521b\u65b0\u7684\u7ecf\u9a8c\u8bb0\u5fc6\u548c\u5956\u52b1\u673a\u5236\u8bbe\u8ba1\uff0c\u6709\u6548\u5e73\u8861\u4e86DeepSearch\u8303\u5f0f\u4e2d\u7684\u51c6\u786e\u6027-\u6548\u7387\u6743\u8861\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u6df1\u5ea6\u63a8\u7406\u7cfb\u7edf\u3002"}}
{"id": "2512.06515", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06515", "abs": "https://arxiv.org/abs/2512.06515", "authors": ["Somnath Banerjee", "Sayan Layek", "Sayantan Adak", "Mykola Pechenizkiy", "Animesh Mukherjee", "Rima Hazra"], "title": "ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models", "comment": null, "summary": "Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned \"harm vector\" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.", "AI": {"tldr": "ProSocialAlign\u662f\u4e00\u4e2a\u6d4b\u8bd5\u65f6\u53c2\u6570\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u8bcd\u5178\u7ea6\u675f\u751f\u6210\u548c\u65b9\u5411\u6027\u8c03\u8282\uff0c\u5728\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5b89\u5168\u3001\u5171\u60c5\u3001\u4ef7\u503c\u5bf9\u9f50\u7684\u54cd\u5e94\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u8303\u5f0f\u5728\u60c5\u611f\u6fc0\u70c8\u6216\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u5b58\u5728\u4e0d\u8db3\uff1a\u4ec5\u62d2\u7edd\u53ef\u80fd\u758f\u8fdc\u7528\u6237\uff0c\u800c\u7b80\u5355\u987a\u4ece\u53ef\u80fd\u653e\u5927\u98ce\u9669\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u4fdd\u6301\u5b89\u5168\u7684\u540c\u65f6\u63d0\u4f9b\u5171\u60c5\u548c\u4eba\u6027\u5316\u56de\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u6d4b\u8bd5\u65f6\u53c2\u6570\u9ad8\u6548\u6846\u67b6\uff0c\u5c06\u5b89\u5168\u5f62\u5f0f\u5316\u4e3a\u8bcd\u5178\u7ea6\u675f\u751f\u6210\uff1a\u9996\u5148\u5e94\u7528\u786c\u7ea6\u675f\u6d88\u9664\u6709\u5bb3\u5ef6\u7eed\uff0c\u7136\u540e\u5728\u5b89\u5168\u96c6\u5408\u5185\u4f18\u5316\u4eb2\u793e\u4f1a\u8d28\u91cf\u3002\u7ed3\u5408(i)\u65b9\u5411\u6027\u8c03\u8282\uff08\u5728\u53c2\u6570\u7a7a\u95f4\u51cf\u53bb\u5b66\u4e60\u7684\"\u4f24\u5bb3\u5411\u91cf\"\uff09\uff0c(ii)\u504f\u597d\u611f\u77e5\u81ea\u56de\u5f52\u5956\u52b1\u5efa\u6a21\uff08\u8de8\u5c5e\u6027\u8054\u5408\u8bad\u7ec3\uff0c\u5e26\u68af\u5ea6\u51b2\u7a81\u89e3\u51b3\uff09\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u3001\u7528\u6237\u53ef\u63a7\u7684\u89e3\u7801\u3002", "result": "\u5728\u4e94\u4e2a\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u4e0d\u5b89\u5168\u6cc4\u6f0f\u5e76\u63d0\u5347\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u5ea6\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "ProSocialAlign\u4e3a\u5728\u63a8\u7406\u65f6\u751f\u6210\u4e0a\u4e0b\u6587\u654f\u611f\u3001\u5b89\u5168\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u54cd\u5e94\u63d0\u4f9b\u4e86\u7a33\u5065\u548c\u6a21\u5757\u5316\u7684\u57fa\u7840\u3002"}}
{"id": "2512.06013", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06013", "abs": "https://arxiv.org/abs/2512.06013", "authors": ["Wenhao Li", "Chengwei Ma", "Weixin Mao"], "title": "VAT: Vision Action Transformer by Unlocking Full Representation of ViT", "comment": null, "summary": "In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.", "AI": {"tldr": "Vision Action Transformer (VAT) \u901a\u8fc7\u5229\u7528ViT\u6240\u6709\u5c42\u7684\u7279\u5f81\u5c42\u6b21\u7ed3\u6784\uff0c\u5b9e\u73b0\u611f\u77e5\u4e0e\u52a8\u4f5c\u751f\u6210\u7684\u6df1\u5ea6\u878d\u5408\uff0c\u5728\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\u4e2d\uff0cVision Transformers\u901a\u5e38\u53ea\u4f7f\u7528\u6700\u540e\u4e00\u5c42\u7684\u7279\u5f81\uff0c\u4e22\u5f03\u4e86\u6709\u4ef7\u503c\u7684\u4fe1\u606f\uff0c\u5bfc\u81f4\u8868\u793a\u80fd\u529b\u4e0d\u8db3", "method": "\u63d0\u51faVision Action Transformer (VAT)\uff0c\u4eceViT\u6269\u5c55\u800c\u6765\uff0c\u5904\u7406\u4e13\u95e8\u7684\u52a8\u4f5c\u4ee4\u724c\u4e0e\u6240\u6709Transformer\u5c42\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5b9e\u73b0\u611f\u77e5\u4e0e\u52a8\u4f5c\u751f\u6210\u7684\u6e10\u8fdb\u5f0f\u6df1\u5ea6\u878d\u5408", "result": "\u5728\u56db\u4e2aLIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523098.15%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4e86OpenVLA-OFT\u7b49\u5148\u524d\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u65b0\u7684SOTA", "conclusion": "VAT\u4e0d\u4ec5\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\uff0c\u8fd8\u8bc1\u660e\u4e86\u5229\u7528\u89c6\u89c9\u6a21\u578b\u5b8c\u6574\"\u8868\u793a\u8f68\u8ff9\"\u5bf9\u4e8e\u63a8\u8fdb\u673a\u5668\u4eba\u7b56\u7565\u7684\u91cd\u8981\u6027"}}
{"id": "2512.06705", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06705", "abs": "https://arxiv.org/abs/2512.06705", "authors": ["Yongyuan He", "Yi Bu"], "title": "Academic journals' AI policies fail to curb the surge in AI-assisted academic writing", "comment": "40 pages, 10 figures, and 9 tables", "summary": "The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e865114\u79cd\u671f\u520a\u548c520\u4e07\u7bc7\u8bba\u6587\uff0c\u53d1\u73b0\u5c3d\u7ba170%\u671f\u520a\u5236\u5b9a\u4e86AI\u4f7f\u7528\u653f\u7b56\uff0c\u4f46AI\u5199\u4f5c\u5de5\u5177\u4f7f\u7528\u7387\u5728\u5404\u5b66\u79d1\u5747\u5927\u5e45\u589e\u957f\uff0c\u4e14\u653f\u7b56\u6709\u65e0\u5bf9\u4f7f\u7528\u7387\u65e0\u663e\u8457\u5f71\u54cd\u3002\u975e\u82f1\u8bed\u56fd\u5bb6\u3001\u7269\u7406\u79d1\u5b66\u548c\u9ad8\u5f00\u653e\u83b7\u53d6\u671f\u520a\u589e\u957f\u6700\u5feb\u3002\u5173\u952e\u53d1\u73b0\u662f\u900f\u660e\u5ea6\u5dee\u8ddd\uff1a2023\u5e74\u540e\u76847.5\u4e07\u7bc7\u8bba\u6587\u4e2d\u4ec576\u7bc7\uff080.1%\uff09\u660e\u786e\u62ab\u9732\u4e86AI\u4f7f\u7528\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u5feb\u901f\u5e94\u7528\uff0c\u671f\u520a\u548c\u51fa\u7248\u5546\u7eb7\u7eb7\u51fa\u53f0\u76f8\u5173\u653f\u7b56\uff0c\u4f46\u8fd9\u4e9b\u653f\u7b56\u7684\u5b9e\u9645\u6548\u679c\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30AI\u4f7f\u7528\u6307\u5357\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u771f\u5b9e\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e865,114\u79cd\u671f\u520a\u548c\u8d85\u8fc7520\u4e07\u7bc7\u8bba\u6587\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u8bc4\u4f30AI\u653f\u7b56\u7684\u6548\u679c\u3002\u7279\u522b\u5bf9164,000\u7bc7\u79d1\u5b66\u51fa\u7248\u7269\u8fdb\u884c\u4e86\u5168\u6587\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce82023\u5e74\u540e\u53d1\u8868\u768475,000\u7bc7\u8bba\u6587\u4e2dAI\u4f7f\u7528\u62ab\u9732\u60c5\u51b5\u3002", "result": "1. \u5c3d\u7ba170%\u7684\u671f\u520a\u91c7\u7528\u4e86AI\u653f\u7b56\uff08\u4e3b\u8981\u662f\u8981\u6c42\u62ab\u9732\uff09\uff0c\u4f46\u7814\u7a76\u4eba\u5458\u4f7f\u7528AI\u5199\u4f5c\u5de5\u5177\u7684\u6bd4\u4f8b\u5728\u5404\u5b66\u79d1\u5747\u5927\u5e45\u589e\u957f\uff1b2. \u6709\u653f\u7b56\u548c\u65e0\u653f\u7b56\u671f\u520a\u4e4b\u95f4\u7684AI\u4f7f\u7528\u7387\u65e0\u663e\u8457\u5dee\u5f02\uff1b3. \u975e\u82f1\u8bed\u56fd\u5bb6\u3001\u7269\u7406\u79d1\u5b66\u548c\u9ad8\u5f00\u653e\u83b7\u53d6\u671f\u520a\u7684AI\u4f7f\u7528\u589e\u957f\u6700\u5feb\uff1b4. \u900f\u660e\u5ea6\u5dee\u8ddd\u663e\u8457\uff1a2023\u5e74\u540e\u53d1\u8868\u768475,000\u7bc7\u8bba\u6587\u4e2d\uff0c\u53ea\u670976\u7bc7\uff080.1%\uff09\u660e\u786e\u62ab\u9732\u4e86AI\u4f7f\u7528\u3002", "conclusion": "\u5f53\u524d\u7684AI\u653f\u7b56\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u80fd\u4fc3\u8fdb\u900f\u660e\u5ea6\u6216\u9650\u5236AI\u7684\u91c7\u7528\u3002\u7814\u7a76\u547c\u5401\u91cd\u65b0\u8bc4\u4f30\u4f26\u7406\u6846\u67b6\uff0c\u4ee5\u4fc3\u8fdb\u79d1\u5b66\u4e2d\u8d1f\u8d23\u4efb\u7684AI\u6574\u5408\u3002"}}
{"id": "2512.06586", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06586", "abs": "https://arxiv.org/abs/2512.06586", "authors": ["Mikhail Zimin", "Milyausha Shamsutdinova", "Georgii Andriushchenko"], "title": "Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract", "comment": null, "summary": "Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.", "AI": {"tldr": "AlignRuScore\uff1a\u5c06AlignScore\u6307\u6807\u9002\u914d\u5230\u4fc4\u8bed\uff0c\u7528\u4e8e\u8bc4\u4f30\u4fc4\u8bed\u6587\u672c\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u586b\u8865\u4e86\u4fc4\u8bed\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u4fc4\u8bed\u6587\u672c\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u5de5\u5177\uff0c\u73b0\u6709\u5de5\u5177\u4e3b\u8981\u9762\u5411\u82f1\u8bed\u8bed\u6599\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u5f00\u53d1\u9002\u7528\u4e8e\u4fc4\u8bed\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u57fa\u4e8eRuBERT\u7684\u5bf9\u9f50\u6a21\u578b\uff0c\u4f7f\u7528\u7279\u5b9a\u4efb\u52a1\u7684\u5206\u7c7b\u548c\u56de\u5f52\u5934\uff0c\u5728\u4fc4\u8bed\u548c\u7ffb\u8bd1\u7684\u82f1\u8bed\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5c06AlignScore\u6307\u6807\u9002\u914d\u5230\u4fc4\u8bed\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u7edf\u4e00\u7684\u5bf9\u9f50\u6307\u6807\u53ef\u4ee5\u6210\u529f\u79fb\u690d\u5230\u4fc4\u8bed\uff0c\u4e3a\u7a33\u5065\u7684\u591a\u8bed\u8a00\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u53d1\u5e03\u4e86\u7ffb\u8bd1\u8bed\u6599\u5e93\u3001\u6a21\u578b\u68c0\u67e5\u70b9\u548c\u4ee3\u7801\u3002", "conclusion": "AlignRuScore\u6210\u529f\u5c06\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u5de5\u5177\u6269\u5c55\u5230\u4fc4\u8bed\uff0c\u4e3a\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u8d44\u6e90\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u5de5\u4f5c\u3002"}}
{"id": "2512.06014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06014", "abs": "https://arxiv.org/abs/2512.06014", "authors": ["Jiho Shin", "Dominic Marshall", "Matthieu Komorowski"], "title": "Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets", "comment": null, "summary": "Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u4e24\u4e2a\u5927\u89c4\u6a21\u80f8\u90e8X\u5149\u5d4c\u5165\u6a21\u578b\uff08CXR-Foundation\u548cMedImageInsight\uff09\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0MedImageInsight\u6027\u80fd\u7565\u4f18\uff0c\u800cCXR-Foundation\u5728\u8de8\u6570\u636e\u96c6\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u8868\u793a\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u8de8\u6570\u636e\u96c6\u4e0a\u7684\u6bd4\u8f83\u884c\u4e3a\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u5bf9\u4e24\u4e2a\u5927\u578b\u80f8\u90e8X\u5149\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u533b\u5b66\u57fa\u7840\u6a21\u578b\u7684\u8bc4\u4f30\u5efa\u7acb\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\u3002", "method": "\u4f7f\u7528\u7edf\u4e00\u7684\u9884\u5904\u7406\u6d41\u7a0b\u548c\u56fa\u5b9a\u7684\u4e0b\u6e38\u5206\u7c7b\u5668\uff0c\u5728MIMIC-CR\u548cNIH ChestX-ray14\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e24\u4e2a\u6a21\u578b\u3002\u76f4\u63a5\u4ece\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u63d0\u53d6\u5d4c\u5165\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7LightGBM\u5206\u7c7b\u5668\u5728\u591a\u4e2a\u75be\u75c5\u6807\u7b7e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u62a5\u544a\u5e73\u5747AUROC\u548cF1\u5206\u6570\u53ca95%\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "MedImageInsight\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e2d\u6027\u80fd\u7565\u9ad8\uff0c\u800cCXR-Foundation\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8de8\u6570\u636e\u96c6\u7a33\u5b9a\u6027\u3002MedImageInsight\u5d4c\u5165\u7684\u65e0\u76d1\u7763\u805a\u7c7b\u663e\u793a\u51fa\u4e0e\u5b9a\u91cf\u7ed3\u679c\u4e00\u81f4\u7684\u75be\u75c5\u7279\u5f02\u6027\u7ed3\u6784\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u533b\u5b66\u57fa\u7840\u6a21\u578b\u6807\u51c6\u5316\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u591a\u6a21\u6001\u548c\u4e34\u5e8a\u6574\u5408\u7814\u7a76\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\u3002"}}
{"id": "2512.06656", "categories": ["cs.CL", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.06656", "abs": "https://arxiv.org/abs/2512.06656", "authors": ["Kwabena Yamoah", "Cass Dykeman"], "title": "The Online Discourse of Virtual Reality and Anxiety", "comment": "Three tables and two figures. Unfortunately, I did not formally register the dataset prior to conducting the analysis", "summary": "VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u8bed\u6599\u5e93\u8bed\u8a00\u5b66\u65b9\u6cd5\u5206\u6790\u5728\u7ebf\u8ba8\u8bba\u4e2d\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u4e0e\u7126\u8651\u76f8\u5173\u7684\u8bdd\u9898\uff0c\u53d1\u73b0VR\u3001Oculus\u548c\u5934\u663e\u662f\u6700\u5e38\u8ba8\u8bba\u7684\u8bcd\u6c47\uff0c\u5e76\u8bc6\u522b\u4e86\u4e0eVR\u8bbe\u8ba1\u3001\u4f53\u9a8c\u548c\u5f00\u53d1\u76f8\u5173\u7684\u4ecb\u8bcd\u642d\u914d\u6a21\u5f0f\u3002", "motivation": "VR\u5728\u6cbb\u7597\u5e7f\u6cdb\u6027\u7126\u8651\u969c\u788d\u6216\u793e\u4ea4\u7126\u8651\u7b49\u4e34\u5e8a\u95ee\u9898\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4e3a\u60a3\u8005\u798f\u7949\u548c\u62a4\u7406\u521b\u9020\u4e86\u65b0\u9014\u5f84\u3002\u4e86\u89e3\u7528\u6237\u5bf9\u8be5\u6280\u672f\u7684\u5728\u7ebf\u8ba8\u8bba\u53ef\u80fd\u8fdb\u4e00\u6b65\u652f\u6301\u5176\u7597\u6548\u3002", "method": "\u91c7\u7528\u8bed\u6599\u5e93\u8bed\u8a00\u5b66\u65b9\u6cd5\uff0c\u4f7f\u7528Sketch Engine\u8f6f\u4ef6\u8bc6\u522b\u5728\u7ebf\u8ba8\u8bba\u4e2d\u9891\u7e41\u4f7f\u7528\u7684\u8bcd\u6c47\u53ca\u5176\u642d\u914d\u5173\u7cfb\uff0c\u57fa\u4e8eEnglish Trends\u8bed\u6599\u5e93\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728VR\u4e0e\u7126\u8651\u5b50\u8bed\u6599\u5e93\u4e2d\uff0cVR\u3001Oculus\u548c\u5934\u663e\u662f\u6700\u5e38\u8ba8\u8bba\u7684\u8bcd\u6c47\uff0c\u8fd9\u4e9b\u7ed3\u679c\u6307\u5411\u865a\u62df\u7cfb\u7edf\u7684\u53d1\u5c55\u4ee5\u53ca\u4f7f\u865a\u62df\u73af\u5883\u89c2\u770b\u548c\u4ea4\u4e92\u6210\u4e3a\u53ef\u80fd\u7684\u7269\u7406\u8bbe\u5907\u3002\u6b64\u5916\u8fd8\u8bc6\u522b\u4e86\"of virtual reality\"\uff08\u8bbe\u8ba1\uff09\u3001\"in virtual reality\"\uff08\u4f53\u9a8c\uff09\u548c\"for virtual reality\"\uff08\u5f00\u53d1\uff09\u7b49\u4ecb\u8bcd\u642d\u914d\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aVR\u4e0e\u7126\u8651\u5728\u4e00\u822c\u8bdd\u8bed\u4e2d\u7684\u8ba8\u8bba\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u4e3a\u901a\u8fc7\u53d1\u5c55\u548c\u53ef\u8bbf\u95ee\u6027\u652f\u6301\u54a8\u8be2\u9700\u6c42\u7684\u672a\u6765\u673a\u4f1a\u63d0\u4f9b\u4e86\u9014\u5f84\u3002"}}
{"id": "2512.06020", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06020", "abs": "https://arxiv.org/abs/2512.06020", "authors": ["Wenyi Mo", "Tianyu Zhang", "Yalong Bai", "Ligong Han", "Ying Ba", "Dimitris N. Metaxas"], "title": "PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation", "comment": "Project Page: \\href{https://prefgen.github.io/}{\\texttt{https://prefgen.github.io}}", "summary": "Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5229\u7528MLLM\u63d0\u53d6\u7528\u6237\u504f\u597d\u8868\u5f81\u5e76\u6ce8\u5165\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u504f\u597d\u5bfc\u5411\u7684\u89c6\u89c9\u95ee\u7b54\u548c\u5224\u522b\u4efb\u52a1\u5b66\u4e60\u7528\u6237\u7279\u5f81\uff0c\u4f7f\u7528MMD\u5bf9\u9f50\u635f\u5931\u786e\u4fdd\u4e0e\u6269\u6563\u7f16\u7801\u5668\u517c\u5bb9\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7528\u6237\u7ec6\u5fae\u7684\u5ba1\u7f8e\u504f\u597d\uff0c\u7f3a\u4e4f\u6709\u6548\u7f16\u7801\u4e2a\u6027\u5316\u89c6\u89c9\u4fe1\u53f7\u7684\u673a\u5236\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u66f4\u597d\u7406\u89e3\u5e76\u53cd\u6620\u7528\u6237\u4e2a\u4eba\u559c\u597d\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u3002", "method": "1) \u8bad\u7ec3MLLM\u8fdb\u884c\u504f\u597d\u5bfc\u5411\u7684\u89c6\u89c9\u95ee\u7b54\u4ee5\u6355\u6349\u8bed\u4e49\u7ebf\u7d22\uff1b2) \u5f15\u5165\u7528\u6237\u95f4\u548c\u7528\u6237\u5185\u5224\u522b\u4efb\u52a1\u5206\u79bb\u504f\u597d\u76f8\u5173\u7279\u5f81\uff1b3) \u8bbe\u8ba1\u57fa\u4e8e\u6700\u5927\u5747\u503c\u5dee\u5f02\u7684\u5bf9\u9f50\u635f\u5931\u8fde\u63a5\u6a21\u6001\u5dee\u5f02\uff1b4) \u5c06\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u7528\u4e8e\u6761\u4ef6\u5316\u6269\u6563\u751f\u6210\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u8868\u5f81\u63d0\u53d6\u548c\u5bf9\u9f50\u7b56\u7565\u5728\u4e2a\u6027\u5316\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u591a\u6a21\u6001\u6846\u67b6\u63d0\u53d6\u4e30\u5bcc\u7684\u7528\u6237\u8868\u5f81\u5e76\u6709\u6548\u5bf9\u9f50\u5230\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u65e2\u80fd\u5fe0\u5b9e\u4e8e\u6587\u672c\u63d0\u793a\u53c8\u80fd\u51c6\u786e\u53cd\u6620\u7528\u6237\u4e2a\u4eba\u504f\u597d\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2512.06679", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06679", "abs": "https://arxiv.org/abs/2512.06679", "authors": ["Smitha Muthya Sudheendra", "Mani Deep Cherukuri", "Jaideep Srivastava"], "title": "CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis", "comment": null, "summary": "Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.", "AI": {"tldr": "CMV-Fuse\u662f\u4e00\u4e2a\u8de8\u6a21\u6001\u89c6\u56fe\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u62bd\u8c61\u610f\u4e49\u8868\u793a\u3001\u6210\u5206\u53e5\u6cd5\u3001\u4f9d\u5b58\u53e5\u6cd5\u548c\u8bed\u4e49\u6ce8\u610f\u529b\u56db\u79cd\u8bed\u8a00\u89c6\u89d2\uff0c\u6a21\u62df\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u8fc7\u7a0b\uff0c\u63d0\u5347\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u901a\u5e38\u5229\u7528\u5b64\u7acb\u7684\u8bed\u8a00\u89c6\u89d2\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u81ea\u7136\u5229\u7528\u7684\u7ed3\u6784\u8868\u5f81\u4e4b\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u3002\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u672c\u8d28\u4e0a\u4f9d\u8d56\u4e8e\u4ece\u8868\u5c42\u53e5\u6cd5\u5230\u6df1\u5c42\u8bed\u4e49\u548c\u4e16\u754c\u77e5\u8bc6\u7684\u591a\u4e2a\u4e92\u8865\u89c6\u89d2\u7684\u6574\u5408\u3002", "method": "\u63d0\u51faCMV-Fuse\u6846\u67b6\uff0c\u7cfb\u7edf\u6574\u5408\u56db\u79cd\u8bed\u8a00\u89c6\u89d2\uff1a\u62bd\u8c61\u610f\u4e49\u8868\u793a\u3001\u6210\u5206\u53e5\u6cd5\u3001\u4f9d\u5b58\u53e5\u6cd5\u548c\u8bed\u4e49\u6ce8\u610f\u529b\uff0c\u5e76\u589e\u5f3a\u5916\u90e8\u77e5\u8bc6\u96c6\u6210\u3002\u901a\u8fc7\u5c40\u90e8\u53e5\u6cd5\u3001\u4e2d\u95f4\u8bed\u4e49\u548c\u5168\u5c40\u77e5\u8bc6\u4e09\u4e2a\u5c42\u6b21\u7684\u5206\u5c42\u95e8\u63a7\u6ce8\u610f\u529b\u878d\u5408\uff0c\u6355\u6349\u7ec6\u7c92\u5ea6\u7ed3\u6784\u6a21\u5f0f\u548c\u5e7f\u6cdb\u4e0a\u4e0b\u6587\u7406\u89e3\u3002\u91c7\u7528\u65b0\u9896\u7684\u7ed3\u6784\u611f\u77e5\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\u786e\u4fdd\u4e92\u8865\u8868\u5f81\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCMV-Fuse\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5206\u6790\u63ed\u793a\u4e86\u6bcf\u79cd\u8bed\u8a00\u89c6\u89d2\u5982\u4f55\u5bf9\u66f4\u7a33\u5065\u7684\u60c5\u611f\u5206\u6790\u505a\u51fa\u8d21\u732e\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u7684\u591a\u89c6\u89d2\u878d\u5408\u65b9\u6cd5\uff0cCMV-Fuse\u80fd\u591f\u66f4\u6709\u6548\u5730\u6574\u5408\u4e92\u8865\u7684\u8bed\u8a00\u4fe1\u606f\uff0c\u63d0\u5347\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.06681", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06681", "abs": "https://arxiv.org/abs/2512.06681", "authors": ["Amartya Hatua"], "title": "Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis", "comment": null, "summary": "We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.", "AI": {"tldr": "GPT-2\u7684\u60c5\u611f\u8ba1\u7b97\u673a\u5236\u7814\u7a76\uff1a\u65e9\u671f\u5c42\u8d1f\u8d23\u8bcd\u6c47\u60c5\u611f\u68c0\u6d4b\uff0c\u4f46\u4e0a\u4e0b\u6587\u6574\u5408\u53d1\u751f\u5728\u665a\u671f\u5c42\u800c\u975e\u4e2d\u671f\u5c42\uff0c\u4e14\u91c7\u7528\u7edf\u4e00\u7684\u975e\u6a21\u5757\u5316\u673a\u5236", "motivation": "\u7814\u7a76GPT-2\u4e2d\u60c5\u611f\u4fe1\u606f\u5904\u7406\u7684\u673a\u5236\uff0c\u7279\u522b\u662f\u9a8c\u8bc1\u5173\u4e8e\u4e24\u9636\u6bb5\u60c5\u611f\u67b6\u6784\u7684\u5047\u8bbe\uff1a\u65e9\u671f\u8bcd\u6c47\u68c0\u6d4b\u548c\u4e2d\u671f\u4e0a\u4e0b\u6587\u6574\u5408", "method": "\u4f7f\u7528\u7cfb\u7edf\u6027\u6fc0\u6d3b\u4fee\u8865\u6280\u672f\uff0c\u5728GPT-2\u7684\u6240\u670912\u4e2a\u5c42\u4e2d\u8fdb\u884c\u56e0\u679c\u5206\u6790\uff0c\u6d4b\u8bd5\u4e09\u79cd\u4e0a\u4e0b\u6587\u6574\u5408\u5047\u8bbe", "result": "\u65e9\u671f\u5c42\uff080-3\uff09\u786e\u5b9e\u4f5c\u4e3a\u8bcd\u6c47\u60c5\u611f\u68c0\u6d4b\u5668\uff0c\u7f16\u7801\u7a33\u5b9a\u7684\u4f4d\u7f6e\u7279\u5b9a\u6781\u6027\u4fe1\u53f7\uff1b\u4f46\u6240\u6709\u4e09\u79cd\u4e2d\u671f\u5c42\u4e0a\u4e0b\u6587\u6574\u5408\u5047\u8bbe\u90fd\u88ab\u8bc1\u4f2a\uff0c\u4e0a\u4e0b\u6587\u6574\u5408\u4e3b\u8981\u53d1\u751f\u5728\u665a\u671f\u5c42\uff088-11\uff09\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u975e\u6a21\u5757\u5316\u673a\u5236", "conclusion": "GPT-2\u7684\u60c5\u611f\u8ba1\u7b97\u4e0e\u9884\u6d4b\u7684\u5c42\u6b21\u6a21\u5f0f\u4e0d\u540c\uff0c\u4e0a\u4e0b\u6587\u6574\u5408\u53d1\u751f\u5728\u665a\u671f\u5c42\uff0c\u8fd9\u7a81\u663e\u4e86\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0a\u4e0b\u6587\u6574\u5408\u673a\u5236\u8fdb\u884c\u8fdb\u4e00\u6b65\u5b9e\u8bc1\u8868\u5f81\u7684\u5fc5\u8981\u6027"}}
{"id": "2512.06032", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06032", "abs": "https://arxiv.org/abs/2512.06032", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation", "comment": null, "summary": "This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86SAM2\u548cSAM3\u4e4b\u95f4\u7684\u6839\u672c\u6027\u5dee\u5f02\uff0c\u89e3\u91ca\u4e86SAM2\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u5206\u5272\u4e13\u4e1a\u77e5\u8bc6\u4e3a\u4f55\u65e0\u6cd5\u8fc1\u79fb\u5230SAM3\u7684\u591a\u6a21\u6001\u6982\u5ff5\u9a71\u52a8\u8303\u5f0f\uff0c\u63ed\u793a\u4e86\u4ece\u51e0\u4f55\u5206\u5272\u5230\u8bed\u4e49\u6982\u5ff5\u7406\u89e3\u7684\u6839\u672c\u8f6c\u53d8\u3002", "motivation": "\u7814\u7a76SAM2\u548cSAM3\u4e4b\u95f4\u7684\u6839\u672c\u6027\u4e0d\u8fde\u7eed\u6027\uff0c\u89e3\u91ca\u4e3a\u4ec0\u4e48SAM2\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u5206\u5272\u4e13\u4e1a\u77e5\u8bc6\u65e0\u6cd5\u8fc1\u79fb\u5230SAM3\u7684\u591a\u6a21\u6001\u6982\u5ff5\u9a71\u52a8\u8303\u5f0f\uff0c\u9610\u660e\u4ece\u7eaf\u51e0\u4f55\u5206\u5272\u5230\u8bed\u4e49\u6982\u5ff5\u7406\u89e3\u7684\u6839\u672c\u8f6c\u53d8\u3002", "method": "\u901a\u8fc7\u4e94\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u8fdb\u884c\u7ed3\u6784\u5316\u5206\u6790\uff1a(1)\u6982\u5ff5\u6027\u65ad\u88c2\uff1a\u5bf9\u6bd4SAM2\u7684\u7a7a\u95f4\u63d0\u793a\u8bed\u4e49\u4e0eSAM3\u7684\u591a\u6a21\u6001\u878d\u5408\u548c\u6587\u672c\u6761\u4ef6\u63a9\u7801\u751f\u6210\uff1b(2)\u67b6\u6784\u5dee\u5f02\uff1a\u8be6\u7ec6\u5bf9\u6bd4\u7eaf\u89c6\u89c9-\u65f6\u95f4\u8bbe\u8ba1\u4e0e\u89c6\u89c9\u8bed\u8a00\u7f16\u7801\u5668\u3001\u51e0\u4f55\u548c\u793a\u4f8b\u7f16\u7801\u5668\u3001\u878d\u5408\u6a21\u5757\u3001DETR\u98ce\u683c\u89e3\u7801\u5668\u3001\u5bf9\u8c61\u67e5\u8be2\u548c\u4e13\u5bb6\u6df7\u5408\u7684\u96c6\u6210\uff1b(3)\u6570\u636e\u96c6\u548c\u6807\u6ce8\u5dee\u5f02\uff1a\u5bf9\u6bd4SA-V\u89c6\u9891\u63a9\u7801\u4e0eSAM3\u7684\u591a\u6a21\u6001\u6982\u5ff5\u6807\u6ce8\u8bed\u6599\u5e93\uff1b(4)\u8bad\u7ec3\u548c\u8d85\u53c2\u6570\u533a\u522b\uff1a\u5c55\u793a\u4e3a\u4ec0\u4e48SAM2\u4f18\u5316\u77e5\u8bc6\u4e0d\u9002\u7528\u4e8eSAM3\uff1b(5)\u8bc4\u4f30\u3001\u6307\u6807\u548c\u5931\u8d25\u6a21\u5f0f\uff1a\u6982\u8ff0\u4ece\u51e0\u4f55IoU\u6307\u6807\u5230\u8bed\u4e49\u5f00\u653e\u8bcd\u6c47\u8bc4\u4f30\u7684\u8f6c\u53d8\u3002", "result": "\u5206\u6790\u786e\u7acb\u4e86SAM3\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u5206\u5272\u57fa\u7840\u6a21\u578b\u7c7b\u522b\uff0c\u63ed\u793a\u4e86\u4ece\u57fa\u4e8e\u63d0\u793a\u7684\u51e0\u4f55\u5206\u5272\u5230\u6982\u5ff5\u9a71\u52a8\u7684\u8bed\u4e49\u5206\u5272\u7684\u6839\u672c\u8f6c\u53d8\uff0c\u5c55\u793a\u4e86\u4e24\u79cd\u6a21\u578b\u5728\u67b6\u6784\u3001\u8bad\u7ec3\u3001\u8bc4\u4f30\u7b49\u65b9\u9762\u7684\u672c\u8d28\u5dee\u5f02\u3002", "conclusion": "SAM3\u4ee3\u8868\u4e86\u5206\u5272\u57fa\u7840\u6a21\u578b\u7684\u65b0\u7c7b\u522b\uff0c\u6807\u5fd7\u7740\u4ece\u51e0\u4f55\u5206\u5272\u5230\u6982\u5ff5\u9a71\u52a8\u5206\u5272\u65f6\u4ee3\u7684\u8f6c\u53d8\uff0c\u4e3a\u65b0\u5174\u7684\u6982\u5ff5\u9a71\u52a8\u5206\u5272\u9886\u57df\u6307\u660e\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2512.06835", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06835", "abs": "https://arxiv.org/abs/2512.06835", "authors": ["Tingyu Li", "Zheng Sun", "Jingxuan Wei", "Siyuan Li", "Conghui He", "Lijun Wu", "Cheng Tan"], "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning", "comment": "25 pages, 5 figures", "summary": "Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.", "AI": {"tldr": "DoGe\u6846\u67b6\u901a\u8fc7\u53cc\u89e3\u8026\u65b9\u6cd5\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u5956\u52b1\u7834\u89e3\u95ee\u9898\uff0c\u5c06\u5b66\u4e60\u8fc7\u7a0b\u5206\u89e3\u4e3a\u601d\u8003\u8005\u548c\u89e3\u51b3\u8005\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\uff08\u5982\u5316\u5b66\u3001\u5730\u7403\u79d1\u5b66\u3001\u591a\u6a21\u6001\u6570\u5b66\uff09\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u65f6\u9762\u4e34\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u5408\u6210\u6570\u636e\u548c\u81ea\u5956\u52b1\u673a\u5236\u5b58\u5728\u5206\u5e03\u6709\u9650\u548c\u5bf9\u9f50\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5956\u52b1\u7834\u89e3\u73b0\u8c61\u2014\u2014\u6a21\u578b\u5229\u7528\u9ad8\u5956\u52b1\u6a21\u5f0f\uff0c\u5bfc\u81f4\u7b56\u7565\u71b5\u5d29\u6e83\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faDoGe\uff08\u89e3\u8026\u4ee5\u6cdb\u5316\uff09\u53cc\u89e3\u8026\u6846\u67b6\uff1a1\uff09\u5c06\u5b66\u4e60\u8fc7\u7a0b\u89e3\u8026\u4e3a\u601d\u8003\u8005\u548c\u89e3\u51b3\u8005\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u5f15\u5bfc\u6a21\u578b\u9996\u5148\u4ece\u4e0a\u4e0b\u6587\u5b66\u4e60\u800c\u975e\u76f4\u63a5\u89e3\u51b3\u95ee\u9898\uff1b2\uff09\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ece\u81ea\u7531\u63a2\u7d22\u4e0a\u4e0b\u6587\u5230\u5b9e\u9645\u89e3\u51b3\u4efb\u52a1\uff1b3\uff09\u6784\u5efa\u6f14\u5316\u8bfe\u7a0b\u5b66\u4e60\u7ba1\u9053\uff0c\u5305\u62ec\u6269\u5c55\u7684\u672c\u571f\u9886\u57df\u77e5\u8bc6\u8bed\u6599\u5e93\u548c\u8fed\u4ee3\u6f14\u5316\u7684\u79cd\u5b50\u95ee\u9898\u6c60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u73b0\u81ea\u6f14\u5316\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002", "conclusion": "DoGe\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u5b66\u4e60\u8fc7\u7a0b\u548c\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u5956\u52b1\u7834\u89e3\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u81ea\u6f14\u5316\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06690", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06690", "abs": "https://arxiv.org/abs/2512.06690", "authors": ["Chengbing Wang", "Yang Zhang", "Wenjie Wang", "Xiaoyan Zhao", "Fuli Feng", "Xiangnan He", "Tat-Seng Chua"], "title": "Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation", "comment": null, "summary": "Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent \"think-then-generate\" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient \"think-while-generating\" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.", "AI": {"tldr": "FlyThinker\u63d0\u51fa\u4e86\u4e00\u79cd\"\u8fb9\u601d\u8003\u8fb9\u751f\u6210\"\u7684\u9ad8\u6548\u4e2a\u6027\u5316\u957f\u6587\u672c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u63a8\u7406\u6a21\u578b\u751f\u6210\u6f5c\u5728token\u7ea7\u63a8\u7406\u6765\u52a8\u6001\u6307\u5bfc\u54cd\u5e94\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7fa4\u4f53\u7ea7\u504f\u597d\uff0c\u5ffd\u89c6\u4e86\u4e2a\u4eba\u7528\u6237\u9700\u6c42\u3002\u73b0\u6709\u4e2a\u6027\u5316\u65b9\u6cd5\u5982\u63d0\u793a\u5b9a\u5236\u6216\u5fae\u8c03\u96be\u4ee5\u63a8\u7406\u9690\u542b\u504f\u597d\uff0c\u800c\"\u5148\u601d\u8003\u540e\u751f\u6210\"\u65b9\u6cd5\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u9759\u6001\u4e00\u6b21\u6027\u63a8\u7406\u9700\u8981\u4e3a\u5b8c\u6574\u54cd\u5e94\u751f\u6210\u6355\u83b7\u6240\u6709\u76f8\u5173\u4fe1\u606f\uff0c\u5b66\u4e60\u56f0\u96be\u4e14\u96be\u4ee5\u9002\u5e94\u5185\u5bb9\u6f14\u5316\u3002", "method": "FlyThinker\u91c7\u7528\"\u8fb9\u601d\u8003\u8fb9\u751f\u6210\"\u6846\u67b6\uff0c\u4f7f\u7528\u72ec\u7acb\u7684\u63a8\u7406\u6a21\u578b\u5e76\u884c\u751f\u6210\u6f5c\u5728token\u7ea7\u63a8\u7406\uff0c\u8fd9\u4e9b\u63a8\u7406\u88ab\u878d\u5408\u5230\u751f\u6210\u6a21\u578b\u4e2d\u52a8\u6001\u6307\u5bfc\u54cd\u5e94\u751f\u6210\u3002\u63a8\u7406\u6a21\u578b\u4ec5\u4f9d\u8d56\u5148\u524d\u54cd\u5e94\u800c\u975e\u81ea\u8eab\u5148\u524d\u8f93\u51fa\uff0c\u4fdd\u6301\u4e86\u4e0d\u540c\u4f4d\u7f6e\u95f4\u7684\u8bad\u7ec3\u5e76\u884c\u6027\uff0c\u6240\u6709\u63a8\u7406token\u53ef\u5728\u5355\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u751f\u6210\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFlyThinker\u5728\u4fdd\u6301\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4e2a\u6027\u5316\u751f\u6210\u6548\u679c\u3002", "conclusion": "FlyThinker\u901a\u8fc7\u5e76\u884c\u63a8\u7406\u548c\u751f\u6210\u7684\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u6587\u672c\u4e2a\u6027\u5316\u751f\u6210\u4e2d\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u95ee\u9898\uff0c\u4e3a\u4e2a\u6027\u5316\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06065", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06065", "abs": "https://arxiv.org/abs/2512.06065", "authors": ["Runjia Li", "Moayed Haji-Ali", "Ashkan Mirzaei", "Chaoyang Wang", "Arpit Sahni", "Ivan Skorokhodov", "Aliaksandr Siarohin", "Tomas Jakab", "Junlin Han", "Sergey Tulyakov", "Philip Torr", "Willi Menapace"], "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing", "comment": "Project page: https://snap-research.github.io/EgoEdit", "summary": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit", "AI": {"tldr": "\u63d0\u51faEgoEdit\u751f\u6001\u7cfb\u7edf\uff0c\u7528\u4e8e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u7684\u6307\u4ee4\u5f15\u5bfc\u7f16\u8f91\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5feb\u901f\u81ea\u6211\u8fd0\u52a8\u548c\u624b\u7269\u4ea4\u4e92\u65b9\u9762\u7684\u6311\u6218\uff0c\u652f\u6301\u5b9e\u65f6\u4ea4\u4e92\u5f0fAR\u5e94\u7528\u3002", "motivation": "\u73b0\u6709AI\u89c6\u9891\u7f16\u8f91\u5668\u5728\u7b2c\u4e09\u4eba\u79f0\u89c6\u9891\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5b58\u5728\u72ec\u7279\u6311\u6218\uff1a\u5feb\u901f\u81ea\u6211\u8fd0\u52a8\u3001\u9891\u7e41\u7684\u624b\u7269\u4ea4\u4e92\u9020\u6210\u663e\u8457\u9886\u57df\u5dee\u8ddd\uff0c\u4e14\u73b0\u6709\u79bb\u7ebf\u7f16\u8f91\u6d41\u7a0b\u5ef6\u8fdf\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u4ea4\u4e92\u3002", "method": "\u6784\u5efa\u5b8c\u6574\u7684\u751f\u6001\u7cfb\u7edf\uff1a1) EgoEditData - \u4e13\u95e8\u4e3a\u7b2c\u4e00\u4eba\u79f0\u7f16\u8f91\u573a\u666f\u8bbe\u8ba1\u7684\u624b\u52a8\u7b56\u5212\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u624b\u7269\u4ea4\u4e92\u5e76\u660e\u786e\u4fdd\u7559\u624b\u90e8\uff1b2) EgoEdit - \u652f\u6301\u6307\u4ee4\u8ddf\u968f\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u7f16\u8f91\u5668\uff0c\u53ef\u5728\u5355GPU\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5f0f\u63a8\u7406\uff1b3) EgoEditBench - \u9488\u5bf9\u6307\u4ee4\u5fe0\u5b9e\u5ea6\u3001\u624b\u90e8\u548c\u4ea4\u4e92\u4fdd\u7559\u3001\u81ea\u6211\u8fd0\u52a8\u4e0b\u65f6\u95f4\u7a33\u5b9a\u6027\u7684\u8bc4\u4f30\u5957\u4ef6\u3002", "result": "EgoEdit\u5728\u7b2c\u4e00\u4eba\u79f0\u548c\u901a\u7528\u7f16\u8f91\u4efb\u52a1\u4e2d\u90fd\u4ea7\u751f\u65f6\u95f4\u7a33\u5b9a\u3001\u6307\u4ee4\u5fe0\u5b9e\u7684\u7ed3\u679c\uff0c\u5177\u6709\u4ea4\u4e92\u5f0f\u5ef6\u8fdf\u3002\u5728\u7b2c\u4e00\u4eba\u79f0\u7f16\u8f91\u57fa\u51c6\u4e0a\u53d6\u5f97\u660e\u663e\u4f18\u52bf\uff08\u73b0\u6709\u65b9\u6cd5\u5728\u6b64\u8868\u73b0\u4e0d\u4f73\uff09\uff0c\u540c\u65f6\u5728\u901a\u7528\u7f16\u8f91\u4efb\u52a1\u4e0a\u4fdd\u6301\u4e0e\u6700\u5f3a\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684EgoEdit\u751f\u6001\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u7f16\u8f91\u7684\u72ec\u7279\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4ea4\u4e92\u5f0fAR\u5e94\u7528\u6240\u9700\u7684\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u8d28\u91cf\u7f16\u8f91\u3002EgoEditData\u548cEgoEditBench\u5c06\u516c\u5f00\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2512.06859", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06859", "abs": "https://arxiv.org/abs/2512.06859", "authors": ["Ce Chi", "Xing Wang", "Zhendong Wang", "Xiaofan Liu", "Ce Li", "Zhiyan Song", "Chen Zhao", "Kexin Yang", "Boshen Shi", "Jingjing Yang", "Chao Deng", "Junlan Feng"], "title": "JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models", "comment": null, "summary": "In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.", "AI": {"tldr": "JT-DA-8B\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u590d\u6742\u8868\u683c\u63a8\u7406\u4efb\u52a1\u76848B\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b34\u4e2a\u8868\u683c\u63a8\u7406\u4efb\u52a1\u7684\u591a\u6837\u5316\u8bad\u7ec3\u8bed\u6599\uff0c\u7ed3\u5408SFT\u548cRL\u4f18\u5316\uff0c\u5728\u591a\u79cd\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u5728\u8868\u683c\u63a8\u7406\u573a\u666f\u4e2d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u8868\u683c\u63a8\u7406\u4efb\u52a1\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6765\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u6837\u5316\u8868\u683c\u5206\u6790\u9700\u6c42\u3002", "method": "1) \u6784\u5efa\u5305\u542b29\u4e2a\u516c\u5f00\u8868\u683cQA\u6570\u636e\u96c6\u548c300\u4e07\u5f20\u8868\u683c\u7684\u591a\u6837\u5316\u8bad\u7ec3\u8bed\u6599\uff1b2) \u63d0\u51fa\u81ea\u52a8\u6d41\u6c34\u7ebf\u751f\u6210\u591a\u6b65\u5206\u6790\u4efb\u52a1\uff1b3) \u57fa\u4e8e\u5f00\u6e90\u7684JT-Coder-8B\u6a21\u578b\uff0c\u91c7\u7528LLM\u8bc4\u5206\u548c\u5de5\u4f5c\u6d41\u5bf9\u9f50\u8fc7\u6ee4\u6765\u63d0\u70bc\u9ad8\u8d28\u91cf\u8868\u683c\u4e2d\u5fc3\u6570\u636e\uff1b4) \u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u4f18\u5316\u6a21\u578b\uff1b5) \u63d0\u51fa\u56db\u9636\u6bb5\u8868\u683c\u63a8\u7406\u5de5\u4f5c\u6d41\uff1a\u8868\u683c\u9884\u5904\u7406\u3001\u8868\u683c\u611f\u77e5\u3001\u5de5\u5177\u96c6\u6210\u63a8\u7406\u548c\u63d0\u793a\u5de5\u7a0b\u3002", "result": "JT-DA-8B\u5728\u5404\u79cd\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u4e2d\u5fc3\u7684\u751f\u6210\u65b9\u6cd5\u548c\u5de5\u4f5c\u6d41\u9a71\u52a8\u7684\u4f18\u5316\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5168\u9762\u7684\u8bad\u7ec3\u8bed\u6599\u3001\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u7684\u751f\u6210\u65b9\u6cd5\u548c\u5de5\u4f5c\u6d41\u9a71\u52a8\u7684\u4f18\u5316\u7b56\u7565\uff0cJT-DA-8B\u6210\u529f\u89e3\u51b3\u4e86\u590d\u6742\u8868\u683c\u63a8\u7406\u4efb\u52a1\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u8868\u683c\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06080", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06080", "abs": "https://arxiv.org/abs/2512.06080", "authors": ["Tzofi Klinghoffer", "Siddharth Somasundaram", "Xiaoyu Xiang", "Yuchen Fan", "Christian Richardt", "Akshat Dave", "Ramesh Raskar", "Rakesh Ranjan"], "title": "Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light", "comment": "SIGGRAPH Asia 2025. Project page: https://shoot-bounce-3d.github.io", "summary": "3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.", "AI": {"tldr": "\u5229\u7528\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u901a\u8fc7\u591a\u8df3\u5149\u4f20\u8f93\u6062\u590d\u906e\u6321\u533a\u57df\u548c\u955c\u9762\u53cd\u5c04\u573a\u666f\u76843D\u51e0\u4f55\u7ed3\u6784\uff0c\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5904\u7406\u591a\u8def\u590d\u7528\u7167\u660e\u4e0b\u7684\u590d\u6742\u5149\u4f20\u8f93\u95ee\u9898\u3002", "motivation": "\u5355\u89c6\u89d23D\u573a\u666f\u91cd\u5efa\u9762\u4e34\u906e\u6321\u533a\u57df\u548c\u955c\u9762\u6750\u6599\uff08\u5982\u955c\u5b50\uff09\u7684\u6311\u6218\uff0c\u4f20\u7edf\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u901a\u5e38\u9700\u8981\u9010\u70b9\u626b\u63cf\uff0c\u800c\u591a\u8def\u590d\u7528\u7167\u660e\u4e0b\u7684\u590d\u6742\u5149\u4f20\u8f93\u96be\u4ee5\u89e3\u6790\u6c42\u89e3\u3002", "method": "\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5904\u7406\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u4e2d\u7684\u5149\u4f20\u8f93\u9006\u95ee\u9898\uff0c\u521b\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u5ba4\u5185\u573a\u666f\u6fc0\u5149\u96f7\u8fbe\u77ac\u6001\u6570\u636e\u96c6\uff08\u7ea610\u4e07\u6837\u672c\uff09\uff0c\u5b66\u4e60\u590d\u6742\u5149\u4f20\u8f93\u5148\u9a8c\uff0c\u5c06\u6d4b\u91cf\u7684\u53cc\u8df3\u5149\u5206\u89e3\u4e3a\u5404\u6fc0\u5149\u70b9\u7684\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u5355\u6b21\u6d4b\u91cf\u4e2d\u63a8\u65ad\u5177\u6709\u906e\u6321\u548c\u955c\u9762\u53cd\u5c04\u573a\u666f\u76843D\u51e0\u4f55\u7ed3\u6784\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5904\u7406\u591a\u8def\u590d\u7528\u7167\u660e\u4e0b\u7684\u590d\u6742\u5149\u4f20\u8f93\uff0c\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u80fd\u591f\u4ece\u5355\u6b21\u6d4b\u91cf\u4e2d\u6062\u590d\u5bc6\u96c6\u6df1\u5ea6\u3001\u906e\u6321\u51e0\u4f55\u548c\u6750\u6599\u5c5e\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.06867", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06867", "abs": "https://arxiv.org/abs/2512.06867", "authors": ["John Licato", "Stephen Steinle", "Brayden Hollis"], "title": "Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?", "comment": "Accepted at IJCNLP-AACL 2025", "summary": "Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4f7f\u7528\u89d2\u8272\u63d0\u793a\u662f\u5426\u80fd\u5728\u5bf9\u6297\u6027\u6218\u7565\u73af\u5883\u4e2d\u4ea7\u751f\u53ef\u6d4b\u91cf\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u901a\u8fc7PERIL\u4e16\u754c\u5f81\u670d\u68cb\u76d8\u6e38\u620f\u6d4b\u8bd5\u53d1\u73b0\uff0c\u67d0\u4e9b\u4e0e\u6218\u7565\u601d\u7ef4\u76f8\u5173\u7684\u89d2\u8272\u80fd\u63d0\u5347\u6e38\u620f\u8868\u73b0\uff0c\u4f46\u9700\u8981\u4e2d\u4ecb\u673a\u5236\u5c06\u89d2\u8272\u8f6c\u5316\u4e3a\u542f\u53d1\u5f0f\u503c\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u89d2\u8272\u63d0\u793a\u4f3c\u4e4e\u80fd\u89e6\u53d1\u4e0d\u540c\u7684\u6587\u672c\u751f\u6210\u98ce\u683c\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u5dee\u5f02\u662f\u5426\u80fd\u8f6c\u5316\u4e3a\u53ef\u6d4b\u91cf\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u5bf9\u6297\u6027\u6218\u7565\u73af\u5883\u4e2d\u3002\u7814\u7a76\u8005\u5e0c\u671b\u4e86\u89e3\u89d2\u8272\u63d0\u793a\u662f\u5426\u5f71\u54cd\u6218\u7565\u51b3\u7b56\u8868\u73b0\u3002", "method": "\u4f7f\u7528PERIL\uff08\u4e16\u754c\u5f81\u670d\u68cb\u76d8\u6e38\u620f\uff09\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6bd4\u8f83\u89d2\u8272\u884d\u751f\u7684\u542f\u53d1\u5f0f\u7b56\u7565\u4e0e\u624b\u52a8\u9009\u62e9\u7684\u7b56\u7565\u3002\u5f15\u5165\u57fa\u4e8e\u63a2\u7d22\u6027\u56e0\u5b50\u5206\u6790\u7684\u7ed3\u6784\u5316\u7ffb\u8bd1\u8fc7\u7a0b\u4f5c\u4e3a\u4e2d\u4ecb\uff0c\u5c06LLM\u751f\u6210\u7684\u6e05\u5355\u54cd\u5e94\u6620\u5c04\u4e3a\u542f\u53d1\u5f0f\u503c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u4e0e\u6218\u7565\u601d\u7ef4\u76f8\u5173\u7684\u89d2\u8272\u786e\u5b9e\u80fd\u63d0\u9ad8\u6e38\u620f\u8868\u73b0\uff0c\u4f46\u524d\u63d0\u662f\u4f7f\u7528\u4e2d\u4ecb\u673a\u5236\u5c06\u89d2\u8272\u8f6c\u5316\u4e3a\u542f\u53d1\u5f0f\u503c\u3002\u4e0e\u76f4\u63a5\u63a8\u65ad\u7684\u542f\u53d1\u5f0f\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u542f\u53d1\u5f0f\u7684\u53ef\u9760\u6027\u548c\u8868\u9762\u6548\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u589e\u8fdb\u4e86\u5bf9\u89d2\u8272\u63d0\u793a\u5982\u4f55\u5f71\u54cd\u57fa\u4e8eLLM\u7684\u51b3\u7b56\u7684\u7406\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5fc3\u7406\u6d4b\u91cf\u5b66\u539f\u7406\u5e94\u7528\u4e8eLLM\u7684\u542f\u53d1\u5f0f\u751f\u6210\u65b9\u6cd5\uff0c\u4e3a\u7814\u7a76\u89d2\u8272\u7c7b\u578b\u5bf9\u51b3\u7b56\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5de5\u5177\u3002"}}
{"id": "2512.06711", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06711", "abs": "https://arxiv.org/abs/2512.06711", "authors": ["Yulin Huang", "Yaxuan Luan", "Jinxu Guo", "Xiangchen Song", "Yuchen Liu"], "title": "Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models", "comment": null, "summary": "This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5dee\u5206\u9690\u79c1\u566a\u58f0\u5206\u914d\u4e0e\u68af\u5ea6\u88c1\u526a\u7684\u534f\u540c\u4f18\u5316\u6846\u67b6\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u8bad\u7ec3\u6548\u7387", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u6307\u4ee4\u5fae\u8c03\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u6548\u7387\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u9690\u79c1\u9884\u7b97\u6d88\u8017\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3", "method": "\u4fdd\u6301\u4e3b\u5e72\u6a21\u578b\u51bb\u7ed3\uff0c\u901a\u8fc7\u4f4e\u7ef4\u6295\u5f71\u5b50\u7a7a\u95f4\u66f4\u65b0\u53c2\u6570\uff0c\u5728\u68af\u5ea6\u8ba1\u7b97\u4e2d\u5f15\u5165\u88c1\u526a\u548c\u81ea\u9002\u5e94\u566a\u58f0\u5206\u914d\uff0c\u7ed3\u5408\u68af\u5ea6\u7ea6\u675f\u3001\u566a\u58f0\u5206\u914d\u548c\u53c2\u6570\u6295\u5f71\u7684\u7edf\u4e00\u6846\u67b6", "result": "\u5728\u51c6\u786e\u6027\u3001\u9690\u79c1\u9884\u7b97\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u591a\u6837\u5316\u548c\u4e0d\u786e\u5b9a\u6570\u636e\u6761\u4ef6\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd", "conclusion": "\u4e30\u5bcc\u4e86\u5dee\u5206\u9690\u79c1\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u7406\u8bba\u6574\u5408\uff0c\u5c55\u793a\u4e86\u5728\u6307\u4ee4\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u9002\u5e94\u6027\uff0c\u4e3a\u590d\u6742\u6307\u4ee4\u73af\u5883\u4e0b\u7684\u5b89\u5168\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.06096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06096", "abs": "https://arxiv.org/abs/2512.06096", "authors": ["Karthik Mohan", "Sonam Singh", "Amit Arvind Kale"], "title": "BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving", "comment": null, "summary": "The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360\u00b0 BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.", "AI": {"tldr": "BeLLA\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u5c06\u7edf\u4e00\u7684360\u00b0\u9e1f\u77b0\u56fe\u8868\u793a\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fde\u63a5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u95ee\u7b54\u4efb\u52a1\uff0c\u5728\u9700\u8981\u7a7a\u95f4\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u4f7f\u7528\u5355\u89c6\u89d2\u7f16\u7801\u5668\u65e0\u6cd5\u5229\u7528\u591a\u6444\u50cf\u5934\u7cfb\u7edf\u7684\u7a7a\u95f4\u7ed3\u6784\uff0c\u8981\u4e48\u4f7f\u7528\u805a\u5408\u7684\u591a\u89c6\u89d2\u7279\u5f81\u7f3a\u4e4f\u7edf\u4e00\u7684\u7a7a\u95f4\u8868\u793a\uff0c\u96be\u4ee5\u8fdb\u884c\u81ea\u6211\u4e2d\u5fc3\u65b9\u5411\u3001\u7269\u4f53\u5173\u7cfb\u548c\u66f4\u5e7f\u6cdb\u4e0a\u4e0b\u6587\u7684\u63a8\u7406\u3002", "method": "\u63d0\u51faBeLLA\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u5c06\u7edf\u4e00\u7684360\u00b0\u9e1f\u77b0\u56fe\u8868\u793a\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fde\u63a5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u95ee\u7b54\u3002\u8be5\u65b9\u6cd5\u5229\u7528BEV\u8868\u793a\u63d0\u4f9b\u7edf\u4e00\u7684\u7a7a\u95f4\u8868\u5f81\uff0c\u7ed3\u5408LLM\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728NuScenes-QA\u548cDriveLM\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBeLLA\u5728\u9700\u8981\u7a7a\u95f4\u63a8\u7406\u7684\u95ee\u9898\u4e0a\uff08\u5982\u76f8\u5bf9\u7269\u4f53\u5b9a\u4f4d\u548c\u9644\u8fd1\u7269\u4f53\u884c\u4e3a\u7406\u89e3\uff09\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u67d0\u4e9b\u4efb\u52a1\u4e0a\u83b7\u5f97\u9ad8\u8fbe+9.3%\u7684\u7edd\u5bf9\u63d0\u5347\u3002\u5728\u5176\u4ed6\u7c7b\u522b\u4e2d\u4e5f\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "BeLLA\u901a\u8fc7\u7ed3\u5408\u7edf\u4e00\u7684360\u00b0BEV\u8868\u793a\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7406\u89e3\u7269\u4f53\u76f8\u5bf9\u4f4d\u7f6e\u548c\u884c\u4e3a\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2512.06983", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06983", "abs": "https://arxiv.org/abs/2512.06983", "authors": ["Eli J. Laird", "Corey Clark"], "title": "On Memory: A comparison of memory mechanisms in world models", "comment": "10 pages, 1 figure", "summary": "World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eTransformer\u7684\u4e16\u754c\u6a21\u578b\u7684\u6709\u6548\u8bb0\u5fc6\u8de8\u5ea6\uff0c\u5206\u6790\u4e86\u591a\u79cd\u8bb0\u5fc6\u589e\u5f3a\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u8bb0\u5fc6\u7f16\u7801\u4e0e\u8bb0\u5fc6\u6ce8\u5165\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u901a\u8fc7\u72b6\u6001\u56de\u5fc6\u4efb\u52a1\u8bc4\u4f30\u4e86\u4e0d\u540c\u673a\u5236\u5728\u6269\u5c55\u4e16\u754c\u6a21\u578b\u8bb0\u5fc6\u548c\u5b9e\u73b0\u8f68\u8ff9\u95ed\u73af\u65b9\u9762\u7684\u6548\u679c\u3002", "motivation": "\u4e16\u754c\u6a21\u578b\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u60f3\u8c61\u73af\u5883\u4e2d\u8fdb\u884c\u89c4\u5212\uff0c\u4f46\u57fa\u4e8eTransformer\u7684\u4e16\u754c\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u89c4\u5212\u4e2d\u5b58\u5728\u6709\u6548\u8bb0\u5fc6\u8de8\u5ea6\u6709\u9650\u7684\u9650\u5236\uff0c\u8fd9\u5bfc\u81f4\u957f\u8f68\u8ff9\u751f\u6210\u4e2d\u7684\u611f\u77e5\u6f02\u79fb\uff0c\u963b\u788d\u4e86\u5728\u60f3\u8c61\u8f68\u8ff9\u4e2d\u5b9e\u73b0\u95ed\u73af\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u591a\u79cd\u8bb0\u5fc6\u589e\u5f3a\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u533a\u5206\u8bb0\u5fc6\u7f16\u7801\u548c\u8bb0\u5fc6\u6ce8\u5165\u673a\u5236\u7684\u5206\u7c7b\u6cd5\uff0c\u4ece\u6b8b\u5dee\u6d41\u52a8\u6001\u7684\u89d2\u5ea6\u7406\u89e3\u8fd9\u4e9b\u673a\u5236\u6269\u5c55\u4e16\u754c\u6a21\u578b\u8bb0\u5fc6\u7684\u4f5c\u7528\u3002\u901a\u8fc7\u72b6\u6001\u56de\u5fc6\u8bc4\u4f30\u4efb\u52a1\uff0c\u6d4b\u91cf\u4e86\u6bcf\u79cd\u673a\u5236\u7684\u8bb0\u5fc6\u56de\u5fc6\u80fd\u529b\u5e76\u5206\u6790\u4e86\u5404\u81ea\u7684\u6743\u8861\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bb0\u5fc6\u673a\u5236\u80fd\u591f\u63d0\u9ad8\u89c6\u89c9Transformer\u7684\u6709\u6548\u8bb0\u5fc6\u8de8\u5ea6\uff0c\u5e76\u4e3a\u5728\u4e16\u754c\u6a21\u578b\u60f3\u8c61\u4e2d\u5b8c\u6210\u8f68\u8ff9\u95ed\u73af\u63d0\u4f9b\u4e86\u8def\u5f84\u3002\u4e0d\u540c\u8bb0\u5fc6\u673a\u5236\u5728\u8bb0\u5fc6\u56de\u5fc6\u65b9\u9762\u8868\u73b0\u51fa\u5404\u81ea\u7684\u4f18\u52bf\u548c\u6743\u8861\u3002", "conclusion": "\u8bb0\u5fc6\u589e\u5f3a\u673a\u5236\u5bf9\u4e8e\u6269\u5c55\u57fa\u4e8eTransformer\u7684\u4e16\u754c\u6a21\u578b\u7684\u8bb0\u5fc6\u8de8\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u7f13\u89e3\u957f\u8f68\u8ff9\u751f\u6210\u4e2d\u7684\u611f\u77e5\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u60f3\u8c61\u8f68\u8ff9\u4e2d\u7684\u95ed\u73af\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2512.06732", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06732", "abs": "https://arxiv.org/abs/2512.06732", "authors": ["Aarushi Wagh", "Saniya Srivastava"], "title": "\"The Dentist is an involved parent, the bartender is not\": Revealing Implicit Biases in QA with Implicit BBQ", "comment": null, "summary": "Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the \"sexual orientation\" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.", "AI": {"tldr": "ImplicitBBQ\u57fa\u51c6\u6d4b\u8bd5\u6269\u5c55\u4e86BBQ\u57fa\u51c6\uff0c\u901a\u8fc7\u9690\u5f0f\u7ebf\u7d22\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u89c1\uff0c\u53d1\u73b0GPT-4o\u5728\u9690\u5f0f\u504f\u89c1\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u663e\u5f0f\u57fa\u51c6\u65e0\u6cd5\u68c0\u6d4b\u7684\u9690\u6027\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u504f\u89c1\u7684\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u7ebf\u7d22\uff08\u5982\u76f4\u63a5\u58f0\u660e\u5b97\u6559\u3001\u79cd\u65cf\u3001\u6027\u522b\u7b49\u53d7\u4fdd\u62a4\u5c5e\u6027\uff09\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4e92\u52a8\u4e2d\u504f\u89c1\u5f80\u5f80\u901a\u8fc7\u59d3\u540d\u3001\u6587\u5316\u7ebf\u7d22\u6216\u7279\u5f81\u7b49\u9690\u5f0f\u65b9\u5f0f\u4f53\u73b0\u3002\u8fd9\u79cd\u5173\u952e\u758f\u5ffd\u5728\u516c\u5e73\u6027\u8bc4\u4f30\u4e2d\u9020\u6210\u4e86\u663e\u8457\u7684\u76f2\u70b9\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86ImplicitBBQ\u57fa\u51c6\uff0c\u6269\u5c55\u4e86\u95ee\u7b54\u504f\u89c1\u57fa\u51c6\uff08BBQ\uff09\uff0c\u57286\u4e2a\u7c7b\u522b\u4e2d\u52a0\u5165\u4e86\u9690\u5f0f\u7ebf\u7d22\u7684\u53d7\u4fdd\u62a4\u5c5e\u6027\u3002\u901a\u8fc7\u8be5\u57fa\u51c6\u8bc4\u4f30\u4e86GPT-4o\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u5176\u5728\u9690\u5f0f\u548c\u663e\u5f0fBBQ\u63d0\u793a\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "GPT-4o\u5728ImplicitBBQ\u4e0a\u7684\u8868\u73b0\u4e0e\u663e\u5f0fBBQ\u63d0\u793a\u76f8\u6bd4\u5b58\u5728\u4ee4\u4eba\u62c5\u5fe7\u7684\u5dee\u5f02\uff1a\"\u6027\u53d6\u5411\"\u5b50\u7c7b\u522b\u7684\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe7%\uff0c\u5927\u591a\u6570\u5176\u4ed6\u7c7b\u522b\u4e5f\u51fa\u73b0\u4e86\u4e00\u81f4\u7684\u4e0b\u964d\u8d8b\u52bf\u3002\u8fd9\u8868\u660e\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5305\u542b\u73b0\u6709\u663e\u5f0f\u57fa\u51c6\u65e0\u6cd5\u68c0\u6d4b\u7684\u9690\u6027\u504f\u89c1\u3002", "conclusion": "ImplicitBBQ\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u7ec6\u81f4\u516c\u5e73\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u4ec5\u4f9d\u8d56\u663e\u5f0f\u504f\u89c1\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u7f13\u89e3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9690\u6027\u504f\u89c1\u3002"}}
{"id": "2512.06103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06103", "abs": "https://arxiv.org/abs/2512.06103", "authors": ["Raghavendra Ramachandra", "Sushma Venkatesh"], "title": "SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection", "comment": "Accepted in IEEE T-BIOM", "summary": "Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \\textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\\,nm, 830\\,nm, 850\\,nm, 870\\,nm, and 980\\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.", "AI": {"tldr": "\u63d0\u51faSpectraIrisPAD\u6846\u67b6\uff0c\u5229\u7528\u591a\u5149\u8c31\u6210\u50cf\u548cDINOv2 Vision Transformer\u8fdb\u884c\u8679\u819c\u5448\u73b0\u653b\u51fb\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u8679\u819c\u8bc6\u522b\u867d\u51c6\u786e\u4f46\u6613\u53d7\u5448\u73b0\u653b\u51fb\uff0c\u4f20\u7edf\u8fd1\u7ea2\u5916\u7cfb\u7edf\u5b58\u5728\u5c40\u9650\u6027\uff0c\u591a\u5149\u8c31\u6210\u50cf\u80fd\u63d0\u4f9b\u4e92\u8865\u53cd\u5c04\u4fe1\u606f\u4ee5\u589e\u5f3a\u653b\u51fb\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u57fa\u4e8eDINOv2 Vision Transformer\u67b6\u6784\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u5149\u8c31\u4f4d\u7f6e\u7f16\u7801\u3001\u4ee4\u724c\u878d\u5408\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u63d0\u53d6\u533a\u5206\u6027\u6ce2\u6bb5\u7279\u5f81\uff1b\u521b\u5efa\u5305\u542b5\u4e2a\u8fd1\u7ea2\u5916\u6ce2\u6bb5\u7684MSIrPAD\u6570\u636e\u96c6", "result": "SpectraIrisPAD\u5728\u672a\u89c1\u653b\u51fb\u8bc4\u4f30\u534f\u8bae\u4e0b\u5168\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5404\u9879\u6027\u80fd\u6307\u6807\u4e0a\u5747\u8868\u73b0\u6700\u4f73\uff0c\u5c55\u793a\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b", "conclusion": "\u591a\u5149\u8c31\u6210\u50cf\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u8679\u819c\u5448\u73b0\u653b\u51fb\u68c0\u6d4b\u6027\u80fd\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\u4e3a\u8679\u819c\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u4fdd\u969c"}}
{"id": "2512.06734", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06734", "abs": "https://arxiv.org/abs/2512.06734", "authors": ["Subrit Dikshit", "Ritu Tiwari", "Priyank Jain"], "title": "A Patient-Doctor-NLP-System to contest inequality for less privileged", "comment": "19 pages, 6 figures", "summary": "Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.", "AI": {"tldr": "PDFTEMRA\u662f\u4e00\u4e2a\u7d27\u51d1\u7684Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u578b\u84b8\u998f\u3001\u9891\u57df\u8c03\u5236\u3001\u96c6\u6210\u5b66\u4e60\u548c\u968f\u673a\u6fc0\u6d3b\u6a21\u5f0f\u7b49\u6280\u672f\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u7406\u89e3\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u533b\u7597NLP\u5e94\u7528\uff0c\u7279\u522b\u662f\u4e3a\u5370\u5730\u8bed\u4f7f\u7528\u8005\u548c\u89c6\u969c\u7528\u6237\u63d0\u4f9b\u533b\u7597\u8f85\u52a9\u3002", "motivation": "\u5c3d\u7ba1\u8fc1\u79fb\u5b66\u4e60\u52a0\u901f\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u7684\u771f\u5b9e\u533b\u7597\u573a\u666f\u4e2d\u8bad\u7ec3\u548c\u90e8\u7f72\u8fd9\u4e9b\u5927\u578b\u6a21\u578b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u89c6\u969c\u7528\u6237\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5370\u5730\u8bed\uff09\u4f7f\u7528\u8005\u5728\u519c\u6751\u533b\u7597\u73af\u5883\u4e2d\u83b7\u5f97\u6709\u9650\u652f\u6301\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86PDFTEMRA\uff08Performant Distilled Frequency Transformer Ensemble Model with Random Activations\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u96c6\u6210\u4e86\u6a21\u578b\u84b8\u998f\u3001\u9891\u57df\u8c03\u5236\u3001\u96c6\u6210\u5b66\u4e60\u548c\u968f\u673a\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u7406\u89e3\u6027\u80fd\u3002", "result": "\u5728\u9488\u5bf9\u5370\u5730\u8bed\u548c\u53ef\u8bbf\u95ee\u6027\u573a\u666f\u5b9a\u5236\u7684\u533b\u7597\u95ee\u7b54\u548c\u54a8\u8be2\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aPDFTEMRA\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u6bd4\u8f83\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "PDFTEMRA\u9002\u7528\u4e8e\u53ef\u8bbf\u95ee\u3001\u5305\u5bb9\u6027\u3001\u4f4e\u8d44\u6e90\u7684\u533b\u7597NLP\u5e94\u7528\uff0c\u8868\u660e\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.06105", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06105", "abs": "https://arxiv.org/abs/2512.06105", "authors": ["Junwen Zheng", "Xinran Xu", "Li Rong Wang", "Chang Cai", "Lucinda Siyun Tan", "Dingyuan Wang", "Hong Liang Tey", "Xiuyi Fan"], "title": "Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation", "comment": "AAAI-26-AIA", "summary": "Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.", "AI": {"tldr": "\u63d0\u51faCEFM\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06\u4e34\u5e8aABC\u6807\u51c6\u6620\u5c04\u5230\u89c6\u89c9\u7279\u5f81\u7a7a\u95f4\uff0c\u751f\u6210\u7ed3\u6784\u5316\u6587\u672c\u89e3\u91ca\uff0c\u63d0\u9ad8\u9ed1\u8272\u7d20\u7624\u5206\u7c7b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u9ed1\u8272\u7d20\u7624\u5206\u7c7b\u4e2d\u5df2\u8fbe\u5230\u4e13\u5bb6\u6c34\u5e73\uff0c\u4f46\u6a21\u578b\u4e0d\u900f\u660e\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u963b\u788d\u4e86\u4e34\u5e8a\u91c7\u7528\uff0c\u533b\u751f\u96be\u4ee5\u4fe1\u4efb\u9ed1\u76d2\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "CEFM\u6846\u67b6\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u4f5c\u4e3a\u6838\u5fc3\u673a\u5236\uff0c\u901a\u8fc7\u53cc\u6295\u5f71\u5934\u5c06\u4e34\u5e8a\u8bca\u65ad\u6807\u51c6\uff08ABC\u89c4\u5219\uff09\u6620\u5c04\u5230Vision Transformer\u5d4c\u5165\u7a7a\u95f4\uff0c\u5c06\u4e34\u5e8a\u8bed\u4e49\u4e0e\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u5c06\u5bf9\u9f50\u7684\u8868\u5f81\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u89e3\u91ca\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fbe\u523092.79%\u51c6\u786e\u7387\u548c0.961\u7684AUC\uff0c\u5728\u591a\u4e2a\u53ef\u89e3\u91ca\u6027\u6307\u6807\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u7a7a\u95f4\u6392\u5217\u4e0e\u533b\u751f\u5e94\u7528ABC\u89c4\u5219\u7684\u65b9\u5f0f\u4e00\u81f4\u3002", "conclusion": "CEFM\u6846\u67b6\u6709\u6548\u5f25\u5408\u4e86\u9ad8\u6027\u80fd\u5206\u7c7b\u4e0e\u4e34\u5e8a\u4fe1\u4efb\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u5c06\u4e34\u5e8a\u6807\u51c6\u4e0e\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\u5e76\u751f\u6210\u53ef\u7406\u89e3\u7684\u89e3\u91ca\uff0c\u63d0\u9ad8\u4e86\u9ed1\u8272\u7d20\u7624\u8bca\u65ad\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2512.07081", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07081", "abs": "https://arxiv.org/abs/2512.07081", "authors": ["Rongjia Zhou", "Chengzhuo Li", "Carl Yang", "Jiaying Lu"], "title": "ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes", "comment": "10 pages, 2 figures. Submitted to AMIA 2026 Informatics Summit Student Paper Track", "summary": "Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.", "AI": {"tldr": "ClinNoteAgents\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4e34\u5e8a\u81ea\u7531\u6587\u672c\u7b14\u8bb0\u4e2d\u63d0\u53d6\u5fc3\u8870\u518d\u5165\u9662\u98ce\u9669\u56e0\u7d20\u5e76\u8fdb\u884c\u9884\u6d4b\uff0c\u51cf\u5c11\u5bf9\u7ed3\u6784\u5316\u6570\u636e\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "motivation": "\u5fc3\u8870\u662f\u7f8e\u56fd\u8001\u5e74\u4eba\u518d\u5165\u9662\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\u3002\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u4e30\u5bcc\u7684\u60a3\u8005\u4fe1\u606f\uff0c\u4f46\u5728\u5fc3\u8870\u518d\u5165\u9662\u98ce\u9669\u5206\u6790\u4e2d\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u4f20\u7edf\u6a21\u578b\u4f9d\u8d56\u4e13\u5bb6\u89c4\u5219\u3001\u533b\u5b66\u672f\u8bed\u5e93\u548c\u672c\u4f53\u6765\u89e3\u91ca\u4e34\u5e8a\u7b14\u8bb0\uff0c\u4f46\u8fd9\u4e9b\u7b14\u8bb0\u901a\u5e38\u5305\u542b\u62fc\u5199\u9519\u8bef\u3001\u7f29\u5199\u548c\u9886\u57df\u7279\u5b9a\u672f\u8bed\u3002", "method": "\u63d0\u51faClinNoteAgents\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u81ea\u7531\u6587\u672c\u4e34\u5e8a\u7b14\u8bb0\u8f6c\u5316\u4e3a\uff1a(1) \u7528\u4e8e\u5173\u8054\u5206\u6790\u7684\u7ed3\u6784\u5316\u4e34\u5e8a\u548c\u793e\u4f1a\u98ce\u9669\u56e0\u7d20\u8868\u793a\uff1b(2) \u7528\u4e8e\u5fc3\u887030\u5929\u518d\u5165\u9662\u9884\u6d4b\u7684\u4e34\u5e8a\u533b\u751f\u98ce\u683c\u62bd\u8c61\u3002", "result": "\u57282,065\u540d\u60a3\u8005\uff08\u518d\u5165\u9662\u738735.16%\uff09\u76843,544\u4efd\u7b14\u8bb0\u4e0a\u8bc4\u4f30\uff0c\u5728\u4ece\u81ea\u7531\u6587\u672c\u63d0\u53d6\u98ce\u9669\u56e0\u7d20\u3001\u8bc6\u522b\u5173\u952e\u8d21\u732e\u56e0\u7d20\u548c\u9884\u6d4b\u518d\u5165\u9662\u98ce\u9669\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u901a\u8fc7\u51cf\u5c11\u5bf9\u7ed3\u6784\u5316\u5b57\u6bb5\u7684\u4f9d\u8d56\u5e76\u6700\u5c0f\u5316\u4eba\u5de5\u6807\u6ce8\u548c\u6a21\u578b\u8bad\u7ec3\uff0cClinNoteAgents\u4e3a\u6570\u636e\u6709\u9650\u7684\u533b\u7597\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u57fa\u4e8e\u7b14\u8bb0\u7684\u5fc3\u8870\u518d\u5165\u9662\u98ce\u9669\u5efa\u6a21\u65b9\u6cd5\u3002"}}
{"id": "2512.06744", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06744", "abs": "https://arxiv.org/abs/2512.06744", "authors": ["Rajeev Ranjan"], "title": "One Word Is Not Enough: Simple Prompts Improve Word Embeddings", "comment": null, "summary": "Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like \"meaning: {word}\" or \"Represent the semantic concept: {word}\" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u5728\u5355\u8bcd\u524d\u6dfb\u52a0\u8bed\u4e49\u63d0\u793a\uff08\u5982\"meaning: {word}\"\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5728\u5355\u8bcd\u76f8\u4f3c\u5ea6\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u8d85\u8d8a\u4f20\u7edf\u9759\u6001\u5d4c\u5165\u65b9\u6cd5\u3002", "motivation": "\u6587\u672c\u5d4c\u5165\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u53e5\u5b50\u7ea7\u5e94\u7528\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f46\u5b83\u4eec\u5728\u5b64\u7acb\u5355\u8bcd\u4e0a\u7684\u8868\u73b0\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u7b80\u5355\u63d0\u793a\u63d0\u5347\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5728\u5355\u8bcd\u76f8\u4f3c\u5ea6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u57287\u4e2a\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff08\u5305\u62ecOpenAI\u3001Cohere\u3001Voyage AI\u7b49\u516c\u53f8\u7684\u6a21\u578b\uff09\u4e0a\u6d4b\u8bd5\u4e863\u4e2a\u6807\u51c6\u5355\u8bcd\u76f8\u4f3c\u5ea6\u57fa\u51c6\uff08SimLex-999\u3001WordSim-353\u3001MEN-3000\uff09\u3002\u901a\u8fc7\u5728\u5355\u8bcd\u524d\u6dfb\u52a0\u8bed\u4e49\u63d0\u793a\uff08\u5982\"meaning: {word}\"\u6216\"Represent the semantic concept: {word}\"\uff09\u6765\u6539\u8fdb\u5d4c\u5165\u6548\u679c\uff0c\u8fd9\u662f\u4e00\u79cd\u96f6\u6837\u672c\u6280\u672f\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u8bed\u4e49\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86\u5355\u8bcd\u76f8\u4f3c\u5ea6\u76f8\u5173\u6027\uff1a\u5728SimLex-999\u4e0a\u6700\u591a\u63d0\u5347+0.29\uff1b\u67d0\u4e9b\u6a21\u578b\u5728\u539f\u59cb\u5355\u8bcd\u4e0a\u5b8c\u5168\u5931\u8d25\uff08\u76f8\u5173\u6027=0\uff09\uff0c\u4f46\u901a\u8fc7\u63d0\u793a\u6062\u590d\uff08\u63d0\u5347+0.73\uff09\u3002\u6700\u4f73\u7ed3\u679c\uff1aCohere\u6a21\u578b\u5728SimLex-999\u4e0a\u8fbe\u52300.692\u76f8\u5173\u6027\uff0cOpenAI\u6a21\u578b\u5728WordSim-353\u4e0a\u8fbe\u52300.811\uff0c\u5728MEN-3000\u4e0a\u8fbe\u52300.855\u3002\u8fd9\u4e9b\u7ed3\u679c\u8d85\u8d8a\u4e86\u4f20\u7edf\u9759\u6001\u5d4c\u5165\u65b9\u6cd5\u5982Word2Vec\uff080.40\uff09\u548cLexVec\uff080.48\uff09\uff0c\u5efa\u7acb\u4e86\u7eaf\u5d4c\u5165\u65b9\u6cd5\u7684\u65b0SOTA\u3002", "conclusion": "\u7b80\u5355\u7684\u8bed\u4e49\u63d0\u793a\u6280\u672f\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5728\u5355\u8bcd\u76f8\u4f3c\u5ea6\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u8d85\u8d8a\u4f20\u7edf\u9759\u6001\u5d4c\u5165\u65b9\u6cd5\uff0c\u4e3a\u5355\u8bcd\u7ea7\u8bed\u4e49\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06158", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06158", "abs": "https://arxiv.org/abs/2512.06158", "authors": ["Su Sun", "Cheng Zhao", "Himangi Mittal", "Gaurav Mittal", "Rohith Kukkala", "Yingjie Victor Chen", "Mei Chen"], "title": "Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation", "comment": "15 pages, 11 figures", "summary": "Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \\emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \\emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \\emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.", "AI": {"tldr": "Track4DGen\uff1a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u89c6\u89d2\u89c6\u9891\u6269\u6563\u6a21\u578b\u3001\u57fa\u7840\u70b9\u8ffd\u8e2a\u5668\u548c\u6df7\u54084D\u9ad8\u65af\u6e85\u5c04\u91cd\u5efa\u5668\uff0c\u4ece\u7a00\u758f\u8f93\u5165\u751f\u6210\u52a8\u60014D\u5bf9\u8c61\uff0c\u89e3\u51b3\u5916\u89c2\u6f02\u79fb\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u4ece\u7a00\u758f\u8f93\u5165\u751f\u6210\u52a8\u60014D\u5bf9\u8c61\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u540c\u65f6\u4fdd\u6301\u5916\u89c2\u548c\u8fd0\u52a8\u5728\u89c6\u89d2\u548c\u65f6\u95f4\u4e0a\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u6291\u5236\u4f2a\u5f71\u548c\u65f6\u95f4\u6f02\u79fb\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u50cf\u7d20\u6216\u6f5c\u5728\u7a7a\u95f4\u7684\u89c6\u9891\u6269\u6563\u635f\u5931\u76d1\u7763\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u65f6\u95f4\u611f\u77e5\u7279\u5f81\u7ea7\u8ffd\u8e2a\u6307\u5bfc\u3002", "method": "Track4DGen\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5728\u591a\u89c6\u89d2\u89c6\u9891\u6269\u6563\u751f\u6210\u5668\u4e2d\u5f3a\u5236\u5bc6\u96c6\u7279\u5f81\u7ea7\u70b9\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ea7\u751f\u65f6\u95f4\u4e00\u81f4\u7684\u7279\u5f81\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u6df7\u5408\u8fd0\u52a8\u7f16\u7801\u91cd\u5efa4D\u9ad8\u65af\u6e85\u5c04\uff0c\u7ed3\u5408\u6269\u6563\u7279\u5f81\uff08\u643a\u5e26\u8ffd\u8e2a\u5148\u9a8c\uff09\u3001Hex-plane\u7279\u5f81\u548c4D\u7403\u8c10\u51fd\u6570\u8fdb\u884c\u9ad8\u4fdd\u771f\u52a8\u6001\u5efa\u6a21\u3002", "result": "Track4DGen\u5728\u591a\u89c6\u89d2\u89c6\u9891\u751f\u6210\u548c4D\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ea7\u751f\u65f6\u95f4\u7a33\u5b9a\u3001\u53ef\u6587\u672c\u7f16\u8f91\u76844D\u8d44\u4ea7\u3002\u6b64\u5916\uff0c\u8fd8\u521b\u5efa\u4e86Sketchfab28\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7528\u4e8e\u5bf9\u8c61\u4e2d\u5fc34D\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u6ce8\u5165\u8ffd\u8e2a\u5668\u884d\u751f\u7684\u8fd0\u52a8\u5148\u9a8c\u5230\u591a\u89c6\u89d2\u89c6\u9891\u751f\u6210\u548c4D\u9ad8\u65af\u6e85\u5c04\u7684\u4e2d\u95f4\u7279\u5f81\u8868\u793a\u4e2d\uff0cTrack4DGen\u6709\u6548\u89e3\u51b3\u4e86\u5916\u89c2\u6f02\u79fb\u548c\u89c6\u89d2\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3a\u52a8\u60014D\u5bf9\u8c61\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06751", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06751", "abs": "https://arxiv.org/abs/2512.06751", "authors": ["Seungyeon Jwa", "Daechul Ahn", "Reokyoung Kim", "Dongyeop Kang", "Jonghyun Choi"], "title": "Becoming Experienced Judges: Selective Test-Time Learning for Evaluators", "comment": null, "summary": "Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.", "AI": {"tldr": "\u63d0\u51faLWE\u6846\u67b6\uff0c\u8ba9LLM\u8bc4\u4f30\u5668\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u81ea\u6211\u53cd\u9988\u6539\u8fdb\u5143\u63d0\u793a\uff0c\u65e0\u9700\u8bad\u7ec3\u96c6\uff0c\u5e76\u5728\u4e0d\u4e00\u81f4\u6848\u4f8b\u4e0a\u9009\u62e9\u6027\u66f4\u65b0\u4ee5\u63d0\u5347\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u5668\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1) \u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u6848\u4f8b\uff0c\u65e0\u6cd5\u79ef\u7d2f\u7ecf\u9a8c\uff1b2) \u4f7f\u7528\u56fa\u5b9a\u63d0\u793a\uff0c\u7f3a\u4e4f\u6837\u672c\u7279\u5b9a\u7684\u8bc4\u4f30\u6807\u51c6\u3002\u9700\u8981\u8ba9\u8bc4\u4f30\u5668\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6301\u7eed\u6539\u8fdb\u3002", "method": "\u63d0\u51faLWE\u6846\u67b6\uff1a\u7ef4\u62a4\u4e00\u4e2a\u6f14\u5316\u7684\u5143\u63d0\u793a\uff0c\u751f\u6210\u6837\u672c\u7279\u5b9a\u7684\u8bc4\u4f30\u6307\u4ee4\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u53cd\u9988\u8fdb\u884c\u6539\u8fdb\u3002\u8fdb\u4e00\u6b65\u63d0\u51faSelective LWE\uff0c\u53ea\u5728\u81ea\u6211\u4e0d\u4e00\u81f4\u7684\u6848\u4f8b\u4e0a\u66f4\u65b0\u5143\u63d0\u793a\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u4e24\u4e2a\u6210\u5bf9\u6bd4\u8f83\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSelective LWE\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u8bc4\u4f30\u5668\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u9009\u62e9\u6027\u66f4\u65b0\u5728\u987a\u5e8f\u6d4b\u8bd5\u4e2d\u6301\u7eed\u6539\u8fdb\uff0c\u4ece\u6700\u56f0\u96be\u7684\u6848\u4f8b\u4e2d\u5b66\u4e60\u6700\u591a\u3002", "conclusion": "LWE\u6846\u67b6\u4f7fLLM\u8bc4\u4f30\u5668\u80fd\u591f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u81ea\u6211\u53cd\u9988\u6301\u7eed\u6539\u8fdb\uff0c\u9009\u62e9\u6027\u66f4\u65b0\u7b56\u7565\u5728\u4fdd\u6301\u987a\u5e8f\u5b66\u4e60\u4f18\u52bf\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u6210\u672c\u6548\u76ca\uff0c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6539\u8fdb\u9014\u5f84\u3002"}}
{"id": "2512.06171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06171", "abs": "https://arxiv.org/abs/2512.06171", "authors": ["Jessica Plassmann", "Nicolas Schuler", "Michael Schuth", "Georg von Freymann"], "title": "Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection", "comment": "11 pages, 4 figures", "summary": "Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u4ece\u526a\u5207\u6563\u6591\u6d4b\u91cf\u4e2d\u751f\u6210\u7f3a\u9677\u6807\u6ce8\uff0c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u6570\u636e\u96c6\u521b\u5efa\u3002", "motivation": "\u526a\u5207\u6563\u6591\u6280\u672f\u867d\u7136\u5bf9\u8868\u9762\u4f4d\u79fb\u68af\u5ea6\u654f\u611f\uff0c\u80fd\u6709\u6548\u68c0\u6d4b\u5b89\u5168\u5173\u952e\u90e8\u4ef6\u7684\u4e9a\u8868\u9762\u7f3a\u9677\uff0c\u4f46\u5176\u5de5\u4e1a\u5e94\u7528\u53d7\u5230\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7f3a\u4e4f\u7684\u9650\u5236\u3002\u4eba\u5de5\u6807\u6ce8\u52b3\u52a8\u5bc6\u96c6\u3001\u4e3b\u89c2\u6027\u5f3a\u4e14\u96be\u4ee5\u6807\u51c6\u5316\u3002", "method": "\u5f15\u5165\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u4ece\u526a\u5207\u6563\u6591\u6d4b\u91cf\u4e2d\u751f\u6210\u7f3a\u9677\u6807\u6ce8\uff0c\u5305\u62ec\u9ad8\u5206\u8fa8\u7387\u5206\u5272\u548c\u8fb9\u754c\u6846\u6807\u7b7e\u3002", "result": "\u4e0e\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u5bf9\u6bd4\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u8db3\u591f\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u652f\u6301\u5f31\u76d1\u7763\u8bad\u7ec3\uff0c\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u80fd\u591f\u652f\u6301\u53ef\u6269\u5c55\u7684\u6570\u636e\u96c6\u521b\u5efa\uff0c\u4e3a\u7a33\u5065\u7684\u7f3a\u9677\u68c0\u6d4b\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2512.07109", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07109", "abs": "https://arxiv.org/abs/2512.07109", "authors": ["Miguel Ingram", "Arthur Joseph Merritt"], "title": "A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy", "comment": "62 pages, 10 figures", "summary": "Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9400\u4e2a\u4efb\u52a1\u76849\u7c7b\u522b\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7CNN\u9a8c\u8bc1\u5176\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u5e76\u63ed\u793a\u4e86Transformer\u67b6\u6784\u5728\u4efb\u52a1\u5408\u6210\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u4e86\u795e\u7ecf\u4eb2\u548c\u529b\u5929\u82b1\u677f\u6548\u5e94\u3002", "motivation": "\u54cd\u5e94Hodel\u7b49\u4eba\u5bf9\u4efb\u52a1\u76f8\u5173\u6027\u6b63\u5f0f\u5b9a\u4e49\u7684\u547c\u5401\uff0c\u5efa\u7acb\u7cfb\u7edf\u5316\u7684\u4efb\u52a1\u5206\u7c7b\u4f53\u7cfb\uff0c\u4ee5\u8bca\u65ad\u5f53\u524dAI\u6a21\u578b\u5728\u62bd\u8c61\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "1. \u5f00\u53d1\u57fa\u4e8e\u89c4\u5219\u4ee3\u7801\u5206\u6790\u76849\u7c7b\u522b\u4efb\u52a1\u5206\u7c7b\u6cd5\uff08400\u4e2a\u4efb\u52a1\uff0c97.5%\u51c6\u786e\u7387\uff09\uff1b2. \u4f7f\u7528\u539f\u59cb\u7f51\u683c\u50cf\u7d20\u8bad\u7ec3CNN\u9a8c\u8bc1\u5206\u7c7b\u6cd5\u7684\u89c6\u89c9\u4e00\u81f4\u6027\uff1b3. \u5728302\u4e2a\u4efb\u52a1\u4e0a\u5fae\u8c03170\u4e07\u53c2\u6570Transformer\uff0c\u5206\u6790\u5c40\u90e8\u6a21\u5f0f\u4e0e\u5168\u5c40\u5408\u6210\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "result": "1. \u5206\u7c7b\u6cd5\u5728S3\u6570\u636e\u96c6\u4e0a\u8fbe\u523095.24%\u51c6\u786e\u7387\uff0c\u6574\u4f5336.25%\uff083.3\u500d\u968f\u673a\u6982\u7387\uff09\uff1b2. \u53d1\u73b035.3%\u4efb\u52a1\u5bf9Transformer\u5177\u6709\u4f4e\u795e\u7ecf\u4eb2\u548c\u529b\uff1b3. \u63ed\u793a\u7ec4\u5408\u6027\u5dee\u8ddd\uff1a69.5%\u4efb\u52a1\u5c40\u90e8\u51c6\u786e\u7387>80%\u4f46\u5168\u5c40\u51c6\u786e\u7387<10%\uff1b4. \u5728\u72ec\u7acb\u7814\u7a76\u4e2d\u9a8c\u8bc1\u9884\u6d4b\u80fd\u529b\uff1a\u4f4e\u4eb2\u548c\u529b\u4efb\u52a151.9% vs \u9ad8\u4eb2\u548c\u529b77.7%\uff08p<0.001\uff09\u3002", "conclusion": "\u5f53\u524dAI\u8fdb\u5c55\u53d7\u9650\u4e8e\u67b6\u6784\u9002\u5e94\u6027\u800c\u975e\u8bad\u7ec3\u6570\u636e\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u4eb2\u548c\u529b\u5bf9\u9f50\u6a21\u5757\u7684\u6df7\u5408\u67b6\u6784\u3002\u5206\u7c7b\u6cd5\u4e3a\u7cbe\u786e\u8bca\u65ad\u4efb\u52a1\u96be\u5ea6\u548c\u6a21\u578b\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2512.06776", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06776", "abs": "https://arxiv.org/abs/2512.06776", "authors": ["Yuchuan Tian", "Yuchen Liang", "Jiacheng Sun", "Shuo Zhang", "Guangwen Yang", "Yingte Shu", "Sibo Fang", "Tianyu Guo", "Kai Han", "Chao Xu", "Hanting Chen", "Xinghao Chen", "Yunhe Wang"], "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs", "comment": "13 pages, 4 figures", "summary": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faNBDiff\u65b9\u6cd5\uff0c\u5c06\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u9ad8\u6548\u9002\u914d\u4e3a\u5757\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u56e0\u679c\u6027\u4e0e\u53cc\u5411\u6027\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u57287B\u53c2\u6570\u89c4\u6a21\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u81ea\u56de\u5f52\u89e3\u7801\u7684\u4e32\u884c\u6027\u5bfc\u81f4\u541e\u5410\u91cf\u74f6\u9888\uff0c\u800c\u4ece\u5934\u8bad\u7ec3\u6269\u6563\u8bed\u8a00\u6a21\u578b\u6210\u672c\u9ad8\u6602\u4e14\u6d6a\u8d39\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u7684\u77e5\u8bc6\u3002\u73b0\u6709\u9002\u914d\u65b9\u6cd5\u672a\u80fd\u89e3\u51b3\u81ea\u56de\u5f52\u56e0\u679c\u6027\u4e0e\u5757\u6269\u6563\u53cc\u5411\u6027\u4e4b\u95f4\u7684\u6839\u672c\u6027\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u5c06\u81ea\u56de\u5f52\u89c6\u4e3a\u5757\u5927\u5c0f\u4e3a1\u7684\u5757\u6269\u6563\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e0a\u4e0b\u6587\u56e0\u679c\u6ce8\u610f\u529b\u63a9\u7801\uff08\u4e0a\u4e0b\u6587\u56e0\u679c\uff0c\u4ec5\u5728\u6d3b\u52a8\u5757\u5185\u53cc\u5411\uff09\uff0c\u91c7\u7528\u9ad8\u6548\u5e76\u884c\u9002\u914d\u6d41\u7a0b\uff0c\u4f7f\u7528\u8f85\u52a9\u81ea\u56de\u5f52\u635f\u5931\u6700\u5927\u5316\u6570\u636e\u5229\u7528\u5e76\u4fdd\u7559\u9884\u8bad\u7ec3\u77e5\u8bc6\uff0c\u9010\u6b65\u589e\u52a0\u751f\u6210\u5757\u5927\u5c0f\u3002", "result": "NBDiff-7B\uff08\u57fa\u7840\u548c\u6307\u4ee4\u7248\u672c\uff09\u7ee7\u627f\u4e86\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u63a8\u7406\u80fd\u529b\uff0c\u57287B\u7c7b\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u901a\u7528\u77e5\u8bc6\u3001\u6570\u5b66\u548c\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u539f\u5219\u6027\u7684\u81ea\u56de\u5f52\u5230\u5757\u6269\u6563\u9002\u914d\u662f\u4e00\u79cd\u6709\u6548\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4f5c\u4e3a\u4ece\u5934\u8bad\u7ec3\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2512.06174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06174", "abs": "https://arxiv.org/abs/2512.06174", "authors": ["Shilin Hu", "Jingyi Xu", "Akshat Dave", "Dimitris Samaras", "Hieu Le"], "title": "Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction", "comment": null, "summary": "Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u663e\u5f0f\u7269\u7406\u5efa\u6a21\uff08\u51e0\u4f55\u4e0e\u5149\u7167\uff09\u5d4c\u5165\u6df1\u5ea6\u5b66\u4e60\u9634\u5f71\u751f\u6210\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u76eeRGB\u56fe\u50cf\u83b7\u53d6\u8fd1\u4f3c3D\u51e0\u4f55\u548c\u4e3b\u5bfc\u5149\u65b9\u5411\uff0c\u57fa\u4e8e\u9634\u5f71\u5f62\u6210\u7269\u7406\u539f\u7406\u751f\u6210\u521d\u59cb\u9634\u5f71\u4f30\u8ba1\uff0c\u518d\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7cbe\u7ec6\u5316\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9634\u5f71\u751f\u6210\u65b9\u6cd5\u5f88\u5c11\u5229\u7528\u663e\u5f0f\u7684\u7269\u7406\u5efa\u6a21\uff0c\u800c\u9634\u5f71\u5f62\u6210\u672c\u8d28\u4e0a\u9075\u5faa\u7269\u7406\u89c4\u5f8b\uff08\u906e\u6321\u7269\u963b\u6321\u5149\u7ebf\u5f62\u6210\u9634\u5f71\uff09\u3002\u8bba\u6587\u65e8\u5728\u5c06\u51e0\u4f55\u548c\u5149\u7167\u7684\u663e\u5f0f\u7269\u7406\u5efa\u6a21\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u751f\u6210\u65e2\u89c6\u89c9\u903c\u771f\u53c8\u7269\u7406\u4e00\u81f4\u7684\u9634\u5f71\u3002", "method": "1. \u4ece\u5355\u76eeRGB\u56fe\u50cf\u83b7\u53d6\u5bc6\u96c6\u70b9\u4e91\u8868\u793a\u7684\u8fd1\u4f3c3D\u51e0\u4f55\uff1b2. \u9884\u6d4b\u4e3b\u5bfc\u5149\u65b9\u5411\uff1b3. \u57fa\u4e8e\u9634\u5f71\u5f62\u6210\u7269\u7406\u539f\u7406\u8ba1\u7b97\u521d\u59cb\u9634\u5f71\u4f4d\u7f6e\u548c\u5f62\u72b6\uff1b4. \u5c06\u7269\u7406\u57fa\u7840\u4f30\u8ba1\u8f93\u5165\u6269\u6563\u6846\u67b6\u8fdb\u884c\u7cbe\u7ec6\u5316\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u9634\u5f71\u5916\u89c2\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u573a\u666f\u51e0\u4f55\u548c\u5149\u7167\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728DESOBAV2\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6a21\u578b\u751f\u6210\u7684\u9634\u5f71\u65e2\u89c6\u89c9\u903c\u771f\u53c8\u7269\u7406\u4e00\u81f4\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u51e0\u4f55\u590d\u6742\u6216\u5149\u7167\u6a21\u7cca\u7684\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u901a\u8fc7\u5c06\u663e\u5f0f\u7269\u7406\u5efa\u6a21\uff08\u51e0\u4f55\u548c\u5149\u7167\uff09\u5d4c\u5165\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u57fa\u7840\u4f30\u8ba1\u548c\u6269\u6563\u6a21\u578b\u7cbe\u7ec6\u5316\uff0c\u80fd\u591f\u751f\u6210\u89c6\u89c9\u903c\u771f\u4e14\u7269\u7406\u4e00\u81f4\u7684\u9634\u5f71\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2512.07178", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07178", "abs": "https://arxiv.org/abs/2512.07178", "authors": ["Latifa Dwiyanti", "Sergio Ryan Wibisono", "Hidetaka Nambo"], "title": "ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation", "comment": "This paper was accepted and presented at the 7th World Symposium on Software Engineering (WSSE) 2025 on 25 October 2025 in Okayama, Japan, and is currently awaiting publication", "summary": "Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2aPython\u5305\uff0c\u5c06SHAP\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT\uff09\u96c6\u6210\uff0c\u4e3a\u7279\u5f81\u91cd\u8981\u6027\u89e3\u91ca\u751f\u6210\u4e0a\u4e0b\u6587\u6587\u672c\u63cf\u8ff0\uff0c\u63d0\u5347\u975e\u6280\u672f\u7528\u6237\u7684\u7406\u89e3\u6027\u3002", "motivation": "SHAP\u867d\u7136\u80fd\u6709\u6548\u53ef\u89c6\u5316\u7279\u5f81\u91cd\u8981\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u975e\u6280\u672f\u7528\u6237\u6709\u610f\u4e49\u7684\u4e0a\u4e0b\u6587\u89e3\u91ca\u3002\u5728\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\uff0c\u53ef\u89e3\u91ca\u6027\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u5f00\u53d1Python\u5305\uff0c\u5c06SHAP\u4e0eOpenAI\u7684GPT\u96c6\u6210\uff0c\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u7684\u53c2\u6570\uff08\u7279\u5f81\u522b\u540d\u3001\u63cf\u8ff0\u3001\u80cc\u666f\u4fe1\u606f\uff09\u751f\u6210\u4e0a\u4e0b\u6587\u6587\u672c\u89e3\u91ca\u3002", "result": "\u5728\u533b\u7597\u76f8\u5173\u6848\u4f8b\u7814\u7a76\u4e2d\u5e94\u7528\u8be5\u5305\uff0c\u901a\u8fc7\u674e\u514b\u7279\u91cf\u8868\u548c\u540e\u7eed\u8bbf\u8c08\u7684\u7528\u6237\u8bc4\u4f30\u663e\u793a\uff0c\u751f\u6210\u7684\u89e3\u91ca\u6bd4\u7eaf\u53ef\u89c6\u5316\u8f93\u51fa\u66f4\u6613\u7406\u89e3\u548c\u4e0a\u4e0b\u6587\u66f4\u5408\u9002\u3002", "conclusion": "\u53ef\u89c6\u5316\u4e0e\u4e0a\u4e0b\u6587\u6587\u672c\u7ed3\u5408\u53ef\u80fd\u652f\u6301\u66f4\u7528\u6237\u53cb\u597d\u548c\u53ef\u4fe1\u8d56\u7684\u6a21\u578b\u89e3\u91ca\uff0c\u5c3d\u7ba1\u7ed3\u679c\u8fd8\u662f\u521d\u6b65\u7684\u3002"}}
{"id": "2512.06787", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06787", "abs": "https://arxiv.org/abs/2512.06787", "authors": ["Ofek Glick", "Vladimir Tchuiev", "Marah Ghoummaid", "Michal Moshkovitz", "Dotan Di-Castro"], "title": "LLM4SFC: Sequential Function Chart Generation via Large Language Models", "comment": null, "summary": "While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.", "AI": {"tldr": "LLM4SFC\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u53ef\u6267\u884c\u7684\u987a\u5e8f\u529f\u80fd\u56fe(SFC)\uff0c\u89e3\u51b3\u4e86\u56fe\u5f62\u5316PLC\u7f16\u7a0b\u8bed\u8a00\u751f\u6210\u7684\u6311\u6218\uff0c\u5728\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8fbe\u523075%-94%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u7528\u4e8e\u751f\u6210\u6587\u672c\u5316PLC\u7f16\u7a0b\u8bed\u8a00\uff08\u5982\u7ed3\u6784\u5316\u6587\u672c\uff09\uff0c\u4f46IEC 61131-3\u6807\u51c6\u7684\u56fe\u5f62\u5316\u8bed\u8a00\uff08\u5982\u987a\u5e8f\u529f\u80fd\u56fe\uff09\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u751f\u6210SFC\u9762\u4e34\u56fe\u5f62\u7279\u6027\u548c\u5d4c\u5165\u5f0fST\u4ee3\u7801\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5e38\u4ea7\u751f\u4e0d\u53ef\u6267\u884c\u6216\u4e0e\u5de5\u4e1a\u5de5\u5177\u94fe\u4e0d\u517c\u5bb9\u7684\u4ee3\u7801\u3002", "method": "LLM4SFC\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u7cbe\u7b80\u7ed3\u6784\u5316\u8868\u793a\uff0c\u6355\u6349SFC\u62d3\u6251\u7ed3\u6784\u548c\u5d4c\u5165\u5f0fST\u4ee3\u7801\uff0c\u51cf\u5c11\u6587\u672c\u5197\u4f59\uff1b2) \u5fae\u8c03\u548c\u5c11\u6837\u672c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u4f7f\u6a21\u578b\u7b26\u5408SFC\u7f16\u7a0b\u89c4\u8303\uff1b3) \u7ed3\u6784\u5316\u751f\u6210\u65b9\u6cd5\uff0c\u5b9e\u65f6\u4fee\u526a\u975e\u6cd5\u4ee4\u724c\uff0c\u786e\u4fdd\u7b26\u5408SFC\u6587\u672c\u683c\u5f0f\u3002", "result": "\u5728\u81ea\u52a8\u5316\u5236\u9020\u9879\u76ee\u7684\u771f\u5b9eSFC\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u5f00\u6e90\u548c\u4e13\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff0cLLM4SFC\u53ef\u9760\u5730\u751f\u6210\u8bed\u6cd5\u6709\u6548\u7684SFC\u7a0b\u5e8f\uff0c\u6210\u529f\u6865\u63a5\u56fe\u5f62\u548c\u6587\u672cPLC\u8bed\u8a00\uff0c\u751f\u6210\u6210\u529f\u7387\u8fbe\u523075%-94%\u3002", "conclusion": "LLM4SFC\u662f\u9996\u4e2a\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u53ef\u6267\u884cSFC\u7684\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u5f62\u5316PLC\u7f16\u7a0b\u8bed\u8a00\u751f\u6210\u7684\u6311\u6218\uff0c\u4e3a\u81ea\u52a8\u5316\u5de5\u4e1a\u7f16\u7a0b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.06179", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06179", "abs": "https://arxiv.org/abs/2512.06179", "authors": ["Shilin Hu", "Jingyi Xu", "Sagnik Das", "Dimitris Samaras", "Hieu Le"], "title": "Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction", "comment": null, "summary": "Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.", "AI": {"tldr": "\u63d0\u51fa\u8054\u5408\u68c0\u6d4b\u6295\u5c04\u9634\u5f71\u548c\u9644\u7740\u9634\u5f71\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5149\u7167\u548c\u51e0\u4f55\u63a8\u7406\u5b9e\u73b0\u4e24\u79cd\u9634\u5f71\u7684\u76f8\u4e92\u589e\u5f3a\u68c0\u6d4b", "motivation": "\u73b0\u6709\u9634\u5f71\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6295\u5c04\u9634\u5f71\uff0c\u7f3a\u4e4f\u4e13\u95e8\u7684\u9644\u7740\u9634\u5f71\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u800c\u9644\u7740\u9634\u5f71\u5bf9\u7406\u89e3\u7269\u4f53\u4e09\u7ef4\u7ed3\u6784\u81f3\u5173\u91cd\u8981", "method": "\u6784\u5efa\u5305\u542b\u9634\u5f71\u68c0\u6d4b\u6a21\u5757\u548c\u5149\u7167\u4f30\u8ba1\u6a21\u5757\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u95ed\u73af\u63a8\u7406\u8fc7\u7a0b\u8fed\u4ee3\u4f18\u5316\u9634\u5f71\u5206\u5272\u548c\u5149\u7167\u4f30\u8ba1", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u9644\u7740\u9634\u5f71\u68c0\u6d4b\u6027\u80fd\uff08BER\u964d\u4f4e\u81f3\u5c1133%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5b8c\u6574\u9634\u5f71\u548c\u6295\u5c04\u9634\u5f71\u7684\u826f\u597d\u68c0\u6d4b\u6548\u679c", "conclusion": "\u901a\u8fc7\u5149\u7167-\u51e0\u4f55\u8054\u5408\u63a8\u7406\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9644\u7740\u9634\u5f71\u68c0\u6d4b\u95ee\u9898\uff0c\u4e3a\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u9634\u5f71\u5206\u6790\u80fd\u529b"}}
{"id": "2512.07179", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.07179", "abs": "https://arxiv.org/abs/2512.07179", "authors": ["Wonbeen Lee", "Channyoung Lee", "Junho Sohn", "Hansam Cho"], "title": "PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations", "comment": "15 pages, 5 figures, 17 tables. Preparing submission for EDM 2026 conference", "summary": "With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.", "AI": {"tldr": "\u63d0\u51faPICKT\u6a21\u578b\u89e3\u51b3\u77e5\u8bc6\u8ffd\u8e2a\u4e2d\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u5904\u7406\u591a\u79cd\u8f93\u5165\u6570\u636e\u683c\u5f0f\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6027\u80fd\u4e0e\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u5b58\u5728\u8f93\u5165\u6570\u636e\u683c\u5f0f\u53d7\u9650\u3001\u65b0\u5b66\u751f/\u65b0\u95ee\u9898\u51b7\u542f\u52a8\u95ee\u9898\u3001\u4ee5\u53ca\u5b9e\u9645\u670d\u52a1\u73af\u5883\u7a33\u5b9a\u6027\u4e0d\u8db3\u7b49\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPICKT\u6a21\u578b\uff0c\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u5316\u6982\u5ff5\u95f4\u5173\u7cfb\uff0c\u8003\u8651\u9898\u76ee\u548c\u6982\u5ff5\u6587\u672c\u4fe1\u606f\uff0c\u6709\u6548\u5904\u7406\u591a\u79cd\u8f93\u5165\u6570\u636e\u683c\u5f0f\uff0c\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6a21\u578b\u5728\u771f\u5b9e\u64cd\u4f5c\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u65b0\u5b66\u751f\u6ce8\u518c\u548c\u65b0\u9898\u76ee\u6dfb\u52a0\u4e24\u4e2a\u6838\u5fc3\u51b7\u542f\u52a8\u6311\u6218\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u7a33\u5b9a\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "PICKT\u6a21\u578b\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u4e0e\u6280\u672f\u57fa\u7840\uff0c\u589e\u5f3a\u4e86\u5728\u771f\u5b9e\u4ea7\u54c1\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2512.06812", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06812", "abs": "https://arxiv.org/abs/2512.06812", "authors": ["Tiago Rodrigues", "Carla Teixeira Lopes"], "title": "Large Language Model-Based Generation of Discharge Summaries", "comment": "17 pages, 6 figures", "summary": "Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u4e94\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff09\u81ea\u52a8\u751f\u6210\u51fa\u9662\u5c0f\u7ed3\u7684\u4efb\u52a1\uff0c\u53d1\u73b0\u4e13\u6709\u6a21\u578b\uff08\u7279\u522b\u662fGemini\uff09\u5728\u5355\u6837\u672c\u63d0\u793a\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u867d\u7136\u4e5f\u6709\u6f5c\u529b\u4f46\u5b58\u5728\u5e7b\u89c9\u548c\u91cd\u590d\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u51fa\u9662\u5c0f\u7ed3\u662f\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u7f16\u5199\u7684\u8be6\u7ec6\u8bb0\u5f55\u60a3\u8005\u5c31\u8bca\u60c5\u51b5\u7684\u6587\u6863\uff0c\u5305\u542b\u5bf9\u60a3\u8005\u62a4\u7406\u81f3\u5173\u91cd\u8981\u7684\u4e30\u5bcc\u4fe1\u606f\u3002\u81ea\u52a8\u5316\u751f\u6210\u51fa\u9662\u5c0f\u7ed3\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u7684\u5de5\u4f5c\u91cf\uff0c\u51cf\u5c11\u9519\u8bef\uff0c\u5e76\u786e\u4fdd\u5173\u952e\u60a3\u8005\u4fe1\u606f\u6613\u4e8e\u83b7\u53d6\u548c\u53ef\u64cd\u4f5c\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4e94\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u5e94\u7528\uff0c\u5305\u62ec\u5f00\u6e90\u6a21\u578b\uff08Mistral\u3001Llama 2\uff09\u548c\u4e13\u6709\u7cfb\u7edf\uff08GPT-3\u3001GPT-4\u3001Gemini 1.5 Pro\uff09\uff0c\u5229\u7528MIMIC-III\u7684\u6458\u8981\u548c\u7b14\u8bb0\u6570\u636e\u3002\u8bc4\u4f30\u65b9\u6cd5\u5305\u62ec\u7cbe\u786e\u5339\u914d\u3001\u8f6f\u91cd\u53e0\u548c\u65e0\u9700\u53c2\u8003\u6307\u6807\u7684\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e13\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u4f7f\u7528\u5355\u6837\u672c\u63d0\u793a\u7684Gemini\uff0c\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u751f\u6210\u7684\u6458\u8981\u4e0e\u9ec4\u91d1\u6807\u51c6\u6458\u8981\u7684\u76f8\u4f3c\u5ea6\u6700\u9ad8\u3002\u5f00\u6e90\u6a21\u578b\u867d\u7136\u4e5f\u6709\u6f5c\u529b\uff08\u7279\u522b\u662f\u5fae\u8c03\u540e\u7684Mistral\uff09\uff0c\u4f46\u5728\u6027\u80fd\u4e0a\u843d\u540e\uff0c\u7ecf\u5e38\u51fa\u73b0\u5e7b\u89c9\u548c\u91cd\u590d\u4fe1\u606f\u7684\u95ee\u9898\u3002\u4e34\u5e8a\u4e13\u5bb6\u7684\u4eba\u5de5\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u4e13\u6709\u6a21\u578b\u751f\u6210\u7684\u6458\u8981\u5177\u6709\u5b9e\u9645\u6548\u7528\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u5e7b\u89c9\u548c\u7f3a\u5931\u4fe1\u606f\u7b49\u6311\u6218\uff0c\u7814\u7a76\u7ed3\u679c\u8868\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7279\u522b\u662f\u4e13\u6709\u6a21\u578b\uff0c\u53ea\u8981\u786e\u4fdd\u6570\u636e\u9690\u79c1\uff0c\u5c31\u662f\u81ea\u52a8\u751f\u6210\u51fa\u9662\u5c0f\u7ed3\u7684\u6709\u5e0c\u671b\u7684\u5019\u9009\u65b9\u6848\u3002"}}
{"id": "2512.06814", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06814", "abs": "https://arxiv.org/abs/2512.06814", "authors": ["Dibyanayan Bandyopadhyay", "Soham Bhattacharjee", "Mohammed Hasanuzzaman", "Asif Ekbal"], "title": "CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation", "comment": "Accepted at Transactions of the Association for Computational Linguistics (TACL). Pre-MIT Press publication version", "summary": "Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE", "AI": {"tldr": "CAuSE\u662f\u4e00\u4e2a\u901a\u8fc7\u56e0\u679c\u62bd\u8c61\u751f\u6210\u5fe0\u5b9e\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u4ea4\u6362\u5e72\u9884\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u591a\u6a21\u6001\u5206\u7c7b\u5668\u901a\u5e38\u88ab\u89c6\u4e3a\u9ed1\u76d2\u6a21\u578b\uff0c\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u4e2d\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff08NLEs\uff09\u6700\u76f4\u89c2\u6613\u7528\uff0c\u4f46\u9700\u8981\u786e\u4fdd\u8fd9\u4e9b\u89e3\u91ca\u80fd\u5fe0\u5b9e\u53cd\u6620\u5206\u7c7b\u5668\u7684\u5185\u90e8\u51b3\u7b56\u884c\u4e3a\uff08\u5fe0\u5b9e\u6027\uff09\u3002", "method": "\u63d0\u51faCAuSE\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u6362\u5e72\u9884\u8bad\u7ec3\uff0c\u4e3a\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u5206\u7c7b\u5668\u751f\u6210\u5fe0\u5b9e\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u5f62\u6210\u5e95\u5c42\u5206\u7c7b\u5668\u7684\u56e0\u679c\u62bd\u8c61\u3002", "result": "CAuSE\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u91cd\u65b0\u8bbe\u8ba1\u7684\u56e0\u679c\u5fe0\u5b9e\u6027\u5ea6\u91cf\u6807\u51c6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5b9a\u6027\u5206\u6790\u4e5f\u652f\u6301\u5176\u4f18\u52bf\u3002", "conclusion": "CAuSE\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u591a\u6a21\u6001\u5206\u7c7b\u5668\u751f\u6210\u5fe0\u5b9e\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u5e76\u901a\u8fc7\u8be6\u7ec6\u7684\u9519\u8bef\u5206\u6790\u6307\u51fa\u4e86\u5176\u5931\u8d25\u6848\u4f8b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.06190", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.06190", "abs": "https://arxiv.org/abs/2512.06190", "authors": ["Shichen Li", "Ahmadreza Eslaminia", "Chenhui Shao"], "title": "Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying", "comment": null, "summary": "Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u989c\u8272\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u9ad8\u7ef4\u65f6\u95f4\u989c\u8272\u4fe1\u606f\u4e0e\u5e72\u71e5\u5de5\u827a\u53c2\u6570\uff0c\u5b9e\u73b0\u51c6\u786e\u4e14\u6570\u636e\u9ad8\u6548\u7684\u989c\u8272\u8f68\u8ff9\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u4f4e\u7ef4\u989c\u8272\u7279\u5f81\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u98df\u54c1\u6837\u54c1\u590d\u6742\u7684\u52a8\u6001\u989c\u8272\u8f68\u8ff9\uff0c\u4e14\u73b0\u6709\u5efa\u6a21\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u672a\u89c1\u5de5\u827a\u6761\u4ef6\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u989c\u8272\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u6574\u5408\u9ad8\u7ef4\u65f6\u95f4\u989c\u8272\u4fe1\u606f\u4e0e\u5e72\u71e5\u5de5\u827a\u53c2\u6570\uff0c\u5b9e\u73b0\u51c6\u786e\u4e14\u6570\u636e\u9ad8\u6548\u7684\u989c\u8272\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5728\u672a\u89c1\u5e72\u71e5\u6761\u4ef6\u4e0b\uff0c\u6a21\u578b\u5728\u997c\u5e72\u5e72\u71e5\u4e2d\u8fbe\u5230RMSE 2.12\uff0c\u82f9\u679c\u5e72\u71e5\u4e2d\u8fbe\u5230RMSE 1.29\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u8bef\u5dee\u51cf\u5c11\u8d85\u8fc790%\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u5177\u6709\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u9884\u6d4b\u98df\u54c1\u5e72\u71e5\u8fc7\u7a0b\u4e2d\u7684\u989c\u8272\u53d8\u5316\u8f68\u8ff9\u3002"}}
{"id": "2512.06848", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06848", "abs": "https://arxiv.org/abs/2512.06848", "authors": ["Sepyan Purnama Kristanto", "Lutfi Hakim", "Hermansyah"], "title": "AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices", "comment": "9Pages, 3 figure, Politeknik Negeri Banyuwangi", "summary": "Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.", "AI": {"tldr": "AquaFusionNet\uff1a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u7edf\u4e00\u663e\u5fae\u955c\u6210\u50cf\u548c\u7269\u7406\u5316\u5b66\u4f20\u611f\u5668\u6570\u636e\uff0c\u7528\u4e8e\u5b9e\u65f6\u76d1\u6d4b\u5c0f\u578b\u996e\u7528\u6c34\u7cfb\u7edf\u4e2d\u7684\u5fae\u751f\u7269\u6c61\u67d3\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\u3002", "motivation": "\u4f4e\u6536\u5165\u548c\u4e2d\u7b49\u6536\u5165\u5730\u533a\u7684\u5c0f\u578b\u996e\u7528\u6c34\u7cfb\u7edf\u4e2d\u5fae\u751f\u7269\u6c61\u67d3\u6ce2\u52a8\u8fc5\u901f\uff0c\u73b0\u6709\u76d1\u6d4b\u5de5\u5177\u53ea\u80fd\u6355\u6349\u90e8\u5206\u4fe1\u606f\u3002\u663e\u5fae\u955c\u6210\u50cf\u63d0\u4f9b\u5fae\u751f\u7269\u7ea7\u53ef\u89c1\u6027\uff0c\u7269\u7406\u5316\u5b66\u4f20\u611f\u5668\u663e\u793a\u77ed\u671f\u6c34\u8d28\u53d8\u5316\uff0c\u4f46\u64cd\u4f5c\u5458\u9700\u8981\u5206\u522b\u89e3\u8bfb\u8fd9\u4e9b\u6570\u636e\u6d41\uff0c\u5bfc\u81f4\u5b9e\u65f6\u51b3\u7b56\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faAquaFusionNet\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u95e8\u4e3a\u4f4e\u529f\u8017\u786c\u4ef6\u8bbe\u8ba1\u7684\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b66\u4e60\u5fae\u751f\u7269\u5916\u89c2\u4e0e\u4f20\u611f\u5668\u52a8\u6001\u4e4b\u95f4\u7684\u7edf\u8ba1\u4f9d\u8d56\u5173\u7cfb\u3002\u4f7f\u7528AquaMicro12K\u6570\u636e\u96c6\uff08\u5305\u542b12,846\u5f20\u996e\u7528\u6c34\u73af\u5883\u6807\u6ce8\u7684\u663e\u5fae\u56fe\u50cf\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u5370\u5ea6\u5c3c\u897f\u4e9a\u4e1c\u722a\u54c77\u4e2a\u8bbe\u65bd\u90e8\u7f726\u4e2a\u6708\uff0c\u5904\u7406184\u4e07\u5e27\u56fe\u50cf\uff0c\u6c61\u67d3\u4e8b\u4ef6\u68c0\u6d4b\u8fbe\u523094.8% mAP@0.5\uff0c\u5f02\u5e38\u9884\u6d4b\u51c6\u786e\u738796.3%\uff0c\u5728Jetson Nano\u4e0a\u529f\u8017\u4ec54.8W\u3002\u76f8\u6bd4\u4ee3\u8868\u6027\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u5668\uff0c\u5728\u76f8\u540c\u6216\u66f4\u4f4e\u529f\u8017\u4e0b\u63d0\u4f9b\u66f4\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8de8\u6a21\u6001\u8026\u5408\u51cf\u5c11\u4e86\u5355\u6a21\u6001\u68c0\u6d4b\u5668\u5e38\u89c1\u7684\u6545\u969c\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u5728\u6c61\u57a2\u3001\u6d4a\u5ea6\u5cf0\u503c\u548c\u4e0d\u4e00\u81f4\u7167\u660e\u6761\u4ef6\u4e0b\u3002\u6240\u6709\u6a21\u578b\u3001\u6570\u636e\u548c\u786c\u4ef6\u8bbe\u8ba1\u5df2\u5f00\u6e90\uff0c\u4fc3\u8fdb\u5206\u6563\u5f0f\u6c34\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\u7684\u590d\u5236\u548c\u9002\u5e94\u3002"}}
{"id": "2512.06206", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06206", "abs": "https://arxiv.org/abs/2512.06206", "authors": ["Akis Linardos", "Sarthak Pati", "Ujjwal Baid", "Brandon Edwards", "Patrick Foley", "Kevin Ta", "Verena Chung", "Micah Sheller", "Muhammad Irfan Khan", "Mojtaba Jafaritadi", "Elina Kontio", "Suleiman Khan", "Leon M\u00e4chler", "Ivan Ezhov", "Suprosanna Shit", "Johannes C. Paetzold", "Gustav Grimberg", "Manuel A. Nickel", "David Naccache", "Vasilis Siomos", "Jonathan Passerat-Palmbach", "Giacomo Tarroni", "Daewoon Kim", "Leonard L. Klausmann", "Prashant Shah", "Bjoern Menze", "Dimitrios Makris", "Spyridon Bakas"], "title": "The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning", "comment": "Published at the Journal of Machine Learning for Biomedical Imaging (MELBA) https://melba-journal.org/2025:033", "summary": "We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.", "AI": {"tldr": "MICCAI FeTS 2024\u6311\u6218\u8d5b\u8bc4\u4f30\u4e86\u7528\u4e8e\u80f6\u8d28\u7624\u4e9a\u533a\u5206\u5272\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0cPID\u63a7\u5236\u5668\u65b9\u6cd5\u5728\u5206\u5272\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8be5\u6311\u6218\u8d5b\u65e8\u5728\u63a8\u52a8\u533b\u5b66\u5f71\u50cf\u4e2d\u8054\u90a6\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u7279\u522b\u5173\u6ce8\u80f6\u8d28\u7624\u4e9a\u533a\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u8bc4\u4f30\u65b0\u7684\u6743\u91cd\u805a\u5408\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u5316\u7684\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u548c\u591a\u673a\u6784\u6570\u636e\u96c6\uff08\u6765\u81eaBraTS\u57fa\u51c6\uff09\uff0c\u5305\u542b1251\u4e2a\u8bad\u7ec3\u6848\u4f8b\u3001219\u4e2a\u9a8c\u8bc1\u6848\u4f8b\u548c570\u4e2a\u9690\u85cf\u6d4b\u8bd5\u6848\u4f8b\u3002\u91c7\u7528\u7d2f\u79ef\u8bc4\u5206\u7cfb\u7edf\uff0c\u7efc\u5408\u8003\u8651\u5206\u5272\u6027\u80fd\uff08DSC\u548cHD95\uff09\u548c\u901a\u4fe1\u6548\u7387\uff08\u6536\u655b\u5206\u6570\uff09\u3002", "result": "\u57fa\u4e8ePID\u63a7\u5236\u5668\u7684\u65b9\u6cd5\u83b7\u5f97\u6700\u9ad8\u6392\u540d\uff0cET\u3001TC\u3001WT\u7684DSC\u5e73\u5747\u503c\u5206\u522b\u4e3a0.733\u30010.761\u30010.751\uff0cHD95\u503c\u5206\u522b\u4e3a33.922mm\u300133.623mm\u300132.309mm\uff0c\u6536\u655b\u5206\u6570\u4e3a0.764\uff0c\u901a\u4fe1\u6548\u7387\u6700\u9ad8\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u8fdb\u4e86\u533b\u5b66\u5f71\u50cf\u8054\u90a6\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u8d85\u8d8a\u4e86\u5148\u524d\u6311\u6218\u8d5b\u7684\u6700\u4f73\u65b9\u6cd5\uff0c\u8bc1\u660ePID\u63a7\u5236\u5668\u662f\u7a33\u5b9a\u548c\u4f18\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u6743\u91cd\u805a\u5408\u7684\u6709\u6548\u673a\u5236\u3002"}}
{"id": "2512.07314", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07314", "abs": "https://arxiv.org/abs/2512.07314", "authors": ["Yuxiao Luo", "Songming Zhang", "Sijie Ruan", "Siran Chen", "Kang Liu", "Yang Xu", "Yu Zheng", "Ling Yin"], "title": "M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling", "comment": null, "summary": "Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.", "AI": {"tldr": "M-STAR\u662f\u4e00\u4e2a\u591a\u5c3a\u5ea6\u65f6\u7a7a\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u7c97\u5230\u7ec6\u7684\u9884\u6d4b\u8fc7\u7a0b\u751f\u6210\u957f\u671f\u8f68\u8ff9\uff0c\u5728\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u901f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u751f\u6210\u957f\u5468\u671f\u8f68\u8ff9\uff08\u5982\u5468\u8f68\u8ff9\uff09\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u7f3a\u4e4f\u663e\u5f0f\u7684\u65f6\u7a7a\u591a\u5c3a\u5ea6\u5efa\u6a21\u80fd\u529b", "method": "\u63d0\u51faM-STAR\u6846\u67b6\uff0c\u5305\u542b\u591a\u5c3a\u5ea6\u65f6\u7a7a\u6807\u8bb0\u5668\u7f16\u7801\u5206\u5c42\u79fb\u52a8\u6a21\u5f0f\uff0c\u4ee5\u53ca\u57fa\u4e8eTransformer\u7684\u89e3\u7801\u5668\u8fdb\u884c\u4e0b\u4e00\u5c3a\u5ea6\u81ea\u56de\u5f52\u9884\u6d4b", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cM-STAR\u5728\u8f68\u8ff9\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u901f\u5ea6", "conclusion": "M-STAR\u901a\u8fc7\u591a\u5c3a\u5ea6\u65f6\u7a7a\u5efa\u6a21\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u8f68\u8ff9\u751f\u6210\u7684\u6548\u7387\u548c\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u4e3a\u4eba\u7c7b\u79fb\u52a8\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5"}}
{"id": "2512.06869", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06869", "abs": "https://arxiv.org/abs/2512.06869", "authors": ["Wanyang Hong", "Zhaoning Zhang", "Yi Chen", "Libo Zhang", "Baihui Liu", "Linbo Qiao", "Zhiliang Tian", "Dongsheng Li"], "title": "Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.", "AI": {"tldr": "Rhea\u6846\u67b6\u901a\u8fc7\u89d2\u8272\u611f\u77e5\u542f\u53d1\u5f0f\u60c5\u666f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u5bf9\u8bdd\u5386\u53f2\u89e3\u8026\u4e3a\u6307\u4ee4\u8bb0\u5fc6\u548c\u60c5\u666f\u8bb0\u5fc6\u4e24\u4e2a\u72ec\u7acb\u6a21\u5757\uff0c\u6709\u6548\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u7d2f\u79ef\u4e0a\u4e0b\u6587\u8870\u51cf\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5355\u8f6e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6027\u80fd\u4f1a\u9010\u6e10\u4e0b\u964d\u3002\u7814\u7a76\u8005\u5c06\u8fd9\u79cd\u73b0\u8c61\u5b9a\u4e49\u4e3a\"\u7d2f\u79ef\u4e0a\u4e0b\u6587\u8870\u51cf\" - \u7531\u6ce8\u610f\u529b\u6c61\u67d3\u3001\u7a00\u91ca\u548c\u6f02\u79fb\u5bfc\u81f4\u7684\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u6e10\u8fdb\u5f0f\u9000\u5316\u3002", "method": "\u63d0\u51faRhea\uff08\u89d2\u8272\u611f\u77e5\u542f\u53d1\u5f0f\u60c5\u666f\u6ce8\u610f\u529b\uff09\u6846\u67b6\uff0c\u5c06\u5bf9\u8bdd\u5386\u53f2\u89e3\u8026\u4e3a\u4e24\u4e2a\u529f\u80fd\u72ec\u7acb\u7684\u8bb0\u5fc6\u6a21\u5757\uff1a1\uff09\u6307\u4ee4\u8bb0\u5fc6\uff08IM\uff09\uff1a\u901a\u8fc7\u7ed3\u6784\u4f18\u5148\u7ea7\u673a\u5236\u6301\u4e45\u5b58\u50a8\u9ad8\u4fdd\u771f\u5168\u5c40\u7ea6\u675f\uff1b2\uff09\u60c5\u666f\u8bb0\u5fc6\uff08EM\uff09\uff1a\u901a\u8fc7\u975e\u5bf9\u79f0\u566a\u58f0\u63a7\u5236\u548c\u542f\u53d1\u5f0f\u4e0a\u4e0b\u6587\u68c0\u7d22\u52a8\u6001\u7ba1\u7406\u7528\u6237-\u6a21\u578b\u4ea4\u4e92\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0cRhea\u5e94\u7528\u4f18\u5148\u7ea7\u6ce8\u610f\u529b\u6784\u5efa\u9ad8\u4fe1\u566a\u6bd4\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u591a\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ecMT-Eval\u548cLong-MT-Bench+\uff09\u4e0a\uff0cRhea\u7f13\u89e3\u4e86\u6027\u80fd\u8870\u51cf\uff0c\u572810\u5206\u5236\u4e0a\u63d0\u9ad8\u4e861.04\u5206\uff08\u76f8\u5bf9\u4e8e\u5f3a\u57fa\u7ebf\u670916%\u7684\u76f8\u5bf9\u589e\u76ca\uff09\u3002\u6b64\u5916\uff0cRhea\u5728\u957f\u65f6\u7a0b\u4ea4\u4e92\u4e2d\u4fdd\u6301\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6307\u4ee4\u4fdd\u771f\u5ea6\uff08IAR > 8.1\uff09\u3002", "conclusion": "Rhea\u4e3a\u6784\u5efa\u66f4\u7cbe\u786e\u3001\u6307\u4ee4\u4e00\u81f4\u7684\u5bf9\u8bdd\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u95ee\u9898\u3002"}}
{"id": "2512.06221", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06221", "abs": "https://arxiv.org/abs/2512.06221", "authors": ["Alena Makarova"], "title": "Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study", "comment": "15 pages, 13 figures. Reproducibility study", "summary": "This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u7ed3\u5408\u5947\u5f02\u503c\u5206\u89e3(SVD)\u548c\u5c0f\u6ce2\u5dee\u5206\u7f29\u51cf(WDR)\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u8fdb\u884c\u4e86\u53ef\u91cd\u590d\u6027\u9a8c\u8bc1\uff0c\u53d1\u73b0\u539f\u8bba\u6587\u58f0\u79f0\u7684\u4f18\u4e8eJPEG2000\u548c\u5355\u72ecWDR\u7684\u7ed3\u679c\u65e0\u6cd5\u590d\u73b0\uff0c\u5b9e\u9645\u6027\u80fd\u5e76\u672a\u8d85\u8d8aJPEG2000\u3002", "motivation": "\u9a8c\u8bc1SVD+WDR\u56fe\u50cf\u538b\u7f29\u6280\u672f\u7684\u539f\u59cb\u6027\u80fd\u58f0\u79f0\uff0c\u68c0\u67e5\u5176\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u8bc6\u522b\u539f\u8bba\u6587\u4e2d\u7f3a\u5931\u7684\u5b9e\u73b0\u7ec6\u8282\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "method": "\u91cd\u65b0\u5b9e\u73b0SVD+WDR\u65b9\u6cd5\uff0c\u4ed4\u7ec6\u586b\u8865\u539f\u8bba\u6587\u4e2d\u7f3a\u5931\u7684\u5b9e\u73b0\u7ec6\u8282\uff08\u5982\u91cf\u5316\u548c\u9608\u503c\u521d\u59cb\u5316\uff09\uff0c\u5c3d\u53ef\u80fd\u590d\u73b0\u539f\u59cb\u5b9e\u9a8c\uff0c\u5e76\u5728\u65b0\u56fe\u50cf\u4e0a\u8fdb\u884c\u989d\u5916\u6d4b\u8bd5\uff0c\u4f7f\u7528PSNR\u548cSSIM\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u590d\u73b0\u7ed3\u679c\u663e\u793aSVD+WDR\u65b9\u6cd5\u5728PSNR\u65b9\u9762\u901a\u5e38\u65e0\u6cd5\u8d85\u8d8aJPEG2000\u6216WDR\uff0c\u4ec5\u5728SSIM\u65b9\u9762\u76f8\u5bf9\u4e8eJPEG2000\u6709\u90e8\u5206\u6539\u8fdb\uff0c\u4e0e\u539f\u8bba\u6587\u58f0\u79f0\u7684\u4f18\u8d8a\u6027\u80fd\u4e0d\u7b26\u3002", "conclusion": "\u539f\u8bba\u6587\u4e2d\u7684\u6a21\u7cca\u63cf\u8ff0\uff08\u5982\u91cf\u5316\u548c\u9608\u503c\u521d\u59cb\u5316\u7ec6\u8282\uff09\u663e\u8457\u5f71\u54cd\u4e86\u65b9\u6cd5\u7684\u53ef\u91cd\u590d\u6027\u548c\u62a5\u544a\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u5728\u5b66\u672f\u7814\u7a76\u4e2d\u63d0\u4f9b\u5b8c\u6574\u5b9e\u73b0\u7ec6\u8282\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.07355", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07355", "abs": "https://arxiv.org/abs/2512.07355", "authors": ["Alexandre Rocchi--Henry", "Thomas Fel", "Gianni Franchi"], "title": "A Geometric Unification of Concept Learning with Concept Cones", "comment": "22 pages", "summary": "Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\\footnote{We adopt the terminology of \\citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u51e0\u4f55\u6846\u67b6\u7edf\u4e00\u4e86\u6982\u5ff5\u74f6\u9888\u6a21\u578b(CBMs)\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAEs)\uff0c\u63d0\u51fa\u4e24\u8005\u90fd\u5b66\u4e60\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u65b9\u5411\u5f62\u6210\u6982\u5ff5\u9525\uff0c\u533a\u522b\u4ec5\u5728\u4e8e\u5982\u4f55\u9009\u62e9\u8fd9\u4e2a\u9525\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u8bc4\u4f30SAEs\u5b66\u4e60\u6982\u5ff5\u4e0eCBM\u6982\u5ff5\u5bf9\u9f50\u7a0b\u5ea6\u7684\u91cf\u5316\u6307\u6807\u3002", "motivation": "\u4e24\u79cd\u53ef\u89e3\u91ca\u6027\u4f20\u7edf\u2014\u2014\u76d1\u7763\u5f0f\u7684\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u548c\u65e0\u76d1\u7763\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\u2014\u2014\u867d\u7136\u5e76\u884c\u53d1\u5c55\u4f46\u5f88\u5c11\u5bf9\u8bdd\u3002CBMs\u901a\u8fc7\u76d1\u7763\u5b9a\u4e49\u6982\u5ff5\uff0cSAEs\u901a\u8fc7\u7a00\u758f\u7f16\u7801\u53d1\u73b0\u6982\u5ff5\uff0c\u9700\u8981\u5efa\u7acb\u4e24\u8005\u4e4b\u95f4\u7684\u7406\u8bba\u6865\u6881\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u6846\u67b6\uff1a\u4e24\u79cd\u65b9\u6cd5\u90fd\u5b66\u4e60\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u65b9\u5411\uff0c\u5176\u975e\u8d1f\u7ec4\u5408\u5f62\u6210\u6982\u5ff5\u9525\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u5c06CBMs\u4f5c\u4e3a\u4eba\u7c7b\u5b9a\u4e49\u7684\u53c2\u8003\u51e0\u4f55\uff0c\u8bc4\u4f30SAEs\u5b66\u4e60\u7684\u6982\u5ff5\u9525\u4e0eCBM\u6982\u5ff5\u9525\u7684\u5305\u542b\u5173\u7cfb\uff0c\u63d0\u51fa\u91cf\u5316\u5bf9\u9f50\u6307\u6807\u3002", "result": "\u53d1\u73b0\u4e86\u7a00\u758f\u5ea6\u548c\u6269\u5c55\u56e0\u5b50\u7684\"\u6700\u4f73\u70b9\"\uff0c\u80fd\u6700\u5927\u5316\u4e0eCBM\u6982\u5ff5\u7684\u51e0\u4f55\u548c\u8bed\u4e49\u5bf9\u9f50\u3002\u63d0\u4f9b\u4e86\u8bc4\u4f30SAEs\u8fdb\u5c55\u548c\u53d1\u73b0\u6982\u5ff5\u4e0e\u4eba\u7c7b\u6982\u5ff5\u5bf9\u9f50\u7a0b\u5ea6\u7684\u539f\u7406\u6027\u6307\u6807\u3002", "conclusion": "\u901a\u8fc7\u5171\u4eab\u7684\u51e0\u4f55\u6846\u67b6\u7edf\u4e00\u4e86\u76d1\u7763\u548c\u65e0\u76d1\u7763\u7684\u6982\u5ff5\u53d1\u73b0\u65b9\u6cd5\uff0c\u4e3a\u8861\u91cfSAEs\u5b66\u4e60\u7684\u6982\u5ff5\u4e0e\u4eba\u7c7b\u6982\u5ff5\u7684\u5bf9\u9f50\u7a0b\u5ea6\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u548c\u91cf\u5316\u5de5\u5177\u3002"}}
{"id": "2512.06874", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06874", "abs": "https://arxiv.org/abs/2512.06874", "authors": ["Ziyun Yu", "Yiru Zhou", "Chen Zhao", "Hongyi Wen"], "title": "An Analysis of Large Language Models for Simulating User Responses in Surveys", "comment": "Accepted to IJCNLP-AACL 2025 (Main Conference)", "summary": "Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u7528\u6237\u610f\u89c1\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u5373\u4f7f\u91c7\u7528CLAIMSIM\u65b9\u6cd5\u589e\u5f3a\u591a\u6837\u6027\uff0cLLMs\u4ecd\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u7528\u6237\u7684\u771f\u5b9e\u89c2\u70b9\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08\u7279\u522b\u662f\u7ecf\u8fc7RLHF\u8bad\u7ec3\u7684\uff09\u5b58\u5728\u504f\u5411\u4e3b\u6d41\u89c2\u70b9\u7684\u504f\u89c1\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5b83\u4eec\u80fd\u5426\u4ee3\u8868\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u548c\u6587\u5316\u80cc\u666f\u7528\u6237\u7684\u62c5\u5fe7\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22LLMs\u5728\u6a21\u62df\u8de8\u9886\u57df\u8c03\u67e5\u95ee\u5377\u56de\u7b54\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u76f4\u63a5\u63d0\u793a\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u63d0\u51faCLAIMSIM\u65b9\u6cd5\uff08\u89c2\u70b9\u591a\u6837\u5316\u65b9\u6cd5\uff09\uff0c\u8be5\u65b9\u6cd5\u4eceLLMs\u7684\u53c2\u6570\u77e5\u8bc6\u4e2d\u63d0\u53d6\u89c2\u70b9\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\u3002\u5728\u8c03\u67e5\u95ee\u5377\u56de\u7b54\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136CLAIMSIM\u80fd\u4ea7\u751f\u66f4\u591a\u6837\u5316\u7684\u56de\u7b54\uff0c\u4f46\u4e24\u79cd\u65b9\u6cd5\u90fd\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u7528\u6237\u3002\u5206\u6790\u53d1\u73b0\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1\uff09LLMs\u503e\u5411\u4e8e\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u95f4\u4fdd\u6301\u56fa\u5b9a\u89c2\u70b9\uff0c\u751f\u6210\u5355\u4e00\u89c6\u89d2\u7684\u58f0\u660e\uff1b2\uff09\u5f53\u9762\u5bf9\u76f8\u4e92\u51b2\u7a81\u7684\u89c2\u70b9\u65f6\uff0cLLMs\u96be\u4ee5\u63a8\u7406\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u95f4\u7684\u7ec6\u5fae\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u9488\u5bf9\u7279\u5b9a\u7528\u6237\u7279\u5f81\u8c03\u6574\u56de\u7b54\u7684\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u591a\u6837\u5316\u7528\u6237\u89c2\u70b9\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u7684\u7ec6\u5fae\u5dee\u5f02\u548c\u751f\u6210\u591a\u89c6\u89d2\u89c2\u70b9\u65b9\u9762\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u6539\u8fdbLLMs\u5728\u6a21\u62df\u771f\u5b9e\u7528\u6237\u610f\u89c1\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2512.07436", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07436", "abs": "https://arxiv.org/abs/2512.07436", "authors": ["Hang He", "Chuhuai Yue", "Chengqi Dong", "Mingxue Tian", "Zhenfeng Liu", "Jiajun Chai", "Xiaohan Wang", "Yufei Zhang", "Qun Liao", "Guojun Yin", "Wei Lin", "Chengcheng Wan", "Haiying Sun", "Ting Su"], "title": "LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services", "comment": null, "summary": "Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86LocalSearchBench\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u672c\u5730\u751f\u6d3b\u670d\u52a1\u7684\u667a\u80fd\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u8d85\u8fc715\u4e07\u6761\u9ad8\u8d28\u91cf\u6570\u636e\u6761\u76ee\u548c300\u4e2a\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u672c\u5730\u751f\u6d3b\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u901a\u7528\u4fe1\u606f\u68c0\u7d22\u9886\u57df\uff0c\u5f88\u5c11\u5173\u6ce8\u5177\u6709\u72ec\u7279\u6311\u6218\u7684\u5782\u76f4\u9886\u57df\u3002\u672c\u5730\u751f\u6d3b\u670d\u52a1\u9886\u57df\u7684\u67e5\u8be2\u901a\u5e38\u5177\u6709\u6a21\u7cca\u6027\uff0c\u9700\u8981\u5728\u5546\u5bb6\u548c\u4ea7\u54c1\u4e4b\u95f4\u8fdb\u884c\u591a\u8df3\u63a8\u7406\uff0c\u8fd9\u4e9b\u6311\u6218\u5c1a\u672a\u5f97\u5230\u5145\u5206\u89e3\u51b3\u3002", "method": "\u6784\u5efa\u4e86LocalSearchBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u6765\u81ea\u4e0d\u540c\u57ce\u5e02\u548c\u4e1a\u52a1\u7c7b\u578b\u7684\u8d85\u8fc7150,000\u6761\u9ad8\u8d28\u91cf\u6761\u76ee\uff0c\u5e76\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u6784\u5efa\u4e86300\u4e2a\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u3002\u540c\u65f6\u5f00\u53d1\u4e86LocalPlayground\u7edf\u4e00\u73af\u5883\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u5de5\u5177\u4f9b\u667a\u80fd\u4f53\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728LocalSearchBench\u4e0a\u4e5f\u8868\u73b0\u4e0d\u4f73\uff1a\u6700\u4f73\u6a21\u578b\uff08DeepSeek-V3.1\uff09\u4ec5\u8fbe\u523034.34%\u7684\u6b63\u786e\u7387\uff0c\u5927\u591a\u6570\u6a21\u578b\u5728\u5b8c\u6574\u6027\uff08\u5e73\u574777.33%\uff09\u548c\u5fe0\u5b9e\u6027\uff08\u5e73\u574761.99%\uff09\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u672c\u5730\u751f\u6d3b\u670d\u52a1\u9886\u57df\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u9886\u57df\u7279\u5b9a\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\uff0c\u4ee5\u5e94\u5bf9\u8be5\u9886\u57df\u7684\u72ec\u7279\u6311\u6218\u3002"}}
{"id": "2512.06919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06919", "abs": "https://arxiv.org/abs/2512.06919", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla"], "title": "Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles", "comment": "13 pages, 2 figures", "summary": "The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.", "AI": {"tldr": "\u5f00\u53d1\u81ea\u52a8\u5316\u65b9\u6cd5\u4ecePRO-CTCAE\u5e93\u4e2d\u9009\u62e9\u6700\u5c0f\u4f46\u5168\u9762\u7684\u75c7\u72b6\u5b50\u96c6\uff0c\u57fa\u4e8e\u5386\u53f2\u5b89\u5168\u6570\u636e\u548cMedDRA\u8bed\u4e49\u5206\u6790\uff0c\u5e73\u8861\u4fe1\u53f7\u8986\u76d6\u4e0e\u60a3\u8005\u8d1f\u62c5", "motivation": "PRO-CTCAE\u7cfb\u7edf\u5305\u542b\u5927\u91cf\u75c7\u72b6\u9879\uff0c\u4f20\u7edf\u57fa\u4e8e\u9884\u671f\u6bd2\u6027\u8c31\u7684\u9009\u62e9\u65b9\u6cd5\u5b58\u5728\u6311\u6218\uff1a\u9009\u62e9\u8fc7\u591a\u9879\u4f1a\u589e\u52a0\u60a3\u8005\u8d1f\u62c5\u964d\u4f4e\u4f9d\u4ece\u6027\uff0c\u9009\u62e9\u8fc7\u5c11\u53ef\u80fd\u9057\u6f0f\u91cd\u8981\u5b89\u5168\u4fe1\u53f7\uff0c\u9700\u8981\u66f4\u5ba2\u89c2\u3001\u53ef\u91cd\u590d\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u8986\u76d6\u8303\u56f4\u4e0e\u60a3\u8005\u8d1f\u62c5", "method": "1) \u5c06PRO-CTCAE\u75c7\u72b6\u9879\u6620\u5c04\u5230\u5bf9\u5e94\u7684MedDRA\u9996\u9009\u672f\u8bed\uff1b2) \u4f7f\u7528Safeterm\u9ad8\u7ef4\u8bed\u4e49\u7a7a\u95f4\u7f16\u7801\u4e34\u5e8a\u548c\u4e0a\u4e0b\u6587\u591a\u6837\u6027\uff1b3) \u57fa\u4e8e\u5386\u53f2\u4e0d\u826f\u4e8b\u4ef6\u6570\u636e\u8ba1\u7b97\u6bcf\u4e2a\u5019\u9009\u9879\u7684\u76f8\u5173\u6027\u548c\u53d1\u751f\u7387\uff1b4) \u5e94\u7528\u8c31\u5206\u6790\u5230\u7ec4\u5408\u6548\u7528\u548c\u591a\u6837\u6027\u77e9\u9635\uff0c\u8bc6\u522b\u6b63\u4ea4\u533b\u5b66\u6982\u5ff5\u96c6\uff1b5) \u6309\u91cd\u8981\u6027\u6392\u5e8f\u75c7\u72b6\u5e76\u57fa\u4e8e\u89e3\u91ca\u4fe1\u606f\u5efa\u8bae\u622a\u65ad\u70b9", "result": "\u8be5\u65b9\u6cd5\u5df2\u4f5c\u4e3aSafeterm\u8bd5\u9a8c\u5b89\u5168\u5e94\u7528\u7684\u4e00\u90e8\u5206\u5b9e\u73b0\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9645\u80bf\u7624\u5b66\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u8bc4\u4f30\uff0c\u8bc1\u660e\u80fd\u591f\u6709\u6548\u5e73\u8861\u4fe1\u53f7\u8986\u76d6\u4e0e\u60a3\u8005\u8d1f\u62c5", "conclusion": "\u8fd9\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u901a\u8fc7\u5229\u7528MedDRA\u8bed\u4e49\u548c\u5386\u53f2\u6570\u636e\uff0c\u4e3aPRO-CTCAE\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5ba2\u89c2\u3001\u53ef\u91cd\u590d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5e73\u8861\u5b89\u5168\u4fe1\u53f7\u8986\u76d6\u4e0e\u60a3\u8005\u62a5\u544a\u8d1f\u62c5\uff0c\u7b80\u5316\u4e34\u5e8a\u8bd5\u9a8c\u8bbe\u8ba1\u6d41\u7a0b"}}
{"id": "2512.06232", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06232", "abs": "https://arxiv.org/abs/2512.06232", "authors": ["Ellen Su", "Solim Legris", "Todd M. Gureckis", "Mengye Ren"], "title": "Opinion: Learning Intuitive Physics May Require More than Visual Data", "comment": null, "summary": "Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u6570\u636e\u5206\u5e03\u800c\u975e\u6570\u636e\u91cf\u662f\u5426\u662f\u5b66\u4e60\u76f4\u89c9\u7269\u7406\u7684\u5173\u952e\uff0c\u901a\u8fc7\u5728\u53d1\u5c55\u73b0\u5b9e\u7684\u513f\u7ae5\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u53d1\u73b0\u4ec5\u4f7f\u7528\u53d1\u5c55\u73b0\u5b9e\u6570\u636e\u4e0d\u8db3\u4ee5\u8ba9\u5f53\u524d\u67b6\u6784\u5b66\u4e60\u76f4\u89c9\u7269\u7406\u8868\u793a\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u57fa\u4e8e\u76f4\u89c9\u7269\u7406\u7406\u89e3\u7684\u4e30\u5bcc\u5185\u90e8\u6a21\u578b\u6765\u719f\u7ec3\u5bfc\u822a\u4e16\u754c\uff0c\u800c\u5c3d\u7ba1\u5728\u5927\u91cf\u4e92\u8054\u7f51\u89c6\u9891\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u76f4\u89c9\u7269\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4ecd\u8fbe\u4e0d\u5230\u4eba\u7c7b\u6c34\u5e73\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u6570\u636e\u5206\u5e03\uff08\u800c\u975e\u6570\u636e\u91cf\uff09\u662f\u5426\u662f\u5b66\u4e60\u8fd9\u4e9b\u7269\u7406\u539f\u7406\u7684\u5173\u952e\u3002", "method": "\u5728SAYCam\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u89c6\u9891\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff08V-JEPA\uff09\u6a21\u578b\u3002SAYCam\u662f\u4e00\u4e2a\u53d1\u5c55\u73b0\u5b9e\u3001\u81ea\u6211\u4e2d\u5fc3\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u90e8\u5206\u6355\u6349\u4e86\u4e09\u4e2a\u513f\u7ae5\u7684\u65e5\u5e38\u89c6\u89c9\u4f53\u9a8c\uff0c\u5176\u6570\u636e\u91cf\u4ec5\u4e3a\u6700\u5148\u8fdb\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u91cf\u76840.01%\u3002", "result": "\u5728\u8fd9\u4e2a\u53d1\u5c55\u73b0\u5b9e\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5e76\u672a\u5bfc\u81f4IntPhys2\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528\u53d1\u5c55\u73b0\u5b9e\u6570\u636e\u96c6\u8bad\u7ec3\u4e0d\u8db3\u4ee5\u8ba9\u5f53\u524d\u67b6\u6784\u5b66\u4e60\u652f\u6301\u76f4\u89c9\u7269\u7406\u7684\u8868\u793a\u3002", "conclusion": "\u4ec5\u6539\u53d8\u89c6\u89c9\u6570\u636e\u91cf\u548c\u5206\u5e03\u53ef\u80fd\u4e0d\u8db3\u4ee5\u6784\u5efa\u5177\u6709\u4eba\u5de5\u76f4\u89c9\u7269\u7406\u7684\u7cfb\u7edf\u3002\u9700\u8981\u66f4\u6df1\u5165\u7684\u65b9\u6cd5\u6765\u4f7fAI\u7cfb\u7edf\u83b7\u5f97\u7c7b\u4f3c\u4eba\u7c7b\u7684\u7269\u7406\u76f4\u89c9\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2512.06922", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.06922", "abs": "https://arxiv.org/abs/2512.06922", "authors": ["George Mikros"], "title": "Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI", "comment": null, "summary": "Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.", "AI": {"tldr": "LLMs\u5728\u53f8\u6cd5\u8bed\u8a00\u5b66\u4e2d\u65e2\u662f\u5206\u6790\u5de5\u5177\u53c8\u662f\u6311\u6218\u6765\u6e90\uff1a\u53ef\u7528\u4e8e\u5927\u89c4\u6a21\u8bed\u6599\u5206\u6790\u548c\u4f5c\u8005\u8bc6\u522b\uff0c\u4f46\u4f1a\u901a\u8fc7\u98ce\u683c\u6a21\u4eff\u3001\u4f5c\u8005\u6df7\u6dc6\u548c\u751f\u6210\u5408\u6210\u6587\u672c\u7834\u574f\u8bed\u8a00\u7279\u5f81\u7684\u72ec\u7279\u6027\u3002\u5f53\u524dAI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u53f8\u6cd5\u8bed\u8a00\u5b66\u9700\u8981\u65b9\u6cd5\u91cd\u6784\u4ee5\u4fdd\u6301\u79d1\u5b66\u53ef\u4fe1\u5ea6\u548c\u6cd5\u5f8b\u53ef\u91c7\u7eb3\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u53f8\u6cd5\u8bed\u8a00\u5b66\u6784\u6210\u53cc\u91cd\u6311\u6218\uff1a\u4e00\u65b9\u9762\u53ef\u4f5c\u4e3a\u5f3a\u5927\u7684\u5206\u6790\u5de5\u5177\uff0c\u53e6\u4e00\u65b9\u9762\u901a\u8fc7\u98ce\u683c\u6a21\u4eff\u3001\u4f5c\u8005\u6df7\u6dc6\u548c\u5408\u6210\u6587\u672c\u751f\u6210\u7834\u574f\u4e86\u8bed\u8a00\u7279\u5f81\u72ec\u7279\u6027\u8fd9\u4e00\u57fa\u672c\u5047\u8bbe\u3002\u5f53\u524dAI\u6587\u672c\u68c0\u6d4b\u6280\u672f\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\uff08\u7279\u522b\u662f\u5bf9\u975e\u6bcd\u8bed\u82f1\u8bed\u5199\u4f5c\u8005\uff09\u548c\u5bf9\u6297\u7b56\u7565\u8106\u5f31\u6027\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u5728\u6cd5\u5f8b\u53ef\u91c7\u7eb3\u6027\u6807\u51c6\u4e0b\u5f15\u53d1\u62c5\u5fe7\u3002", "method": "\u6587\u7ae0\u5206\u6790\u4e86LLMs\u5728\u53f8\u6cd5\u8bed\u8a00\u5b66\u4e2d\u7684\u53cc\u91cd\u89d2\u8272\uff0c\u8bc4\u4f30\u4e86\u5f53\u524dAI\u6587\u672c\u68c0\u6d4b\u6280\u672f\uff08\u57fa\u4e8e\u5206\u7c7b\u5668\u3001\u98ce\u683c\u8ba1\u91cf\u5b66\u548c\u6c34\u5370\u65b9\u6cd5\uff09\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u65b9\u6cd5\u91cd\u6784\u65b9\u6848\u3002\u7814\u7a76\u57fa\u4e8e\u73b0\u6709\u98ce\u683c\u8ba1\u91cf\u5b66\u7814\u7a76\uff0c\u63a2\u8ba8LLMs\u6a21\u62df\u4eba\u7c7b\u5199\u4f5c\u98ce\u683c\u7684\u80fd\u529b\u53ca\u5176\u53ef\u68c0\u6d4b\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u80fd\u591f\u8fd1\u4f3c\u8868\u9762\u98ce\u683c\u7279\u5f81\uff0c\u4f46\u4e0e\u4eba\u7c7b\u5199\u4f5c\u8005\u5b58\u5728\u53ef\u68c0\u6d4b\u5dee\u5f02\u3002\u5f53\u524dAI\u6587\u672c\u68c0\u6d4b\u6280\u672f\u9762\u4e34\u91cd\u5927\u9650\u5236\uff1a\u5bf9\u975e\u6bcd\u8bed\u82f1\u8bed\u5199\u4f5c\u8005\u7684\u9ad8\u8bef\u62a5\u7387\uff0c\u4ee5\u53ca\u5bf9\u540c\u5f62\u5f02\u4e49\u66ff\u6362\u7b49\u5bf9\u6297\u7b56\u7565\u7684\u8106\u5f31\u6027\u3002\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u5728\u6cd5\u5f8b\u53ef\u91c7\u7eb3\u6027\u6807\u51c6\u4e0b\u5b58\u5728\u95ee\u9898\u3002", "conclusion": "\u53f8\u6cd5\u8bed\u8a00\u5b66\u9700\u8981\u8fdb\u884c\u65b9\u6cd5\u91cd\u6784\u4ee5\u4fdd\u6301\u79d1\u5b66\u53ef\u4fe1\u5ea6\u548c\u6cd5\u5f8b\u53ef\u91c7\u7eb3\u6027\u3002\u5efa\u8bae\u7684\u8c03\u6574\u5305\u62ec\uff1a\u6df7\u5408\u4eba\u673a\u5de5\u4f5c\u6d41\u7a0b\u3001\u8d85\u8d8a\u4e8c\u5143\u5206\u7c7b\u7684\u53ef\u89e3\u91ca\u68c0\u6d4b\u8303\u5f0f\uff0c\u4ee5\u53ca\u6d4b\u91cf\u4e0d\u540c\u4eba\u7fa4\u9519\u8bef\u548c\u504f\u89c1\u7684\u9a8c\u8bc1\u673a\u5236\u3002\u8be5\u9886\u57df\u7684\u6838\u5fc3\u89c1\u89e3\uff08\u8bed\u8a00\u63ed\u793a\u5176\u751f\u4ea7\u8005\u4fe1\u606f\uff09\u4ecd\u7136\u6709\u6548\uff0c\u4f46\u5fc5\u987b\u9002\u5e94\u65e5\u76ca\u590d\u6742\u7684\u4eba\u7c7b\u548c\u673a\u5668\u4f5c\u8005\u94fe\u3002"}}
{"id": "2512.07611", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07611", "abs": "https://arxiv.org/abs/2512.07611", "authors": ["Yongsheng Lian"], "title": "Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement", "comment": null, "summary": "This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.\n  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e09\u79cd\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08PPO\u3001GRPO\u3001DAPO\uff09\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u590d\u6742\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6548\u679c\uff0c\u901a\u8fc7\u63a7\u5236\u6027\u8fc1\u79fb\u5b66\u4e60\u8bc4\u4f30\u53d1\u73b0RL\u8bad\u7ec3\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u6539\u8fdb\u7a0b\u5ea6\u56e0\u57fa\u51c6\u800c\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4e0d\u540c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5982\u4f55\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e3aRL-based LLM\u8bad\u7ec3\u63d0\u4f9b\u5b9e\u7528\u7684\u53c2\u6570\u914d\u7f6e\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u63a7\u5236\u6027\u8fc1\u79fb\u5b66\u4e60\u8bc4\u4f30\u65b9\u6cd5\uff1a\u9996\u5148\u5728\u4e13\u95e8\u7684Countdown Game\u4e0a\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u7136\u540e\u5728\u901a\u7528\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u5bf9GRPO\u548cDAPO\u4e2d\u7684\u7ec4\u5927\u5c0f\u3001KL\u60e9\u7f5a\u7cfb\u6570\u7b49\u53c2\u6570\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "result": "\u6240\u6709RL\u8bad\u7ec3\u6a21\u578b\u5728\u5404\u9879\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u5bf9\u5e94\u7684\u57fa\u7840\u6a21\u578b\uff1b\u589e\u52a0GRPO\u548cDAPO\u4e2d\u7684\u7ec4\u5927\u5c0f\u80fd\u5e26\u6765\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u52a8\u6001\u548c\u66f4\u9ad8\u51c6\u786e\u7387\uff1bKL\u60e9\u7f5a\u7cfb\u6570\u7684\u5f71\u54cd\u662f\u975e\u5355\u8c03\u7684\uff1bDAPO\u4e2d\u7684\u52a8\u6001\u91c7\u6837\u7ec4\u4ef6\u5e76\u672a\u63d0\u5347\u6027\u80fd\uff0c\u7981\u7528DS\u65f6DAPO\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4e0d\u540c\u7b97\u6cd5\u548c\u53c2\u6570\u914d\u7f6e\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\u3002\u7814\u7a76\u4e3aRL-based LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u53c2\u6570\u914d\u7f6e\u6307\u5bfc\uff0c\u7279\u522b\u662f\u5173\u4e8e\u7ec4\u5927\u5c0f\u4f18\u5316\u548c\u52a8\u6001\u91c7\u6837\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u8bc4\u4f30\u3002"}}
{"id": "2512.06924", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06924", "abs": "https://arxiv.org/abs/2512.06924", "authors": ["Milad Alshomary", "Anisha Bhatnagar", "Peter Zeng", "Smaranda Muresan", "Owen Rambow", "Kathleen McKeown"], "title": "XAM: Interactive Explainability for Authorship Attribution Models", "comment": null, "summary": "We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.", "AI": {"tldr": "IXAM\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u7d22\u4f5c\u8005\u5f52\u5c5e\u6a21\u578b\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u591a\u7c92\u5ea6\u5199\u4f5c\u98ce\u683c\u7279\u5f81\u89e3\u91ca\u6a21\u578b\u9884\u6d4b", "motivation": "\u73b0\u6709\u7684\u4f5c\u8005\u5f52\u5c5e\u6a21\u578b\u7f3a\u4e4f\u4ea4\u4e92\u5f0f\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff0c\u7528\u6237\u96be\u4ee5\u7406\u89e3\u6a21\u578b\u5982\u4f55\u57fa\u4e8e\u5199\u4f5c\u98ce\u683c\u505a\u51fa\u9884\u6d4b\u51b3\u7b56", "method": "\u5f00\u53d1\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u5141\u8bb8\u7528\u6237\u63a2\u7d22\u6a21\u578b\u5d4c\u5165\u7a7a\u95f4\uff0c\u6784\u5efa\u591a\u7c92\u5ea6\u5199\u4f5c\u98ce\u683c\u7279\u5f81\u4f5c\u4e3a\u89e3\u91ca\uff0c\u5e76\u4e0e\u9884\u5b9a\u4e49\u98ce\u683c\u89e3\u91ca\u8fdb\u884c\u5bf9\u6bd4", "result": "\u901a\u8fc7\u7528\u6237\u8bc4\u4f30\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u9884\u5b9a\u4e49\u98ce\u683c\u89e3\u91ca\u5177\u6709\u66f4\u9ad8\u4ef7\u503c\uff0c\u80fd\u66f4\u597d\u5730\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u6a21\u578b\u9884\u6d4b\u673a\u5236", "conclusion": "IXAM\u6846\u67b6\u4e3a\u4f5c\u8005\u5f52\u5c5e\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4ea4\u4e92\u5f0f\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u7528\u6237\u5bf9\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u7684\u7406\u89e3\u548c\u4fe1\u4efb"}}
{"id": "2512.06255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06255", "abs": "https://arxiv.org/abs/2512.06255", "authors": ["Shijie Wang", "Xin Yu", "Yadan Luo", "Zijian Wang", "Pengfei Zhang", "Zi Huang"], "title": "Language-driven Fine-grained Retrieval", "comment": null, "summary": "Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer", "AI": {"tldr": "LaFG\u662f\u4e00\u4e2a\u8bed\u8a00\u9a71\u52a8\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u7c7b\u522b\u540d\u79f0\u8f6c\u6362\u4e3a\u5c5e\u6027\u7ea7\u76d1\u7763\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758fone-hot\u6807\u7b7e\u3001\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u7c7b\u522b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8e\u7c7b\u522b\u540d\u79f0\u7684\u7a00\u758fone-hot\u6807\u7b7e\u4f5c\u4e3a\u76d1\u7763\uff0c\u867d\u7136\u5bf9\u5df2\u89c1\u7c7b\u522b\u6709\u6548\uff0c\u4f46\u5ffd\u7565\u4e86\u7c7b\u522b\u540d\u79f0\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u963b\u788d\u4e86\u8de8\u7c7b\u522b\u7ec6\u8282\u53ef\u6bd4\u6027\u7684\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "LaFG\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u7c7b\u522b\u540d\u79f0\u8f6c\u6362\u4e3a\u8be6\u7ec6\u7684\u5c5e\u6027\u5bfc\u5411\u63cf\u8ff0\uff1b2\uff09\u5229\u7528\u51bb\u7ed3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u8fd9\u4e9b\u63cf\u8ff0\u6295\u5f71\u5230\u89c6\u89c9\u5bf9\u9f50\u7a7a\u95f4\uff0c\u805a\u7c7b\u5f62\u6210\u6570\u636e\u96c6\u8303\u56f4\u7684\u5c5e\u6027\u8bcd\u6c47\u8868\uff1b3\uff09\u901a\u8fc7\u5168\u5c40\u63d0\u793a\u6a21\u677f\u9009\u62e9\u7c7b\u522b\u76f8\u5173\u5c5e\u6027\uff0c\u805a\u5408\u6210\u7c7b\u522b\u7279\u5b9a\u7684\u8bed\u8a00\u539f\u578b\uff1b4\uff09\u7528\u8fd9\u4e9b\u539f\u578b\u76d1\u7763\u68c0\u7d22\u6a21\u578b\u3002", "result": "\u8bba\u6587\u672a\u5728\u6458\u8981\u4e2d\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u7684\u5c5e\u6027\u7ea7\u76d1\u7763\uff0c\u63d0\u5347\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u6a21\u578b\u5bf9\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LaFG\u901a\u8fc7\u5c06\u7c7b\u522b\u540d\u79f0\u8f6c\u6362\u4e3a\u4e30\u5bcc\u7684\u5c5e\u6027\u7ea7\u76d1\u7763\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u6316\u6398\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u6709\u671b\u63d0\u5347\u6a21\u578b\u5bf9\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2512.06938", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06938", "abs": "https://arxiv.org/abs/2512.06938", "authors": ["Ivanho\u00e9 Botcazou", "Tassadit Amghar", "Sylvain Lamprier", "Fr\u00e9d\u00e9ric Saubion"], "title": "Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation", "comment": null, "summary": "Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faProgress Ratio Embeddings(PRE)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u7684\u4e09\u89d2\u51fd\u6570\u4fe1\u53f7\u5b9e\u73b0\u7a33\u5b9a\u7684\u6587\u672c\u751f\u6210\u957f\u5ea6\u63a7\u5236\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u65f6\u8868\u73b0\u66f4\u7a33\u5065\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u5bf9\u751f\u6210\u957f\u5ea6\u7684\u7cbe\u786e\u63a7\u5236\u4ecd\u7136\u4e0d\u8db3\u3002\u73b0\u6709\u57fa\u4e8e\u53cd\u5411\u4f4d\u7f6e\u5d4c\u5165(RPE)\u7684\u65b9\u6cd5\u5728\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u65f6\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faProgress Ratio Embeddings(PRE)\u65b9\u6cd5\uff0c\u4f7f\u7528\u8fde\u7eed\u7684\u4e09\u89d2\u51fd\u6570\"\u4e0d\u8010\u70e6\u4fe1\u53f7\"\u5d4c\u5165\uff0c\u4e0e\u6807\u51c6Transformer\u67b6\u6784\u65e0\u7f1d\u96c6\u6210\uff0c\u63d0\u4f9b\u7a33\u5b9a\u7684\u957f\u5ea6\u63a7\u5236\u800c\u4e0d\u964d\u4f4e\u6587\u672c\u8d28\u91cf\u3002", "result": "PRE\u65b9\u6cd5\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u65b0\u95fb\u6458\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u591f\u7a33\u5b9a\u63a7\u5236\u751f\u6210\u957f\u5ea6\uff0c\u4e14\u5728\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u4e0b\u4e0d\u964d\u4f4e\u6587\u672c\u51c6\u786e\u6027\uff0c\u8fd8\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u957f\u5ea6\u3002", "conclusion": "PRE\u65b9\u6cd5\u4e3a\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u957f\u5ea6\u63a7\u5236\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u65f6\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2512.06258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06258", "abs": "https://arxiv.org/abs/2512.06258", "authors": ["Chaoyang Wang", "Yangfan He", "Yiyang Zhou", "Yixuan Wang", "Jiaqi Liu", "Peng Xia", "Zhengzhong Tu", "Mohit Bansal", "Huaxiu Yao"], "title": "Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs", "comment": null, "summary": "We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8def\u5f84\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff1a\u5373\u4f7f\u77e5\u9053\u6b63\u786e\u7b54\u6848\uff0c\u4e5f\u5e38\u901a\u8fc7\u9519\u8bef\u63a8\u7406\u8def\u5f84\u5f97\u51fa\u7ed3\u679c\u3002\u4f5c\u8005\u63d0\u51fa\u4e86PSO\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u5f84\u9009\u62e9\u4f18\u5316\u63d0\u5347\u63a8\u7406\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u7f3a\u9677\uff1a\u5373\u4f7f\u6a21\u578b\u77e5\u9053\u6b63\u786e\u7b54\u6848\uff0c\u4e5f\u7ecf\u5e38\u901a\u8fc7\u9519\u8bef\u7684\u63a8\u7406\u8def\u5f84\u5230\u8fbe\u90a3\u91cc\u3002\u6838\u5fc3\u95ee\u9898\u4e0d\u662f\u7f3a\u4e4f\u77e5\u8bc6\uff0c\u800c\u662f\u5728\u5e7f\u9614\u7684\u63a8\u7406\u641c\u7d22\u7a7a\u95f4\u4e2d\u7684\u8def\u5f84\u9009\u62e9\u504f\u5dee\u3002\u6a21\u578b\u867d\u7136\u80fd\u591f\u91c7\u6837\u6b63\u786e\u7684\u89e3\u51b3\u65b9\u6848\u8f68\u8ff9\uff0c\u4f46\u4e0d\u6210\u6bd4\u4f8b\u5730\u503e\u5411\u4e8e\u4e0d\u7a33\u5b9a\u6216\u903b\u8f91\u4e0d\u4e00\u81f4\u7684\u8def\u5f84\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u7a33\u5b9a\u548c\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u4e86PSO\uff08\u8def\u5f84\u9009\u62e9\u4f18\u5316\uff09\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6846\u67b6\u3002\u7b2c\u4e00\u9636\u6bb5\uff1a\u4f7f\u7528\u5e26\u6709\u6a21\u677f\u548c\u7b54\u6848\u5956\u52b1\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u6765\u57f9\u517b\u7ed3\u6784\u5316\u3001\u9010\u6b65\u63a8\u7406\u3002\u7b2c\u4e8c\u9636\u6bb5\uff1a\u8fdb\u884c\u5728\u7ebf\u504f\u597d\u4f18\u5316\uff0c\u6a21\u578b\u4eceGRPO\u751f\u6210\u7684\u6570\u636e\u4e2d\u91c7\u6837\u63a8\u7406\u8def\u5f84\uff0c\u81ea\u6211\u8bc4\u4f30\u5b83\u4eec\uff0c\u5e76\u5411\u9996\u9009\u8f68\u8ff9\u5bf9\u9f50\u3002\u9519\u8bef\u6216\u6b21\u4f18\u8def\u5f84\u540c\u65f6\u5b58\u50a8\u5728\u8d1f\u5411\u56de\u653e\u8bb0\u5fc6\u4e2d\u4f5c\u4e3a\u786c\u8d1f\u6837\u672c\uff0c\u5b9a\u671f\u91cd\u65b0\u8bbf\u95ee\u4ee5\u9632\u6b62\u6a21\u578b\u91cd\u590d\u5148\u524d\u9519\u8bef\u5e76\u4fc3\u8fdb\u6301\u7eed\u63a8\u7406\u6539\u8fdb\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPSO\u6709\u6548\u4fee\u526a\u65e0\u6548\u63a8\u7406\u8def\u5f84\uff0c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\uff08\u5e73\u5747\u63d0\u53477.4%\uff09\uff0c\u5e76\u4ea7\u751f\u66f4\u7a33\u5b9a\u548c\u4e00\u81f4\u7684\u601d\u7ef4\u94fe\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86LVLMs\u4e2d\u8def\u5f84\u9009\u62e9\u504f\u5dee\u7684\u5173\u952e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684PSO\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\uff0c\u8fd8\u589e\u5f3a\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u548c\u4e00\u81f4\u6027\uff0c\u4e3a\u6539\u8fdb\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.07710", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07710", "abs": "https://arxiv.org/abs/2512.07710", "authors": ["Anxiang Zeng", "Haibo Zhang", "Hailing Zhang", "Kaixiang Mo", "Liang Yao", "Ling Hu", "Long Zhang", "Shuman Liu", "Shuyi Xie", "Yanshi Li", "Yizhang Chen", "Yuepeng Sheng", "Yuwei Huang", "Zhaochen Xu", "Zhiqiang Zhou", "Ziqin Liew"], "title": "Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE", "comment": null, "summary": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.", "AI": {"tldr": "\u63d0\u51fa\u4e86CompassMax-V3-Thinking\uff0c\u4e00\u4e2a\u5343\u4ebf\u89c4\u6a21\u7684MoE\u63a8\u7406\u6a21\u578b\uff0c\u91c7\u7528\u57fa\u4e8e\"\u6bcf\u4e2a\u63d0\u793a\u90fd\u5fc5\u987b\u91cd\u8981\"\u539f\u5219\u7684\u65b0RL\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21RL\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5c06RL\u6269\u5c55\u5230\u5343\u4ebf\u89c4\u6a21\u65f6\u66b4\u9732\u51fa\u5173\u952e\u6548\u7387\u95ee\u9898\uff1a\u96f6\u65b9\u5dee\u63d0\u793a\u6d6a\u8d39rollout\u8d44\u6e90\u3001\u957f\u65f6\u57df\u91cd\u8981\u6027\u91c7\u6837\u4e0d\u7a33\u5b9a\u3001\u6807\u51c6\u5956\u52b1\u6a21\u578b\u5bfc\u81f4\u4f18\u52bf\u53cd\u8f6c\uff0c\u4ee5\u53carollout\u5904\u7406\u7684\u7cfb\u7edf\u6027\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u56db\u9879\u7edf\u4e00\u521b\u65b0\uff1a1) \u591a\u9636\u6bb5\u96f6\u65b9\u5dee\u6d88\u9664\uff0c\u8fc7\u6ee4\u975e\u4fe1\u606f\u6027\u63d0\u793a\u5e76\u7a33\u5b9a\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\uff1b2) ESPO\u71b5\u81ea\u9002\u5e94\u4f18\u5316\u65b9\u6cd5\uff0c\u5e73\u8861token\u7ea7\u548c\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u91c7\u6837\uff1b3) \u8def\u7531\u5668\u91cd\u653e\u7b56\u7565\uff0c\u5bf9\u9f50\u8bad\u7ec3\u65f6MoE\u8def\u7531\u5668\u51b3\u7b56\u4e0e\u63a8\u7406\u65f6\u884c\u4e3a\uff1b4) \u9ad8\u541e\u5410RL\u7cfb\u7edf\uff0c\u91c7\u7528FP8\u7cbe\u5ea6rollout\u3001\u91cd\u53e0\u5956\u52b1\u8ba1\u7b97\u548c\u957f\u5ea6\u611f\u77e5\u8c03\u5ea6\u3002", "result": "\u8fd9\u4e9b\u8d21\u732e\u5f62\u6210\u4e86\u7edf\u4e00\u7684\u7ba1\u9053\uff0c\u4f7f\u5343\u4ebf\u89c4\u6a21MoE\u6a21\u578b\u7684RL\u8bad\u7ec3\u53d8\u5f97\u7a33\u5b9a\u9ad8\u6548\u3002\u6700\u7ec8\u6a21\u578b\u5728\u5185\u90e8\u548c\u516c\u5f00\u8bc4\u4f30\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u901a\u8fc7\u89e3\u51b3\u5927\u89c4\u6a21RL\u8bad\u7ec3\u4e2d\u7684\u5173\u952e\u6548\u7387\u95ee\u9898\uff0c\u6210\u529f\u6784\u5efa\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u5343\u4ebf\u89c4\u6a21MoE\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06991", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06991", "abs": "https://arxiv.org/abs/2512.06991", "authors": ["Jing Jie Tan", "Ban-Hoe Kwan", "Danny Wee-Kiat Ng", "Yan-Chai Hum", "Anissa Mokraoui", "Shih-Yu Lo"], "title": "Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models", "comment": "16 pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel \"Prompting-in-a-Series\" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \\textit{gpt4o} from OpenAI and \\textit{gemini} from Google, along with open-source models like \\textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.", "AI": {"tldr": "PICEPR\u662f\u4e00\u79cd\u57fa\u4e8e\u5fc3\u7406\u5b66\u5185\u5bb9\u5d4c\u5165\u7684\u4eba\u683c\u8bc6\u522b\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\"\u7cfb\u5217\u63d0\u793a\"\u65b9\u6cd5\u7ed3\u5408\u5185\u5bb9\u548c\u5d4c\u5165\u4e24\u6761\u7ba1\u9053\uff0c\u5229\u7528\u6a21\u5757\u5316\u89e3\u7801\u5668LLM\u8fdb\u884c\u5185\u5bb9\u751f\u6210\u548c\u4eba\u683c\u7279\u5f81\u63d0\u53d6\uff0c\u5728\u4eba\u683c\u8bc6\u522b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e865-15%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4eba\u683c\u8bc6\u522b\u9886\u57df\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9\u4eba\u683c\u8bc6\u522b\u7684\u7b97\u6cd5\uff0c\u7ed3\u5408\u5fc3\u7406\u5b66\u77e5\u8bc6\u548cLLM\u7684\u80fd\u529b\uff0c\u63d0\u5347\u4eba\u683c\u7279\u5f81\u63d0\u53d6\u548c\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faPICEPR\u7b97\u6cd5\uff0c\u91c7\u7528\"Prompting-in-a-Series\"\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u6761\u7ba1\u9053\uff1a(a)\u5185\u5bb9\u7ba1\u9053\uff1a\u5229\u7528\u6a21\u5757\u5316\u89e3\u7801\u5668LLM\u603b\u7ed3\u6216\u751f\u6210\u5185\u5bb9\uff1b(b)\u5d4c\u5165\u7ba1\u9053\uff1a\u4f5c\u4e3a\u4eba\u683c\u7279\u5f81\u63d0\u53d6\u5668\u548c\u4eba\u683c\u4e30\u5bcc\u5185\u5bb9\u751f\u6210\u5668\u3002\u540c\u65f6\u5bf9\u6bd4\u4e86\u95ed\u6e90\u6a21\u578b(gpt4o\u3001gemini)\u548c\u5f00\u6e90\u6a21\u578b(mistral)\u7684\u751f\u6210\u8d28\u91cf\u3002", "result": "PICEPR\u7b97\u6cd5\u5728\u4eba\u683c\u8bc6\u522b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u67095-15%\u7684\u6539\u8fdb\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u5408\u7406\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u3002", "conclusion": "PICEPR\u7b97\u6cd5\u6210\u529f\u5730\u5c06\u5fc3\u7406\u5b66\u77e5\u8bc6\u4e0eLLM\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u521b\u65b0\u7684\"\u7cfb\u5217\u63d0\u793a\"\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4eba\u683c\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4e3aLLM\u5728\u4eba\u683c\u5206\u6790\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u5de5\u5177\u3002"}}
{"id": "2512.06269", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06269", "abs": "https://arxiv.org/abs/2512.06269", "authors": ["Quan Tran", "Tuan Dang"], "title": "TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting", "comment": "10 pages", "summary": "3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in \"floater\" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7ea6\u675f\u591a\u89c6\u89d2\u4e09\u89d2\u6d4b\u91cf\u589e\u5f3a\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u4ec5\u4f9d\u8d56\u5149\u5ea6\u635f\u5931\u5bfc\u81f4\u7684\u51e0\u4f55\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5728DTU\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.50mm\u7684\u5e73\u5747\u5012\u89d2\u8ddd\u79bb\uff0c\u4f18\u4e8e\u540c\u7c7b\u663e\u5f0f\u65b9\u6cd5\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u867d\u7136\u80fd\u5b9e\u65f6\u5408\u6210\u65b0\u89c6\u89d2\u4e14\u6e32\u67d3\u903c\u771f\uff0c\u4f46\u4ec5\u4f9d\u8d56\u5149\u5ea6\u635f\u5931\u6784\u5efa3D\u9ad8\u65af\u4f1a\u5bfc\u81f4\u91cd\u5efa\u4e0d\u4e00\u81f4\uff0c\u4ea7\u751f\"\u6f02\u6d6e\u7269\"\u4f2a\u5f71\u548c\u975e\u7ed3\u6784\u5316\u51e0\u4f55\uff0c\u963b\u788d\u9ad8\u8d28\u91cf\u8868\u9762\u63d0\u53d6\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u7ea6\u675f\u591a\u89c6\u89d2\u4e09\u89d2\u6d4b\u91cf\u589e\u5f3a\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u4e2a\u4f30\u8ba1\u89c6\u89d2\u8fbe\u6210\u7269\u7406\u4e16\u754c3D\u8868\u793a\u7684\u5171\u8bc6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\u4ece\u76f8\u90bb\u89c6\u89d2\u675f\u91cd\u65b0\u4e09\u89d2\u5316\u5f97\u5230\u9c81\u68d2\u5171\u8bc6\u70b9\uff0c\u5e76\u60e9\u7f5a\u6e32\u67d33D\u70b9\u4e0e\u5171\u8bc6\u70b9\u7684\u504f\u5dee\u6765\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u5728DTU\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747\u5012\u89d2\u8ddd\u79bb\u8fbe\u52300.50mm\uff0c\u4f18\u4e8e\u540c\u7c7b\u663e\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u589e\u5f3a\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7684\u51e0\u4f55\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u4ee5\u4fbf\u793e\u533a\u9a8c\u8bc1\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2512.07761", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07761", "abs": "https://arxiv.org/abs/2512.07761", "authors": ["Xiqiao Xiong", "Ouxiang Li", "Zhuo Liu", "Moxin Li", "Wentao Shi", "Fuli Feng", "Xiangnan He"], "title": "RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models", "comment": "19 pages, 15 figures", "summary": "Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6700\u7ec8\u8f93\u51fa\u7684\u6709\u5bb3\u6027\u4f5c\u4e3a\u5956\u52b1\uff0c\u5e76\u5f15\u5165\u8fc7\u7a0b\u5956\u52b1\u6765\u63d0\u5347\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u8f6e\u4f18\u5316\uff0c\u4e0d\u8db3\u4ee5\u5b66\u4e60\u957f\u671f\u653b\u51fb\u7b56\u7565\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u6709\u6548\u653b\u51fb\u9ed1\u76d2\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u95ee\u9898\u6784\u5efa\u4e3a\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u76f4\u63a5\u4f18\u5316\u6700\u7ec8\u8f93\u51fa\u7684\u6709\u5bb3\u6027\u4f5c\u4e3a\u7ed3\u679c\u5956\u52b1\u3002\u63d0\u51fa\u4e24\u79cd\u542f\u53d1\u5f0f\u8fc7\u7a0b\u5956\u52b1\uff1a1) \u63a7\u5236\u4e2d\u95f4\u8f93\u51fa\u7684\u6709\u5bb3\u6027\u4ee5\u907f\u514d\u89e6\u53d1\u9ed1\u76d2\u6a21\u578b\u7684\u62d2\u7edd\u673a\u5236\uff1b2) \u4fdd\u6301\u4e2d\u95f4\u8f93\u51fa\u7684\u8bed\u4e49\u76f8\u5173\u6027\u4ee5\u907f\u514d\u504f\u79bb\u4e3b\u9898\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u6301\u7eed\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u591a\u8f6e\u4f18\u5316\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5bf9\u9ed1\u76d2\u8bed\u8a00\u6a21\u578b\u7684\u8d8a\u72f1\u653b\u51fb\u6210\u529f\u7387\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2512.07015", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.07015", "abs": "https://arxiv.org/abs/2512.07015", "authors": ["Mayank Ravishankara"], "title": "FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to \"hallucinate with citations.\"\n  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing \"Self-Correction\" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates \"Kill Queries\"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this \"Anti-Context.\" Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time \"Red Team\" for factual generation.", "AI": {"tldr": "FVA-RAG\u6846\u67b6\u901a\u8fc7\u4ece\u5f52\u7eb3\u9a8c\u8bc1\u8f6c\u5411\u6f14\u7ece\u8bc1\u4f2a\uff0c\u4f7f\u7528\u5bf9\u6297\u6027\u68c0\u7d22\u7b56\u7565\u751f\u6210\"\u6740\u6b7b\u67e5\u8be2\"\u6765\u5bfb\u627e\u77db\u76fe\u8bc1\u636e\uff0c\u6709\u6548\u51cf\u5c11\u68c0\u7d22\u5949\u627f\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u5b58\u5728\"\u68c0\u7d22\u5949\u627f\"\u6f0f\u6d1e\uff1a\u5f53\u67e5\u8be2\u57fa\u4e8e\u9519\u8bef\u524d\u63d0\u6216\u5e38\u89c1\u8bef\u89e3\u65f6\uff0c\u5411\u91cf\u68c0\u7d22\u5668\u503e\u5411\u4e8e\u83b7\u53d6\u7b26\u5408\u7528\u6237\u504f\u89c1\u7684\u6587\u6863\u800c\u975e\u5ba2\u89c2\u4e8b\u5b9e\uff0c\u5bfc\u81f4\u6a21\u578b\"\u5e26\u5f15\u7528\u7684\u5e7b\u89c9\"\u3002", "method": "\u63d0\u51faFVA-RAG\u6846\u67b6\uff0c\u5c06\u68c0\u7d22\u8303\u5f0f\u4ece\u5f52\u7eb3\u9a8c\u8bc1\uff08\u5bfb\u6c42\u652f\u6301\uff09\u8f6c\u5411\u6f14\u7ece\u8bc1\u4f2a\uff08\u5bfb\u6c42\u53cd\u9a73\uff09\u3002\u90e8\u7f72\u72ec\u7acb\u7684\u5bf9\u6297\u6027\u68c0\u7d22\u7b56\u7565\uff0c\u4e3b\u52a8\u751f\u6210\"\u6740\u6b7b\u67e5\u8be2\"\u6765\u5bfb\u627e\u77db\u76fe\u8bc1\u636e\uff0c\u5e76\u5f15\u5165\u53cc\u91cd\u9a8c\u8bc1\u673a\u5236\uff0c\u660e\u786e\u6743\u8861\u8349\u7a3f\u7b54\u6848\u4e0e\"\u53cd\u4e0a\u4e0b\u6587\"\u3002", "result": "\u5728\u5e38\u89c1\u8bef\u89e3\u6570\u636e\u96c6\u4e0a\u7684\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0cFVA-RAG\u76f8\u6bd4\u6807\u51c6RAG\u57fa\u7ebf\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u5949\u627f\u5e7b\u89c9\u7684\u9c81\u68d2\u6027\uff0c\u6709\u6548\u5145\u5f53\u4e8b\u5b9e\u751f\u6210\u7684\u63a8\u7406\u65f6\"\u7ea2\u961f\"\u3002", "conclusion": "FVA-RAG\u901a\u8fc7\u8bc1\u4f2a\u5bfc\u5411\u7684\u68c0\u7d22\u548c\u5bf9\u6297\u6027\u9a8c\u8bc1\uff0c\u4e3a\u51cf\u5c11RAG\u7cfb\u7edf\u4e2d\u7684\u68c0\u7d22\u5949\u627f\u548c\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06275", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06275", "abs": "https://arxiv.org/abs/2512.06275", "authors": ["Kegang Wang", "Jiankai Tang", "Yuntao Wang", "Xin Liu", "Yuxuan Fan", "Jiatong Ji", "Yuanchun Shi", "Daniel McDuff"], "title": "FacePhys: State of the Heart Learning", "comment": null, "summary": "Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \\emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\\% to 99\\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.", "AI": {"tldr": "FacePhys\u662f\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u72b6\u6001\u7a7a\u95f4\u5bf9\u5076\u6027\u7684\u5185\u5b58\u9ad8\u6548\u8fdc\u7a0b\u5149\u7535\u5bb9\u79ef\u63cf\u8bb0\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u6a21\u578b\u53ef\u6269\u5c55\u6027\u3001\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u548c\u5b9e\u65f6\u64cd\u4f5c\u7684\u4e09\u96be\u95ee\u9898\uff0c\u5b9e\u73b0\u4e8649%\u7684\u9519\u8bef\u7387\u964d\u4f4e\u548c\u5b9e\u65f6\u63a8\u7406\u3002", "motivation": "\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u751f\u547d\u4f53\u5f81\u6d4b\u91cf\u6280\u672f\uff08\u7279\u522b\u662f\u8fdc\u7a0b\u5149\u7535\u5bb9\u79ef\u63cf\u8bb0\uff09\u4e3a\u8212\u9002\u3001\u666e\u9002\u7684\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u53d7\u5230\u524d\u7aef\u8bbe\u5907\u8ba1\u7b97\u9650\u5236\u548c\u6570\u636e\u538b\u7f29\u4f20\u8f93\u5bfc\u81f4\u4fe1\u53f7\u8d28\u91cf\u4e0b\u964d\u7684\u5236\u7ea6\u3002", "method": "\u63d0\u51faFacePhys\u7b97\u6cd5\uff0c\u57fa\u4e8e\u65f6\u7a7a\u72b6\u6001\u7a7a\u95f4\u5bf9\u5076\u6027\uff0c\u5229\u7528\u53ef\u8f6c\u79fb\u7684\u5fc3\u810f\u72b6\u6001\u6355\u6349\u89c6\u9891\u5e27\u95f4\u7684\u7ec6\u5fae\u5468\u671f\u6027\u53d8\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u652f\u6301\u957f\u89c6\u9891\u5e8f\u5217\u8bad\u7ec3\u548c\u4f4e\u5ef6\u8fdf\u63a8\u7406\u3002", "result": "FacePhys\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9519\u8bef\u7387\u964d\u4f4e49%\uff0c\u5185\u5b58\u5360\u7528\u4ec53.6MB\uff0c\u6bcf\u5e27\u5ef6\u8fdf9.46ms\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534783%\u523099%\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u63d0\u4f9b\u53ef\u9760\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "FacePhys\u901a\u8fc7\u521b\u65b0\u7684\u65f6\u7a7a\u72b6\u6001\u7a7a\u95f4\u5bf9\u5076\u6027\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86rPPG\u6280\u672f\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u8ba1\u7b97\u9650\u5236\u548c\u7cbe\u5ea6\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u5065\u5eb7\u76d1\u6d4b\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07795", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07795", "abs": "https://arxiv.org/abs/2512.07795", "authors": ["Nearchos Potamitis", "Lars Klein", "Akhil Arora"], "title": "ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning", "comment": "11 pages, 3 tables, 4 figures", "summary": "Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .", "AI": {"tldr": "ReasonBENCH\uff1a\u9996\u4e2a\u91cf\u5316LLM\u63a8\u7406\u4e0d\u7a33\u5b9a\u6027\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u8f6e\u8bc4\u4f30\u534f\u8bae\u63d0\u4f9b\u7edf\u8ba1\u53ef\u9760\u7684\u6027\u80fd\u4e0e\u6210\u672c\u6307\u6807\uff0c\u63ed\u793a\u5f53\u524d\u63a8\u7406\u65b9\u6cd5\u666e\u904d\u5b58\u5728\u9ad8\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u8bc4\u4f30\u4e3b\u8981\u62a5\u544a\u5355\u6b21\u8fd0\u884c\u51c6\u786e\u7387\uff0c\u5ffd\u7565\u4e86\u968f\u673a\u89e3\u7801\u5e26\u6765\u7684\u5185\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u65e0\u6cd5\u53ef\u9760\u8bc4\u4f30\u65b9\u6cd5\u6027\u80fd\u7684\u7a33\u5b9a\u6027\u3001\u53ef\u590d\u73b0\u6027\u548c\u6210\u672c\u4e00\u81f4\u6027\u3002", "method": "\u5f00\u53d1ReasonBENCH\u57fa\u51c6\uff0c\u5305\u542b\uff1a(1)\u6807\u51c6\u5316\u63a8\u7406\u6846\u67b6\u3001\u6a21\u578b\u548c\u4efb\u52a1\u7684\u6a21\u5757\u5316\u8bc4\u4f30\u5e93\uff1b(2)\u62a5\u544a\u8d28\u91cf\u548c\u6210\u672c\u7edf\u8ba1\u53ef\u9760\u6307\u6807\u7684\u591a\u8f6e\u8bc4\u4f30\u534f\u8bae\uff1b(3)\u9f13\u52b1\u65b9\u5dee\u611f\u77e5\u62a5\u544a\u7684\u516c\u5f00\u6392\u884c\u699c\u3002", "result": "\u8de8\u591a\u4e2a\u9886\u57df\u4efb\u52a1\u53d1\u73b0\uff0c\u7edd\u5927\u591a\u6570\u63a8\u7406\u7b56\u7565\u548c\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u4e0d\u7a33\u5b9a\u6027\u3002\u5373\u4f7f\u5e73\u5747\u6027\u80fd\u76f8\u4f3c\u7684\u7b56\u7565\uff0c\u7f6e\u4fe1\u533a\u95f4\u5bbd\u5ea6\u53ef\u8fbe\u56db\u500d\u5dee\u5f02\uff1b\u6027\u80fd\u6700\u4f73\u7684\u65b9\u6cd5\u901a\u5e38\u6210\u672c\u66f4\u9ad8\u4e14\u66f4\u4e0d\u7a33\u5b9a\u3002", "conclusion": "\u63a8\u7406\u4e0d\u7a33\u5b9a\u6027\u635f\u5bb3\u4e86\u8de8\u8fd0\u884c\u7684\u53ef\u590d\u73b0\u6027\u548c\u62a5\u544a\u6027\u80fd\u7684\u53ef\u9760\u6027\u3002\u53ef\u590d\u73b0\u6027\u662f\u53ef\u9760LLM\u63a8\u7406\u7684\u5173\u952e\u7ef4\u5ea6\uff0cReasonBENCH\u4e3a\u672a\u6765\u63a8\u7406\u65b9\u6cd5\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.07059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07059", "abs": "https://arxiv.org/abs/2512.07059", "authors": ["Richard Young"], "title": "Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models", "comment": "30 pages, 11 figures, 5 tables. Code and data: https://github.com/ricyoung/tempest-replication", "summary": "Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.", "AI": {"tldr": "\u4f7f\u7528TEMPEST\u591a\u8f6e\u653b\u51fb\u6846\u67b6\u8bc4\u4f3010\u4e2a\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u5f53\u524d\u5bf9\u9f50\u6280\u672f\u5bf9\u81ea\u9002\u5e94\u591a\u8f6e\u653b\u51fb\u5b58\u5728\u6839\u672c\u6027\u8106\u5f31\u6027\uff0c\u6a21\u578b\u89c4\u6a21\u4e0d\u80fd\u9884\u6d4b\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u800c\u601d\u7ef4\u6a21\u5f0f\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u5b89\u5168\u589e\u5f3a\u624b\u6bb5\u3002", "motivation": "\u5c3d\u7ba1\u5728\u5b89\u5168\u5bf9\u9f50\u65b9\u9762\u6295\u5165\u4e86\u5927\u91cf\u8d44\u6e90\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u590d\u6742\u591a\u8f6e\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\u4ecd\u7136\u7f3a\u4e4f\u5145\u5206\u7814\u7a76\uff0c\u4e14\u4e0d\u6e05\u695a\u6a21\u578b\u89c4\u6a21\u6216\u63a8\u7406\u6a21\u5f0f\u662f\u5426\u4f1a\u5f71\u54cd\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528TEMPEST\u591a\u8f6e\u653b\u51fb\u6846\u67b6\u8bc4\u4f30\u6765\u81ea8\u4e2a\u4f9b\u5e94\u5546\u768410\u4e2a\u524d\u6cbf\u6a21\u578b\uff0c\u9488\u5bf91000\u79cd\u6709\u5bb3\u884c\u4e3a\u751f\u6210\u8d85\u8fc797,000\u4e2aAPI\u67e5\u8be2\uff0c\u901a\u8fc7\u72ec\u7acb\u5b89\u5168\u5206\u7c7b\u5668\u8fdb\u884c\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\u8106\u5f31\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1a6\u4e2a\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387(ASR)\u8fbe\u523096%-100%\uff0c4\u4e2a\u6a21\u578b\u8868\u73b0\u51fa\u6709\u610f\u4e49\u7684\u62b5\u6297\u80fd\u529b(ASR 42%-78%)\uff1b\u5728\u76f8\u540c\u67b6\u6784\u4e0a\u542f\u7528\u6269\u5c55\u63a8\u7406\u53ef\u5c06ASR\u4ece97%\u964d\u81f342%\u3002", "conclusion": "\u5f53\u524d\u5bf9\u9f50\u6280\u672f\u5bf9\u81ea\u9002\u5e94\u591a\u8f6e\u653b\u51fb\u5b58\u5728\u6839\u672c\u6027\u8106\u5f31\u6027\uff0c\u6a21\u578b\u89c4\u6a21\u4e0d\u80fd\u9884\u6d4b\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u800c\u5ba1\u614e\u63a8\u7406\uff08\u601d\u7ef4\u6a21\u5f0f\uff09\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u9632\u5fa1\u65b9\u5411\u3002"}}
{"id": "2512.06276", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06276", "abs": "https://arxiv.org/abs/2512.06276", "authors": ["Tianyi Gao", "Hao Li", "Han Fang", "Xin Wei", "Xiaodong Dong", "Hongbo Sun", "Ye Yuan", "Zhongjiang He", "Jinglin Xu", "Jingmin Xin", "Hao Sun"], "title": "RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension", "comment": null, "summary": "Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.", "AI": {"tldr": "RefBench-PRO\uff1a\u4e00\u4e2a\u5c06\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u5206\u89e3\u4e3a\u611f\u77e5\u548c\u63a8\u7406\u4e24\u4e2a\u7ef4\u5ea6\u3001\u516d\u4e2a\u5b50\u4efb\u52a1\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5305\u542b\u81ea\u52a8\u6570\u636e\u751f\u6210\u6d41\u7a0b\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684Ref-R1\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709REC\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u611f\u77e5\u80fd\u529b\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7684\u8bc4\u5206\u673a\u5236\uff0c\u65e0\u6cd5\u63ed\u793a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u8ba4\u77e5\u80fd\u529b\u4e0a\u7684\u5b9a\u4f4d\u80fd\u529b\u3002\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u5168\u9762\u8bc4\u4f30MLLM\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4e2d\u7684\u8868\u73b0\u3002", "method": "1. \u63d0\u51faRefBench-PRO\u57fa\u51c6\uff0c\u5c06\u6307\u4ee3\u8868\u8fbe\u5206\u89e3\u4e3a\u611f\u77e5\u548c\u63a8\u7406\u4e24\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff0c\u8fdb\u4e00\u6b65\u7ec6\u5206\u4e3a\u516d\u4e2a\u6e10\u8fdb\u6311\u6218\u6027\u4efb\u52a1\uff1a\u5c5e\u6027\u3001\u4f4d\u7f6e\u3001\u4ea4\u4e92\u3001\u5e38\u8bc6\u3001\u5173\u7cfb\u548c\u62d2\u7edd\u30022. \u5f00\u53d1\u5168\u81ea\u52a8\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u4e3a\u516d\u4e2a\u5b50\u7ef4\u5ea6\u751f\u6210\u591a\u6837\u5316\u7684\u6307\u4ee3\u8868\u8fbe\u30023. \u63d0\u51faRef-R1\u5b66\u4e60\u65b9\u6848\uff0c\u91c7\u7528\u57fa\u4e8e\u52a8\u6001IoU\u7684GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u63d0\u9ad8\u590d\u6742\u63a8\u7406\u6761\u4ef6\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRefBench-PRO\u80fd\u591f\u5bf9MLLM\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4e0a\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\uff0c\u5728\u611f\u77e5\u548c\u63a8\u7406\u65b9\u9762\u90fd\u63d0\u51fa\u4e86\u66f4\u5927\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u4e3aREC\u5efa\u7acb\u4e86\u66f4\u5f3a\u7684\u57fa\u7ebf\u3002", "conclusion": "RefBench-PRO\u662f\u4e00\u4e2a\u5168\u9762\u7684REC\u57fa\u51c6\uff0c\u901a\u8fc7\u5206\u89e3\u6307\u4ee3\u8868\u8fbe\u7684\u8ba4\u77e5\u7ef4\u5ea6\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30MLLM\u7684\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u80fd\u529b\u3002\u63d0\u51fa\u7684Ref-R1\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u6761\u4ef6\u4e0b\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e3aREC\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u3002"}}
{"id": "2512.07796", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07796", "abs": "https://arxiv.org/abs/2512.07796", "authors": ["Sridhar Mahadevan"], "title": "Large Causal Models from Large Language Models", "comment": "29 pages", "summary": "We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.", "AI": {"tldr": "DEMOCRITUS\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u5927\u89c4\u6a21\u56e0\u679c\u6a21\u578b\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u63d0\u53d6\u591a\u9886\u57df\u6587\u672c\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5c06\u5176\u6574\u5408\u6210\u7edf\u4e00\u7684\u56e0\u679c\u77e5\u8bc6\u56fe\u8c31\u3002", "motivation": "\u4f20\u7edf\u56e0\u679c\u63a8\u7406\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u548c\u5047\u8bbe\u9a71\u52a8\uff0c\u4f9d\u8d56\u5b9e\u9a8c\u4ea7\u751f\u7684\u6570\u503c\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4ece\u5e7f\u6cdb\u6587\u672c\u4e2d\u63d0\u53d6\u591a\u9886\u57df\u56e0\u679c\u77e5\u8bc6\uff0c\u6784\u5efa\u8de8\u8d8a\u4e0d\u540c\u9886\u57df\u7684\u5927\u89c4\u6a21\u56e0\u679c\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u9ad8\u8d28\u91cfLLM\u63d0\u51fa\u4e3b\u9898\u3001\u751f\u6210\u56e0\u679c\u95ee\u9898\u3001\u4ece\u591a\u9886\u57df\u6587\u672c\u4e2d\u63d0\u53d6\u56e0\u679c\u9648\u8ff0\u3002\u901a\u8fc7\u516d\u4e2a\u6a21\u5757\u7684\u6d41\u6c34\u7ebf\u5c06\u788e\u7247\u5316\u3001\u53ef\u80fd\u51b2\u7a81\u7684\u56e0\u679c\u4e3b\u5f20\u8f6c\u5316\u4e3a\u5173\u7cfb\u56e0\u679c\u4e09\u5143\u7ec4\uff0c\u5e76\u5d4c\u5165\u5927\u89c4\u6a21\u56e0\u679c\u6a21\u578b\u4e2d\u3002\u91c7\u7528\u65b0\u7684\u8303\u7574\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6574\u5408\u8fd9\u4e9b\u56e0\u679c\u77e5\u8bc6\u3002", "result": "DEMOCRITUS\u7cfb\u7edf\u5df2\u5728\u8003\u53e4\u5b66\u3001\u751f\u7269\u5b66\u3001\u6c14\u5019\u53d8\u5316\u3001\u7ecf\u6d4e\u5b66\u3001\u533b\u5b66\u548c\u6280\u672f\u7b49\u591a\u4e2a\u9886\u57df\u6210\u529f\u5e94\u7528\uff0c\u80fd\u591f\u6784\u5efa\u8de8\u8d8a\u4e0d\u540c\u9886\u57df\u7684\u5927\u89c4\u6a21\u56e0\u679c\u6a21\u578b\u3002\u5206\u6790\u4e86\u7cfb\u7edf\u7684\u8ba1\u7b97\u6210\u672c\u5206\u5e03\uff0c\u786e\u5b9a\u4e86\u6269\u5c55\u5230\u66f4\u5927\u6a21\u578b\u7684\u5f53\u524d\u74f6\u9888\u3002", "conclusion": "DEMOCRITUS\u5c55\u793a\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u5927\u89c4\u6a21\u56e0\u679c\u6a21\u578b\u7684\u65b0\u8303\u5f0f\u6f5c\u529b\uff0c\u80fd\u591f\u6574\u5408\u591a\u9886\u57df\u56e0\u679c\u77e5\u8bc6\u3002\u7cfb\u7edf\u76ee\u524d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u4e3a\u6269\u5c55\u5176\u80fd\u529b\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.07075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07075", "abs": "https://arxiv.org/abs/2512.07075", "authors": ["Shiwei Guo", "Sihang Jiang", "Qianxi He", "Yanghua Xiao", "Jiaqing Liang", "Bi Yude", "Minggui He", "Shimin Tao", "Li Zhang"], "title": "Do Large Language Models Truly Understand Cross-cultural Differences?", "comment": null, "summary": "In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u57fa\u4e8e\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8de8\u6587\u5316\u6838\u5fc3\u6982\u5ff5\u5bf9\u9f50\u548c\u751f\u6210\u5f0f\u4efb\u52a1\u8bbe\u8ba1\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6587\u5316\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5305\u542b4530\u4e2a\u6d4b\u8bd5\u9879\uff0c\u8986\u76d69\u4e2a\u7ef4\u5ea6\u548c15\u4e2a\u73b0\u5b9e\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30LLMs\u8de8\u6587\u5316\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u573a\u666f\u3001\u8de8\u6587\u5316\u6982\u5ff5\u6620\u5c04\u4e0d\u8db3\u3001\u6df1\u5c42\u6587\u5316\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u5168\u9762\u8bc4\u4f30LLMs\u662f\u5426\u771f\u6b63\u5177\u5907\u8de8\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u6587\u5316\u7406\u8bba\u5c06\u8de8\u6587\u5316\u80fd\u529b\u5206\u4e3a9\u4e2a\u7ef4\u5ea6\uff0c\u7b5b\u9009210\u4e2a\u6838\u5fc3\u6982\u5ff5\uff0c\u572815\u4e2a\u5177\u4f53\u73b0\u5b9e\u573a\u666f\u4e2d\u6784\u5efa4530\u4e2a\u6d4b\u8bd5\u9879\uff0c\u9075\u5faa\u65e2\u5b9a\u7684\u9879\u76ee\u8bbe\u8ba1\u539f\u5219\uff0c\u652f\u6301\u6570\u636e\u96c6\u6301\u7eed\u6269\u5c55\u3002", "result": "SAGE\u6570\u636e\u96c6\u8bc1\u5b9e\u53ef\u8fc1\u79fb\u5230\u5176\u4ed6\u8bed\u8a00\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u7ef4\u5ea6\u548c\u573a\u666f\u4e0a\u7684\u5f31\u70b9\uff0c\u66b4\u9732\u4e86\u8de8\u6587\u5316\u63a8\u7406\u7684\u7cfb\u7edf\u6027\u9650\u5236\u3002LLMs\u8ddd\u79bb\u771f\u6b63\u7684\u7ec6\u81f4\u8de8\u6587\u5316\u7406\u89e3\u4ecd\u6709\u5dee\u8ddd\u3002", "conclusion": "\u867d\u7136\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u8981\u8fbe\u5230\u771f\u6b63\u7ec6\u81f4\u7684\u8de8\u6587\u5316\u7406\u89e3\u4ecd\u6709\u8ddd\u79bb\u3002SAGE\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347LLMs\u7684\u8de8\u6587\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\u3002"}}
{"id": "2512.06282", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.06282", "abs": "https://arxiv.org/abs/2512.06282", "authors": ["Lyn Chao-ling Chen", "Kuan-Wen Chen", "Yi-Ping Hung"], "title": "A Sleep Monitoring System Based on Audio, Video and Depth Information", "comment": "Accepted in the Computer Vision, Graphics and Image Processing (CVGIP 2013)", "summary": "For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u4e8b\u4ef6\u65b9\u6cd5\u7684\u975e\u4fb5\u5165\u5f0f\u7761\u7720\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ea2\u5916\u6df1\u5ea6\u4f20\u611f\u5668\u3001RGB\u6444\u50cf\u5934\u548c\u56db\u9ea6\u514b\u98ce\u9635\u5217\u68c0\u6d4b\u8fd0\u52a8\u3001\u5f00\u5173\u706f\u548c\u566a\u97f3\u4e09\u7c7b\u4e8b\u4ef6\uff0c\u7528\u4e8e\u5bb6\u5ead\u73af\u5883\u4e0b\u7684\u7761\u7720\u969c\u788d\u5b9a\u91cf\u8bc4\u4f30\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u975e\u4fb5\u5165\u5f0f\u7684\u5b9a\u91cf\u8bc4\u4f30\u7761\u7720\u969c\u788d\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u76d1\u6d4b\u7761\u7720\u5e72\u6270\u4e8b\u4ef6\uff0c\u4e3a\u7761\u7720\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u5ba2\u89c2\u6570\u636e\u3002", "method": "\u4f7f\u7528\u7ea2\u5916\u6df1\u5ea6\u4f20\u611f\u5668\u3001RGB\u6444\u50cf\u5934\u548c\u56db\u9ea6\u514b\u98ce\u9635\u5217\u8bbe\u5907\uff0c\u5728\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u76d1\u6d4b\u7761\u7720\u3002\u5efa\u7acb\u6df1\u5ea6\u4fe1\u53f7\u7684\u80cc\u666f\u6a21\u578b\u68c0\u6d4b\u8fd0\u52a8\u5e45\u5ea6\uff0c\u5efa\u7acb\u5f69\u8272\u56fe\u50cf\u7684\u80cc\u666f\u6a21\u578b\u68c0\u6d4b\u5149\u7167\u53d8\u5316\uff0c\u91c7\u7528\u4e8b\u4ef6\u68c0\u6d4b\u7b97\u6cd5\u4ece\u4e09\u7c7b\u4f20\u611f\u5668\u5904\u7406\u6570\u636e\u4e2d\u68c0\u6d4b\u4e8b\u4ef6\u53d1\u751f\u3002", "result": "\u7cfb\u7edf\u5728\u7761\u7720\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u8fd0\u52a8\u3001\u5149\u7167\u53d8\u5316\u548c\u566a\u97f3\u4e09\u7c7b\u7761\u7720\u5e72\u6270\u4e8b\u4ef6\u3002", "conclusion": "\u57fa\u4e8e\u4e8b\u4ef6\u7684\u591a\u4f20\u611f\u5668\u975e\u4fb5\u5165\u5f0f\u7761\u7720\u76d1\u6d4b\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u91cf\u5316\u8bc4\u4f30\u7761\u7720\u969c\u788d\uff0c\u4e3a\u5bb6\u5ead\u73af\u5883\u4e0b\u7684\u7761\u7720\u8d28\u91cf\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2512.07090", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07090", "abs": "https://arxiv.org/abs/2512.07090", "authors": ["Jungmin Lee", "Gwangeun Byeon", "Yulhwa Kim", "Seokin Hong"], "title": "Leveraging KV Similarity for Online Structured Pruning in LLMs", "comment": null, "summary": "Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.", "AI": {"tldr": "Token Filtering\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5728\u7ebf\u7ed3\u6784\u5316\u526a\u679d\u6280\u672f\uff0c\u901a\u8fc7\u6d4b\u91cftoken\u5197\u4f59\u5ea6\u5e76\u8df3\u8fc7\u5197\u4f59\u6ce8\u610f\u529b\u8ba1\u7b97\u6765\u52a0\u901fLLM\u63a8\u7406\uff0c\u65e0\u9700\u6821\u51c6\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u4f9d\u8d56\u79bb\u7ebf\u6821\u51c6\u6570\u636e\uff0c\u5728\u4e0d\u540c\u8f93\u5165\u95f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6821\u51c6\u6570\u636e\u3001\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u505a\u51fa\u526a\u679d\u51b3\u7b56\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faToken Filtering\u6280\u672f\uff1a1) \u901a\u8fc7\u8054\u5408\u952e\u503c\u76f8\u4f3c\u5ea6\u6d4b\u91cftoken\u5197\u4f59\u5ea6\uff1b2) \u8bbe\u8ba1\u65b9\u5dee\u611f\u77e5\u878d\u5408\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u52a0\u6743\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u7684\u952e\u503c\u76f8\u4f3c\u5ea6\uff1b3) \u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8df3\u8fc7\u5197\u4f59\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728LLaMA-2 (7B/13B)\u3001LLaMA-3 (8B)\u548cMistral (7B)\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cToken Filtering\u5728\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u4e0a\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u5728MMLU\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u5373\u4f7f\u526a\u679d50%\u4ecd\u4fdd\u6301\u5f3a\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u3002", "conclusion": "Token Filtering\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6821\u51c6\u6570\u636e\u7684\u7a33\u5b9a\u5728\u7ebf\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cftoken\u5197\u4f59\u5ea6\u548c\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2512.07132", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07132", "abs": "https://arxiv.org/abs/2512.07132", "authors": ["Nithin Sivakumaran", "Justin Chih-Yao Chen", "David Wan", "Yue Zhang", "Jaehong Yoon", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning", "comment": "Code: https://github.com/nsivaku/dart", "summary": "Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.", "AI": {"tldr": "DART\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u8fa9\u8bba\u5206\u6b67\u6765\u8bc6\u522b\u6709\u7528\u7684\u89c6\u89c9\u5de5\u5177\uff0c\u5229\u7528\u5de5\u5177\u4fe1\u606f\u4fc3\u8fdb\u8ba8\u8bba\u5e76\u9009\u62e9\u6700\u4f73\u7b54\u6848\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e13\u4e1a\u89c6\u89c9\u5de5\u5177\u53ef\u4ee5\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u6216\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e13\u5bb6\u77e5\u8bc6\uff0c\u4f46\u786e\u5b9a\u4f55\u65f6\u8c03\u7528\u54ea\u4e9b\u5de5\u5177\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5de5\u5177\u8c03\u7528\u65f6\u673a\u548c\u9009\u62e9\u4e0a\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51faDART\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1\uff09\u5229\u7528\u591a\u4e2a\u8fa9\u8bba\u89c6\u89c9\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u5206\u6b67\u8bc6\u522b\u6709\u7528\u7684\u89c6\u89c9\u5de5\u5177\uff1b2\uff09\u5de5\u5177\u5f15\u5165\u65b0\u4fe1\u606f\u5e76\u63d0\u4f9b\u5de5\u5177\u5bf9\u9f50\u7684\u540c\u610f\u5206\u6570\uff1b3\uff09\u4f7f\u7528\u805a\u5408\u667a\u80fd\u4f53\u57fa\u4e8e\u667a\u80fd\u4f53\u8f93\u51fa\u548c\u5de5\u5177\u4fe1\u606f\u9009\u62e9\u6700\u4f73\u7b54\u6848\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDART\u4f18\u4e8e\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u548c\u5355\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\u6846\u67b6\uff1a\u5728A-OKVQA\u4e0a\u6bd4\u6b21\u4f18\u57fa\u7ebf\uff08\u5e26\u6cd5\u5b98\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff09\u63d0\u53473.4%\uff0c\u5728MMMU\u4e0a\u63d0\u53472.4%\u3002\u5728M3D\u533b\u7597\u6570\u636e\u96c6\u4e0a\u6bd4\u5176\u4ed6\u57fa\u7ebf\u63d0\u53471.3%\u3002\u6587\u672c\u91cd\u53e0\u5ea6\u5206\u6790\u663e\u793aDART\u8ba8\u8bba\u66f4\u4e30\u5bcc\uff0c\u5de5\u5177\u8c03\u7528\u5206\u5e03\u663e\u793a\u591a\u6837\u5316\u5de5\u5177\u88ab\u53ef\u9760\u4f7f\u7528\u3002", "conclusion": "DART\u901a\u8fc7\u5229\u7528\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u5206\u6b67\u6765\u6307\u5bfc\u5de5\u5177\u8c03\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u5de5\u5177\u9009\u62e9\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u80fd\u826f\u597d\u9002\u5e94\u65b0\u5de5\u5177\u3002"}}
{"id": "2512.07134", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07134", "abs": "https://arxiv.org/abs/2512.07134", "authors": ["Lauren Levine", "Amir Zeldes"], "title": "GUMBridge: a Corpus for Varieties of Bridging Anaphora", "comment": null, "summary": "Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in \"There is 'a house'. 'The door' is red,\" where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.", "AI": {"tldr": "GUMBridge\u662f\u4e00\u4e2a\u65b0\u7684\u82f1\u8bed\u6865\u63a5\u6307\u4ee3\u8d44\u6e90\uff0c\u6db5\u76d616\u79cd\u4e0d\u540c\u6587\u4f53\uff0c\u63d0\u4f9b\u6865\u63a5\u73b0\u8c61\u5e7f\u6cdb\u8986\u76d6\u548c\u7ec6\u7c92\u5ea6\u5b50\u7c7b\u578b\u5206\u7c7b\u6807\u6ce8\uff0c\u8bc4\u4f30\u663e\u793a\u6865\u63a5\u6d88\u89e3\u548c\u5b50\u7c7b\u578b\u5206\u7c7b\u5bf9\u5f53\u4ee3LLMs\u4ecd\u662f\u6311\u6218\u6027\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u82f1\u8bed\u6865\u63a5\u6307\u4ee3\u8d44\u6e90\u5927\u591a\u89c4\u6a21\u5c0f\u3001\u73b0\u8c61\u8986\u76d6\u6709\u9650\u3001\u6587\u4f53\u8986\u76d6\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5168\u9762\u3001\u591a\u6837\u5316\u7684\u6865\u63a5\u6307\u4ee3\u6807\u6ce8\u8d44\u6e90\u6765\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u3002", "method": "\u6784\u5efaGUMBridge\u8d44\u6e90\uff0c\u5305\u542b16\u79cd\u4e0d\u540c\u6587\u4f53\u7684\u82f1\u8bed\u6587\u672c\uff0c\u63d0\u4f9b\u6865\u63a5\u73b0\u8c61\u7684\u5e7f\u6cdb\u8986\u76d6\u548c\u7ec6\u7c92\u5ea6\u5b50\u7c7b\u578b\u5206\u7c7b\u6807\u6ce8\uff0c\u5e76\u8fdb\u884c\u6807\u6ce8\u8d28\u91cf\u8bc4\u4f30\u3002", "result": "\u521b\u5efa\u4e86GUMBridge\u8d44\u6e90\uff0c\u6db5\u76d616\u79cd\u6587\u4f53\uff0c\u63d0\u4f9b\u4e86\u6865\u63a5\u73b0\u8c61\u7684\u5168\u9762\u8986\u76d6\u548c\u8be6\u7ec6\u5b50\u7c7b\u578b\u5206\u7c7b\u3002\u4f7f\u7528\u5f00\u6e90\u548c\u95ed\u6e90LLMs\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u57fa\u7ebf\u6027\u80fd\u8bc4\u4f30\u663e\u793a\uff0c\u6865\u63a5\u6d88\u89e3\u548c\u5b50\u7c7b\u578b\u5206\u7c7b\u5bf9\u5f53\u4ee3LLMs\u4ecd\u662f\u56f0\u96be\u4efb\u52a1\u3002", "conclusion": "GUMBridge\u4e3a\u6865\u63a5\u6307\u4ee3\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u591a\u6837\u5316\u7684\u8d44\u6e90\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8d44\u6e90\u7684\u4e0d\u8db3\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u6865\u63a5\u6d88\u89e3\u548c\u5206\u7c7b\u4efb\u52a1\u5bf9\u5f53\u524dLLMs\u7684\u6311\u6218\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2512.06328", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06328", "abs": "https://arxiv.org/abs/2512.06328", "authors": ["Jiahao Li", "Yusheng Luo", "Yunzhong Lou", "Xiangdong Zhou"], "title": "ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models", "comment": "Accepted as an Oral presentation at AAAI 2026", "summary": "We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.", "AI": {"tldr": "ReCAD\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5bfc\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u751f\u6210\u7cbe\u786e\u7684\u53c2\u6570\u5316CAD\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230CAD\u548c\u56fe\u50cf\u5230CAD\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\u6ce8\u5165\u77e5\u8bc6\uff0c\u5bf9\u53ef\u7f16\u8f91\u6027\u652f\u6301\u6709\u9650\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u5f3a\u5927\u751f\u6210\u5148\u9a8c\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u5229\u7528\u5927\u6a21\u578b\u5185\u5728\u751f\u6210\u80fd\u529b\u3001\u652f\u6301\u590d\u6742CAD\u64cd\u4f5c\u5e76\u786e\u4fdd\u51e0\u4f55\u7cbe\u5ea6\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u7684\u6846\u67b6\u3002", "method": "1. \u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u5177\u5907\u57fa\u672cCAD\u6a21\u578b\u751f\u6210\u80fd\u529b\uff0c\u5c06CAD\u811a\u672c\u91cd\u5199\u4e3a\u53c2\u6570\u5316\u4ee3\u7801\u7528\u4e8e\u76d1\u7763\u751f\u6210\u51c6\u786e\u6587\u672c\u63cf\u8ff0\uff1b2. \u63d0\u51fa\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u53c2\u6570\u5316\u4ee3\u7801\u4e3a\u6307\u5bfc\u589e\u5f3a\u6a21\u578b\u5728\u6311\u6218\u6027\u95ee\u9898\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff1b3. \u91c7\u7528\u5206\u5c42\u57fa\u5143\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5728\u7edf\u4e00\u5956\u52b1\u51fd\u6570\u4e0b\u9010\u6b65\u6559\u6388\u7ed3\u6784\u5316\u7ec4\u5408\u6280\u80fd\u3002", "result": "ReCAD\u5728\u6587\u672c\u5230CAD\u548c\u56fe\u50cf\u5230CAD\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51e0\u4f55\u7cbe\u5ea6\u3002\u5728\u56fe\u50cf\u5230CAD\u4efb\u52a1\u4e2d\uff0c\u5c06\u5e73\u5747Chamfer\u8ddd\u79bb\u4ece73.47\u964d\u81f329.61\uff08\u5206\u5e03\u5185\uff09\u548c\u4ece272.06\u964d\u81f380.23\uff08\u5206\u5e03\u5916\uff09\uff0c\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ReCAD\u6846\u67b6\u6210\u529f\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u751f\u6210\u7cbe\u786e\u7684\u53c2\u6570\u5316CAD\u6a21\u578b\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u4ee3\u7801\u6307\u5bfc\u548c\u5206\u5c42\u57fa\u5143\u5b66\u4e60\u5b9e\u73b0\u4e86\u51e0\u4f55\u7cbe\u5ea6\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u7684\u53cc\u91cd\u63d0\u5347\uff0c\u4e3a\u591a\u6a21\u6001\u8f93\u5165\u5230CAD\u6a21\u578b\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07195", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.07195", "abs": "https://arxiv.org/abs/2512.07195", "authors": ["Xuan Zhang", "Wenxuan Zhang", "Anxu Wang", "See-Kiong Ng", "Yang Deng"], "title": "MASim: Multilingual Agent-Based Simulation for Social Science", "comment": null, "summary": "Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.", "AI": {"tldr": "MASim\u662f\u9996\u4e2a\u652f\u6301\u591a\u8bed\u8a00\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u8de8\u8bed\u8a00\u793e\u4f1a\u884c\u4e3a\uff0c\u5305\u542b\u516c\u5171\u8206\u8bba\u5efa\u6a21\u548c\u5a92\u4f53\u5f71\u54cd\u529b\u5206\u6790\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\u6a21\u62df\u5927\u591a\u662f\u5355\u8bed\u8a00\u7684\uff0c\u65e0\u6cd5\u6a21\u62df\u73b0\u5b9e\u793e\u4f1a\u4e2d\u91cd\u8981\u7684\u8de8\u8bed\u8a00\u4e92\u52a8\uff0c\u9650\u5236\u4e86\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u7684\u7814\u7a76\u8303\u56f4\u3002", "method": "\u5f00\u53d1\u4e86MASim\u591a\u8bed\u8a00\u667a\u80fd\u4f53\u6a21\u62df\u6846\u67b6\uff0c\u652f\u6301\u5177\u6709\u4e0d\u540c\u793e\u4f1a\u8bed\u8a00\u7279\u5f81\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u8fdb\u884c\u591a\u8f6e\u4ea4\u4e92\u3002\u6784\u5efa\u4e86MAPS\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408\u5168\u7403\u4eba\u53e3\u5206\u5e03\u7684\u8c03\u67e5\u95ee\u9898\u548c\u4eba\u53e3\u7edf\u8ba1\u89d2\u8272\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMASim\u80fd\u591f\u590d\u73b0\u793e\u4f1a\u6587\u5316\u73b0\u8c61\uff0c\u6821\u51c6\u3001\u654f\u611f\u6027\u3001\u4e00\u81f4\u6027\u548c\u6587\u5316\u6848\u4f8b\u7814\u7a76\u8868\u660e\u8be5\u6846\u67b6\u6709\u6548\uff0c\u7a81\u51fa\u4e86\u591a\u8bed\u8a00\u6a21\u62df\u5bf9\u4e8e\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u7684\u91cd\u8981\u6027\u3002", "conclusion": "MASim\u586b\u8865\u4e86\u591a\u8bed\u8a00\u667a\u80fd\u4f53\u6a21\u62df\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u8de8\u8bed\u8a00\u793e\u4f1a\u4e92\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.06330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06330", "abs": "https://arxiv.org/abs/2512.06330", "authors": ["Haoyu Zhang", "Junhan Luo", "Yugang Cao", "Siran Peng", "Jie Huang", "Liangjian-Deng"], "title": "S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening", "comment": null, "summary": "Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.", "AI": {"tldr": "S2WMamba\uff1a\u4e00\u79cd\u901a\u8fc72D/1D\u5c0f\u6ce2\u53d8\u6362\u663e\u5f0f\u89e3\u8026\u9891\u7387\u4fe1\u606f\uff0c\u7ed3\u5408Mamba\u8de8\u6a21\u6001\u4ea4\u4e92\u7684\u8f7b\u91cf\u7ea7\u5168\u8272\u9510\u5316\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u5168\u8272\u9510\u5316\u4e2d\u8054\u5408\u5904\u7406PAN\u548cMS\u56fe\u50cf\u65f6\uff0c\u7a7a\u95f4\u7ec6\u8282\u4e0e\u5149\u8c31\u4fdd\u771f\u5ea6\u5bb9\u6613\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u663e\u5f0f\u89e3\u8026\u9891\u7387\u4fe1\u606f\u5e76\u8fdb\u884c\u6709\u6548\u8de8\u6a21\u6001\u4ea4\u4e92\u7684\u65b9\u6cd5\u3002", "method": "1) \u5bf9PAN\u56fe\u50cf\u5e94\u75282D Haar DWT\u5b9a\u4f4d\u7a7a\u95f4\u8fb9\u7f18\u548c\u7eb9\u7406\uff1b2) \u5bf9\u6bcf\u4e2a\u50cf\u7d20\u7684\u5149\u8c31\u5e94\u7528\u901a\u9053\u7ea71D Haar DWT\u5206\u79bb\u4f4e\u9891/\u9ad8\u9891\u5206\u91cf\u4ee5\u9650\u5236\u5149\u8c31\u5931\u771f\uff1b3) \u6784\u5efa\u5149\u8c31\u5206\u652f\uff08\u6ce8\u5165\u5c0f\u6ce2\u63d0\u53d6\u7684\u7a7a\u95f4\u7ec6\u8282\u5230MS\u7279\u5f81\uff09\u548c\u7a7a\u95f4\u5206\u652f\uff08\u4f7f\u75281D\u91d1\u5b57\u5854\u7684\u5149\u8c31\u7ec6\u5316PAN\u7279\u5f81\uff09\uff1b4) \u901a\u8fc7\u57fa\u4e8eMamba\u7684\u8de8\u8c03\u5236\u5b9e\u73b0\u5206\u652f\u95f4\u4fe1\u606f\u4ea4\u6362\uff0c\u5efa\u6a21\u957f\u7a0b\u4f9d\u8d56\u4e14\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\uff1b5) \u4f7f\u7528\u591a\u5c3a\u5ea6\u52a8\u6001\u95e8\uff08\u4e58\u6cd5+\u52a0\u6cd5\uff09\u81ea\u9002\u5e94\u878d\u5408\u5206\u652f\u8f93\u51fa\u3002", "result": "\u5728WV3\u3001GF2\u548cQB\u6570\u636e\u96c6\u4e0a\uff0cS2WMamba\u5339\u914d\u6216\u8d85\u8d8a\u4e86FusionMamba\u3001CANNet\u3001U2Net\u3001ARConv\u7b49\u5f3a\u57fa\u7ebf\uff0cPSNR\u63d0\u5347\u6700\u9ad8\u8fbe0.23 dB\uff0c\u5728WV3\u5168\u5206\u8fa8\u7387\u4e0a\u8fbe\u5230HQNR 0.956\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e862D/1D DWT\u5e03\u5c40\u3001\u5e76\u884c\u53cc\u5206\u652f\u548c\u878d\u5408\u95e8\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "S2WMamba\u901a\u8fc7\u663e\u5f0f\u9891\u7387\u89e3\u8026\u548c\u8f7b\u91cf\u7ea7\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5168\u8272\u9510\u5316\u4e2d\u7a7a\u95f4\u7ec6\u8282\u4e0e\u5149\u8c31\u4fdd\u771f\u5ea6\u7684\u7ea0\u7f20\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2512.07218", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07218", "abs": "https://arxiv.org/abs/2512.07218", "authors": ["Feng Liang", "Weixin Zeng", "Runhao Zhao", "Xiang Zhao"], "title": "NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models", "comment": "Accepted by AAAI 2026", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.", "AI": {"tldr": "NeSTR\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u65f6\u5e8f\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7b26\u53f7\u8868\u793a\u548c\u6df7\u5408\u53cd\u601d\u63a8\u7406\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u5e8f\u654f\u611f\u6027\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u5347\u65f6\u5e8f\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u5e8f\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5904\u7406\u590d\u6742\u65f6\u5e8f\u7ea6\u675f\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7b26\u53f7\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8981\u4e48\u53cd\u601d\u65b9\u6cd5\u7f3a\u4e4f\u7ed3\u6784\u5316\u65f6\u5e8f\u8868\u793a\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6216\u5e7b\u89c9\u63a8\u7406\u3002\u5373\u4f7f\u6709\u65f6\u5e8f\u4e0a\u4e0b\u6587\u53ef\u7528\uff0cLLM\u4ecd\u53ef\u80fd\u8bef\u89e3\u6216\u8bef\u7528\u65f6\u5e8f\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u65f6\u5e8f\u63a8\u7406\uff08NeSTR\uff09\u6846\u67b6\uff0c\u96c6\u6210\u7ed3\u6784\u5316\u7b26\u53f7\u8868\u793a\u4e0e\u6df7\u5408\u53cd\u601d\u63a8\u7406\u3002\u901a\u8fc7\u7b26\u53f7\u7f16\u7801\u4fdd\u7559\u663e\u5f0f\u65f6\u5e8f\u5173\u7cfb\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5f3a\u5236\u6267\u884c\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u4f7f\u7528\u6eaf\u56e0\u53cd\u601d\u7ea0\u6b63\u9519\u8bef\u63a8\u7406\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u65f6\u5e8f\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeSTR\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\u5c31\u80fd\u6301\u7eed\u6539\u8fdb\u65f6\u5e8f\u63a8\u7406\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u5728\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u5e8f\u7406\u89e3\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "NeSTR\u6846\u67b6\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u65f6\u5e8f\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u5e8f\u654f\u611f\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.07246", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07246", "abs": "https://arxiv.org/abs/2512.07246", "authors": ["Mengqi Wang", "Jianwei Wang", "Qing Liu", "Xiwei Xu", "Zhenchang Xing", "Liming Zhu", "Wenjie Zhang"], "title": "Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection", "comment": "14 pages, 8 figures", "summary": "Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.", "AI": {"tldr": "\u63d0\u51faTreeED\u548cForestED\u6846\u67b6\uff0c\u4f7f\u7528LLM\u8bf1\u5bfc\u51b3\u7b56\u6811\u8fdb\u884c\u9519\u8bef\u68c0\u6d4b\uff0c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u5e73\u5747F1\u5206\u6570\u63d0\u9ad816.1%", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4f9d\u8d56\u9ed1\u76d2\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff1b2) \u5bf9\u63d0\u793a\u8bcd\u654f\u611f\uff0c\u8f93\u51fa\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002\u9700\u8981\u65e2\u80fd\u5229\u7528LLM\u77e5\u8bc6\u53c8\u80fd\u63d0\u4f9b\u900f\u660e\u51b3\u7b56\u8fc7\u7a0b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLLM-as-an-inducer\u6846\u67b6\uff1a1) TreeED\u4f7f\u7528LLM\u6839\u636e\u6570\u636e\u4e0a\u4e0b\u6587\u3001\u51b3\u7b56\u6811\u89c4\u8303\u548c\u8f93\u51fa\u8981\u6c42\u8bf1\u5bfc\u51b3\u7b56\u6811\u9aa8\u67b6\uff0c\u5305\u542b\u89c4\u5219\u8282\u70b9\uff08\u7b80\u5355\u9a8c\u8bc1\uff09\u3001GNN\u8282\u70b9\uff08\u590d\u6742\u6a21\u5f0f\uff09\u548c\u53f6\u5b50\u8282\u70b9\uff08\u6700\u7ec8\u51b3\u7b56\uff09\uff1b2) ForestED\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u83b7\u53d6\u591a\u4e2a\u884c\u5b50\u96c6\uff0c\u4e3a\u6bcf\u4e2a\u5b50\u96c6\u6784\u5efa\u51b3\u7b56\u6811\uff0c\u4f7f\u7528\u57fa\u4e8e\u671f\u671b\u6700\u5927\u5316\u7684\u7b97\u6cd5\u8054\u5408\u4f30\u8ba1\u6811\u53ef\u9760\u6027\u548c\u4f18\u5316\u5171\u8bc6\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\uff0c\u5e73\u5747F1\u5206\u6570\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u9ad816.1%\u3002", "conclusion": "\u63d0\u51fa\u7684LLM-as-an-inducer\u6846\u67b6\u901a\u8fc7\u8bf1\u5bfc\u51b3\u7b56\u6811\u89e3\u51b3\u4e86\u73b0\u6709LLM-as-a-labeler\u65b9\u6cd5\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u8868\u683c\u6570\u636e\u9519\u8bef\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06344", "abs": "https://arxiv.org/abs/2512.06344", "authors": ["Kaile Wang", "Lijun He", "Haisheng Fu", "Haixia Bi", "Fan Li"], "title": "Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate", "comment": null, "summary": "Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.", "AI": {"tldr": "MTGC\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u5f15\u5bfc\u589e\u5f3a\u8d85\u4f4e\u7801\u7387\u751f\u6210\u5f0f\u56fe\u50cf\u538b\u7f29\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u751f\u6210\u5e7b\u89c9\u95ee\u9898", "motivation": "\u751f\u6210\u5f0f\u56fe\u50cf\u538b\u7f29\u5728\u8d85\u4f4e\u7801\u7387\u4e0b\u5b58\u5728\u8bed\u4e49\u504f\u5dee\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u57286G\u8bed\u4e49\u901a\u4fe1\u4e2d\u7684\u53ef\u9760\u90e8\u7f72\uff0c\u9700\u8981\u589e\u5f3a\u8bed\u4e49\u4e00\u81f4\u6027", "method": "\u63d0\u51faMTGC\u6846\u67b6\uff0c\u96c6\u6210\u4e09\u79cd\u5f15\u5bfc\u6a21\u6001\uff1a\u6587\u672c\u63cf\u8ff0\u3001\u9ad8\u538b\u7f29\u56fe\u50cf\u548c\u8bed\u4e49\u4f2a\u8bcd\uff1b\u8bbe\u8ba1\u4efb\u52a1\u611f\u77e5\u8bed\u4e49\u538b\u7f29\u6a21\u5757\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u8bed\u4e49\uff1b\u91c7\u7528\u53cc\u8def\u5f84\u534f\u540c\u5f15\u5bfc\u673a\u5236\u7684\u591a\u6a21\u6001\u5f15\u5bfc\u6269\u6563\u89e3\u7801\u5668", "result": "\u5728DIV2K\u6570\u636e\u96c6\u4e0aDISTS\u6307\u6807\u4e0b\u964d10.59%\uff0c\u5728\u8d85\u4f4e\u7801\u7387\u4e0b\u663e\u8457\u63d0\u5347\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u611f\u77e5\u8d28\u91cf\u548c\u50cf\u7d20\u7ea7\u4fdd\u771f\u5ea6", "conclusion": "MTGC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u4f4e\u7801\u7387\u751f\u6210\u5f0f\u56fe\u50cf\u538b\u7f29\u7684\u8bed\u4e49\u504f\u5dee\u95ee\u9898\uff0c\u4e3a6G\u8bed\u4e49\u901a\u4fe1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u56fe\u50cf\u538b\u7f29\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.07265", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.07265", "abs": "https://arxiv.org/abs/2512.07265", "authors": ["Bhavana Akkiraju", "Srihari Bandarupalli", "Swathi Sambangi", "Vasavi Ravuri", "R Vijaya Saraswathi", "Anil Kumar Vuppala"], "title": "TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation", "comment": "Submitted to AACL IJCNLP 2025", "summary": "Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u6cf0\u5362\u56fa\u8bed-\u82f1\u8bed\u8bed\u97f3\u7ffb\u8bd1\u521b\u5efa\u4e86\u9ad8\u8d28\u91cf\u57fa\u51c6\uff0c\u6bd4\u8f83\u4e86\u7ea7\u8054\u4e0e\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u53d1\u73b0\u7aef\u5230\u7aef\u7cfb\u7edf\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6cf0\u5362\u56fa\u8bed\u4f5c\u4e3a\u62e5\u6709\u8d85\u8fc78000\u4e07\u4f7f\u7528\u8005\u7684\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\uff0c\u5176\u8bed\u97f3\u7ffb\u8bd1\u7814\u7a76\u4e25\u91cd\u4e0d\u8db3\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u6cf0\u5362\u56fa\u8bed-\u82f1\u8bed\u8bed\u97f3\u7ffb\u8bd1\u5efa\u7acb\u9ad8\u8d28\u91cf\u57fa\u51c6\u3002", "method": "\u4f7f\u752846\u5c0f\u65f6\u624b\u52a8\u9a8c\u8bc1\u7684CSTD\u8bed\u6599\u5e93\u6570\u636e\uff0830h/8h/8h\u8bad\u7ec3/\u5f00\u53d1/\u6d4b\u8bd5\u5206\u5272\uff09\uff0c\u7cfb\u7edf\u6bd4\u8f83\u7ea7\u8054\u67b6\u6784\uff08IndicWhisper + IndicMT\uff09\u4e0e\u7aef\u5230\u7aef\u67b6\u6784\uff08\u5fae\u8c03SeamlessM4T\u6a21\u578b\uff09\uff0c\u5e76\u8bc4\u4f30BLEU\u3001METEOR\u3001ChrF++\u3001ROUGE-L\u3001TER\u548cBERTScore\u7b49\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7ea7\u8054\u67b6\u6784\u56e0\u4f7f\u7528\u5927\u91cf\u6cf0\u5362\u56fa\u8bed\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u800c\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5fae\u8c03\u7684\u7aef\u5230\u7aef\u6a21\u578b\u4f7f\u7528\u663e\u8457\u66f4\u5c11\u7684\u6cf0\u5362\u56fa\u8bed\u6570\u636e\u4ecd\u8868\u73b0\u51fa\u8272\u3002\u7814\u7a76\u53d1\u73b0\u4f20\u7edf\u6307\u6807\u6bd4BERTScore\u80fd\u66f4\u597d\u5730\u533a\u5206\u6cf0\u5362\u56fa\u8bed-\u82f1\u8bed\u7ffb\u8bd1\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4ed4\u7ec6\u7684\u8d85\u53c2\u6570\u8c03\u6574\u548c\u8db3\u591f\u7684\u5e73\u884c\u6570\u636e\uff08\u53ef\u80fd\u5c11\u4e8e100\u5c0f\u65f6\uff09\uff0c\u7aef\u5230\u7aef\u7cfb\u7edf\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u53ef\u4ee5\u8fbe\u5230\u4e0e\u7ea7\u8054\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u3001\u7aef\u5230\u7aef\u7cfb\u7edf\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7ade\u4e89\u529b\u7684\u5b9e\u8bc1\u8bc1\u636e\uff0c\u4ee5\u53ca\u5f62\u6001\u590d\u6742\u8bed\u8a00\u5bf9\u81ea\u52a8\u8bc4\u4f30\u7684\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2512.06345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06345", "abs": "https://arxiv.org/abs/2512.06345", "authors": ["Xiangshuai Song", "Jun-Jie Huang", "Tianrui Liu", "Ke Liang", "Chang Tang"], "title": "CLUENet: Cluster Attention Makes Neural Networks Have Eyes", "comment": "10 pages, 6 figures, 2026 Association for the Advancement of Artificial Intelligence", "summary": "Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.", "AI": {"tldr": "CLUENet\u662f\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u8303\u5f0f\u7684\u900f\u660e\u89c6\u89c9\u8bed\u4e49\u7406\u89e3\u67b6\u6784\uff0c\u901a\u8fc7\u5168\u5c40\u8f6f\u805a\u5408\u786c\u5206\u914d\u3001\u6e29\u5ea6\u7f29\u653e\u4f59\u5f26\u6ce8\u610f\u529b\u3001\u95e8\u63a7\u6b8b\u5dee\u8fde\u63a5\u3001\u786c\u5171\u4eab\u7279\u5f81\u8c03\u5ea6\u548c\u6539\u8fdb\u7684\u805a\u7c7b\u6c60\u5316\u7b56\u7565\uff0c\u5728\u51c6\u786e\u7387\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u5377\u79ef\u548c\u6ce8\u610f\u529b\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u867d\u7136\u6210\u529f\uff0c\u4f46\u5176\u56fa\u5b9a\u7684\u611f\u53d7\u91ce\u548c\u590d\u6742\u67b6\u6784\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u4e0d\u89c4\u5219\u7a7a\u95f4\u6a21\u5f0f\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u805a\u7c7b\u8303\u5f0f\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u7684\u8bed\u4e49\u5efa\u6a21\uff0c\u4f46\u5b58\u5728\u51c6\u786e\u7387\u6709\u9650\u3001\u6548\u7387\u4f4e\u548c\u8bad\u7ec3\u65f6\u68af\u5ea6\u6d88\u5931\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86CLUENet\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a(1) \u5168\u5c40\u8f6f\u805a\u5408\u548c\u786c\u5206\u914d\uff0c\u7ed3\u5408\u6e29\u5ea6\u7f29\u653e\u4f59\u5f26\u6ce8\u610f\u529b\u548c\u95e8\u63a7\u6b8b\u5dee\u8fde\u63a5\u4ee5\u589e\u5f3a\u5c40\u90e8\u5efa\u6a21\uff1b(2) \u5757\u95f4\u786c\u5171\u4eab\u7279\u5f81\u8c03\u5ea6\uff1b(3) \u6539\u8fdb\u7684\u805a\u7c7b\u6c60\u5316\u7b56\u7565\u3002", "result": "\u5728CIFAR-100\u548cMini-ImageNet\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCLUENet\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u805a\u7c7b\u65b9\u6cd5\u548c\u4e3b\u6d41\u89c6\u89c9\u6a21\u578b\uff0c\u5728\u5206\u7c7b\u6027\u80fd\u548c\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CLUENet\u4e3a\u89c6\u89c9\u8bed\u4e49\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u6df1\u5ea6\u67b6\u6784\uff0c\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u900f\u660e\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u5e73\u8861\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.07277", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.07277", "abs": "https://arxiv.org/abs/2512.07277", "authors": ["Srihari Bandarupalli", "Bhavana Akkiraju", "Charan Devarakonda", "Vamsiraghusimha Narsinga", "Anil Kumar Vuppala"], "title": "Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data", "comment": "Accepted in AACL IJCNLP 2025", "summary": "Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.", "AI": {"tldr": "\u901a\u8fc7\u8de8\u8bed\u8a00\u6301\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u7528\u65e0\u6807\u6ce8\u8bed\u97f3\u6570\u636e\u548c\u5f62\u6001\u611f\u77e5\u5206\u8bcd\uff0c\u4e3a\u4f4e\u8d44\u6e90\u6ce2\u65af-\u963f\u62c9\u4f2f\u8bed\u7cfb\u8bed\u8a00\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff0c\u5728\u53c2\u6570\u91cf\u51cf\u5c115\u500d\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\u3002", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u8ba1\u7b97\u8d44\u6e90\u4e0d\u8db3\u7684\u53cc\u91cd\u9650\u5236\uff0c\u73b0\u6709\u5927\u6a21\u578b\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u8bed\u8a00\u73af\u5883\u3002", "method": "\u6784\u5efa3000\u5c0f\u65f6\u591a\u8bed\u8a00\u65e0\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u91c7\u7528\u8de8\u8bed\u8a00\u6301\u7eed\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u5f62\u6001\u611f\u77e5\u5206\u8bcd\u6280\u672f\uff0c\u5f00\u53d1\u4e86300M\u53c2\u6570\u7684\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u6ce2\u65af\u8bed\u4e0a\u8d85\u8d8aWhisper Large v3\uff081.5B\u53c2\u6570\uff09\uff0c\u5728\u963f\u62c9\u4f2f\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u53c2\u6570\u91cf\u51cf\u5c115\u500d\u4e14\u6807\u6ce8\u6570\u636e\u9700\u6c42\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "ASR\u8d28\u91cf\u5e76\u975e\u4e3b\u8981\u4f9d\u8d56\u6a21\u578b\u89c4\u6a21\uff0c\u6570\u636e\u76f8\u5173\u6027\u548c\u7b56\u7565\u6027\u9884\u8bad\u7ec3\u5bf9\u4f4e\u8d44\u6e90\u573a\u666f\u66f4\u4e3a\u5173\u952e\uff0c\u4e3a\u8fb9\u7f18\u5316\u8bed\u8a00\u63d0\u4f9b\u4e86\u4e0d\u4f9d\u8d56\u5927\u89c4\u6a21\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u7684\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2512.06353", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06353", "abs": "https://arxiv.org/abs/2512.06353", "authors": ["Kaicheng Yang", "Kaisen Yang", "Baiting Wu", "Xun Zhang", "Qianrui Yang", "Haotong Qin", "He Zhang", "Yulun Zhang"], "title": "TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search", "comment": "Code and Supplementary Material could be found at https://github.com/racoonykc/TreeQ", "summary": "Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ", "AI": {"tldr": "TreeQ\u662f\u4e00\u4e2a\u9488\u5bf9\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u7edf\u4e00\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6811\u7ed3\u6784\u641c\u7d22\u3001\u73af\u5883\u566a\u58f0\u5f15\u5bfc\u548c\u901a\u7528Monarch\u5206\u652f\u4e09\u4e2a\u521b\u65b0\u6280\u672f\uff0c\u5b9e\u73b0\u4e86DiT\u6a21\u578b\u5728\u8d85\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u7684\u9ad8\u6027\u80fd\uff0c\u9996\u6b21\u5728DiT\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u65e0\u635f\u76844\u4f4dPTQ\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\uff08DiTs\uff09\u867d\u7136\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u6311\u6218\u3002\u73b0\u6709\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\uff08MPQ\uff09\u65b9\u6cd5\u5728U-Net\u4e0a\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728DiT\u67b6\u6784\u4e0a\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u4e14\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86TreeQ\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1\uff09\u6811\u7ed3\u6784\u641c\u7d22\uff08TSS\uff09\uff0c\u5229\u7528DiT\u7684\u7ebf\u6027\u7279\u6027\u5728O(n)\u65f6\u95f4\u5185\u904d\u5386\u89e3\u7a7a\u95f4\uff1b2\uff09\u73af\u5883\u566a\u58f0\u5f15\u5bfc\uff08ENG\uff09\uff0c\u4f7f\u7528\u5355\u4e00\u8d85\u53c2\u6570\u7edf\u4e00PTQ\u548cQAT\u7684\u4f18\u5316\u76ee\u6807\uff1b3\uff09\u901a\u7528Monarch\u5206\u652f\uff08GMB\uff09\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7a00\u758f\u5206\u652f\u9632\u6b62\u8d85\u4f4e\u6bd4\u7279\u4e0b\u7684\u4fe1\u606f\u74f6\u9888\u3002", "result": "TreeQ\u5728DiT-XL/2\u6a21\u578b\u4e0a\uff0c\u5728W3A3\u548cW4A4\u7684PTQ/PEFT\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u9996\u6b21\u5728DiT\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u65e0\u635f\u76844\u4f4dPTQ\u6027\u80fd\u3002", "conclusion": "TreeQ\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86DiT\u91cf\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u641c\u7d22\u7b56\u7565\u3001\u4f18\u5316\u76ee\u6807\u7edf\u4e00\u548c\u4fe1\u606f\u74f6\u9888\u7f13\u89e3\u6280\u672f\uff0c\u4e3aDiT\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07288", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07288", "abs": "https://arxiv.org/abs/2512.07288", "authors": ["Tomoki Doi", "Masaru Isonuma", "Hitomi Yanaka"], "title": "Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models", "comment": "To appear in the Proceedings of the Asia-Pacific Chapter of the Association for Computational Linguistics: Student Research Workshop (AACL-SRW 2025)", "summary": "Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u8bad\u7ec3\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u89e3\u91ca\u7684\u5fe0\u5b9e\u5ea6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u6539\u8fdb\u5728\u4e0d\u540c\u89e3\u91ca\u98ce\u683c\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u6839\u636e\u7528\u6237\u6307\u4ee4\u751f\u6210\u4e0d\u540c\u98ce\u683c\u7684\u81ea\u6211\u89e3\u91ca\uff0c\u4f46\u8fd9\u4e9b\u89e3\u91ca\u5f80\u5f80\u7f3a\u4e4f\u5fe0\u5b9e\u5ea6\uff08\u5373\u4e0d\u80fd\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\uff09\u3002\u73b0\u6709\u7814\u7a76\u5bf9\u5982\u4f55\u63d0\u5347\u5fe0\u5b9e\u5ea6\u4ee5\u53ca\u4e0d\u540c\u89e3\u91ca\u98ce\u683c\u95f4\u7684\u6539\u8fdb\u662f\u5426\u5177\u6709\u6cdb\u5316\u6027\u8fd9\u4e24\u4e2a\u95ee\u9898\u5c1a\u672a\u6df1\u5165\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e09\u4e2a\u5206\u7c7b\u4efb\u52a1\u548c\u4e09\u79cd\u89e3\u91ca\u98ce\u683c\u3002\u9996\u5148\u901a\u8fc7\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u6784\u5efa\u53ef\u80fd\u5177\u6709\u5fe0\u5b9e\u5ea6\u7684\u5355\u8bcd\u7ea6\u675f\u89e3\u91ca\uff08\u4f2a\u5fe0\u5b9e\u81ea\u6211\u89e3\u91ca\uff09\uff0c\u7136\u540e\u5bf9\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u8fdb\u884c\u6301\u7eed\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u8bad\u7ec3\u80fd\u63d0\u5347\u6240\u6709\u5206\u7c7b\u4efb\u52a1\u548c\u89e3\u91ca\u98ce\u683c\u7684\u81ea\u6211\u89e3\u91ca\u5fe0\u5b9e\u5ea6\uff1b2\uff09\u8fd9\u79cd\u6539\u8fdb\u5728\u591a\u8bcd\u8bbe\u7f6e\u548c\u672a\u89c1\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u6cdb\u5316\u8ff9\u8c61\uff1b3\uff09\u4e09\u79cd\u98ce\u683c\u95f4\u5b58\u5728\u4e00\u81f4\u7684\u8de8\u98ce\u683c\u6cdb\u5316\uff0c\u8868\u660e\u8bad\u7ec3\u53ef\u80fd\u4fc3\u8fdb\u5fe0\u5b9e\u81ea\u6211\u89e3\u91ca\u80fd\u529b\u7684\u5e7f\u6cdb\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u57fa\u4e8e\u4f2a\u5fe0\u5b9e\u81ea\u6211\u89e3\u91ca\u7684\u6301\u7eed\u5b66\u4e60\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u89e3\u91ca\u7684\u5fe0\u5b9e\u5ea6\uff0c\u4e14\u8fd9\u79cd\u6539\u8fdb\u5728\u4e0d\u540c\u89e3\u91ca\u98ce\u683c\u95f4\u5177\u6709\u6cdb\u5316\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.06358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06358", "abs": "https://arxiv.org/abs/2512.06358", "authors": ["Mingjia Li", "Jin Hu", "Hainuo Wang", "Qiming Hu", "Jiarui Wang", "Xiaojie Guo"], "title": "Rectifying Latent Space for Generative Single-Image Reflection Removal", "comment": null, "summary": "Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u5355\u56fe\u50cf\u53cd\u5c04\u53bb\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u6784\u5efa\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u6765\u66f4\u597d\u5730\u5904\u7406\u9ad8\u5ea6\u6a21\u7cca\u7684\u590d\u5408\u56fe\u50cf\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u5355\u56fe\u50cf\u53cd\u5c04\u53bb\u9664\u662f\u4e00\u4e2a\u9ad8\u5ea6\u4e0d\u9002\u5b9a\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u63a8\u7406\u88ab\u6c61\u67d3\u533a\u57df\u7684\u7ec4\u6210\u7ed3\u6784\uff0c\u5bfc\u81f4\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6062\u590d\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u672c\u6587\u53d1\u73b0\u5173\u952e\u95ee\u9898\u5728\u4e8e\u8bed\u4e49\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u7f3a\u4e4f\u89e3\u91ca\u590d\u5408\u56fe\u50cf\u4f5c\u4e3a\u5176\u7ec4\u6210\u5c42\u7ebf\u6027\u53e0\u52a0\u7684\u5185\u5728\u7ed3\u6784\u3002", "method": "\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1a1\uff09\u53cd\u5c04\u7b49\u53d8VAE\uff0c\u5c06\u6f5c\u5728\u7a7a\u95f4\u4e0e\u53cd\u5c04\u5f62\u6210\u7684\u7ebf\u6027\u7269\u7406\u7279\u6027\u5bf9\u9f50\uff1b2\uff09\u53ef\u5b66\u4e60\u7684\u4efb\u52a1\u7279\u5b9a\u6587\u672c\u5d4c\u5165\uff0c\u63d0\u4f9b\u7cbe\u786e\u6307\u5bfc\u5e76\u7ed5\u8fc7\u6a21\u7cca\u8bed\u8a00\uff1b3\uff09\u6df1\u5ea6\u5f15\u5bfc\u7684\u65e9\u671f\u5206\u652f\u91c7\u6837\u7b56\u7565\uff0c\u5229\u7528\u751f\u6210\u968f\u673a\u6027\u83b7\u5f97\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684SOTA\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u6784\u5efa\u7f16\u8f91\u76ee\u7684\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u6709\u6548\u611f\u77e5\u548c\u5904\u7406\u9ad8\u5ea6\u6a21\u7cca\u7684\u5206\u5c42\u56fe\u50cf\u8f93\u5165\uff0c\u89e3\u51b3\u4e86\u5355\u56fe\u50cf\u53cd\u5c04\u53bb\u9664\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.07367", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07367", "abs": "https://arxiv.org/abs/2512.07367", "authors": ["Revekka Kyriakoglou", "Anna Pappa"], "title": "Multilingual corpora for the study of new concepts in the social sciences and humanities:", "comment": "in French language", "summary": "This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u652f\u6301\u4eba\u6587\u793e\u4f1a\u79d1\u5b66\u4e2d\u65b0\u5174\u6982\u5ff5\u7684\u7814\u7a76\uff0c\u4ee5\"\u975e\u6280\u672f\u521b\u65b0\"\u4e3a\u4f8b\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u516c\u53f8\u7f51\u7ad9\u6587\u672c\u548c\u5e74\u5ea6\u62a5\u544a\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5904\u7406\u6d41\u7a0b\u521b\u5efa\u53ef\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6807\u6ce8\u6570\u636e\u96c6\u3002", "motivation": "\u4e3a\u652f\u6301\u4eba\u6587\u793e\u4f1a\u79d1\u5b66\u4e2d\u65b0\u5174\u6982\u5ff5\u7684\u7814\u7a76\uff0c\u9700\u8981\u6784\u5efa\u4e13\u95e8\u7684\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u65b0\u5174\u6982\u5ff5\u7684\u8bcd\u6c47\u53d8\u5f02\u6027\uff0c\u4e14\u7f3a\u4e4f\u9002\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u7684\u7ed3\u6784\u5316\u6570\u636e\u8d44\u6e90\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u6784\u5efa\u8bed\u6599\u5e93\uff1a1) \u4ece\u516c\u53f8\u7f51\u7ad9\u81ea\u52a8\u63d0\u53d6\u5e76\u6e05\u6d17\u6cd5\u82f1\u53cc\u8bed\u6587\u672c\uff1b2) \u6536\u96c6\u5e74\u5ea6\u62a5\u544a\u5e76\u6309\u6587\u6863\u6807\u51c6\u81ea\u52a8\u7b5b\u9009\u3002\u5904\u7406\u6d41\u7a0b\u5305\u62ec\u8bed\u8a00\u68c0\u6d4b\u3001\u5185\u5bb9\u8fc7\u6ee4\u3001\u76f8\u5173\u7247\u6bb5\u63d0\u53d6\u548c\u7ed3\u6784\u5316\u5143\u6570\u636e\u6807\u6ce8\u3002\u4ece\u521d\u59cb\u8bed\u6599\u5e93\u521b\u5efa\u82f1\u6587\u6570\u636e\u96c6\uff0c\u4e3a\u4e13\u5bb6\u8bcd\u5178\u4e2d\u7684\u6bcf\u4e2a\u672f\u8bed\u63d0\u53d6\u5305\u542b\u524d\u540e\u5404\u4e24\u53e5\u7684\u4e0a\u4e0b\u6587\u5757\uff0c\u5e76\u6807\u6ce8\u76f8\u5173\u4e3b\u9898\u7c7b\u522b\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u4e14\u53ef\u6269\u5c55\u7684\u8d44\u6e90\uff0c\u65e2\u53ef\u7528\u4e8e\u5206\u6790\u65b0\u5174\u6982\u5ff5\u5468\u56f4\u7684\u8bcd\u6c47\u53d8\u5f02\u6027\uff0c\u4e5f\u53ef\u751f\u6210\u4e13\u95e8\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u7684\u6570\u636e\u96c6\u3002\u521b\u5efa\u4e86\u9002\u5408\u76d1\u7763\u5206\u7c7b\u4efb\u52a1\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u672f\u8bed\u51fa\u73b0\u90fd\u5e26\u6709\u4e0a\u4e0b\u6587\u548c\u4e3b\u9898\u7c7b\u522b\u6807\u6ce8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7814\u7a76\u4eba\u6587\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u65b0\u5174\u6982\u5ff5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bed\u6599\u5e93\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6570\u636e\u6e90\u548c\u81ea\u52a8\u5316\u5904\u7406\u6d41\u7a0b\uff0c\u521b\u5efa\u4e86\u65e2\u9002\u5408\u6982\u5ff5\u5206\u6790\u53c8\u9002\u5408\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u591a\u529f\u80fd\u8d44\u6e90\u3002"}}
{"id": "2512.06363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06363", "abs": "https://arxiv.org/abs/2512.06363", "authors": ["Jiabao Guo", "Yadian Wang", "Hui Ma", "Yuhao Fu", "Ju Jia", "Hui Liu", "Shengeng Tang", "Lechao Cheng", "Yunfeng Diao", "Ajian Liu"], "title": "Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection", "comment": null, "summary": "Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSPL-UAD\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7269\u7406\u653b\u51fb\u548c\u6570\u5b57\u653b\u51fb\u7684\u4f18\u5316\u5206\u652f\uff0c\u5b9e\u73b0\u7edf\u4e00\u7684\u653b\u51fb\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u4e24\u7c7b\u653b\u51fb\u68c0\u6d4b\u4f18\u5316\u65b9\u5411\u51b2\u7a81\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u540c\u65f6\u9762\u4e34\u7269\u7406\u5448\u73b0\u653b\u51fb\u548c\u6570\u5b57\u4f2a\u9020\u653b\u51fb\u7684\u53cc\u91cd\u5a01\u80c1\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528CLIP\u52a0\u6b63\u5219\u5316\u7ea6\u675f\u6765\u63d0\u5347\u6a21\u578b\u5728\u4e24\u7c7b\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u76f8\u540c\u7c7b\u522b\u63d0\u793a\u7a7a\u95f4\u4e0b\u5b58\u5728\u7269\u7406\u548c\u6570\u5b57\u653b\u51fb\u68c0\u6d4b\u4f18\u5316\u65b9\u5411\u51b2\u7a81\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSPL-UAD\u6846\u67b6\uff1a1\uff09\u6784\u5efa\u53ef\u5b66\u4e60\u7684\u5e76\u884c\u63d0\u793a\u5206\u652f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6b3a\u9a97\u4e0a\u4e0b\u6587\u63d0\u793a\u751f\u6210\u589e\u5f3a\uff0c\u5b9e\u73b0\u5bf9\u6bcf\u79cd\u653b\u51fb\u7c7b\u578b\u7684\u72ec\u7acb\u4f18\u5316\u63a7\u5236\uff1b2\uff09\u8bbe\u8ba1\u7ebf\u7d22\u611f\u77e5\u589e\u5f3a\uff0c\u5229\u7528\u53cc\u63d0\u793a\u673a\u5236\u5728\u6570\u636e\u4e0a\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u6837\u672c\u6316\u6398\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u672a\u89c1\u653b\u51fb\u7c7b\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5927\u578bUniAttackDataPlus\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u7edf\u4e00\u653b\u51fb\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SPL-UAD\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u7269\u7406\u548c\u6570\u5b57\u653b\u51fb\u7684\u4f18\u5316\u5206\u652f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u4f18\u5316\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u7684\u751f\u7269\u7279\u5f81\u6570\u636e\u4fdd\u62a4\uff0c\u5728\u7edf\u4e00\u653b\u51fb\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.07454", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07454", "abs": "https://arxiv.org/abs/2512.07454", "authors": ["Amir Mohammad Akhlaghi", "Amirhossein Shabani", "Mostafa Abdolmaleki", "Saeed Reza Kheradpisheh"], "title": "Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning", "comment": null, "summary": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique \"warm-up\" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.", "AI": {"tldr": "\u6ce2\u65af-Phi\u662f\u4e00\u4e2a3.8B\u53c2\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bfe\u7a0b\u5b66\u4e60\u6d41\u7a0b\u5c06\u82f1\u8bed\u5355\u8bed\u6a21\u578bPhi-3 Mini\u6709\u6548\u9002\u914d\u5230\u6ce2\u65af\u8bed\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u6269\u5c55\u65b9\u6848", "motivation": "\u5f53\u524dAI\u6c11\u4e3b\u5316\u53d7\u5230\u8bad\u7ec3\u4f4e\u8d44\u6e90\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u6240\u9700\u5de8\u5927\u8ba1\u7b97\u6210\u672c\u7684\u963b\u788d\uff0c\u9700\u8981\u4e3a\u6ce2\u65af\u8bed\u7b49\u8d44\u6e90\u6709\u9650\u8bed\u8a00\u5f00\u53d1\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u80fd\u529b\u6269\u5c55\u65b9\u6848", "method": "\u91c7\u7528\u8d44\u6e90\u9ad8\u6548\u7684\u8bfe\u7a0b\u5b66\u4e60\u6d41\u7a0b\uff1a\u9996\u5148\u4f7f\u7528\u53cc\u8bed\u53d9\u4e8b\uff08Tiny Stories\uff09\u8fdb\u884c\"\u9884\u70ed\"\u9636\u6bb5\u4ee5\u5bf9\u9f50\u5d4c\u5165\uff0c\u7136\u540e\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18", "result": "\u6ce2\u65af-Phi\u5728HuggingFace\u7684Open Persian LLM Leaderboard\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u7d27\u51d1\u6a21\u578b\u4e5f\u80fd\u5177\u5907\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u80fd\u529b", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u786c\u4ef6\u8d44\u6e90\u5c06\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u5230\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\uff0c\u6311\u6218\u4e86\u5f3a\u5927\u591a\u8bed\u8a00\u80fd\u529b\u9700\u8981\u5927\u89c4\u6a21\u6a21\u578b\u6216\u591a\u8bed\u8a00\u57fa\u7ebf\u7684\u5047\u8bbe"}}
{"id": "2512.06373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06373", "abs": "https://arxiv.org/abs/2512.06373", "authors": ["Yuji Wang", "Wenlong Liu", "Jingxuan Niu", "Haoji Zhang", "Yansong Tang"], "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning", "comment": "The project page is [this url](https://github.com/VoyageWang/VG-Refiner)", "summary": "Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.", "AI": {"tldr": "VG-Refiner\uff1a\u9996\u4e2a\u5de5\u5177\u7cbe\u70bc\u7684\u6307\u4ee3\u63a5\u5730\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u601d\u8003-\u518d\u601d\u8003\u673a\u5236\u5904\u7406\u4e0d\u53ef\u9760\u5de5\u5177\u8f93\u51fa\uff0c\u63d0\u5347\u6307\u4ee3\u548c\u63a5\u5730\u4efb\u52a1\u7684\u51c6\u786e\u6027", "motivation": "\u73b0\u6709\u5de5\u5177\u96c6\u6210\u89c6\u89c9\u63a8\u7406\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u96c6\u6210\u5404\u79cd\u89c6\u89c9\u5de5\u5177\uff0c\u4f46\u7f3a\u4e4f\u5904\u7406\u4e0d\u53ef\u9760\u6216\u9519\u8bef\u5de5\u5177\u8f93\u51fa\u7684\u6709\u6548\u54cd\u5e94\u673a\u5236\u3002\u8fd9\u5728\u6307\u4ee3\u548c\u63a5\u5730\u4efb\u52a1\u4e2d\u5c24\u4e3a\u7a81\u51fa\uff0c\u4e0d\u51c6\u786e\u7684\u68c0\u6d4b\u5de5\u5177\u9884\u6d4b\u5e38\u5bfc\u81f4\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u63a8\u7406\u3002", "method": "\u63d0\u51faVG-Refiner\u6846\u67b6\uff0c\u5f15\u5165\u4e24\u9636\u6bb5\u601d\u8003-\u518d\u601d\u8003\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u663e\u5f0f\u5206\u6790\u548c\u54cd\u5e94\u5de5\u5177\u53cd\u9988\uff1b\u8bbe\u8ba1\u7cbe\u70bc\u5956\u52b1\u673a\u5236\uff0c\u9f13\u52b1\u5bf9\u4e0d\u826f\u5de5\u5177\u7ed3\u679c\u8fdb\u884c\u6709\u6548\u4fee\u6b63\uff1b\u63d0\u51fa\u4e24\u4e2a\u65b0\u6307\u6807\u548c\u516c\u5e73\u8bc4\u4f30\u534f\u8bae\u6765\u7cfb\u7edf\u8861\u91cf\u6a21\u578b\u7684\u7cbe\u70bc\u80fd\u529b\u3002", "result": "\u4f7f\u7528\u5c11\u91cf\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u589e\u5f3aVG-Refiner\u7684\u7cbe\u70bc\u80fd\u529b\uff0c\u5728\u6307\u4ee3\u548c\u63a8\u7406\u63a5\u5730\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u548c\u4fee\u6b63\u80fd\u529b\u7684\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "conclusion": "VG-Refiner\u662f\u9996\u4e2a\u9488\u5bf9\u5de5\u5177\u7cbe\u70bc\u6307\u4ee3\u63a5\u5730\u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5904\u7406\u5de5\u5177\u53cd\u9988\u548c\u7cbe\u70bc\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709TiVR\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u53ef\u9760\u5de5\u5177\u8f93\u51fa\u65f6\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6307\u4ee3\u548c\u63a5\u5730\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2512.07461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07461", "abs": "https://arxiv.org/abs/2512.07461", "authors": ["Tong Wu", "Yang Liu", "Jun Bai", "Zixia Jia", "Shuyi Zhang", "Ziyong Lin", "Yanting Wang", "Song-Chun Zhu", "Zilong Zheng"], "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "comment": null, "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "AI": {"tldr": "NPR\u662f\u4e00\u4e2a\u65e0\u9700\u6559\u5e08\u6307\u5bfc\u7684\u6846\u67b6\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u81ea\u6211\u8fdb\u5316\u83b7\u5f97\u771f\u6b63\u7684\u5e76\u884c\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u4ece\u987a\u5e8f\u6a21\u62df\u5230\u539f\u751f\u5e76\u884c\u8ba4\u77e5\u7684\u8f6c\u53d8\uff0c\u57288\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u6027\u80fd\u63d0\u5347\u8fbe24.5%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53474.6\u500d\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u901a\u5e38\u662f\u987a\u5e8f\u6a21\u62df\u7684\uff0c\u7f3a\u4e4f\u771f\u6b63\u7684\u5e76\u884c\u8ba4\u77e5\u80fd\u529b\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u6846\u67b6\uff0c\u8ba9LLMs\u80fd\u591f\u81ea\u6211\u8fdb\u5316\u51fa\u771f\u6b63\u7684\u5e76\u884c\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u63a8\u7406\u3002", "method": "NPR\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u81ea\u6211\u84b8\u998f\u7684\u6e10\u8fdb\u8bad\u7ec3\u8303\u5f0f\uff0c\u4ece\"\u51b7\u542f\u52a8\"\u683c\u5f0f\u53d1\u73b0\u8fc7\u6e21\u5230\u4e25\u683c\u7684\u62d3\u6251\u7ea6\u675f\uff1b2\uff09\u5e76\u884c\u611f\u77e5\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u76f4\u63a5\u5728\u6267\u884c\u56fe\u4e2d\u4f18\u5316\u5206\u652f\u7b56\u7565\uff1b3\uff09\u91cd\u6784SGLang\u5185\u5b58\u7ba1\u7406\u548c\u6d41\u7a0b\u63a7\u5236\u7684NPR\u5f15\u64ce\uff0c\u652f\u6301\u7a33\u5b9a\u7684\u5927\u89c4\u6a21\u5e76\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u57288\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eQwen3-4B\u8bad\u7ec3\u7684NPR\u5b9e\u73b0\u4e86\u6700\u9ad824.5%\u7684\u6027\u80fd\u63d0\u5347\u548c\u6700\u9ad84.6\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002\u4e0e\u4e4b\u524d\u57fa\u7ebf\u7ecf\u5e38\u56de\u9000\u5230\u81ea\u56de\u5f52\u89e3\u7801\u4e0d\u540c\uff0cNPR\u5c55\u793a\u4e86100%\u771f\u6b63\u7684\u5e76\u884c\u6267\u884c\u3002", "conclusion": "NPR\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u6807\u51c6\uff0c\u5b9e\u73b0\u4e86\u81ea\u6211\u8fdb\u5316\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u63a8\u7406\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u987a\u5e8f\u6a21\u62df\u8f6c\u5411\u539f\u751f\u5e76\u884c\u8ba4\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2512.06376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06376", "abs": "https://arxiv.org/abs/2512.06376", "authors": ["Xinhao Xiang", "Abhijeet Rastogi", "Jiawei Zhang"], "title": "Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework", "comment": null, "summary": "Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8bca\u65ad\u6846\u67b6\u8bc4\u4f30AI\u751f\u6210\u9a7e\u9a76\u89c6\u9891\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u7684\u53ef\u9760\u6027\uff0c\u8bc6\u522bAIGV\u5e38\u89c1\u6545\u969c\u6a21\u5f0f\uff0c\u6784\u5efaADGV-Bench\u57fa\u51c6\uff0c\u5f00\u53d1ADGVE\u8bc4\u4f30\u5668\uff0c\u8bc1\u660e\u7b5b\u9009\u540e\u7684AIGV\u53ef\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u7684\u6709\u76ca\u8865\u5145\u3002", "motivation": "\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u80fd\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u9a7e\u9a76\u573a\u666f\uff0cAI\u751f\u6210\u9a7e\u9a76\u89c6\u9891\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5173\u952e\u95ee\u9898\u5728\u4e8e\u8fd9\u4e9b\u89c6\u9891\u662f\u5426\u80fd\u53ef\u9760\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "method": "1. \u63d0\u51faAIGV\u6545\u969c\u6a21\u5f0f\u5206\u7c7b\u6cd5\uff08\u89c6\u89c9\u4f2a\u5f71\u3001\u7269\u7406\u4e0d\u5408\u7406\u8fd0\u52a8\u3001\u4ea4\u901a\u8bed\u4e49\u8fdd\u89c4\uff09\uff1b2. \u6784\u5efaADGV-Bench\u57fa\u51c6\uff0c\u5305\u542b\u4eba\u5de5\u8d28\u91cf\u6807\u6ce8\u548c\u5bc6\u96c6\u6807\u7b7e\uff1b3. \u5f00\u53d1ADGVE\u8bc4\u4f30\u5668\uff0c\u7ed3\u5408\u9759\u6001\u8bed\u4e49\u3001\u65f6\u5e8f\u7ebf\u7d22\u3001\u8f66\u9053\u9075\u5b88\u4fe1\u53f7\u548cVLM\u5f15\u5bfc\u63a8\u7406\u751f\u6210\u8d28\u91cf\u8bc4\u5206\u3002", "result": "\u76f2\u76ee\u6dfb\u52a0\u539f\u59cbAIGV\u4f1a\u964d\u4f4e\u611f\u77e5\u6027\u80fd\uff0c\u4f46\u4f7f\u7528ADGVE\u7b5b\u9009\u540e\u80fd\u540c\u65f6\u63d0\u5347\u901a\u7528\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u548c\u4e0b\u6e38\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u6027\u80fd\uff0c\u4f7fAIGV\u6210\u4e3a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u6709\u76ca\u8865\u5145\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86AIGV\u7684\u98ce\u9669\u548c\u6f5c\u529b\uff0c\u4e3a\u5728\u81ea\u52a8\u9a7e\u9a76\u6d41\u7a0b\u4e2d\u5b89\u5168\u5229\u7528\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u7b5b\u9009\u540e\u7684AIGV\u80fd\u6709\u6548\u8865\u5145\u771f\u5b9e\u6570\u636e\u3002"}}
{"id": "2512.07515", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07515", "abs": "https://arxiv.org/abs/2512.07515", "authors": ["Pengqian Lu", "Jie Lu", "Anjin Liu", "Guangquan Zhang"], "title": "SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG", "comment": null, "summary": "Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance", "AI": {"tldr": "SPAD\u662f\u4e00\u79cd\u65b0\u7684RAG\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06token\u6982\u7387\u5206\u89e3\u4e3a7\u4e2a\u6765\u6e90\u5e76\u805a\u5408POS\u6807\u7b7e\u6765\u8bc6\u522b\u5f02\u5e38\uff0c\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06RAG\u4e2d\u7684\u5e7b\u89c9\u5f52\u56e0\u4e8e\u5185\u90e8\u77e5\u8bc6\uff08FFN\uff09\u4e0e\u68c0\u7d22\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u4e8c\u5143\u51b2\u7a81\uff0c\u8fd9\u79cd\u89c6\u89d2\u4e0d\u5b8c\u6574\uff0c\u5ffd\u7565\u4e86\u751f\u6210\u8fc7\u7a0b\u4e2d\u5176\u4ed6\u7ec4\u4ef6\u7684\u5f71\u54cd\uff0c\u5982\u7528\u6237\u67e5\u8be2\u3001\u5148\u524d\u751f\u6210\u7684token\u3001\u5f53\u524dtoken\u672c\u8eab\u4ee5\u53ca\u6700\u540e\u7684LayerNorm\u8c03\u6574", "method": "SPAD\u65b9\u6cd5\u9996\u5148\u5c06\u6bcf\u4e2atoken\u7684\u6982\u7387\u6570\u5b66\u5f52\u56e0\u52307\u4e2a\u4e0d\u540c\u6765\u6e90\uff1a\u67e5\u8be2\u3001RAG\u3001\u8fc7\u53bbtoken\u3001\u5f53\u524dtoken\u3001FFN\u3001\u6700\u7ec8LayerNorm\u548c\u521d\u59cb\u5d4c\u5165\uff0c\u91cf\u5316\u6bcf\u4e2a\u6765\u6e90\u5bf9\u5f53\u524dtoken\u751f\u6210\u7684\u8d21\u732e\uff1b\u7136\u540e\u901a\u8fc7POS\u6807\u7b7e\u805a\u5408\u8fd9\u4e9b\u5206\u6570\uff0c\u91cf\u5316\u4e0d\u540c\u7ec4\u4ef6\u5982\u4f55\u9a71\u52a8\u7279\u5b9a\u8bed\u8a00\u7c7b\u522b\uff1b\u901a\u8fc7\u8bc6\u522b\u5f02\u5e38\uff08\u5982\u540d\u8bcd\u4f9d\u8d56\u6700\u7ec8LayerNorm\uff09\u6765\u6709\u6548\u68c0\u6d4b\u5e7b\u89c9", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSPAD\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "SPAD\u901a\u8fc7\u66f4\u5168\u9762\u7684\u6982\u7387\u5f52\u56e0\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u4e8c\u5143\u51b2\u7a81\u89c6\u89d2\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u68c0\u6d4bRAG\u4e2d\u7684\u5e7b\u89c9"}}
{"id": "2512.07522", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07522", "abs": "https://arxiv.org/abs/2512.07522", "authors": ["Sebastian Sztwiertnia", "Felix Friedrich", "Kristian Kersting", "Patrick Schramowski", "Bj\u00f6rn Deiseroth"], "title": "LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings", "comment": null, "summary": "Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.", "AI": {"tldr": "LIME\u662f\u4e00\u79cd\u901a\u8fc7\u5143\u6570\u636e\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u8bed\u6cd5\u3001\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u5c5e\u6027\u7684\u5143\u6570\u636e\u6765\u4e30\u5bcc\u8bcd\u5143\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u6548\u7387\u548c\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4ec5\u89e3\u7801\u5668\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4f9d\u8d56\u5927\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u4f46\u8fd9\u7c7b\u6570\u636e\u7684\u53ef\u7528\u6027\u5df2\u63a5\u8fd1\u6781\u9650\u3002\u867d\u7136\u5143\u6570\u636e\u5e38\u7528\u4e8e\u521b\u5efa\u548c\u6574\u7406\u6570\u636e\u96c6\uff0c\u4f46\u5176\u4f5c\u4e3a\u76f4\u63a5\u8bad\u7ec3\u4fe1\u53f7\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faLIME\u65b9\u6cd5\uff0c\u5c06\u6355\u83b7\u8bed\u6cd5\u3001\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u5c5e\u6027\u7684\u5143\u6570\u636e\u5d4c\u5165\u5230\u8bcd\u5143\u5d4c\u5165\u4e2d\u3002\u8fd8\u5f00\u53d1\u4e86LIME+1\u53d8\u4f53\uff0c\u4f7f\u7528\u79fb\u4f4d\u5143\u6570\u636e\u6765\u6307\u5bfc\u8bcd\u5143\u751f\u6210\u3002", "result": "LIME\u4f7f\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u7684\u9002\u5e94\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe56%\uff0c\u4ec5\u589e\u52a00.01%\u7684\u53c2\u6570\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u3002\u6539\u5584\u4e86\u5206\u8bcd\u6548\u679c\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u548c\u751f\u6210\u4efb\u52a1\u6027\u80fd\uff0c\u8fd9\u4e9b\u4f18\u52bf\u5728500M\u52302B\u89c4\u6a21\u7684\u6a21\u578b\u4e2d\u5747\u5b58\u5728\u3002LIME+1\u5c06\u63a8\u7406\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe38%\uff0c\u7b97\u672f\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe35%\u3002", "conclusion": "\u5143\u6570\u636e\u4f5c\u4e3a\u76f4\u63a5\u8bad\u7ec3\u4fe1\u53f7\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0cLIME\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u6548\u7387\u548c\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6LIME+1\u53d8\u4f53\u8fd8\u80fd\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2512.06400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06400", "abs": "https://arxiv.org/abs/2512.06400", "authors": ["Jing Tao", "Yonghong Zong", "Banglei Guana", "Pengju Sun", "Taihang Lei", "Yang Shanga", "Qifeng Yu"], "title": "Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement", "comment": "The paper has been accepted and officially published by IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT", "summary": "In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u533a\u57df\u611f\u77e5\u7684\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u878d\u5408\u6846\u67b6\uff0c\u4f7f\u7528\u7a7a\u95f4\u53d8\u5316\u66dd\u5149\u76f8\u673a\u7ed3\u5408\u591a\u66dd\u5149\u4e0e\u591a\u6a21\u6001\u6210\u50cf\uff0c\u5728\u6781\u7aef\u73af\u5883\u4e0b\u4fdd\u6301\u53ef\u89c1\u5149\u51e0\u4f55\u4fdd\u771f\u5ea6\u5e76\u878d\u5165\u70ed\u8f90\u5c04\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u878d\u5408\u7ea2\u5916\u548c\u53ef\u89c1\u5149\u8c31\u65f6\uff0c\u5f80\u5f80\u635f\u5bb3\u53ef\u89c1\u5149\u56fe\u50cf\u8d28\u91cf\uff0c\u5f71\u54cd\u6d4b\u91cf\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u70ed\u8f90\u5c04\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u533a\u57df\u611f\u77e5\u7684\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u7a7a\u95f4\u53d8\u5316\u66dd\u5149\u76f8\u673a\u8fdb\u884c\u591a\u66dd\u5149\u548c\u591a\u6a21\u6001\u6210\u50cf\u3002\u9996\u5148\u8fdb\u884c\u533a\u57df\u611f\u77e5\u7684\u7279\u5f81\u878d\u5408\u786e\u4fdd\u7cbe\u786e\u7684\u591a\u6a21\u6001\u914d\u51c6\uff0c\u7136\u540e\u8fdb\u884c\u81ea\u9002\u5e94\u878d\u5408\u4e0e\u5bf9\u6bd4\u5ea6\u589e\u5f3a\uff0c\u6700\u540e\u901a\u8fc7\u533a\u57df\u663e\u8457\u6027\u56fe\u5f15\u5bfc\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u8865\u507f\u673a\u5236\u4f18\u5316\u7ea2\u5916-\u53ef\u89c1\u5149\u8c31\u878d\u5408\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u6e05\u6670\u5ea6\u548c\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9a\u91cf\u548c\u89c6\u89c9\u8bc4\u4f30\u5747\u8bc1\u5b9e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u533a\u57df\u611f\u77e5\u878d\u5408\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6781\u7aef\u73af\u5883\u4e0b\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u878d\u5408\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u53ef\u89c1\u5149\u51e0\u4f55\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u6210\u529f\u878d\u5165\u70ed\u8f90\u5c04\u4fe1\u606f\uff0c\u4e3a\u6444\u5f71\u6d4b\u91cf\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u591a\u6a21\u6001\u6210\u50cf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06421", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06421", "abs": "https://arxiv.org/abs/2512.06421", "authors": ["Gengze Zhou", "Chongjian Ge", "Hao Tan", "Feng Liu", "Yicong Hong"], "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation", "comment": null, "summary": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSelf-Autoregressive Refinement (SAR)\u65b9\u6cd5\uff0c\u901a\u8fc7Stagger-Scale Rollout\u673a\u5236\u548cContrastive Student-Forcing Loss\uff0c\u89e3\u51b3\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u4e2d\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u5728\u5a92\u4f53\u5408\u6210\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5c3a\u5ea6\u7ea7\u81ea\u56de\u5f52\u6a21\u578b\u5b58\u5728\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002\u4f5c\u8005\u8bc6\u522b\u51fa\u4e24\u4e2a\u4e3b\u8981\u539f\u56e0\uff1a\u8bad\u7ec3-\u6d4b\u8bd5\u4e0d\u5339\u914d\uff08\u63a8\u7406\u65f6\u6a21\u578b\u4f9d\u8d56\u81ea\u8eab\u4e0d\u5b8c\u7f8e\u9884\u6d4b\uff09\u548c\u5c3a\u5ea6\u5b66\u4e60\u96be\u5ea6\u4e0d\u5e73\u8861\uff08\u67d0\u4e9b\u5c3a\u5ea6\u4f18\u5316\u590d\u6742\u5ea6\u8fc7\u9ad8\uff09\u3002", "method": "\u63d0\u51faSelf-Autoregressive Refinement (SAR)\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) Stagger-Scale Rollout (SSR)\u673a\u5236\uff0c\u6267\u884c\u8f7b\u91cf\u7ea7\u81ea\u56de\u5f52\u5c55\u5f00\uff0c\u8ba9\u6a21\u578b\u63a5\u89e6\u81ea\u8eab\u4e2d\u95f4\u9884\u6d4b\u4ee5\u5bf9\u9f50\u8bad\u7ec3-\u6d4b\u8bd5\u6a21\u5f0f\uff1b2) Contrastive Student-Forcing Loss (CSFL)\uff0c\u4e3a\u81ea\u751f\u6210\u4e0a\u4e0b\u6587\u63d0\u4f9b\u5145\u5206\u76d1\u7763\u4ee5\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06SAR\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u7684\u81ea\u56de\u5f52\u6a21\u578b\u80fd\u6301\u7eed\u63d0\u5347\u751f\u6210\u8d28\u91cf\u4e14\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002\u4f8b\u5982\uff0c\u5728ImageNet 256\u4e0a\u8bad\u7ec3\u7684FlexVAR-d16\u6a21\u578b\uff0cSAR\u572810\u4e2aepoch\u5185\uff0832xA100 GPU\u4e0a5\u5c0f\u65f6\uff09\u5b9e\u73b0\u4e865.2%\u7684FID\u964d\u4f4e\u3002", "conclusion": "SAR\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u671b\u6210\u4e3a\u89c6\u89c9\u81ea\u56de\u5f52\u751f\u6210\u7684\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07525", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07525", "abs": "https://arxiv.org/abs/2512.07525", "authors": ["Xiaoran Liu", "Yuerong Song", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Zhaoxiang Liu", "Shiguo Lian", "Ziwei He", "Xipeng Qiu"], "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "comment": "20 pages, 6 figures, under review", "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684RoPE\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u590d\u6570\u70b9\u79ef\u7684\u865a\u90e8\u4fe1\u606f\u6765\u589e\u5f3a\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u6807\u51c6RoPE\u5b9e\u73b0\u53ea\u4f7f\u7528\u590d\u6570\u70b9\u79ef\u7684\u5b9e\u90e8\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\uff0c\u4e22\u5f03\u4e86\u5305\u542b\u91cd\u8981\u76f8\u4f4d\u4fe1\u606f\u7684\u865a\u90e8\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5efa\u6a21\u4e2d\u5173\u7cfb\u7ec6\u8282\u7684\u635f\u5931\u3002", "method": "\u63d0\u51fa\u6269\u5c55\u65b9\u6cd5\u91cd\u65b0\u6574\u5408\u88ab\u4e22\u5f03\u7684\u865a\u90e8\uff0c\u5229\u7528\u5b8c\u6574\u7684\u590d\u6570\u8868\u793a\u521b\u5efa\u53cc\u7ec4\u4ef6\u6ce8\u610f\u529b\u5206\u6570\uff0c\u589e\u5f3a\u4f4d\u7f6e\u4fe1\u606f\u7684\u4fdd\u7559\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6RoPE\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\uff0c\u6539\u8fdb\u6548\u679c\u66f4\u52a0\u663e\u8457\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u590d\u6570\u70b9\u79ef\u7684\u5b8c\u6574\u4fe1\u606f\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u5efa\u6a21\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e3a\u4f4d\u7f6e\u7f16\u7801\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06426", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06426", "abs": "https://arxiv.org/abs/2512.06426", "authors": ["Nzakiese Mbongo", "Kailash A. Hambarde", "Hugo Proen\u00e7a"], "title": "When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition", "comment": "12 pages, 9 figures", "summary": "Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u8def\u5f84Transformer\u6846\u67b6\uff0c\u5229\u7528CLIP\u8054\u5408\u5efa\u6a21\u89c6\u89c9\u548c\u5c5e\u6027\u7ebf\u7d22\uff0c\u5b9e\u73b0\u8fdc\u8ddd\u79bb\u6027\u522b\u8bc6\u522b\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8fdc\u8ddd\u79bb\u56fe\u50cf\u4e2d\u7684\u6027\u522b\u8bc6\u522b\u9762\u4e34\u7a7a\u95f4\u5206\u8fa8\u7387\u6709\u9650\u3001\u89c6\u89d2\u591a\u53d8\u548c\u9762\u90e8\u7279\u5f81\u7f3a\u5931\u7b49\u6311\u6218\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53cc\u8def\u5f84Transformer\u6846\u67b6\uff1a1) \u89c6\u89c9\u8def\u5f84\u901a\u8fc7\u9009\u62e9\u6027\u5fae\u8c03CLIP\u56fe\u50cf\u7f16\u7801\u5668\u4e0a\u5c42\uff1b2) \u5c5e\u6027\u8def\u5f84\u901a\u8fc7\u8f6f\u751f\u7269\u7279\u5f81\u63d0\u793a\uff08\u53d1\u578b\u3001\u670d\u88c5\u3001\u914d\u9970\uff09\u5728CLIP\u6587\u672c-\u56fe\u50cf\u7a7a\u95f4\u4e2d\u63a8\u7406\u6027\u522b\u3002\u52a0\u5165\u7a7a\u95f4\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u906e\u6321\u548c\u4f4e\u5206\u8fa8\u7387\u4e0b\u7684\u5224\u522b\u5b9a\u4f4d\u3002", "result": "\u6784\u5efaU-DetAGReID\u6570\u636e\u96c6\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u5728\u591a\u4e2a\u6307\u6807\uff08macro-F1\u3001\u51c6\u786e\u7387\u3001AUC\uff09\u4e0a\u8d85\u8d8a\u73b0\u6709\u884c\u4eba\u5c5e\u6027\u548c\u91cd\u8bc6\u522b\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5bf9\u8ddd\u79bb\u3001\u89d2\u5ea6\u548c\u9ad8\u5ea6\u53d8\u5316\u5177\u6709\u4e00\u81f4\u9c81\u68d2\u6027\u3002", "conclusion": "\u8bed\u8a00\u5f15\u5bfc\u7684\u53cc\u8def\u5f84\u5b66\u4e60\u4e3a\u65e0\u7ea6\u675f\u8fdc\u8ddd\u79bb\u573a\u666f\u4e0b\u7684\u8d1f\u8d23\u4efb\u6027\u522b\u8bc6\u522b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u8bc1\u5b9e\u4e86\u53ef\u89e3\u91ca\u7684\u5c5e\u6027\u5b9a\u4f4d\u548c\u8d1f\u8d23\u4efb\u7684\u5f03\u6743\u884c\u4e3a\u3002"}}
{"id": "2512.07538", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07538", "abs": "https://arxiv.org/abs/2512.07538", "authors": ["Michelle Wastl", "Jannis Vamvas", "Rico Sennrich"], "title": "SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents", "comment": "30 pages", "summary": "Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SwissGov-RSD\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u8de8\u8bed\u8a00\u6587\u6863\u7ea7\u8bed\u4e49\u5dee\u5f02\u8bc6\u522b\u7684\u81ea\u7136\u6570\u636e\u96c6\uff0c\u5305\u542b224\u4e2a\u591a\u8bed\u8a00\u5e73\u884c\u6587\u6863\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cdLLM\u548c\u7f16\u7801\u5668\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u8bc6\u522b\u8de8\u6587\u6863\uff08\u5c24\u5176\u662f\u4e0d\u540c\u8bed\u8a00\u95f4\uff09\u7684\u8bed\u4e49\u5dee\u5f02\u5bf9\u4e8e\u6587\u672c\u751f\u6210\u8bc4\u4f30\u548c\u591a\u8bed\u8a00\u5185\u5bb9\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8fd9\u4e00\u4efb\u52a1\u672c\u8eab\u5f88\u5c11\u53d7\u5230\u5173\u6ce8\uff0c\u7f3a\u4e4f\u5408\u9002\u7684\u81ea\u7136\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e86SwissGov-RSD\u6570\u636e\u96c6\uff0c\u5305\u542b\u82f1\u8bed-\u5fb7\u8bed\u3001\u82f1\u8bed-\u6cd5\u8bed\u3001\u82f1\u8bed-\u610f\u5927\u5229\u8bed\u7684224\u4e2a\u591a\u5e73\u884c\u6587\u6863\uff0c\u7531\u4eba\u5de5\u8fdb\u884c\u8bcd\u7ea7\u5dee\u5f02\u6807\u6ce8\u3002\u8bc4\u4f30\u4e86\u5f00\u6e90\u548c\u95ed\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4ee5\u53ca\u7f16\u7801\u5668\u6a21\u578b\u5728\u4e0d\u540c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5f53\u524d\u81ea\u52a8\u65b9\u6cd5\u5728\u8be5\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u8fdc\u4f4e\u4e8e\u5b83\u4eec\u5728\u5355\u8bed\u3001\u53e5\u5b50\u7ea7\u548c\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86LLM\u548c\u7f16\u7801\u5668\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u8de8\u8bed\u8a00\u6587\u6863\u7ea7\u8bed\u4e49\u5dee\u5f02\u8bc6\u522b\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u4f5c\u8005\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2512.07540", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07540", "abs": "https://arxiv.org/abs/2512.07540", "authors": ["Boxuan Lyu", "Haiyue Song", "Hidetaka Kamigaito", "Chenchen Ding", "Hideki Tanaka", "Masao Utiyama", "Kotaro Funakoshi", "Manabu Okumura"], "title": "Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation", "comment": null, "summary": "Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u751f\u6210\u5f0f\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5e94\u7528\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\u89e3\u7801\uff0c\u4ee5\u89e3\u51b3\u6700\u5927\u540e\u9a8c\u6982\u7387\u89e3\u7801\u4e2d\u6a21\u578b\u6982\u7387\u4e0e\u4eba\u5de5\u6807\u6ce8\u76f8\u4f3c\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u84b8\u998f\u6280\u672f\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u751f\u6210\u5f0f\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u6700\u5927\u540e\u9a8c\u6982\u7387\u89e3\u7801\uff0c\u5047\u8bbe\u6a21\u578b\u4f30\u8ba1\u7684\u6982\u7387\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u76f8\u4f3c\u5ea6\u5b8c\u7f8e\u76f8\u5173\u3002\u7136\u800c\uff0c\u4f5c\u8005\u89c2\u5bdf\u5230\u4e0e\u4eba\u5de5\u6807\u6ce8\u4e0d\u76f8\u4f3c\u7684\u6807\u6ce8\u53ef\u80fd\u83b7\u5f97\u6bd4\u4eba\u5de5\u6807\u6ce8\u66f4\u9ad8\u7684\u6a21\u578b\u4f3c\u7136\uff0c\u8fd9\u8868\u660eMAP\u89e3\u7801\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f5c\u8005\u5c06\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\u89e3\u7801\u5e94\u7528\u4e8e\u751f\u6210\u5f0f\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\u6a21\u578b\uff0c\u4f7f\u7528\u53e5\u5b50\u7ea7\u548c\u8de8\u5ea6\u7ea7\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u4f5c\u4e3a\u6548\u7528\u51fd\u6570\uff0c\u6839\u636e\u5019\u9009\u5047\u8bbe\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u8fd1\u4f3c\u76f8\u4f3c\u5ea6\u8fdb\u884c\u9009\u62e9\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u964d\u4f4eMBR\u89e3\u7801\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4f5c\u8005\u5e94\u7528\u4e86MBR\u84b8\u998f\u6280\u672f\uff0c\u4f7f\u6807\u51c6\u8d2a\u5a6a\u6a21\u578b\u80fd\u591f\u5339\u914dMBR\u89e3\u7801\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMBR\u89e3\u7801\u5728\u7cfb\u7edf\u7ea7\u3001\u53e5\u5b50\u7ea7\u548c\u8de8\u5ea6\u7ea7\u5747\u4f18\u4e8eMAP\u57fa\u7ebf\u3002\u901a\u8fc7MBR\u84b8\u998f\uff0c\u6807\u51c6\u8d2a\u5a6a\u6a21\u578b\u80fd\u591f\u5339\u914dMBR\u89e3\u7801\u6027\u80fd\uff0c\u6709\u6548\u6d88\u9664\u4e86\u63a8\u7406\u65f6\u7684\u5ef6\u8fdf\u74f6\u9888\u3002", "conclusion": "MBR\u89e3\u7801\u80fd\u591f\u6709\u6548\u89e3\u51b3\u751f\u6210\u5f0f\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\u4e2d\u6a21\u578b\u6982\u7387\u4e0e\u4eba\u5de5\u6807\u6ce8\u76f8\u4f3c\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u901a\u8fc7\u84b8\u998f\u6280\u672f\u53ef\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f7f\u8be5\u65b9\u6cd5\u66f4\u5177\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.06424", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06424", "abs": "https://arxiv.org/abs/2512.06424", "authors": ["Tianshan Zhang", "Zeyu Zhang", "Hao Tang"], "title": "DragMesh: Interactive 3D Generation Made Easy", "comment": null, "summary": "While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.", "AI": {"tldr": "DragMesh\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u4ea4\u4e92\u5f0f3D\u5173\u8282\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7684\u52a8\u529b\u5b66\u63a8\u7406\u548c\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u53cc\u56db\u5143\u6570VAE\u548cFiLM\u6761\u4ef6\u6ce8\u5165\uff0c\u5b9e\u73b0\u4e86\u65e2\u7b26\u5408\u7269\u7406\u7ea6\u675f\u53c8\u5b9e\u65f6\u53ef\u7528\u7684\u5173\u8282\u8fd0\u52a8\u751f\u6210\u3002", "motivation": "\u5f53\u524d3D\u5185\u5bb9\u751f\u6210\u65b9\u6cd5\u5728\u9759\u6001\u5185\u5bb9\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7406\u89e3\u7269\u4f53\u5982\u4f55\u79fb\u52a8\u548c\u54cd\u5e94\u4ea4\u4e92\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u5173\u8282\u8fd0\u52a8\u65b9\u6cd5\u8981\u4e48\u7269\u7406\u4e00\u81f4\u4f46\u901f\u5ea6\u592a\u6162\u65e0\u6cd5\u5b9e\u65f6\u4f7f\u7528\uff0c\u8981\u4e48\u751f\u6210\u901f\u5ea6\u5feb\u4f46\u8fdd\u53cd\u57fa\u672c\u8fd0\u52a8\u5b66\u7ea6\u675f\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u7b26\u5408\u7269\u7406\u7ea6\u675f\u53c8\u5b9e\u65f6\u53ef\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u89e3\u8026\u7684\u52a8\u529b\u5b66\u63a8\u7406\u6846\u67b6\uff1a\u5148\u901a\u8fc7\u8bed\u4e49\u610f\u56fe\u63a8\u7406\u786e\u5b9a\u5173\u8282\u7c7b\u578b\uff0c\u518d\u901a\u8fc7KPP-Net\u8fdb\u884c\u51e0\u4f55\u56de\u5f52\u786e\u5b9a\u8f74\u548c\u539f\u70b9\uff1b2. \u5f00\u53d1\u53cc\u56db\u5143\u6570VAE\uff08DQ-VAE\uff09\u5229\u7528\u53cc\u56db\u5143\u6570\u7684\u7d27\u51d1\u3001\u8fde\u7eed\u548c\u65e0\u5947\u5f02\u6027\u7279\u6027\u8868\u793a\u521a\u4f53\u8fd0\u52a8\uff1b3. \u4f7f\u7528FiLM\u6761\u4ef6\u6ce8\u5165\u5728DQ-VAE\u7684\u6bcf\u4e00\u5c42\u6ce8\u5165\u5173\u8282\u5148\u9a8c\uff0c\u786e\u4fdd\u4e25\u683c\u9075\u5faa\u8fd0\u52a8\u5b66\u7ea6\u675f\uff1b4. \u91c7\u7528\u6570\u503c\u7a33\u5b9a\u7684\u53c9\u79ef\u635f\u5931\u4fdd\u8bc1\u8f74\u5bf9\u9f50\u3002", "result": "DragMesh\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u80fd\u591f\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5bf9\u65b0\u9896\u7269\u4f53\u8fdb\u884c\u5408\u7406\u7684\u751f\u6210\u5f0f\u5173\u8282\u8fd0\u52a8\uff0c\u4e3a\u751f\u6210\u5f0f3D\u667a\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4e00\u6b65\u3002", "conclusion": "DragMesh\u901a\u8fc7\u89e3\u8026\u7684\u52a8\u529b\u5b66\u63a8\u7406\u548c\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u56db\u5143\u6570\u8868\u793a\u548c\u6301\u7eed\u7684\u591a\u5c3a\u5ea6\u6307\u5bfc\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5b9e\u65f6\u4ea4\u4e92\u5f0f3D\u5173\u8282\u8fd0\u52a8\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u65e2\u7b26\u5408\u7269\u7406\u7ea6\u675f\u53c8\u5177\u6709\u5b9e\u65f6\u6027\u80fd\u7684\u751f\u6210\u5f0f\u5173\u8282\u8fd0\u52a8\u7cfb\u7edf\u3002"}}
{"id": "2512.06504", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06504", "abs": "https://arxiv.org/abs/2512.06504", "authors": ["Andrii Lysyi", "Anatoliy Sachenko", "Pavlo Radiuk", "Mykola Lysyi", "Oleksandr Melnychenko", "Diana Zahorodnia"], "title": "Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion", "comment": null, "summary": "The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u5149\u4f0f\u57fa\u7840\u8bbe\u65bd\u81ea\u52a8\u68c0\u6d4b\u7684\u667a\u80fd\u96c6\u6210\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u7f3a\u9677\uff0c\u5305\u62ec\u70ed\u8c03\u8272\u677f\u504f\u5dee\u3001\u6570\u636e\u5197\u4f59\u548c\u9ad8\u901a\u4fe1\u5e26\u5bbd\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u5149\u4f0f\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u70ed\u8c03\u8272\u677f\u504f\u5dee\u3001\u6570\u636e\u5197\u4f59\u548c\u9ad8\u901a\u4fe1\u5e26\u5bbd\u9700\u6c42\u7b49\u5173\u952e\u7f3a\u9677\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u6765\u81ea\u52a8\u5316\u6574\u4e2a\u76d1\u6d4b\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u534f\u540c\u67b6\u6784\uff1a1) \u901a\u8fc7\u5f3a\u5236\u8868\u793a\u4e00\u81f4\u6027\u5b66\u4e60\u8c03\u8272\u677f\u4e0d\u53d8\u7684\u70ed\u5d4c\u5165\uff1b2) \u901a\u8fc7\u95e8\u63a7\u673a\u5236\u4e0e\u5bf9\u6bd4\u5ea6\u5f52\u4e00\u5316\u7684RGB\u6d41\u878d\u5408\uff1b3) \u4f7f\u7528\u7f57\u5fb7\u91cc\u683c\u65af\u66f4\u65b0\u7684\u95ed\u73af\u81ea\u9002\u5e94\u91cd\u91c7\u96c6\u63a7\u5236\u5668\uff1b4) \u57fa\u4e8eDBSCAN\u548c\u534a\u6b63\u77e2\u8ddd\u79bb\u7684\u5730\u7406\u7a7a\u95f4\u53bb\u91cd\u6a21\u5757\u3002", "result": "\u5728\u516c\u5f00PVF-10\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52300.903\u7684\u5e73\u5747\u7cbe\u5ea6(mAP@0.5)\uff0c\u6bd4\u5355\u6a21\u6001\u57fa\u7ebf\u63d0\u9ad812-15%\uff1b\u73b0\u573a\u9a8c\u8bc1\u53ec\u56de\u7387\u8fbe96%\uff1b\u53bb\u91cd\u8fc7\u7a0b\u51cf\u5c11\u91cd\u590d\u5f15\u8d77\u7684\u8bef\u62a515-20%\uff1b\u4ec5\u76f8\u5173\u9065\u6d4b\u5c06\u7a7a\u4e2d\u6570\u636e\u4f20\u8f93\u51cf\u5c1160-70%\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4e3b\u52a8\u5f0f\u5149\u4f0f\u68c0\u6d4b\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u65b0\u8303\u5f0f\uff0c\u7cfb\u7edf\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5df2\u901a\u8fc7\u73b0\u573a\u9a8c\u8bc1\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u51c6\u5907\u5ea6\u3002"}}
{"id": "2512.07543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07543", "abs": "https://arxiv.org/abs/2512.07543", "authors": ["Frederic Blum"], "title": "Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects", "comment": "Accepted with minor revisions at *Linguistic Typology*, expected to be fully published in 2026", "summary": "The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.", "AI": {"tldr": "\u8be5\u7814\u7a76\u91cd\u65b0\u68c0\u9a8c\u4e86\u57fa\u672c\u8bcd\u6c47\u4e2d\u8bed\u97f3\u7279\u5f81\u7edf\u8ba1\u8fc7\u8868\u5f81\u73b0\u8c61\uff0c\u53d1\u73b0\u5148\u524d\u5927\u591a\u6570\u5173\u4e8e\u8bed\u97f3\u8c61\u5f81\u6a21\u5f0f\u7684\u7814\u7a76\u7ed3\u679c\u5728\u63a7\u5236\u8bed\u8a00\u95f4\u7684\u8c31\u7cfb\u548c\u5730\u57df\u4f9d\u8d56\u5173\u7cfb\u540e\u4e0d\u518d\u7a33\u5065\uff0c\u4ec5\u6709\u5c11\u6570\u6a21\u5f0f\u4fdd\u6301\u7a33\u5b9a\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u8bed\u8a00\u57fa\u672c\u8bcd\u6c47\u4e2d\u8bed\u97f3\u8c61\u5f81\u6a21\u5f0f\u7684\u7814\u7a76\u53ef\u80fd\u5b58\u5728\u6837\u672c\u504f\u5dee\u3001\u6a21\u578b\u7f3a\u9677\uff0c\u4e14\u672a\u5145\u5206\u63a7\u5236\u8bed\u8a00\u95f4\u7684\u8c31\u7cfb\u548c\u5730\u57df\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u7ed3\u679c\u7a33\u5065\u6027\u5b58\u7591\u3002", "method": "\u4f7f\u7528Lexibank\u6570\u636e\u5e93\u4e2d2864\u79cd\u8bed\u8a00\u7684\u6570\u636e\uff08\u539f\u7814\u7a76\u4e3a245\u79cd\uff09\uff0c\u5728\u539f\u59cb\u6a21\u578b\u57fa\u7840\u4e0a\u589e\u52a0\u7a7a\u95f4\u548c\u8c31\u7cfb\u4f9d\u8d56\u5173\u7cfb\u7684\u7edf\u8ba1\u63a7\u5236\uff0c\u91cd\u65b0\u68c0\u9a8c\u8bed\u97f3\u8c61\u5f81\u6a21\u5f0f\u3002", "result": "\u5927\u591a\u6570\u5148\u524d\u89c2\u5bdf\u5230\u7684\u8bed\u97f3\u8c61\u5f81\u6a21\u5f0f\u5728\u63a7\u5236\u8c31\u7cfb\u548c\u5730\u57df\u56e0\u7d20\u540e\u4e0d\u518d\u7a33\u5065\uff0c\u8bb8\u591a\u6a21\u5f0f\u5b8c\u5168\u6d88\u5931\uff1b\u4ec5\u6709\u5c11\u6570\u6a21\u5f0f\u5728\u65b0\u6837\u672c\u4e2d\u4fdd\u6301\u9ad8\u5ea6\u7a33\u5b9a\u3002", "conclusion": "\u8bed\u97f3\u8c61\u5f81\u73b0\u8c61\u5728\u57fa\u672c\u8bcd\u6c47\u4e2d\u7684\u5206\u5e03\u53ef\u80fd\u6bd4\u5148\u524d\u8ba4\u4e3a\u7684\u66f4\u6709\u9650\uff0c\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u8bed\u8a00\u666e\u904d\u6027\u4e3b\u5f20\u4e2d\u9700\u8981\u8fdb\u884c\u591a\u5c42\u6b21\u7a33\u5065\u6027\u68c0\u9a8c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.07544", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07544", "abs": "https://arxiv.org/abs/2512.07544", "authors": ["Kyungro Lee", "Dongha Choi", "Hyunju Lee"], "title": "MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue", "comment": "18 pages", "summary": "As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.", "AI": {"tldr": "MoCoRP\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4eba\u8bbe\u53e5\u5b50\u4e0e\u56de\u590d\u4e4b\u95f4\u7684NLI\u5173\u7cfb\uff0c\u63d0\u5347\u57fa\u4e8e\u4eba\u8bbe\u5bf9\u8bdd\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4eba\u8bbe\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\u7f3a\u4e4f\u4eba\u8bbe\u53e5\u5b50\u4e0e\u56de\u590d\u4e4b\u95f4\u7684\u663e\u5f0f\u5173\u7cfb\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6355\u6349\u4eba\u8bbe\u4fe1\u606f\uff0c\u5f71\u54cd\u5bf9\u8bdd\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf", "method": "\u63d0\u51faMoCoRP\u6846\u67b6\uff0c\u5229\u7528NLI\u4e13\u5bb6\u663e\u5f0f\u63d0\u53d6\u4eba\u8bbe\u53e5\u5b50\u4e0e\u56de\u590d\u4e4b\u95f4\u7684NLI\u5173\u7cfb\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6709\u6548\u5c06\u9002\u5f53\u7684\u4eba\u8bbe\u4fe1\u606f\u878d\u5165\u56de\u590d\uff1b\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8eBART\u7b49\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5bf9\u9f50\u8c03\u4f18\u6269\u5c55\u5230\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b", "result": "\u5728ConvAI2\u548cMPChat\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cMoCoRP\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u4eba\u8bbe\u4e00\u81f4\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5bf9\u8bdd\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff1b\u4e0d\u4ec5\u5728\u5b9a\u91cf\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5728\u5b9a\u6027\u65b9\u9762\u4e5f\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u4eba\u8bbe-\u56de\u590d\u5173\u7cfb\u5728\u57fa\u4e8e\u4eba\u8bbe\u7684\u5bf9\u8bdd\u4e2d\u662f\u6709\u6548\u7684\uff0cMoCoRP\u6846\u67b6\u80fd\u591f\u751f\u6210\u66f4\u5177\u4e00\u81f4\u6027\u548c\u5438\u5f15\u529b\u7684\u5bf9\u8bdd"}}
{"id": "2512.06562", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06562", "abs": "https://arxiv.org/abs/2512.06562", "authors": ["Dung Thuy Nguyen", "Quang Nguyen", "Preston K. Robinette", "Eli Jiang", "Taylor T. Johnson", "Kevin Leach"], "title": "SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities", "comment": null, "summary": "Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.", "AI": {"tldr": "SUGAR\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u6a21\u578b\u9057\u5fd8\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u6216\u987a\u5e8f\u79fb\u9664\u591a\u4e2a\u8eab\u4efd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "motivation": "\u968f\u77403D\u611f\u77e5\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u80fd\u591f\u5408\u6210\u9ad8\u4fdd\u771f\u7684\u4eba\u7c7b\u8eab\u4efd\u56fe\u50cf\uff0c\u8fd9\u5f15\u53d1\u4e86\u5173\u4e8e\u7528\u6237\u540c\u610f\u548c\u4ece\u6a21\u578b\u8f93\u51fa\u7a7a\u95f4\u4e2d\u79fb\u9664\u7279\u5b9a\u4e2a\u4f53\u7684\u7d27\u8feb\u95ee\u9898\u3002", "method": "SUGAR\u4e3a\u6bcf\u4e2a\u8eab\u4efd\u5b66\u4e60\u4e2a\u6027\u5316\u7684\u66ff\u4ee3\u6f5c\u5728\u8868\u793a\uff0c\u5c06\u91cd\u5efa\u7ed3\u679c\u8f6c\u79fb\u5230\u89c6\u89c9\u4e0a\u8fde\u8d2f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u800c\u4e0d\u662f\u5c06\u4e0d\u9700\u8981\u7684\u8eab\u4efd\u6295\u5c04\u5230\u4e0d\u73b0\u5b9e\u7684\u8f93\u51fa\u6216\u4f9d\u8d56\u9759\u6001\u6a21\u677f\u9762\u90e8\u3002\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u6301\u7eed\u6548\u7528\u4fdd\u62a4\u76ee\u6807\uff0c\u9632\u6b62\u968f\u7740\u66f4\u591a\u8eab\u4efd\u88ab\u9057\u5fd8\u800c\u5bfc\u81f4\u7684\u6027\u80fd\u9000\u5316\u3002", "result": "SUGAR\u5728\u79fb\u9664\u591a\u8fbe200\u4e2a\u8eab\u4efd\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u4fdd\u7559\u6548\u7528\u65b9\u9762\u63d0\u4f9b\u4e86\u9ad8\u8fbe700%\u7684\u6539\u8fdb\u3002", "conclusion": "SUGAR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u751f\u6210\u9057\u5fd8\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u79fb\u9664\u591a\u4e2a\u8eab\u4efd\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u6a21\u578b\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u6a21\u578b\u4e2d\u7528\u6237\u540c\u610f\u548c\u8eab\u4efd\u79fb\u9664\u7684\u91cd\u8981\u95ee\u9898\u3002"}}
{"id": "2512.07571", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.07571", "abs": "https://arxiv.org/abs/2512.07571", "authors": ["Nicolas Calbucura", "Valentin Barriere"], "title": "A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification", "comment": null, "summary": "This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u5c06\u8bed\u97f3\u4fe1\u606f\u6709\u6548\u96c6\u6210\u5230\u6587\u672c\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u7528\u4e8e\u7279\u5b9a\u5206\u7c7b\u4efb\u52a1\uff0c\u5728\u8bba\u8fa9\u8c2c\u8bef\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA", "motivation": "\u89e3\u51b3\u97f3\u9891\u5e8f\u5217\u957f\u5ea6\u8fdc\u5927\u4e8e\u6587\u672c\u5e8f\u5217\u5bfc\u81f4\u7684\u878d\u5408\u56f0\u96be\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u8bed\u97f3\u5206\u8bcd\u5668\u8f93\u51fa\u957f\u5e8f\u5217\u96be\u4ee5\u4f4e\u6210\u672c\u96c6\u6210\u5230\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6311\u6218", "method": "\u4f7f\u7528\u57fa\u4e8elasso\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u5728\u591a\u6a21\u6001\u8bcd\u888b\u8868\u793a\u4e2d\u4fdd\u7559\u6700\u91cd\u8981\u7684\u97f3\u9891token\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\u4f7f\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u8fd9\u4e9btoken\uff0c\u7136\u540e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03", "result": "\u76f8\u6bd4\u5355\u6a21\u6001\u6a21\u578b\u3001\u66f4\u5927\u7684SpeechLM\u6a21\u578b\u6216\u901a\u8fc7\u5b66\u4e60\u8868\u793a\u96c6\u6210\u97f3\u9891\u7684\u65b9\u6cd5\uff0c\u6027\u80fd\u5747\u6709\u63d0\u5347\uff1b\u5728\u8bba\u8fa9\u8c2c\u8bef\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230state-of-the-art\u7ed3\u679c", "conclusion": "\u63d0\u51fa\u7684\u7b80\u5355\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u6587\u672c\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5373\u4f7f\u968f\u673a\u9009\u62e9\u97f3\u9891token\u4e5f\u80fd\u63d0\u5347\u5355\u6a21\u6001\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u878d\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.06447", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06447", "abs": "https://arxiv.org/abs/2512.06447", "authors": ["Jiuyi Chen", "Mingkui Tan", "Haifeng Lu", "Qiuna Xu", "Zhihua Wang", "Runhao Zeng", "Xiping Hu"], "title": "Towards Stable Cross-Domain Depression Recognition under Missing Modalities", "comment": null, "summary": "Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a33\u5b9a\u8de8\u57df\u6291\u90c1\u8bc6\u522b\u6846\u67b6SCD-MLLM\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u3001\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u5f31\u3001\u5bf9\u7f3a\u5931\u6a21\u6001\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u6291\u90c1\u75c7\u7b5b\u67e5\u5177\u6709\u91cd\u8981\u516c\u5171\u536b\u751f\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u591a\u6a21\u6001\u81ea\u52a8\u6291\u90c1\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7684\u901a\u7528\u6846\u67b6\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u6291\u90c1\u8bc6\u522b\u573a\u666f\uff0c\u4e14\u5bf9\u73b0\u5b9e\u6570\u636e\u4e2d\u5e38\u89c1\u7684\u7f3a\u5931\u6a21\u6001\u7f3a\u4e4f\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faSCD-MLLM\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u591a\u6e90\u6570\u636e\u8f93\u5165\u9002\u914d\u5668(MDIA)\uff0c\u4f7f\u7528\u63a9\u7801\u673a\u5236\u548c\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u5c06\u5f02\u6784\u6570\u636e\u8f6c\u6362\u4e3a\u7edf\u4e00\u6807\u8bb0\u5e8f\u5217\uff1b2) \u6a21\u6001\u611f\u77e5\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757(MAFM)\uff0c\u901a\u8fc7\u5171\u4eab\u6295\u5f71\u673a\u5236\u81ea\u9002\u5e94\u878d\u5408\u89c6\u542c\u7279\u5f81\uff0c\u589e\u5f3a\u5bf9\u7f3a\u5931\u6a21\u6001\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u6291\u90c1\u6570\u636e\u96c6(CMDC\u3001AVEC2014\u3001DAIC-WOZ\u3001DVlog\u3001EATD)\u4e0a\u8fdb\u884c\u591a\u6570\u636e\u96c6\u8054\u5408\u8bad\u7ec3\u5b9e\u9a8c\uff0c\u5728\u5b8c\u6574\u548c\u90e8\u5206\u6a21\u6001\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709SOTA\u6a21\u578b\u53ca\u5546\u4e1aLLM(Gemini\u548cGPT)\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3001\u591a\u6a21\u6001\u6291\u90c1\u7ebf\u7d22\u6355\u6349\u80fd\u529b\u548c\u7f3a\u5931\u6a21\u6001\u7a33\u5b9a\u6027\u3002", "conclusion": "SCD-MLLM\u4e3a\u591a\u6a21\u6001\u6291\u90c1\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u7a33\u5b9a\u4e14\u53ef\u6cdb\u5316\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5f02\u6784\u6570\u636e\u548c\u7f3a\u5931\u6a21\u6001\u95ee\u9898\uff0c\u5728\u8de8\u57df\u6291\u90c1\u8bc6\u522b\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.06642", "categories": ["cs.CV", "astro-ph.CO", "astro-ph.IM", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06642", "abs": "https://arxiv.org/abs/2512.06642", "authors": ["Achmad Ardani Prasha", "Clavino Ourizqi Rachmadi", "Muhamad Fauzan Ibnu Syahlan", "Naufal Rahfi Anugerah", "Nanda Garin Raditya", "Putri Amelia", "Sabrina Laila Mutiara", "Hilman Syachr Ramadhan"], "title": "Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution", "comment": "21 pages, 7 figures, 3 table", "summary": "Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.", "AI": {"tldr": "\u4f7f\u7528\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u5728\u5f3a\u5f15\u529b\u900f\u955c\u6a21\u62df\u56fe\u50cf\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u8868\u793a\uff0c\u7528\u4e8e\u6697\u7269\u8d28\u6a21\u578b\u5206\u7c7b\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u3002", "motivation": "\u5f3a\u5f15\u529b\u900f\u955c\u53ef\u4ee5\u63ed\u793a\u6697\u7269\u8d28\u4e9a\u7ed3\u6784\u7684\u5f71\u54cd\uff0c\u4f46\u4ece\u566a\u58f0\u5927\u3001\u5206\u8fa8\u7387\u4f4e\u7684\u56fe\u50cf\u4e2d\u5206\u6790\u8fd9\u4e9b\u6548\u5e94\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u7269\u7406\u4e30\u5bcc\u7684\u6a21\u62df\u6570\u636e\u4e2d\u5b66\u4e60\u901a\u7528\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eDeepLense ML4SCI\u57fa\u51c6\u7684\u6a21\u62df\u5f3a\u900f\u955c\u56fe\u50cf\u7684MAE\u9884\u8bad\u7ec3\u7b56\u7565\u3002\u4f7f\u7528Vision Transformer\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u76ee\u6807\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u4e3a\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u5206\u522b\u5fae\u8c03\u7f16\u7801\u5668\uff1a1\uff09\u6697\u7269\u8d28\u6a21\u578b\u5206\u7c7b\uff08\u51b7\u6697\u7269\u8d28\u3001\u8f74\u5b50\u6837\u6216\u65e0\u4e9a\u7ed3\u6784\uff09\uff1b2\uff09\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0816\u00d716\u523064\u00d764\uff09\u3002", "result": "\u572890%\u63a9\u7801\u7387\u4e0b\uff0c\u5fae\u8c03\u5206\u7c7b\u5668\u8fbe\u5230\u5b8f\u89c2AUC 0.968\u548c\u51c6\u786e\u738788.65%\uff0c\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684\u57fa\u7ebf\uff08AUC 0.957\uff0c\u51c6\u786e\u738782.46%\uff09\u3002\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\uff0cMAE\u9884\u8bad\u7ec3\u6a21\u578b\u91cd\u5efa\u56fe\u50cf\u7684PSNR\u7ea633 dB\uff0cSSIM 0.961\uff0c\u7565\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u3002\u63a9\u7801\u7387\u5206\u6790\u663e\u793a\uff1a\u66f4\u9ad8\u63a9\u7801\u7387\u6539\u5584\u5206\u7c7b\u4f46\u8f7b\u5fae\u964d\u4f4e\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "conclusion": "MAE\u9884\u8bad\u7ec3\u5728\u7269\u7406\u4e30\u5bcc\u7684\u6a21\u62df\u6570\u636e\u4e0a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u91cd\u7528\u7684\u7f16\u7801\u5668\uff0c\u9002\u7528\u4e8e\u591a\u4e2a\u5f3a\u900f\u955c\u5206\u6790\u4efb\u52a1\uff0c\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2512.07583", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07583", "abs": "https://arxiv.org/abs/2512.07583", "authors": ["Navid Asgari", "Benjamin M. Cole"], "title": "Complementary Learning Approach for Text Classification using Large Language Models", "comment": "67 pages", "summary": "In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6210\u672c\u6548\u76ca\u7814\u7a76\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b\u4e0e\u673a\u5668\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u548c\u5c11\u6837\u672c\u5b66\u4e60\u6765\u6539\u5584\u4eba\u673a\u534f\u4f5c", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7814\u7a76\u4e2d\u5e94\u7528\u6210\u672c\u8f83\u9ad8\u4e14\u5b58\u5728\u56fa\u6709\u5f31\u70b9\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u53d1\u6325\u5b66\u8005\u548c\u673a\u5668\u5404\u81ea\u4f18\u52bf\uff0c\u53c8\u80fd\u5f25\u8865\u5404\u81ea\u4e0d\u8db3\u7684\u6210\u672c\u6548\u76ca\u65b9\u6cd5", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u65b9\u6cd5\u8bba\uff0c\u7ed3\u5408\u601d\u7ef4\u94fe\u548c\u5c11\u6837\u672c\u5b66\u4e60\u63d0\u793a\u6280\u672f\uff0c\u5c06\u5b9a\u6027\u7814\u7a76\u4e2d\u5408\u4f5c\u56e2\u961f\u7684\u6700\u4f73\u5b9e\u8df5\u6269\u5c55\u5230\u5b9a\u91cf\u7814\u7a76\u7684\u4eba\u673a\u534f\u4f5c\u4e2d\uff0c\u8ba9\u4eba\u7c7b\u80fd\u591f\u8fd0\u7528\u6eaf\u56e0\u63a8\u7406\u548c\u81ea\u7136\u8bed\u8a00\u6765\u5ba1\u67e5\u673a\u5668\u548c\u4eba\u7c7b\u7684\u5de5\u4f5c", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7ba1\u7406LLMs\u7684\u56fa\u6709\u5f31\u70b9\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u7814\u7a76\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5206\u67901990-2017\u5e74\u95f41,934\u4efd\u5236\u836f\u8054\u76df\u65b0\u95fb\u7a3f\u4e2d\u7684\u4eba\u673a\u8bc4\u5206\u5dee\u5f02", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u4eba\u673a\u534f\u4f5c\u7814\u7a76\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u65b9\u6cd5\u8bba\u4f7f\u5b66\u8005\u80fd\u591f\u6709\u6548\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u7ba1\u7406\u5176\u56fa\u6709\u5f31\u70b9\uff0c\u4e3a\u5b9a\u91cf\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u4eba\u673a\u534f\u4f5c\u8303\u5f0f"}}
{"id": "2512.07608", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07608", "abs": "https://arxiv.org/abs/2512.07608", "authors": ["Jing Wang", "Jie Shen", "Xing Niu", "Tong Zhang", "Jeremy Weiss"], "title": "Metric-Fair Prompting: Treating Similar Samples Similarly", "comment": null, "summary": "We introduce \\emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \\emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \\((\\text{question}, \\text{option})\\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.", "AI": {"tldr": "\u63d0\u51faMetric-Fair Prompting\u6846\u67b6\uff0c\u901a\u8fc7\u5ea6\u91cf\u516c\u5e73\u6027\u7ea6\u675f\u6307\u5bfcLLM\u5728\u533b\u5b66\u591a\u9009\u9898\u4e2d\u505a\u51fa\u51b3\u7b56\uff0c\u5229\u7528\u76f8\u4f3c\u95ee\u9898\u8054\u5408\u5904\u7406\u63d0\u5347\u4e2a\u4f53\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027", "motivation": "\u5728\u9ad8\u98ce\u9669\u7684\u533b\u5b66\u591a\u9009\u9898\u56de\u7b54\u4e2d\uff0c\u9700\u8981\u786e\u4fdd\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u7684\u516c\u5e73\u6027\uff0c\u7279\u522b\u662f\u4fdd\u8bc1\u76f8\u4f3c\u95ee\u9898\u5f97\u5230\u76f8\u4f3c\u5904\u7406\uff08\u4e2a\u4f53\u516c\u5e73\u6027\uff09\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u51c6\u786e\u6027", "method": "1) \u5c06\u6bcf\u4e2a(\u95ee\u9898,\u9009\u9879)\u5bf9\u89c6\u4e3a\u4e8c\u5143\u5b9e\u4f8b\uff1b2) \u4f7f\u7528NLP\u5d4c\u5165\u8ba1\u7b97\u95ee\u9898\u76f8\u4f3c\u5ea6\uff1b3) \u5728\u76f8\u4f3c\u95ee\u9898\u7684\u8054\u5408\u5bf9\u4e2d\u800c\u975e\u5b64\u7acb\u5730\u89e3\u51b3\u95ee\u9898\uff1b4) \u8bbe\u8ba1\u63d0\u793a\u5f3a\u5236\u5168\u5c40\u51b3\u7b56\u534f\u8bae\uff1a\u63d0\u53d6\u5173\u952e\u4e34\u5e8a\u7279\u5f81\uff0c\u5c06\u6bcf\u4e2a(\u95ee\u9898,\u9009\u9879)\u6620\u5c04\u5230\u7f6e\u4fe1\u5ea6\u5206\u6570f(x)\uff0c\u5e76\u65bd\u52a0Lipschitz\u5f0f\u7ea6\u675f\u786e\u4fdd\u76f8\u4f3c\u8f93\u5165\u83b7\u5f97\u76f8\u4f3c\u5206\u6570", "result": "\u5728MedQA(US)\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMetric-Fair Prompting\u76f8\u6bd4\u6807\u51c6\u5355\u9879\u76ee\u63d0\u793a\u65b9\u6cd5\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u8868\u660e\u516c\u5e73\u6027\u5f15\u5bfc\u3001\u7f6e\u4fe1\u5ea6\u5bfc\u5411\u7684\u63a8\u7406\u80fd\u591f\u589e\u5f3aLLM\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u591a\u9009\u9898\u4e0a\u7684\u51c6\u786e\u6027", "conclusion": "\u63d0\u51fa\u7684\u5ea6\u91cf\u516c\u5e73\u6027\u63d0\u793a\u6846\u67b6\u901a\u8fc7\u5f3a\u5236\u76f8\u4f3c\u95ee\u9898\u83b7\u5f97\u4e00\u81f4\u5904\u7406\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u7684\u516c\u5e73\u6027\uff0c\u8fd8\u610f\u5916\u5730\u63d0\u9ad8\u4e86\u5728\u533b\u5b66\u591a\u9009\u9898\u4e0a\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2512.07612", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07612", "abs": "https://arxiv.org/abs/2512.07612", "authors": ["Kairong Luo", "Zhenbo Sun", "Xinyu Shi", "Shengqi Chen", "Bowen Yu", "Yunyi Chen", "Chenyi Dang", "Hengtao Tao", "Hui Wang", "Fangming Liu", "Kaifeng Lyu", "Wenguang Chen"], "title": "PCMind-2.1-Kaiyuan-2B Technical Report", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.", "AI": {"tldr": "PCMind-2.1-Kaiyuan-2B\u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u768420\u4ebf\u53c2\u6570\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u5f00\u6e90\u793e\u533a\u4e0e\u884c\u4e1a\u4e4b\u95f4\u7684\u77e5\u8bc6\u5dee\u8ddd\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u6df7\u5408\u7b56\u7565\u3001\u9009\u62e9\u6027\u91cd\u590d\u8bad\u7ec3\u548c\u591a\u9886\u57df\u8bfe\u7a0b\u8bad\u7ec3\u7b49\u65b9\u6cd5\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u5f00\u6e90\u793e\u533a\u4e0e\u884c\u4e1a\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u77e5\u8bc6\u5dee\u8ddd\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u884c\u4e1a\u4f9d\u8d56\u95ed\u6e90\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u9700\u8981\u5f00\u53d1\u5b8c\u5168\u5f00\u6e90\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u8d44\u6e90\u6709\u9650\u7684\u56e2\u961f\u4e5f\u80fd\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\u3002", "method": "1. \u5206\u4f4d\u6570\u6570\u636e\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff1a\u7cfb\u7edf\u6bd4\u8f83\u5f02\u6784\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u4e3a\u6570\u636e\u6df7\u5408\u7b56\u7565\u63d0\u4f9b\u89c1\u89e3\uff1b2. \u6218\u7565\u9009\u62e9\u6027\u91cd\u590d\u65b9\u6848\uff1a\u5728\u591a\u9636\u6bb5\u8303\u5f0f\u4e2d\u6709\u6548\u5229\u7528\u7a00\u758f\u7684\u9ad8\u8d28\u91cf\u6570\u636e\uff1b3. \u591a\u9886\u57df\u8bfe\u7a0b\u8bad\u7ec3\u7b56\u7565\uff1a\u6309\u8d28\u91cf\u6392\u5e8f\u6837\u672c\u3002\u6b64\u5916\u8fd8\u5305\u62ec\u9ad8\u5ea6\u4f18\u5316\u7684\u6570\u636e\u9884\u5904\u7406\u6d41\u7a0b\u548c\u9488\u5bf9FP16\u7a33\u5b9a\u6027\u7684\u67b6\u6784\u4fee\u6539\u3002", "result": "Kaiyuan-2B\u5728\u6027\u80fd\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u5b8c\u5168\u5f00\u6e90\u6a21\u578b\u7ade\u4e89\uff0c\u8bc1\u660e\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u9884\u8bad\u7ec3\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u6240\u6709\u8d44\u4ea7\uff08\u5305\u62ec\u6a21\u578b\u6743\u91cd\u3001\u6570\u636e\u548c\u4ee3\u7801\uff09\u90fd\u5728Apache 2.0\u8bb8\u53ef\u4e0b\u53d1\u5e03\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u7ba1\u7406\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u4f7f\u5f00\u6e90\u793e\u533a\u80fd\u591f\u5f00\u53d1\u51fa\u4e0e\u884c\u4e1a\u6a21\u578b\u7ade\u4e89\u7684\u9ad8\u8d28\u91cf\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2512.06726", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06726", "abs": "https://arxiv.org/abs/2512.06726", "authors": ["Shuo Li", "Jiajun Sun", "Zhihao Zhang", "Xiaoran Fan", "Senjie Jin", "Hui Li", "Yuming Yang", "Junjie Ye", "Lixing Shen", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "The Role of Entropy in Visual Grounding: Analysis and Optimization", "comment": null, "summary": "Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faECVGPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u71b5\u63a7\u5236\u4f18\u5316\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6743\u8861", "motivation": "\u867d\u7136\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u71b5\u5728\u89c6\u89c9\u5b9a\u4f4d\u7b49\u611f\u77e5\u5bfc\u5411\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\u548c\u7279\u6027\uff0c\u4ee5\u53ca\u6709\u6548\u63a7\u5236\u7b56\u7565\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22", "method": "\u63d0\u51faECVGPO\uff08\u71b5\u63a7\u5236\u89c6\u89c9\u5b9a\u4f4d\u7b56\u7565\u4f18\u5316\uff09\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u71b5\u8c03\u8282\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u71b5\u7684\u7279\u6027\uff0c\u8bbe\u8ba1\u6709\u6548\u7684\u71b5\u63a7\u5236\u7b56\u7565", "result": "\u5b9e\u9a8c\u8868\u660eECVGPO\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\u90fd\u53d6\u5f97\u4e86\u5e7f\u6cdb\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "\u901a\u8fc7\u71b5\u63a7\u5236\u53ef\u4ee5\u66f4\u597d\u5730\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6743\u8861\uff0cECVGPO\u7b97\u6cd5\u4e3a\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u71b5\u8c03\u8282\u65b9\u6cd5"}}
{"id": "2512.07666", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07666", "abs": "https://arxiv.org/abs/2512.07666", "authors": ["Zeqi Chen", "Zhaoyang Chu", "Yi Gui", "Feng Guo", "Yao Wan", "Chuan Shi"], "title": "Bridging Code Graphs and Large Language Models for Better Code Understanding", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.", "AI": {"tldr": "CGBridge\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u5916\u90e8\u53ef\u8bad\u7ec3\u7684\u6865\u63a5\u6a21\u5757\u5c06\u4ee3\u7801\u56fe\u4fe1\u606f\u6ce8\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u4ee3\u7801\u7406\u89e3\u7684\u7ed3\u6784\u8bed\u4e49\u80fd\u529b\uff0c\u5728\u4ee3\u7801\u6458\u8981\u548c\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u6a21\u578b\u548c\u56fe\u589e\u5f3a\u63d0\u793a\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f9d\u8d56\u7ebf\u6027\u5316token\u5e8f\u5217\u9650\u5236\u4e86\u5176\u5bf9\u7a0b\u5e8f\u7ed3\u6784\u8bed\u4e49\u7684\u7406\u89e3\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u53d7\u63d0\u793a\u957f\u5ea6\u9650\u5236\uff0c\u8981\u4e48\u9700\u8981\u7279\u5b9a\u4efb\u52a1\u67b6\u6784\u8c03\u6574\uff0c\u4e0d\u517c\u5bb9\u5927\u89c4\u6a21\u6307\u4ee4\u8ddf\u968fLLM\u3002", "method": "\u63d0\u51faCGBridge\u65b9\u6cd5\uff1a1) \u572827\u4e07\u4ee3\u7801\u56fe\u6570\u636e\u96c6\u4e0a\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4ee3\u7801\u56fe\u7f16\u7801\u5668\u5b66\u4e60\u7ed3\u6784\u8bed\u4e49\uff1b2) \u8bad\u7ec3\u5916\u90e8\u6865\u63a5\u6a21\u5757\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5bf9\u9f50\u4ee3\u7801\u3001\u56fe\u548c\u6587\u672c\u8bed\u4e49\uff1b3) \u6865\u63a5\u6a21\u5757\u751f\u6210\u7ed3\u6784\u611f\u77e5\u63d0\u793a\u6ce8\u5165\u51bb\u7ed3LLM\uff0c\u5fae\u8c03\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u5728\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u4e0a\u76f8\u5bf9\u63d0\u534716.19%\u548c9.12%\uff0c\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u76f8\u5bf9\u63d0\u53479.84%\u548c38.87%\u3002\u63a8\u7406\u901f\u5ea6\u6bd4LoRA\u8c03\u4f18\u6a21\u578b\u5feb4\u500d\u4ee5\u4e0a\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u7ed3\u6784\u611f\u77e5\u4ee3\u7801\u7406\u89e3\u3002", "conclusion": "CGBridge\u901a\u8fc7\u5916\u90e8\u6865\u63a5\u6a21\u5757\u6709\u6548\u589e\u5f3aLLM\u7684\u7ed3\u6784\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u4e0e\u5927\u89c4\u6a21\u6307\u4ee4\u8ddf\u968fLLM\u517c\u5bb9\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u6027\u80fd\uff0c\u517c\u5177\u6548\u679c\u548c\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2512.06530", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06530", "abs": "https://arxiv.org/abs/2512.06530", "authors": ["Mohammed Wattad", "Tamir Shor", "Alex Bronstein"], "title": "On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization", "comment": null, "summary": "Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.", "AI": {"tldr": "\u5b66\u4e60\u578bk\u7a7a\u95f4\u91c7\u6837\u6a21\u5f0f\u5728\u8de8\u57dfMRI\u91cd\u5efa\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u5f15\u5165\u91c7\u96c6\u4e0d\u786e\u5b9a\u6027\u589e\u5f3a\u57df\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u5173\u6ce8\u9488\u5bf9\u5355\u4e00\u6570\u636e\u96c6\u6216\u6a21\u6001\u4f18\u5316\u7684\u91c7\u96c6\u6a21\u5f0f\uff0c\u7f3a\u4e4f\u5bf9\u8de8\u6210\u50cf\u57df\u53ef\u8fc1\u79fb\u6027\u7684\u8003\u8651\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5b66\u4e60\u578bk\u7a7a\u95f4\u91c7\u6837\u5728\u57df\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u589e\u5f3a\u57df\u9c81\u68d2\u6027\u7684\u65b0\u65b9\u6cd5\uff1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u91c7\u96c6\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u968f\u673a\u6270\u52a8k\u7a7a\u95f4\u8f68\u8ff9\u6765\u6a21\u62df\u4e0d\u540c\u626b\u63cf\u4eea\u548c\u6210\u50cf\u6761\u4ef6\u7684\u53d8\u5f02\u6027\u3002", "result": "\u901a\u8fc7\u8de8\u6570\u636e\u96c6\u548c\u91c7\u96c6\u8303\u5f0f\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u8868\u660e\u4f7f\u7528\u5b66\u4e60\u578b\u91c7\u6837\u6a21\u5f0f\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u8de8\u57df\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "k\u7a7a\u95f4\u8f68\u8ff9\u8bbe\u8ba1\u4e0d\u4ec5\u662f\u52a0\u901f\u673a\u5236\uff0c\u66f4\u662f\u63d0\u9ad8MRI\u91cd\u5efa\u4e2d\u57df\u6cdb\u5316\u80fd\u529b\u7684\u4e3b\u52a8\u81ea\u7531\u5ea6\uff0c\u5b66\u4e60\u578b\u91c7\u6837\u6a21\u5f0f\u5177\u6709\u8de8\u57df\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.07687", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07687", "abs": "https://arxiv.org/abs/2512.07687", "authors": ["Sujoy Nath", "Arkaprabha Basu", "Sharanya Dasgupta", "Swagatam Das"], "title": "HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \\textsc{\\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHalluShift++\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5c42\u52a8\u6001\u4e2d\u7684\u53ef\u6d4b\u91cf\u5f02\u5e38\u6765\u68c0\u6d4b\u5e7b\u89c9\u95ee\u9898\uff0c\u5c06\u5e7b\u89c9\u68c0\u6d4b\u4ece\u6587\u672cLLMs\u6269\u5c55\u5230\u591a\u6a21\u6001\u573a\u666f\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u4e0e\u89c6\u89c9\u5185\u5bb9\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u7684\u5e7b\u89c9\u63cf\u8ff0\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5916\u90e8LLM\u8bc4\u4f30\u5668\uff0c\u4f46\u8fd9\u4e9b\u8bc4\u4f30\u5668\u672c\u8eab\u4e5f\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u4e14\u5b58\u5728\u9886\u57df\u9002\u5e94\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5047\u8bbe\u8ba4\u4e3a\u5e7b\u89c9\u8868\u73b0\u4e3aMLLMs\u5185\u90e8\u5c42\u52a8\u6001\u4e2d\u7684\u53ef\u6d4b\u91cf\u5f02\u5e38\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5206\u5e03\u504f\u79fb\u3002\u901a\u8fc7\u5c42\u95f4\u5206\u6790\u7279\u5b9a\u5047\u8bbe\uff0cHalluShift++\u5c06\u5e7b\u89c9\u68c0\u6d4b\u4ece\u6587\u672cLLMs\u6269\u5c55\u5230\u591a\u6a21\u6001\u573a\u666f\u3002", "result": "HalluShift++\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u5728GitHub\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790MLLMs\u5185\u90e8\u5c42\u52a8\u6001\u5f02\u5e38\u6765\u68c0\u6d4b\u5e7b\u89c9\u662f\u6709\u6548\u7684\uff0cHalluShift++\u4e3a\u591a\u6a21\u6001\u573a\u666f\u4e0b\u7684\u5e7b\u89c9\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u4f9d\u8d56\u5916\u90e8LLM\u8bc4\u4f30\u5668\u7684\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2512.06746", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06746", "abs": "https://arxiv.org/abs/2512.06746", "authors": ["Ruoxin Chen", "Jiahui Gao", "Kaiqing Lin", "Keyue Zhang", "Yandan Zhao", "Isabel Guan", "Taiping Yao", "Shouhong Ding"], "title": "Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection", "comment": null, "summary": "Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAlignGemini\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u4efb\u52a1-\u6a21\u578b\u5bf9\u9f50\u539f\u5219\u5c06AIGI\u68c0\u6d4b\u5206\u4e3a\u8bed\u4e49\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u50cf\u7d20\u4f2a\u5f71\u68c0\u6d4b\u4e24\u4e2a\u4e92\u8865\u4efb\u52a1\uff0c\u5206\u522b\u7528VLM\u548c\u50cf\u7d20\u4f2a\u5f71\u4e13\u5bb6\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8d44\u6e90\u4e14\u5b58\u5728\u4e25\u91cd\u5e7b\u89c9\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0VLM\u5bf9\u8bed\u4e49\u654f\u611f\u4f46\u5bf9\u50cf\u7d20\u4f2a\u5f71\u4e0d\u654f\u611f\uff0c\u800c\u4f20\u7edf\u50cf\u7d20\u4f2a\u5f71\u68c0\u6d4b\u5668\u7f3a\u4e4f\u8bed\u4e49\u610f\u8bc6\uff0c\u4efb\u52a1\u4e0e\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u9519\u914d\u3002", "method": "\u63d0\u51fa\u4efb\u52a1-\u6a21\u578b\u5bf9\u9f50\u539f\u5219\uff0c\u5c06AIGI\u68c0\u6d4b\u5f62\u5f0f\u5316\u4e3a\u4e24\u4e2a\u4e92\u8865\u4efb\u52a1\uff1a\u8bed\u4e49\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u50cf\u7d20\u4f2a\u5f71\u68c0\u6d4b\u3002\u5b9e\u73b0\u4e3aAlignGemini\u53cc\u5206\u652f\u68c0\u6d4b\u5668\uff0c\u4e00\u4e2a\u5206\u652f\u4f7f\u7528\u7eaf\u8bed\u4e49\u76d1\u7763\u5fae\u8c03VLM\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u4f7f\u7528\u7eaf\u50cf\u7d20\u4f2a\u5f71\u76d1\u7763\u8bad\u7ec3\u50cf\u7d20\u4f2a\u5f71\u4e13\u5bb6\u3002", "result": "\u5728\u4e94\u4e2a\u91ce\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAlignGemini\u5b9e\u73b0\u4e86\u5e73\u5747\u51c6\u786e\u7387+9.5%\u7684\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u4efb\u52a1-\u6a21\u578b\u5bf9\u9f50\u539f\u5219\u5728\u53ef\u6cdb\u5316AIGI\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4efb\u52a1-\u6a21\u578b\u5bf9\u9f50\u662f\u63d0\u5347AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u6cdb\u5316\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\uff0c\u901a\u8fc7\u5c06\u68c0\u6d4b\u5206\u89e3\u4e3a\u4e92\u8865\u7684\u8bed\u4e49\u548c\u50cf\u7d20\u4f2a\u5f71\u4efb\u52a1\uff0c\u5e76\u5206\u522b\u4f7f\u7528\u6700\u9002\u5408\u7684\u6a21\u578b\u5904\u7406\uff0c\u53ef\u4ee5\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.07694", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07694", "abs": "https://arxiv.org/abs/2512.07694", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla", "Elena Hadjicosta"], "title": "Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map", "comment": "12 pages, 4 figures", "summary": "In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.", "AI": {"tldr": "SafeTerm\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u7ef4\u5411\u91cf\u7a7a\u95f4\u5d4c\u5165\u533b\u5b66\u67e5\u8be2\u672f\u8bed\u548cMedDRA\u9996\u9009\u672f\u8bed\uff0c\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u6781\u503c\u805a\u7c7b\u81ea\u52a8\u751f\u6210\u76f8\u5173\u672f\u8bed\u7684\u6392\u540d\u5217\u8868\uff0c\u4e3a\u836f\u7269\u5b89\u5168\u5ba1\u67e5\u4e2d\u7684MedDRA\u67e5\u8be2\u751f\u6210\u63d0\u4f9b\u8865\u5145\u65b9\u6cd5\u3002", "motivation": "\u5728\u836f\u7269\u4e0a\u5e02\u524d\u5b89\u5168\u5ba1\u67e5\u4e2d\uff0c\u5c06\u76f8\u5173\u4e0d\u826f\u4e8b\u4ef6\u672f\u8bed\u5206\u7ec4\u5230\u6807\u51c6\u5316MedDRA\u67e5\u8be2\u6216FDA OCMQs\u5bf9\u4e8e\u4fe1\u53f7\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u64cd\u4f5c\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u7cfb\u7edf\u6765\u6539\u8fdb\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "SafeTerm\u7cfb\u7edf\u5c06\u533b\u5b66\u67e5\u8be2\u672f\u8bed\u548cMedDRA\u9996\u9009\u672f\u8bed\u5d4c\u5165\u591a\u7ef4\u5411\u91cf\u7a7a\u95f4\uff0c\u7136\u540e\u5e94\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u6781\u503c\u805a\u7c7b\u65b9\u6cd5\uff0c\u6839\u636e\u76f8\u5173\u6027\u8bc4\u5206\u751f\u6210\u6392\u540d\u5217\u8868\u3002\u7cfb\u7edf\u4f7f\u7528\u591a\u6807\u51c6\u7edf\u8ba1\u65b9\u6cd5\u5bf9\u672f\u8bed\u8fdb\u884c\u6392\u540d\u3002", "result": "\u5728FDA OCMQ v3.0\uff08104\u4e2a\u67e5\u8be2\uff09\u4e0a\u7684\u9a8c\u8bc1\u663e\u793a\uff0c\u5728\u4e2d\u7b49\u9608\u503c\u4e0b\u53ec\u56de\u7387>95%\uff0c\u8f83\u9ad8\u9608\u503c\u4e0b\u7cbe\u786e\u5ea6\u53ef\u8fbe86%\u3002\u6700\u4f18\u9608\u503c\uff08~0.70-0.75\uff09\u4e0b\u53ec\u56de\u7387\u7ea650%\uff0c\u7cbe\u786e\u5ea6\u7ea633%\u3002\u7a84\u672f\u8bed\u5b50\u96c6\u8868\u73b0\u76f8\u4f3c\u4f46\u9700\u8981\u7a0d\u9ad8\u7684\u76f8\u4f3c\u5ea6\u9608\u503c\u3002", "conclusion": "SafeTerm AI\u9a71\u52a8\u7cfb\u7edf\u4e3a\u81ea\u52a8\u5316MedDRA\u67e5\u8be2\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8865\u5145\u65b9\u6cd5\u3002\u5efa\u8bae\u521d\u59cb\u4f7f\u7528~0.60\u7684\u76f8\u4f3c\u5ea6\u9608\u503c\uff0c\u5bf9\u4e8e\u7cbe\u70bc\u672f\u8bed\u9009\u62e9\u53ef\u589e\u52a0\u9608\u503c\u3002"}}
{"id": "2512.07777", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07777", "abs": "https://arxiv.org/abs/2512.07777", "authors": ["Karin de Langis", "P\u00fcren \u00d6ncel", "Ryan Peters", "Andrew Elfenbein", "Laura Kristen Allen", "Andreas Schramm", "Dongyeop Kang"], "title": "Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?", "comment": null, "summary": "Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u80fd\u591f\u901a\u8fc7\u5185\u90e8\u8868\u5f81\u8bc6\u522b\u4e0d\u8fde\u8d2f\u53d9\u4e8b\uff0c\u4f46\u5728\u751f\u6210\u8bc4\u5206\u56de\u7b54\u65f6\u65e0\u6cd5\u6709\u6548\u533a\u5206\u8fde\u8d2f\u4e0e\u4e0d\u8fde\u8d2f\u6545\u4e8b\uff0c\u8868\u660eLLMs\u5bf9\u53d9\u4e8b\u8fde\u8d2f\u6027\u7684\u7406\u89e3\u5b58\u5728\u7f3a\u9677", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u5426\u53ef\u9760\u5730\u533a\u5206\u8fde\u8d2f\u4e0e\u4e0d\u8fde\u8d2f\u7684\u53d9\u4e8b\uff0c\u8bc4\u4f30LLMs\u5bf9\u6545\u4e8b\u8fde\u8d2f\u6027\u7684\u7406\u89e3\u80fd\u529b", "method": "\u4f7f\u7528\u914d\u5bf9\u53d9\u4e8b\u6570\u636e\u96c6\u8fdb\u884c\u7814\u7a76\uff0c\u901a\u8fc7\u63a2\u6d4b\u7814\u7a76\u53d1\u73b0LLMs\u7684\u5185\u90e8\u8868\u5f81\u80fd\u591f\u8bc6\u522b\u4e0d\u8fde\u8d2f\u53d9\u4e8b\uff0c\u4f46\u901a\u8fc7\u591a\u79cd\u63d0\u793a\u53d8\u4f53\u8ba9LLMs\u751f\u6210\u8bc4\u5206\u56de\u7b54\u6765\u6d4b\u8bd5\u5176\u533a\u5206\u80fd\u529b", "result": "LLMs\u7684\u5185\u90e8\u8868\u5f81\u80fd\u53ef\u9760\u8bc6\u522b\u4e0d\u8fde\u8d2f\u53d9\u4e8b\uff0c\u4f46\u751f\u6210\u7684\u8bc4\u5206\u56de\u7b54\u65e0\u6cd5\u6709\u6548\u533a\u5206\u8fde\u8d2f\u4e0e\u4e0d\u8fde\u8d2f\u6545\u4e8b\uff1bLLMs\u5bf9\u8fdd\u53cd\u573a\u666f\u7684\u4e0d\u8fde\u8d2f\u6027\uff08\u5982\u6c99\u6f20\u4e2d\u7684\u96e8\u5929\uff09\u6bd4\u5bf9\u8fdd\u53cd\u89d2\u8272\u7279\u8d28\u7684\u4e0d\u8fde\u8d2f\u6027\uff08\u5982\u7d20\u98df\u8005\u70b9\u6c49\u5821\uff09\u66f4\u654f\u611f\uff1b\u601d\u7ef4\u94fe\u63a8\u7406\u65e0\u6cd5\u6d88\u9664\u8fd9\u4e9b\u7f3a\u9677", "conclusion": "LLMs\u5bf9\u53d9\u4e8b\u8fde\u8d2f\u6027\u7684\u7406\u89e3\u4e0d\u5b8c\u6574\uff0c\u53ef\u80fd\u66f4\u4f9d\u8d56\u539f\u578b\u4e16\u754c\u77e5\u8bc6\u800c\u975e\u57fa\u4e8e\u610f\u4e49\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\u6784\u5efa\uff0c\u5185\u90e8\u72b6\u6001\u4e0e\u884c\u4e3a\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd"}}
{"id": "2512.06565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06565", "abs": "https://arxiv.org/abs/2512.06565", "authors": ["Xiujin Liu"], "title": "GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation", "comment": "1 figures, 2 tables, 14pages", "summary": "We present GNC-Pose, a fully learning-free monocular 6D object pose estimation pipeline for textured objects that combines rendering-based initialization, geometry-aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D-3D correspondences obtained through feature matching and rendering-based alignment, our method builds upon the Graduated Non-Convexity (GNC) principle and introduces a geometry-aware, cluster-based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC-Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC-Pose achieves competitive accuracy compared with both learning-based and learning-free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.", "AI": {"tldr": "GNC-Pose\u662f\u4e00\u4e2a\u65e0\u9700\u5b66\u4e60\u7684\u5355\u76ee6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e32\u67d3\u521d\u59cb\u5316\u3001\u51e0\u4f55\u611f\u77e5\u5bf9\u5e94\u70b9\u52a0\u6743\u548cGNC\u4f18\u5316\u5b9e\u73b0\uff0c\u5728YCB\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e0e\u5b66\u4e60\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76846D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216\u5b66\u4e60\u7279\u5f81\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u5b8c\u5168\u65e0\u9700\u5b66\u4e60\u7684\u3001\u9c81\u68d2\u76846D\u59ff\u6001\u4f30\u8ba1\u7ba1\u9053\uff0c\u80fd\u591f\u5728\u4e25\u91cd\u5f02\u5e38\u503c\u6c61\u67d3\u4e0b\u7a33\u5b9a\u5de5\u4f5c\u3002", "method": "1) \u901a\u8fc7\u7279\u5f81\u5339\u914d\u548c\u6e32\u67d3\u5bf9\u9f50\u83b7\u5f97\u7c97\u7565\u76842D-3D\u5bf9\u5e94\u70b9\uff1b2) \u5f15\u5165\u51e0\u4f55\u611f\u77e5\u7684\u805a\u7c7b\u52a0\u6743\u673a\u5236\uff0c\u57fa\u4e8e3D\u7ed3\u6784\u4e00\u81f4\u6027\u4e3a\u6bcf\u4e2a\u70b9\u5206\u914d\u7f6e\u4fe1\u5ea6\uff1b3) \u91c7\u7528\u6e10\u8fdb\u975e\u51f8\u6027(GNC)\u4f18\u5316\u539f\u5219\uff1b4) \u6700\u540e\u4f7f\u7528LM\u7ec6\u5316\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5728YCB\u7269\u4f53\u548c\u6a21\u578b\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u5c3d\u7ba1\u4e0d\u9700\u8981\u5b66\u4e60\u7279\u5f81\u3001\u8bad\u7ec3\u6570\u636e\u6216\u7c7b\u522b\u7279\u5b9a\u5148\u9a8c\uff0cGNC-Pose\u5728\u7cbe\u5ea6\u4e0a\u4e0e\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u548c\u65e0\u9700\u5b66\u4e60\u7684\u65b9\u6cd5\u90fd\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "GNC-Pose\u4e3a\u65e0\u9700\u5b66\u4e60\u76846D\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u9c81\u68d2\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u51e0\u4f55\u5148\u9a8c\u548c\u52a0\u6743\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u4e25\u91cd\u5f02\u5e38\u503c\u6c61\u67d3\u4e0b\u7684\u4f18\u5316\u7a33\u5b9a\u6027\u3002"}}
{"id": "2512.06759", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06759", "abs": "https://arxiv.org/abs/2512.06759", "authors": ["Wenbo Lyu", "Yingjun Du", "Jinglin Zhao", "Xianton Zhen", "Ling Shao"], "title": "VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors", "comment": "12 pages,13figures", "summary": "Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench", "AI": {"tldr": "VisChainBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u3001\u591a\u8f6e\u573a\u666f\u4e0b\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,457\u4e2a\u4efb\u52a1\u548c\u8d85\u8fc720,000\u5f20\u56fe\u50cf\uff0c\u8986\u76d6\u65e5\u5e38\u573a\u666f\u548c\u5de5\u7a0b\u6545\u969c\u6392\u9664\u7b49\u9886\u57df\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u6216\u6a2a\u5411\u6bd4\u8f83\uff0c\u5982\u53d1\u73b0\u89c6\u89c9\u5dee\u5f02\u6216\u8bc4\u4f30\u9002\u5f53\u6027\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u7ebf\u7d22\uff0c\u5ffd\u89c6\u4e86\u6e10\u8fdb\u5f0f\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u63a8\u7406\u4ee5\u53ca\u89c6\u89c9\u5230\u89c6\u89c9\u7684\u63a8\u7406\u6311\u6218\u3002\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u751f\u6210\u6d41\u6c34\u7ebf\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u786e\u4fdd\u89c6\u89c9\u591a\u6837\u6027\u548c\u63a7\u5236\u8bed\u8a00\u504f\u5dee\u3002\u57fa\u51c6\u5305\u542b\u4e09\u4e2a\u4e0d\u540c\u9886\u57df\uff08\u65e5\u5e38\u573a\u666f\u3001\u5de5\u7a0b\u6545\u969c\u6392\u9664\u7b49\uff09\u76841,457\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u8d85\u8fc720,000\u5f20\u56fe\u50cf\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u521b\u5efa\u4e86VisChainBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u5927\u89c4\u6a21\u4efb\u52a1\u96c6\u548c\u56fe\u50cf\u6570\u636e\uff0c\u6240\u6709\u57fa\u51c6\u6570\u636e\u548c\u6784\u5efa\u4ee3\u7801\u5df2\u901a\u8fc7Hugging Face\u5e73\u53f0\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "VisChainBench\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u80fd\u591f\u4e25\u683c\u8bc4\u4f30LVLMs\u5728\u987a\u5e8f\u3001\u76f8\u4e92\u4f9d\u8d56\u4efb\u52a1\u4e2d\u8fdb\u884c\u591a\u6b65\u89c6\u89c9\u63a8\u7406\u7684\u80fd\u529b\uff0c\u4e3a\u7814\u7a76\u591a\u56fe\u50cf\u3001\u591a\u8f6e\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2512.07783", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07783", "abs": "https://arxiv.org/abs/2512.07783", "authors": ["Charlie Zhang", "Graham Neubig", "Xiang Yue"], "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models", "comment": null, "summary": "Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b8c\u5168\u53d7\u63a7\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u3001\u4e2d\u671f\u8bad\u7ec3\u548cRL\u540e\u8bad\u7ec3\u5728\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u4e2d\u7684\u56e0\u679c\u8d21\u732e\uff0c\u660e\u786e\u4e86RL\u4ec5\u5728\u9884\u8bad\u7ec3\u7559\u6709\u8db3\u591f\u7a7a\u95f4\u4e14\u9488\u5bf9\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u65f6\u624d\u80fd\u4ea7\u751f\u771f\u6b63\u80fd\u529b\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u867d\u7136\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u96be\u4ee5\u786e\u5b9a\u8fd9\u79cd\u63d0\u5347\u662f\u771f\u6b63\u6269\u5c55\u4e86\u6a21\u578b\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u5229\u7528\u4e86\u9884\u8bad\u7ec3\u4e2d\u5df2\u6709\u7684\u77e5\u8bc6\u3002\u7531\u4e8e\u73b0\u4ee3\u8bad\u7ec3\u6d41\u7a0b\u7f3a\u4e4f\u63a7\u5236\uff08\u9884\u8bad\u7ec3\u8bed\u6599\u4e0d\u900f\u660e\u3001\u4e2d\u671f\u8bad\u7ec3\u7814\u7a76\u4e0d\u8db3\u3001RL\u76ee\u6807\u4e0e\u672a\u77e5\u5148\u9a8c\u77e5\u8bc6\u590d\u6742\u4ea4\u4e92\uff09\uff0c\u9700\u8981\u5f00\u53d1\u5b8c\u5168\u53d7\u63a7\u7684\u5b9e\u9a8c\u6846\u67b6\u6765\u6f84\u6e05\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u5b8c\u5168\u53d7\u63a7\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u4f7f\u7528\u5408\u6210\u63a8\u7406\u4efb\u52a1\uff0c\u5305\u542b\u660e\u786e\u7684\u539f\u5b50\u64cd\u4f5c\u3001\u53ef\u89e3\u6790\u7684\u9010\u6b65\u63a8\u7406\u8f68\u8ff9\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u64cd\u7eb5\u8bad\u7ec3\u5206\u5e03\u3002\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6a21\u578b\uff1a1) \u5916\u63a8\u6cdb\u5316\u5230\u66f4\u590d\u6742\u7684\u7ec4\u5408\uff1b2) \u8de8\u8868\u9762\u4e0a\u4e0b\u6587\u7684\u8bed\u5883\u6cdb\u5316\u3002\u901a\u8fc7\u8be5\u6846\u67b6\u5206\u79bb\u9884\u8bad\u7ec3\u3001\u4e2d\u671f\u8bad\u7ec3\u548cRL\u540e\u8bad\u7ec3\u7684\u56e0\u679c\u8d21\u732e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) RL\u4ec5\u5728\u9884\u8bad\u7ec3\u7559\u6709\u8db3\u591f\u7a7a\u95f4\u4e14\u9488\u5bf9\u6a21\u578b\u80fd\u529b\u8fb9\u754c\uff08\u56f0\u96be\u4f46\u5c1a\u672a\u8d85\u51fa\u80fd\u529b\u8303\u56f4\u7684\u4efb\u52a1\uff09\u65f6\u624d\u80fd\u4ea7\u751f\u771f\u6b63\u7684\u80fd\u529b\u63d0\u5347\uff08pass@128\uff09\uff1b2) \u8bed\u5883\u6cdb\u5316\u9700\u8981\u6700\u5c0f\u4f46\u8db3\u591f\u7684\u9884\u8bad\u7ec3\u66b4\u9732\uff0c\u4e4b\u540eRL\u53ef\u4ee5\u53ef\u9760\u5730\u8fc1\u79fb\uff1b3) \u5728\u56fa\u5b9a\u8ba1\u7b97\u91cf\u4e0b\uff0c\u4e2d\u671f\u8bad\u7ec3\u76f8\u6bd4\u4ec5\u4f7f\u7528RL\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1b4) \u8fc7\u7a0b\u7ea7\u5956\u52b1\u51cf\u5c11\u4e86\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u5e76\u63d0\u9ad8\u4e86\u63a8\u7406\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u9610\u660e\u4e86\u9884\u8bad\u7ec3\u3001\u4e2d\u671f\u8bad\u7ec3\u548cRL\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u4e86\u57fa\u7840\u3002\u7279\u522b\u5f3a\u8c03\u4e86\u4e2d\u671f\u8bad\u7ec3\u5728\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u7684\u6838\u5fc3\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u4f5c\u7528\uff0c\u4ee5\u53caRL\u6709\u6548\u6027\u7684\u8fb9\u754c\u6761\u4ef6\u3002"}}
{"id": "2512.06769", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06769", "abs": "https://arxiv.org/abs/2512.06769", "authors": ["Hang Yin", "Xiaomin He", "PeiWen Yuan", "Yiwei Li", "Jiayi Shi", "Wenxiao Fan", "Shaoxiong Feng", "Kan Li"], "title": "Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding", "comment": null, "summary": "Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\\text{MME}_{\\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.", "AI": {"tldr": "\u63d0\u51faStitch and Tell\u65b9\u6cd5\uff0c\u901a\u8fc7\u62fc\u63a5\u56fe\u50cf\u5e76\u751f\u6210\u7a7a\u95f4\u611f\u77e5\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u5373\u53ef\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u51cf\u5c11\u7a7a\u95f4\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u7a7a\u95f4\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u5bf9\u56fe\u50cf\u4e2d\u7269\u4f53\u76f8\u5bf9\u4f4d\u7f6e\u63cf\u8ff0\u9519\u8bef\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e3b\u8981\u6e90\u4e8e\u56fe\u50cf\u548c\u6587\u672c\u4e4b\u95f4\u7684\u4e0d\u5bf9\u79f0\u7279\u6027\uff0c\u9700\u8981\u589e\u5f3a\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faStitch and Tell\u65b9\u6cd5\uff1a1\uff09\u6cbf\u7a7a\u95f4\u8f74\u62fc\u63a5\u56fe\u50cf\u521b\u5efa\u62fc\u63a5\u56fe\u50cf\uff1b2\uff09\u57fa\u4e8e\u62fc\u63a5\u56fe\u50cf\u7684\u5e03\u5c40\u751f\u6210\u7a7a\u95f4\u611f\u77e5\u7684\u6807\u9898\u6216\u95ee\u7b54\u5bf9\uff1b3\uff09\u65e0\u9700\u989d\u5916\u6807\u6ce8\u3001\u65e0\u9700\u6602\u8d35\u7684\u9ad8\u7ea7\u6a21\u578b\u6216\u4eba\u5de5\u53c2\u4e0e\uff1b4\uff09\u53ef\u5373\u63d2\u5373\u7528\u5730\u6ce8\u5165\u7ed3\u6784\u5316\u7a7a\u95f4\u76d1\u7763\u5230\u8bad\u7ec3\u6570\u636e\u4e2d\u3002", "result": "\u5728LLaVA-v1.5-7B\u3001LLaVA-Qwen2-1.5B\u548cHALVA-7B\u4e09\u79cd\u67b6\u6784\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e24\u4e2a\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u3002\u7ed3\u679c\u663e\u793a\uff1a\u7a7a\u95f4\u7406\u89e3\u4efb\u52a1\u663e\u8457\u63d0\u5347\uff08MME_Position +5.50%\uff0cSpatial-MM +4.19%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6027\u80fd\uff08COCO-QA +1.02%\uff0cMMBench +4.76%\uff09\u3002", "conclusion": "\u5c06\u7a7a\u95f4\u611f\u77e5\u7ed3\u6784\u663e\u5f0f\u6ce8\u5165\u8bad\u7ec3\u6570\u636e\u662f\u51cf\u5c11\u7a7a\u95f4\u5e7b\u89c9\u3001\u63d0\u5347\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u540c\u65f6\u80fd\u4fdd\u6301\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u65e0\u9700\u6807\u6ce8\u3001\u53ef\u5373\u63d2\u5373\u7528\u3002"}}
{"id": "2512.07801", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07801", "abs": "https://arxiv.org/abs/2512.07801", "authors": ["Raunak Jain", "Mudita Khurana"], "title": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support", "comment": null, "summary": "LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u534f\u4f5c\u56e0\u679c\u610f\u4e49\u5efa\u6784(CCS)\"\u4f5c\u4e3a\u51b3\u7b56\u652f\u6301AI\u7684\u7814\u7a76\u8bae\u7a0b\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524dAI\u52a9\u624b\u4e0e\u4e13\u5bb6\u5408\u4f5c\u65f6\u51fa\u73b0\u7684\u9a8c\u8bc1\u5faa\u73af\u3001\u8fc7\u5ea6\u4f9d\u8d56\u7b49\u95ee\u9898\uff0c\u5f3a\u8c03AI\u5e94\u6210\u4e3a\u8ba4\u77e5\u5de5\u4f5c\u7684\u5408\u4f5c\u4f19\u4f34\u800c\u975e\u5de5\u5177\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684AI\u52a9\u624b\u5728\u590d\u6742\u9ad8\u98ce\u9669\u51b3\u7b56\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4eba\u7c7b-AI\u56e2\u961f\u5f80\u5f80\u8868\u73b0\u4e0d\u5982\u6700\u4f73\u4e2a\u4f53\uff0c\u4e13\u5bb6\u5728\u9a8c\u8bc1\u5faa\u73af\u548c\u8fc7\u5ea6\u4f9d\u8d56\u4e4b\u95f4\u6447\u6446\uff0c\u627f\u8bfa\u7684\u4e92\u8865\u6027\u672a\u80fd\u5b9e\u73b0\u3002\u8fd9\u4e0d\u4ec5\u662f\u51c6\u786e\u6027\u95ee\u9898\uff0c\u66f4\u662fAI\u8f85\u52a9\u6982\u5ff5\u7684\u6839\u672c\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u56e0\u679c\u610f\u4e49\u5efa\u6784(CCS)\u4f5c\u4e3a\u7814\u7a76\u8bae\u7a0b\u548c\u7ec4\u7ec7\u6846\u67b6\uff0c\u8bbe\u8ba1\u4f5c\u4e3a\u8ba4\u77e5\u5de5\u4f5c\u4f19\u4f34\u7684\u7cfb\u7edf\uff1a\u7ef4\u62a4\u4e13\u5bb6\u63a8\u7406\u7684\u6f14\u5316\u6a21\u578b\uff0c\u5e2e\u52a9\u8868\u8fbe\u548c\u4fee\u8ba2\u76ee\u6807\uff0c\u5171\u540c\u6784\u5efa\u548c\u538b\u529b\u6d4b\u8bd5\u56e0\u679c\u5047\u8bbe\uff0c\u4ece\u8054\u5408\u51b3\u7b56\u7ed3\u679c\u4e2d\u5b66\u4e60\uff0c\u5b9e\u73b0\u4eba\u7c7b\u548cAI\u7684\u5171\u540c\u6539\u8fdb\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86CCS\u6846\u67b6\uff0c\u5e76\u52fe\u52d2\u4e86\u76f8\u5173\u6311\u6218\uff1a\u4f7f\u534f\u4f5c\u601d\u7ef4\u5177\u6709\u5de5\u5177\u4ef7\u503c\u7684\u8bad\u7ec3\u751f\u6001\u3001\u5171\u540c\u6784\u5efa\u6a21\u578b\u7684\u8868\u793a\u548c\u4ea4\u4e92\u534f\u8bae\u3001\u4ee5\u4fe1\u4efb\u548c\u4e92\u8865\u6027\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u65b9\u5411\u53ef\u4ee5\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\u91cd\u65b0\u5b9a\u4f4d\u4e3a\u53c2\u4e0e\u534f\u4f5c\u610f\u4e49\u5efa\u6784\u7684\u667a\u80fd\u4f53\uff0c\u4f7f\u5176\u6210\u4e3a\u4e0e\u4eba\u7c7b\u4f19\u4f34\u5171\u540c\u601d\u8003\u7684AI\u961f\u53cb\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5de5\u5177\u3002"}}
{"id": "2512.06581", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06581", "abs": "https://arxiv.org/abs/2512.06581", "authors": ["Yuhao Su", "Anwesa Choudhuri", "Zhongpai Gao", "Benjamin Planche", "Van Nguyen Nguyen", "Meng Zheng", "Yuhan Shen", "Arun Innanje", "Terrence Chen", "Ehsan Elhamifar", "Ziyan Wu"], "title": "MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding", "comment": null, "summary": "Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \\textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \\textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \\emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \\emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MedVidBench\u533b\u5b66\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u548cMedGRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u89c6\u9891\u7406\u89e3\u4e2d\u7a7a\u95f4\u7cbe\u5ea6\u3001\u65f6\u5e8f\u63a8\u7406\u548c\u4e34\u5e8a\u8bed\u4e49\u7684\u6311\u6218\uff0c\u901a\u8fc7\u8de8\u6570\u636e\u96c6\u5956\u52b1\u5f52\u4e00\u5316\u548c\u533b\u5b66LLM\u8bc4\u5224\u5668\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u89c6\u9891\u7406\u89e3\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u7a7a\u95f4\u7cbe\u5ea6\u3001\u65f6\u5e8f\u63a8\u7406\u548c\u4e34\u5e8a\u8bed\u4e49\u65b9\u9762\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u533b\u5b66\u89c6\u9891\u7684\u591a\u5c42\u6b21\u4efb\u52a1\uff08\u89c6\u9891\u7ea7\u3001\u7247\u6bb5\u7ea7\u3001\u5e27\u7ea7\uff09\uff0c\u4e14\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u5728\u8de8\u6570\u636e\u96c6\u8bad\u7ec3\u65f6\u56e0\u5956\u52b1\u5c3a\u5ea6\u4e0d\u5e73\u8861\u800c\u5931\u8d25\u3002", "method": "1. \u6784\u5efaMedVidBench\u57fa\u51c6\uff1a\u5305\u542b531,850\u4e2a\u89c6\u9891-\u6307\u4ee4\u5bf9\uff0c\u6db5\u76d68\u4e2a\u533b\u5b66\u6765\u6e90\uff0c\u901a\u8fc7\u4e13\u5bb6\u5f15\u5bfc\u63d0\u793a\u548c\u53cc\u6a21\u578b\u9a8c\u8bc1\u7684\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\uff1b2. \u63d0\u51faMedGRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u5305\u542b\u8de8\u6570\u636e\u96c6\u5956\u52b1\u5f52\u4e00\u5316\uff08\u5c06\u5404\u6570\u636e\u96c6\u7684\u4e2d\u4f4d\u6027\u80fd\u6620\u5c04\u5230\u5171\u540c\u5956\u52b1\u503c\uff09\u548c\u533b\u5b66LLM\u8bc4\u5224\u5668\uff08\u901a\u8fc7\u6bd4\u8f83\u76f8\u4f3c\u6027\u8bc4\u5206\u5728\u4e94\u4e2a\u4e34\u5e8a\u7ef4\u5ea6\u8bc4\u4f30\u5b57\u5e55\u8d28\u91cf\uff09\u3002", "result": "\u5728MedVidBench\u4e0a\u76d1\u7763\u5fae\u8c03Qwen2.5-VL-7B\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4.1\u548cGemini-2.5-Flash\uff1bMedGRPO\u6846\u67b6\u8fdb\u4e00\u6b65\u5728\u5b9a\u4f4d\u548c\u5b57\u5e55\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86SFT\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u57fa\u51c6\u7684\u6709\u6548\u6027\u548c\u8bad\u7ec3\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u533b\u5b66\u9886\u57df\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u4e86\u57fa\u7840\u6027\u57fa\u51c6\u548c\u7a33\u5065\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7MedVidBench\u57fa\u51c6\u548cMedGRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63a8\u52a8\u4e86\u533b\u5b66AI\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.07832", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07832", "abs": "https://arxiv.org/abs/2512.07832", "authors": ["Matteo Boglioni", "Andrea Sgobbi", "Gabriel Tavernini", "Francesco Rita", "Marius Mosbach", "Tiago Pimentel"], "title": "Do Generalisation Results Generalise?", "comment": null, "summary": "A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8d28\u7591\u4ec5\u7528\u5355\u4e00OOD\u6570\u636e\u96c6\u8bc4\u4f30LLM\u6cdb\u5316\u80fd\u529b\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u901a\u8fc7\u5206\u6790\u591a\u4e2aOOD\u6d4b\u8bd5\u96c6\u95f4\u7684\u504f\u76f8\u5173\u6027\u6765\u66f4\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30LLM\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00OOD\u6570\u636e\u96c6\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u4e2d\u6a21\u578b\u4f1a\u9047\u5230\u66f4\u590d\u6742\u591a\u6837\u7684\u6570\u636e\u5206\u5e03\u53d8\u5316\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u51c6\u786e\u8861\u91cf\u6a21\u578b\u7684\u771f\u5b9e\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u4e2aOOD\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u7136\u540e\u8ba1\u7b97\u8fd9\u4e9b\u6d4b\u8bd5\u96c6\u6027\u80fd\u4e4b\u95f4\u7684\u504f\u76f8\u5173\u6027\uff08\u63a7\u5236\u57df\u5185\u6027\u80fd\u7684\u5f71\u54cd\uff09\uff0c\u4ee5\u6b64\u8bc4\u4f30\u6cdb\u5316\u6027\u80fd\u7684\u76f8\u5173\u6027\u3002", "result": "\u5206\u6790OLMo2\u548cOPT\u6a21\u578b\u53d1\u73b0\uff0c\u4e0d\u540cOOD\u6d4b\u8bd5\u96c6\u95f4\u7684\u6cdb\u5316\u7ed3\u679c\u6ca1\u6709\u7edf\u4e00\u7684\u8d8b\u52bf\uff1a\u4e24\u4e2aOOD\u6d4b\u8bd5\u96c6\u4e4b\u95f4\u662f\u5426\u5b58\u5728\u6b63\u76f8\u5173\u6216\u8d1f\u76f8\u5173\uff0c\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u6240\u5206\u6790\u7684\u5177\u4f53\u6a21\u578b\u9009\u62e9\u3002", "conclusion": "LLM\u7684OOD\u6cdb\u5316\u6027\u80fd\u8bc4\u4f30\u9700\u8981\u66f4\u5168\u9762\u7684\u65b9\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u5355\u4e00OOD\u6570\u636e\u96c6\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u7684\u771f\u5b9e\u6cdb\u5316\u80fd\u529b\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540cOOD\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002"}}
{"id": "2512.06612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06612", "abs": "https://arxiv.org/abs/2512.06612", "authors": ["Kazuya Nishimura", "Haruka Hirose", "Ryoma Bise", "Kaito Shiku", "Yasuhiro Kojima"], "title": "Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics", "comment": "Neurips 2025", "summary": "Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.", "AI": {"tldr": "\u63d0\u51faSTRank\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u76f8\u5bf9\u8868\u8fbe\u6a21\u5f0f\u800c\u975e\u7edd\u5bf9\u8868\u8fbe\u503c\u6765\u4f30\u8ba1\u57fa\u56e0\u8868\u8fbe\uff0c\u4ee5\u5e94\u5bf9RNA\u6d4b\u5e8f\u4e2d\u7684\u968f\u673a\u566a\u58f0\u548c\u6279\u6b21\u6548\u5e94\u3002", "motivation": "\u4ece\u75c5\u7406\u56fe\u50cf\u4f30\u8ba1\u57fa\u56e0\u8868\u8fbe\u53ef\u964d\u4f4eRNA\u6d4b\u5e8f\u6210\u672c\uff0c\u4f46\u4f20\u7edf\u70b9\u5bf9\u70b9\u635f\u5931\u51fd\u6570\u96be\u4ee5\u51c6\u786e\u4f30\u8ba1\u7edd\u5bf9\u8868\u8fbe\u503c\uff0c\u56e0\u4e3a\u6d4b\u5e8f\u6280\u672f\u590d\u6742\u6027\u548c\u7ec6\u80de\u5185\u5728\u53d8\u5f02\u6027\u5bfc\u81f4\u89c2\u6d4b\u6570\u636e\u5305\u542b\u968f\u673a\u566a\u58f0\u548c\u6279\u6b21\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u5b66\u4e60\u76f8\u5bf9\u8868\u8fbe\u6a21\u5f0f\u800c\u975e\u7edd\u5bf9\u6c34\u5e73\u7684\u65b0\u76ee\u6807\uff0c\u5047\u8bbe\u57fa\u56e0\u76f8\u5bf9\u8868\u8fbe\u6c34\u5e73\u5728\u72ec\u7acb\u5b9e\u9a8c\u4e2d\u5448\u73b0\u4e00\u81f4\u6a21\u5f0f\u3002\u57fa\u4e8e\u6b64\u5047\u8bbe\uff0c\u63d0\u51faSTRank\u635f\u5931\u51fd\u6570\uff0c\u5bf9\u566a\u58f0\u548c\u6279\u6b21\u6548\u5e94\u5177\u6709\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5173\u6ce8\u76f8\u5bf9\u8868\u8fbe\u6a21\u5f0f\u800c\u975e\u7edd\u5bf9\u8868\u8fbe\u503c\uff0cSTRank\u635f\u5931\u51fd\u6570\u80fd\u591f\u66f4\u6709\u6548\u5730\u4ece\u75c5\u7406\u56fe\u50cf\u4f30\u8ba1\u57fa\u56e0\u8868\u8fbe\uff0c\u51cf\u5c11\u566a\u58f0\u548c\u6279\u6b21\u6548\u5e94\u7684\u5f71\u54cd\u3002"}}
{"id": "2512.06811", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.06811", "abs": "https://arxiv.org/abs/2512.06811", "authors": ["Xiang Lin", "Weixin Li", "Shu Guo", "Lihong Wang", "Di Huang"], "title": "RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models", "comment": "Accepted by AAAI 2026(Oral)", "summary": "Pre-trained Vision-Language Models (VLMs), \\textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.", "AI": {"tldr": "RMAdapter\uff1a\u4e00\u79cd\u57fa\u4e8e\u91cd\u6784\u7684\u591a\u6a21\u6001\u9002\u914d\u5668\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u5e73\u8861\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u4e0e\u901a\u7528\u77e5\u8bc6\u4fdd\u7559\uff0c\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u5fae\u8c03\u9762\u4e34\u5e73\u8861\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u6311\u6218\u3002\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u800c\u57fa\u4e8e\u9002\u914d\u5668\u7684\u65b9\u6cd5\u7814\u7a76\u4e0d\u8db3\u4e14\u6027\u80fd\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u91cd\u6784\u591a\u6a21\u6001\u9002\u914d\u5668\uff08RMAdapter\uff09\uff0c\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff1a1\uff09\u9002\u5e94\u5206\u652f\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6ce8\u5165\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\uff1b2\uff09\u91cd\u6784\u5206\u652f\u901a\u8fc7\u5c06\u6f5c\u5728\u7a7a\u95f4\u7279\u5f81\u91cd\u6784\u56de\u539f\u59cb\u7279\u5f81\u7a7a\u95f4\u6765\u4fdd\u7559\u901a\u7528\u77e5\u8bc6\u3002\u91c7\u7528\u5c40\u90e8\u91cd\u6784\u635f\u5931\u8ba1\u7b97\u548c\u5171\u4eab\u6295\u5f71\u6a21\u5757\u4fdd\u6301\u8f7b\u91cf\u5316\uff0c\u5e76\u52a0\u5165\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u8c03\u8282\u5224\u522b\u6027\u4e0e\u6cdb\u5316\u6027\u7684\u5e73\u8861\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\u4e0a\u5168\u9762\u8bc4\u4f30\uff1a\u65b0\u7c7b\u522b\u6cdb\u5316\u3001\u65b0\u76ee\u6807\u6570\u636e\u96c6\u6cdb\u5316\u3001\u9886\u57df\u6cdb\u5316\u3002\u5728\u4e0d\u4f9d\u8d56\u6570\u636e\u589e\u5f3a\u6216\u91cd\u590d\u63d0\u793a\u8bbe\u8ba1\u7684\u60c5\u51b5\u4e0b\uff0cRMAdapter\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "RMAdapter\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u6709\u6548\u5e73\u8861\u4e86\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u4e0e\u901a\u7528\u77e5\u8bc6\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u5316\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u9002\u914d\u5668\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.06866", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06866", "abs": "https://arxiv.org/abs/2512.06866", "authors": ["Yulin Li", "Haokun Gui", "Ziyang Fan", "Junjie Wang", "Bin Kang", "Bin Chen", "Zhuotao Tian"], "title": "Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior", "comment": "Accepted by NeurIPS 2025", "summary": "Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .", "AI": {"tldr": "DyToK\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684VLLM\u52a8\u6001token\u538b\u7f29\u65b9\u6cd5\uff0c\u5229\u7528VLLM\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u5173\u952e\u5e27\u5148\u9a8c\uff0c\u52a8\u6001\u8c03\u6574\u6bcf\u5e27token\u4fdd\u7559\u6bd4\u4f8b\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b04.3\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u9762\u4e34\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u589e\u957f\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u5173\u952e\u5e27\u91c7\u6837\u65b9\u6cd5\u5728\u7279\u5f81\u7f16\u7801\u524d\u5f15\u5165\u989d\u5916\u8ba1\u7b97\u6210\u672c\uff0c\u4e14\u4e8c\u5143\u5e27\u9009\u62e9\u8303\u5f0f\u6548\u679c\u6b20\u4f73\u3002", "method": "DyToK\u901a\u8fc7\u5206\u6790\u53d1\u73b0VLLM\u6ce8\u610f\u529b\u5c42\u81ea\u7136\u7f16\u7801\u4e86\u67e5\u8be2\u6761\u4ef6\u7684\u5173\u952e\u5e27\u5148\u9a8c\uff0c\u5229\u7528\u8fd9\u4e00\u7279\u6027\u52a8\u6001\u8c03\u6574\u6bcf\u5e27token\u4fdd\u7559\u6bd4\u4f8b\uff0c\u4f18\u5148\u4fdd\u7559\u8bed\u4e49\u4e30\u5bcc\u7684\u5e27\uff0c\u6291\u5236\u5197\u4f59\u4fe1\u606f\u3002", "result": "DyToK\u5728\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e0e\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\uff08\u5982VisionZip\u548cFastV\uff09\u517c\u5bb9\uff0c\u5728LLaVA-OneVision\u548cQwen2.5-VL\u7b49\u591a\u4e2aVLLM\u4e0a\u5b9e\u73b04.3\u500d\u63a8\u7406\u52a0\u901f\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "DyToK\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u52a8\u6001token\u538b\u7f29\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLLM\u5904\u7406\u957f\u89c6\u9891\u65f6\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u517c\u5bb9\u6027\u3002"}}
{"id": "2512.06885", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06885", "abs": "https://arxiv.org/abs/2512.06885", "authors": ["Wancheng Feng", "Chen An", "Zhenliang He", "Meina Kan", "Shiguang Shan", "Lukun Wang"], "title": "JoPano: Unified Panorama Generation via Joint Modeling", "comment": "Code: https://github.com/VIPL-GENUN/JoPano", "summary": "Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.", "AI": {"tldr": "JoPano\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDiT\u7684\u7edf\u4e00\u5168\u666f\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u9762\u9002\u914d\u5668\u548c\u6761\u4ef6\u5207\u6362\u673a\u5236\uff0c\u5c06\u6587\u672c\u5230\u5168\u666f\u56fe\u548c\u89c6\u89d2\u5230\u5168\u666f\u56fe\u4e24\u4e2a\u6838\u5fc3\u4efb\u52a1\u7edf\u4e00\u5728\u4e00\u4e2a\u6a21\u578b\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u89c6\u89c9\u8d28\u91cf\u53d7\u9650\u548c\u4efb\u52a1\u72ec\u7acb\u5efa\u6a21\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5168\u666f\u56fe\u751f\u6210\u65b9\u6cd5\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1\uff09\u57fa\u4e8eU-Net\u7684\u67b6\u6784\u9650\u5236\u4e86\u751f\u6210\u5168\u666f\u56fe\u7684\u89c6\u89c9\u8d28\u91cf\uff1b2\uff09\u901a\u5e38\u5c06\u6587\u672c\u5230\u5168\u666f\u56fe\u548c\u89c6\u89d2\u5230\u5168\u666f\u56fe\u4e24\u4e2a\u6838\u5fc3\u4efb\u52a1\u72ec\u7acb\u5904\u7406\uff0c\u5bfc\u81f4\u5efa\u6a21\u5197\u4f59\u548c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eDiT\u7684\u8054\u5408\u9762\u5168\u666f\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1\uff09\u57fa\u4e8e\u7acb\u65b9\u4f53\u8d34\u56fe\u8868\u793a\u7684\u8054\u5408\u9762\u9002\u914d\u5668\uff0c\u5c06\u9884\u8bad\u7ec3DiT\u7684\u751f\u6210\u80fd\u529b\u8fc1\u79fb\u5230\u5168\u666f\u56fe\u9886\u57df\uff1b2\uff09\u6cca\u677e\u878d\u5408\u51cf\u5c11\u7acb\u65b9\u4f53\u9762\u8fb9\u754c\u7684\u4e0d\u4e00\u81f4\u6027\uff1b3\uff09\u6761\u4ef6\u5207\u6362\u673a\u5236\u7edf\u4e00\u6587\u672c\u5230\u5168\u666f\u56fe\u548c\u89c6\u89d2\u5230\u5168\u666f\u56fe\u4efb\u52a1\u3002", "result": "JoPano\u5728\u6587\u672c\u5230\u5168\u666f\u56fe\u548c\u89c6\u89d2\u5230\u5168\u666f\u56fe\u751f\u6210\u4efb\u52a1\u4e0a\u90fd\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u5168\u666f\u56fe\uff0c\u5728FID\u3001CLIP-FID\u3001IS\u548cCLIP-Score\u7b49\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86Seam-SSIM\u548cSeam-Sobel\u6307\u6807\u5b9a\u91cf\u8bc4\u4f30\u63a5\u7f1d\u4e00\u81f4\u6027\u3002", "conclusion": "JoPano\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u89e3\u51b3\u4e86\u5168\u666f\u56fe\u751f\u6210\u4e2d\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u4efb\u52a1\u5197\u4f59\u95ee\u9898\uff0c\u57fa\u4e8eDiT\u7684\u8054\u5408\u5efa\u6a21\u65b9\u6cd5\u4e3a\u5168\u666f\u56fe\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06662", "abs": "https://arxiv.org/abs/2512.06662", "authors": ["Ruoyu Xue", "Hieu Le", "Jingyi Xu", "Sounak Mondal", "Abe Leite", "Gregory Zelinsky", "Minh Hoai", "Dimitris Samaras"], "title": "Personalized Image Descriptions from Attention Sequences", "comment": "10 pages, 4 figures", "summary": "People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.", "AI": {"tldr": "DEPER\u6a21\u578b\u901a\u8fc7\u540c\u65f6\u5efa\u6a21\u8bed\u8a00\u98ce\u683c\u548c\u4e2a\u6027\u5316\u89c6\u89c9\u5173\u6ce8\u6a21\u5f0f\u6765\u751f\u6210\u66f4\u7b26\u5408\u4e2a\u4eba\u7279\u70b9\u7684\u56fe\u50cf\u63cf\u8ff0\uff0c\u76f8\u6bd4\u4ec5\u5173\u6ce8\u8bed\u8a00\u98ce\u683c\u7684\u65b9\u6cd5\u5e73\u5747\u63d0\u534724%", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u53ea\u5173\u6ce8\u8bed\u8a00\u98ce\u683c\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u4eba\u89c2\u770b\u56fe\u50cf\u65f6\u7684\u89c6\u89c9\u5173\u6ce8\u6a21\u5f0f\u5dee\u5f02\u3002\u4eba\u4eec\u89c2\u770b\u540c\u4e00\u56fe\u50cf\u65f6\u4f1a\u5173\u6ce8\u4e0d\u540c\u533a\u57df\u3001\u5bf9\u8c61\u548c\u7ec6\u8282\uff0c\u5e76\u4ee5\u4e0d\u540c\u987a\u5e8f\u548c\u8bed\u8a00\u98ce\u683c\u8fdb\u884c\u63cf\u8ff0\uff0c\u5bfc\u81f4\u63cf\u8ff0\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "method": "\u63d0\u51faDEPER\u65b9\u6cd5\uff0c\u5b66\u4e60\u540c\u65f6\u6355\u6349\u8bed\u8a00\u98ce\u683c\u548c\u89c2\u770b\u884c\u4e3a\u7684\u4e3b\u4f53\u5d4c\u5165\u8868\u793a\uff0c\u901a\u8fc7\u8f85\u52a9\u6ce8\u610f\u529b\u9884\u6d4b\u4efb\u52a1\u8fdb\u884c\u5f15\u5bfc\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5c06\u8fd9\u4e9b\u5d4c\u5165\u4e0e\u51bb\u7ed3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u4e2a\u6027\u5316\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u6db5\u76d6\u4e0d\u540c\u89c2\u770b\u4efb\u52a1\u548c\u63cf\u8ff0\u7c7b\u578b\u7684\u6570\u636e\u96c6\u4e0a\uff0cDEPER\u5e73\u5747\u63d0\u534724%\uff0c\u8868\u660e\u5efa\u6a21\u4e2a\u6027\u5316\u6ce8\u610f\u529b\u80fd\u4ea7\u751f\u66f4\u7b26\u5408\u4eba\u7c7b\u7279\u70b9\u4e14\u8d28\u91cf\u66f4\u9ad8\u7684\u63cf\u8ff0\u3002", "conclusion": "\u7406\u89e3\u4eba\u4eec\u5982\u4f55\u89c2\u770b\u56fe\u50cf\u6709\u52a9\u4e8e\u9884\u6d4b\u4ed6\u4eec\u4f1a\u8bf4\u4ec0\u4e48\uff1b\u5efa\u6a21\u4eba\u7c7b\u611f\u77e5\u591a\u6837\u6027\u53ef\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u4e0e\u4eba\u7c7b\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002"}}
{"id": "2512.06921", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06921", "abs": "https://arxiv.org/abs/2512.06921", "authors": ["Ziyang Song", "Zelin Zang", "Xiaofan Ye", "Boqiang Xu", "Long Bai", "Jinlin Wu", "Hongliang Ren", "Hongbin Liu", "Jiebo Luo", "Zhen Lei"], "title": "NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification", "comment": "Accepted by IEEE ICIA 2025", "summary": "Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u795e\u7ecf\u5916\u79d1\u9886\u57df\u7684\u591a\u6a21\u6001\u89e3\u5256\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5NeuroABench\uff0c\u5305\u542b9\u5c0f\u65f6\u6807\u6ce8\u89c6\u9891\uff0c\u8bc4\u4f3068\u4e2a\u89e3\u5256\u7ed3\u6784\u8bc6\u522b\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u4f73MLLM\u6a21\u578b\u51c6\u786e\u7387\u4ec540.87%\uff0c\u663e\u8457\u4f4e\u4e8e\u795e\u7ecf\u5916\u79d1\u5b66\u5458\u5e73\u5747\u8868\u73b0\u3002", "motivation": "\u73b0\u6709MLLM\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u624b\u672f\u6d41\u7a0b\u7406\u89e3\uff0c\u5ffd\u89c6\u4e86\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u89e3\u5256\u5b66\u7406\u89e3\u80fd\u529b\u3002\u795e\u7ecf\u5916\u79d1\u533b\u751f\u4f9d\u8d56\u7cbe\u786e\u7684\u89e3\u5256\u77e5\u8bc6\u6765\u89e3\u8bfb\u624b\u672f\u89c6\u9891\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u8bc4\u4f30\u89e3\u5256\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f00\u53d1\u4e86NeuroABench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b9\u5c0f\u65f6\u6807\u6ce8\u7684\u795e\u7ecf\u5916\u79d1\u89c6\u9891\uff0c\u6db5\u76d689\u4e2a\u4e0d\u540c\u624b\u672f\u3002\u91c7\u7528\u65b0\u578b\u591a\u6a21\u6001\u6807\u6ce8\u6d41\u7a0b\u548c\u591a\u8f6e\u5ba1\u67e5\u673a\u5236\uff0c\u8bc4\u4f3068\u4e2a\u4e34\u5e8a\u89e3\u5256\u7ed3\u6784\u7684\u8bc6\u522b\u80fd\u529b\u3002\u6d4b\u8bd5\u4e8610\u591a\u4e2aSOTA MLLM\u6a21\u578b\uff0c\u5e76\u4e0e4\u540d\u795e\u7ecf\u5916\u79d1\u5b66\u5458\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u6700\u4f73MLLM\u6a21\u578b\u5728\u89e3\u5256\u8bc6\u522b\u4efb\u52a1\u4e2d\u4ec5\u8fbe\u523040.87%\u51c6\u786e\u7387\u3002\u795e\u7ecf\u5916\u79d1\u5b66\u5458\u6d4b\u8bd5\u4e2d\uff0c\u6700\u4f73\u5b66\u5458\u51c6\u786e\u738756%\uff0c\u6700\u4f4e28%\uff0c\u5e73\u574746.5%\u3002\u6700\u4f73MLLM\u8868\u73b0\u4e0e\u6700\u4f4e\u5206\u5b66\u5458\u76f8\u5f53\uff0c\u4f46\u663e\u8457\u4f4e\u4e8e\u5b66\u5458\u5e73\u5747\u6c34\u5e73\u3002", "conclusion": "MLLM\u5728\u89e3\u5256\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4e0e\u4eba\u7c7b\u6c34\u5e73\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\u3002NeuroABench\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbMLLM\u7684\u89e3\u5256\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u624b\u672f\u6559\u80b2\u548c\u8f85\u52a9\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.06663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06663", "abs": "https://arxiv.org/abs/2512.06663", "authors": ["Yu Qi", "Yumeng Zhang", "Chenting Gong", "Xiao Tan", "Weiming Zhang", "Wei Zhang", "Jingdong Wang"], "title": "CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.", "AI": {"tldr": "CoT4Det\u5c06\u611f\u77e5\u4efb\u52a1\u91cd\u6784\u4e3a\u5206\u7c7b\u3001\u8ba1\u6570\u548c\u5b9a\u4f4d\u4e09\u4e2a\u53ef\u89e3\u91ca\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u68c0\u6d4b\u7b49\u611f\u77e5\u4efb\u52a1\u4e0a\u7684\u6027\u80fd", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u611f\u77e5\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\uff09\u4e0a\u6027\u80fd\u8fdc\u4f4e\u4e8e\u4efb\u52a1\u4e13\u7528\u4e13\u5bb6\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u5bc6\u96c6\u573a\u666f\u548c\u5c0f\u7269\u4f53\u53ec\u56de\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3", "method": "\u63d0\u51faChain-of-Thought for Detection (CoT4Det)\u7b56\u7565\uff0c\u5c06\u611f\u77e5\u4efb\u52a1\u91cd\u6784\u4e3a\u4e09\u4e2a\u66f4\u7b26\u5408\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u53ef\u89e3\u91ca\u6b65\u9aa4\uff1a\u5206\u7c7b\u3001\u8ba1\u6570\u548c\u5b9a\u4f4d", "result": "\u5728Qwen2.5-VL-7B-Instruct\u6a21\u578b\u4e0a\uff0cCoT4Det\u5c06COCO2017 val\u7684mAP\u4ece19.0%\u63d0\u5347\u523033.0%\uff0c\u5728RefCOCO\u7cfb\u5217\u4e0a\u8d85\u8d8a\u57fa\u7ebf2%\uff0c\u5728Flickr30k entities\u4e0a\u63d0\u534719%", "conclusion": "CoT4Det\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u5176\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u80fd\u529b\uff0c\u4e3a\u5c06\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.06673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06673", "abs": "https://arxiv.org/abs/2512.06673", "authors": ["Shida Gao", "Feng Xue", "Xiangfeng Wang", "Anlong Ming", "Teng Long", "Yihua Shao", "Haozhe Wang", "Zhaowen Lin", "Wei Wang", "Nicu Sebe"], "title": "1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning", "comment": null, "summary": "Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.", "AI": {"tldr": "DEViL\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u8003\u8bed\u4e49\u4ee4\u724c\u5b9e\u73b0\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u89e3\u51b3\u65f6\u7a7a\u5b9a\u4f4d\u4e2d\u81ea\u56de\u5f52\u7a7a\u95f4\u89e3\u7801\u5bfc\u81f4\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u7a7a\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u91c7\u7528\u81ea\u56de\u5f52\u65b9\u5f0f\u751f\u6210\u8fb9\u754c\u6846\u4f5c\u4e3a\u6587\u672c\u6807\u8bb0\uff0c\u8fd9\u5bfc\u81f4\u8f93\u51fa\u5e8f\u5217\u8fc7\u957f\uff0c\u7a7a\u95f4\u8bef\u5dee\u968f\u65f6\u95f4\u7d2f\u79ef\uff0c\u5b9a\u4f4d\u7ed3\u679c\u5728\u89c6\u9891\u4e2d\u9010\u6e10\u6f02\u79fb\u3002", "method": "DEViL\u5c06\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u8026\u5408\uff0c\u901a\u8fc7\u53c2\u8003\u8bed\u4e49\u4ee4\u724c\u5c06\u7528\u6237\u67e5\u8be2\u63d0\u70bc\u4e3a\u4e30\u5bcc\u7684\u8bed\u4e49\u8868\u793a\uff0c\u8be5\u4ee4\u724c\u65e2\u4f5c\u4e3a\u63a7\u5236\u4fe1\u53f7\u53c8\u66ff\u4ee3\u68c0\u6d4b\u5668\u7684\u6587\u672c\u5d4c\u5165\u3002\u540c\u65f6\u63d0\u51fa\u7ba1\u9053\u6316\u6398\u65f6\u95f4\u6b63\u5219\u5316\uff0c\u786e\u4fdd\u68c0\u6d4b\u5668\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u76ee\u6807\u5bf9\u8c61\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDEViL\u5728\u5404\u79cd\u7ec6\u7c92\u5ea6\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u65f6\u7a7a\u89c6\u89c9\u5b9a\u4f4d\u548c\u57fa\u4e8e\u89c6\u89c9\u95ee\u7b54\u7684\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "conclusion": "DEViL\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u7a7a\u5b9a\u4f4d\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u65f6\u7a7a\u5173\u8054\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u3002"}}
{"id": "2512.06810", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06810", "abs": "https://arxiv.org/abs/2512.06810", "authors": ["Yueqian Wang", "Songxiang Liu", "Disong Wang", "Nuo Xu", "Guanglu Wan", "Huishuai Zhang", "Dongyan Zhao"], "title": "MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning", "comment": null, "summary": "Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e3b\u52a8\u4ea4\u4e92\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u5230\u6587\u672c\u7684\u65b9\u5f0f\u8ba9\u6a21\u578b\u5728\u89c6\u9891\u64ad\u653e\u8fc7\u7a0b\u4e2d\u81ea\u4e3b\u51b3\u5b9a\u4f55\u65f6\u56de\u5e94\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u9608\u503c\u6216\u7cbe\u786e\u6807\u6ce8\u56de\u590d\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u89c6\u9891MLLM\u7cfb\u7edf\u591a\u4e3a\u56de\u5408\u5236\uff0c\u53ea\u80fd\u5728\u7528\u6237\u53d1\u8a00\u540e\u56de\u590d\uff0c\u800c\u5b9e\u65f6\u5e94\u7528\u9700\u8981\u6a21\u578b\u5728\u89c6\u9891\u64ad\u653e\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u51b3\u5b9a\u4f55\u65f6\u56de\u5e94\u3002\u8fd9\u662f\u4e00\u4e2a\u6709\u524d\u666f\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u65b9\u5411\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6587\u672c\u5230\u6587\u672c\u7684\u4e3b\u52a8\u4ea4\u4e92\u65b9\u6cd5\uff0c\u6a21\u578b\u6839\u636e\u5bf9\u8bdd\u5386\u53f2\u548c\u5f53\u524d\u89c6\u9891\u5e27\u7684\u89c6\u89c9\u4e0a\u4e0b\u6587\u81ea\u4e3b\u51b3\u5b9a\u662f\u56de\u5e94\u8fd8\u662f\u4fdd\u6301\u6c89\u9ed8\u3002\u91c7\u7528\u591a\u56de\u5408\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9f13\u52b1\u53ca\u65f6\u51c6\u786e\u7684\u56de\u5e94\uff0c\u65e0\u9700\u7cbe\u786e\u7684\u56de\u590d\u65f6\u95f4\u6807\u6ce8\u3002\u5728\u5305\u542b52k\u89c6\u9891\u548c\u4e24\u79cd\u5bf9\u8bdd\u7c7b\u578b\u7684\u6570\u636e\u96c6\u4e0a\u901a\u8fc7SFT\u548cRL\u8bad\u7ec3MMDuet2\u6a21\u578b\u3002", "result": "MMDuet2\u5728\u56de\u5e94\u65f6\u673a\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u4e3b\u52a8\u89c6\u9891MLLM\u57fa\u7ebf\uff0c\u5728ProactiveVideoQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e3b\u52a8\u4ea4\u4e92\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u56de\u5408\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u8c03\u6574\u9608\u503c\u548c\u7cbe\u786e\u6807\u6ce8\u56de\u590d\u65f6\u95f4\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u89c6\u9891\u4ea4\u4e92\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06674", "abs": "https://arxiv.org/abs/2512.06674", "authors": ["Songping Wang", "Rufan Qian", "Yueming Lyu", "Qinglong Liu", "Linzhuang Zou", "Jie Qin", "Songhua Liu", "Caifeng Shan"], "title": "RunawayEvil: Jailbreaking the Image-to-Video Generative Models", "comment": null, "summary": "Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a \"Strategy-Tactic-Action\" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.", "AI": {"tldr": "RunawayEvil\uff1a\u9996\u4e2a\u9488\u5bf9\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u6846\u67b6\uff0c\u91c7\u7528\"\u7b56\u7565-\u6218\u672f-\u884c\u52a8\"\u8303\u5f0f\uff0c\u5177\u5907\u52a8\u6001\u6f14\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548cLLM\u5b9e\u73b0\u81ea\u6211\u8fdb\u5316\u7684\u653b\u51fb\u7b56\u7565\uff0c\u5728\u5546\u4e1aI2V\u6a21\u578b\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u867d\u7136\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u521b\u610f\u63a7\u5236\u80fd\u529b\uff0c\u4f46\u5176\u5b89\u5168\u6027\u7279\u522b\u662f\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u8106\u5f31\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u591a\u6a21\u6001I2V\u6a21\u578b\u7684\u8d8a\u72f1\u653b\u51fb\u6846\u67b6\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51faRunawayEvil\u6846\u67b6\uff0c\u57fa\u4e8e\"\u7b56\u7565-\u6218\u672f-\u884c\u52a8\"\u8303\u5f0f\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u7b56\u7565\u611f\u77e5\u547d\u4ee4\u5355\u5143\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u7b56\u7565\u5b9a\u5236\u548c\u57fa\u4e8eLLM\u7684\u7b56\u7565\u63a2\u7d22\u5b9e\u73b0\u653b\u51fb\u7b56\u7565\u7684\u81ea\u6211\u6f14\u5316\uff1b2) \u591a\u6a21\u6001\u6218\u672f\u89c4\u5212\u5355\u5143\uff1a\u6839\u636e\u9009\u5b9a\u7b56\u7565\u751f\u6210\u534f\u8c03\u7684\u6587\u672c\u8d8a\u72f1\u6307\u4ee4\u548c\u56fe\u50cf\u7be1\u6539\u6307\u5357\uff1b3) \u6218\u672f\u884c\u52a8\u5355\u5143\uff1a\u6267\u884c\u548c\u8bc4\u4f30\u591a\u6a21\u6001\u534f\u8c03\u653b\u51fb\u3002\u8fd9\u79cd\u81ea\u6f14\u5316\u67b6\u6784\u4f7f\u6846\u67b6\u80fd\u591f\u6301\u7eed\u9002\u5e94\u548c\u5f3a\u5316\u653b\u51fb\u7b56\u7565\u800c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "result": "\u5728Open-Sora 2.0\u548cCogVideoX\u7b49\u5546\u4e1aI2V\u6a21\u578b\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cRunawayEvil\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\u3002\u5728COCO2017\u6570\u636e\u96c6\u4e0a\uff0cRunawayEvil\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa58.5%\u523079%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aI2V\u6a21\u578b\u7684\u6f0f\u6d1e\u5206\u6790\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002RunawayEvil\u6846\u67b6\u5c55\u793a\u4e86\u591a\u6a21\u6001\u7cfb\u7edf\u9762\u4e34\u7684\u65b0\u578b\u5b89\u5168\u6311\u6218\uff0c\u5f3a\u8c03\u4e86\u5728\u5f00\u53d1\u521b\u610fAI\u5de5\u5177\u65f6\u9700\u8981\u8003\u8651\u5b89\u5168\u9632\u62a4\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.07034", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07034", "abs": "https://arxiv.org/abs/2512.07034", "authors": ["Tuan-Anh Vu", "Hai Nguyen-Truong", "Ziqiang Zheng", "Binh-Son Hua", "Qing Guo", "Ivor Tsang", "Sai-Kit Yeung"], "title": "Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues", "comment": "Accepted to WACV 2026", "summary": "Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.", "AI": {"tldr": "TransCues\u662f\u4e00\u4e2a\u7528\u4e8e\u900f\u660e\u7269\u4f53\u5206\u5272\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u754c\u7279\u5f81\u589e\u5f3a\u548c\u53cd\u5c04\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73bb\u7483\u7b49\u900f\u660e\u7269\u4f53\u7531\u4e8e\u900f\u660e\u6027\u548c\u53cd\u5c04\u7279\u6027\uff0c\u73b0\u6709\u5206\u5272\u65b9\u6cd5\u96be\u4ee5\u5c06\u5176\u4e0e\u4e0d\u900f\u660e\u6750\u6599\u533a\u5206\u3002\u4eba\u7c7b\u611f\u77e5\u4f9d\u8d56\u8fb9\u754c\u548c\u53cd\u5c04\u7269\u4f53\u7279\u5f81\u6765\u8bc6\u522b\u900f\u660e\u7269\u4f53\uff0c\u4f46\u73b0\u6709\u6587\u732e\u672a\u80fd\u5145\u5206\u6355\u6349\u8fd9\u4e24\u79cd\u7279\u6027\u3002", "method": "\u63d0\u51faTransCues\u6846\u67b6\uff0c\u91c7\u7528\u91d1\u5b57\u5854\u5f0ftransformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5305\u542b\u8fb9\u754c\u7279\u5f81\u589e\u5f3a\u6a21\u5757\u548c\u53cd\u5c04\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u4ee5\u76f8\u4e92\u4fc3\u8fdb\u7684\u65b9\u5f0f\u6574\u5408\u8fd9\u4e24\u79cd\u89c6\u89c9\u7ebf\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff1aTrans10K-v2 (+4.2% mIoU)\u3001MSD (+5.6% mIoU)\u3001RGBD-Mirror (+10.1% mIoU)\u3001TROSD (+13.1% mIoU)\u3001Stanford2D3D (+8.3% mIoU)\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u8fb9\u754c\u548c\u53cd\u5c04\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0cTransCues\u80fd\u6709\u6548\u5206\u5272\u900f\u660e\u7269\u4f53\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5bf9\u73bb\u7483\u7269\u4f53\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.06684", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06684", "abs": "https://arxiv.org/abs/2512.06684", "authors": ["Yumeng He", "Zanwei Zhou", "Yekun Zheng", "Chen Liang", "Yunbo Wang", "Xiaokang Yang"], "title": "EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy", "comment": null, "summary": "Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.", "AI": {"tldr": "EMGauss\uff1a\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u76843D\u91cd\u5efa\u6846\u67b6\uff0c\u5c06\u5207\u7247\u52303D\u91cd\u5efa\u91cd\u65b0\u5b9a\u4e49\u4e3a3D\u52a8\u6001\u573a\u666f\u6e32\u67d3\u95ee\u9898\uff0c\u89e3\u51b3\u5404\u5411\u5f02\u6027\u7ed3\u6784\u91cd\u5efa\u96be\u9898", "motivation": "\u4f53\u79ef\u7535\u5b50\u663e\u5fae\u955c\uff08vEM\uff09\u5728\u7eb3\u7c73\u5c3a\u5ea63D\u6210\u50cf\u4e2d\u5b58\u5728\u91c7\u96c6\u6743\u8861\uff0c\u5bfc\u81f4\u5404\u5411\u5f02\u6027\u4f53\u79ef\u548c\u6709\u9650\u7684\u8f74\u5411\u5206\u8fa8\u7387\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6a2a\u5411\u5148\u9a8c\u6765\u6062\u590d\u5404\u5411\u540c\u6027\uff0c\u4f46\u5bf9\u4e8e\u5f62\u6001\u5404\u5411\u5f02\u6027\u7ed3\u6784\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5c06\u5207\u7247\u52303D\u91cd\u5efa\u91cd\u65b0\u5b9a\u4e49\u4e3a\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u76843D\u52a8\u6001\u573a\u666f\u6e32\u67d3\u95ee\u9898\uff0c\u5c06\u8f74\u5411\u5207\u7247\u8fdb\u5c55\u5efa\u6a21\u4e3a2D\u9ad8\u65af\u70b9\u4e91\u7684\u65f6\u95f4\u6f14\u5316\u3002\u5f15\u5165\u6559\u5e08-\u5b66\u751f\u5f15\u5bfc\u673a\u5236\uff0c\u5229\u7528\u672a\u89c2\u6d4b\u5207\u7247\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u4f5c\u4e3a\u4f2a\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u76f8\u6bd4\u57fa\u4e8e\u6269\u6563\u548cGAN\u7684\u91cd\u5efa\u65b9\u6cd5\uff0cEMGauss\u663e\u8457\u63d0\u9ad8\u4e86\u63d2\u503c\u8d28\u91cf\uff0c\u5b9e\u73b0\u4e86\u8fde\u7eed\u5207\u7247\u5408\u6210\uff0c\u4e14\u65e0\u9700\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002\u5728\u6570\u636e\u7a00\u758f\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u3002", "conclusion": "EMGauss\u4e3avEM\u63d0\u4f9b\u4e86\u4e00\u79cd\u89c4\u907f\u5404\u5411\u540c\u6027\u65b9\u6cd5\u56fa\u6709\u5c40\u9650\u6027\u7684\u901a\u7528\u6846\u67b6\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8evEM\uff0c\u8fd8\u53ef\u80fd\u4e3a\u8de8\u4e0d\u540c\u6210\u50cf\u9886\u57df\u7684\u5207\u7247\u52303D\u91cd\u5efa\u63d0\u4f9b\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07051", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07051", "abs": "https://arxiv.org/abs/2512.07051", "authors": ["Adnan Munir", "Shujaat Khan"], "title": "DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation", "comment": "11 pages, 7 figures", "summary": "Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.", "AI": {"tldr": "DAUNet\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7UNet\u53d8\u4f53\uff0c\u7ed3\u5408\u53ef\u53d8\u5f62\u5377\u79efV2\u548c\u65e0\u53c2\u6570\u6ce8\u610f\u529bSimAM\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u590d\u6742\u5ea6\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u81ea\u52a8\u5316\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u5904\u7406\u51e0\u4f55\u53d8\u5316\u53c8\u80fd\u4fdd\u6301\u9ad8\u6548\u53c2\u6570\u5229\u7528\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b", "method": "\u63d0\u51faDAUNet\uff0c\u5728\u74f6\u9888\u5c42\u4f7f\u7528\u52a8\u6001\u53ef\u53d8\u5f62\u5377\u79ef\u5904\u7406\u51e0\u4f55\u53d8\u5316\uff0c\u5728\u89e3\u7801\u5668\u548c\u8df3\u8dc3\u8fde\u63a5\u4e2d\u4f7f\u7528SimAM\u6ce8\u610f\u529b\u6a21\u5757\u8fdb\u884c\u663e\u8457\u6027\u611f\u77e5\u7684\u7279\u5f81\u878d\u5408", "result": "\u5728\u4e24\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\uff08FH-PS-AoP\u8d85\u58f0\u548cFUMPE CT\u80ba\u6813\u585e\u68c0\u6d4b\uff09\u4e0a\uff0cDAUNet\u5728Dice\u5206\u6570\u3001HD95\u548cASD\u6307\u6807\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u8d8a\u7684\u53c2\u6570\u6548\u7387", "conclusion": "DAUNet\u5bf9\u7f3a\u5931\u4e0a\u4e0b\u6587\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u533a\u57df\u7684\u9c81\u68d2\u6027\u4f7f\u5176\u9002\u5408\u5728\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u53ef\u53d8\u5f62\u5377\u79ef\u548cSimAM\u6ce8\u610f\u529b\u7684\u7ec4\u5408\u6709\u6548\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd"}}
{"id": "2512.06689", "categories": ["cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.06689", "abs": "https://arxiv.org/abs/2512.06689", "authors": ["Jisoo Park", "Seonghak Lee", "Guisik Kim", "Taewoo Kim", "Junseok Kwon"], "title": "Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation", "comment": "Accepted to ASRU 2025", "summary": "Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.", "AI": {"tldr": "UniVoiceLite\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u65e0\u76d1\u7763\u7684\u89c6\u542c\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u8bed\u97f3\u589e\u5f3a\u548c\u8bed\u97f3\u5206\u79bb\u4efb\u52a1\uff0c\u5229\u7528\u5507\u90e8\u8fd0\u52a8\u548c\u9762\u90e8\u8eab\u4efd\u7ebf\u7d22\uff0c\u65e0\u9700\u914d\u5bf9\u566a\u58f0-\u5e72\u51c0\u6570\u636e\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u97f3\u9891\u901a\u5e38\u540c\u65f6\u5305\u542b\u80cc\u666f\u566a\u58f0\u548c\u91cd\u53e0\u8bf4\u8bdd\u4eba\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u8bed\u97f3\u589e\u5f3a\u548c\u8bed\u97f3\u5206\u79bb\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u73b0\u6709\u96c6\u6210\u65b9\u6cd5\u901a\u5e38\u590d\u6742\u3001\u53c2\u6570\u91cf\u5927\u4e14\u4f9d\u8d56\u76d1\u7763\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faUniVoiceLite\u6846\u67b6\uff0c\u5229\u7528\u5507\u90e8\u8fd0\u52a8\u548c\u9762\u90e8\u8eab\u4efd\u7ebf\u7d22\u5f15\u5bfc\u8bed\u97f3\u63d0\u53d6\uff0c\u91c7\u7528Wasserstein\u8ddd\u79bb\u6b63\u5219\u5316\u7a33\u5b9a\u6f5c\u5728\u7a7a\u95f4\uff0c\u65e0\u9700\u914d\u5bf9\u566a\u58f0-\u5e72\u51c0\u6570\u636e\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u65e0\u76d1\u7763\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUniVoiceLite\u5728\u566a\u58f0\u548c\u591a\u8bf4\u8bdd\u4eba\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u7684\u7ed3\u5408\u3002", "conclusion": "UniVoiceLite\u6210\u529f\u7edf\u4e00\u4e86\u8bed\u97f3\u589e\u5f3a\u548c\u8bed\u97f3\u5206\u79bb\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u65e0\u76d1\u7763\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2512.07141", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07141", "abs": "https://arxiv.org/abs/2512.07141", "authors": ["Fenghua Weng", "Chaochao Lu", "Xia Hu", "Wenqi Shao", "Wenjie Wang"], "title": "Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models", "comment": null, "summary": "As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.", "AI": {"tldr": "\u63d0\u51faThink-Reflect-Revise (TRR)\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u5f15\u5bfc\u7684\u81ea\u6211\u53cd\u601d\u589e\u5f3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u80fd", "motivation": "\u73b0\u6709\u5355\u6b21\"\u601d\u8003-\u56de\u7b54\"\u8303\u5f0f\u867d\u7136\u63d0\u9ad8\u4e86\u5b89\u5168\u610f\u8bc6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u4ecd\u5bb9\u6613\u53d7\u5230\u4e0a\u4e0b\u6587\u6216\u89c6\u89c9\u8d8a\u72f1\u653b\u51fb\uff0c\u56e0\u4e3a\u5355\u6b21\u63a8\u7406\u53ef\u80fd\u5ffd\u7565\u81ea\u8eab\u8f93\u51fa\u4e2d\u7684\u663e\u5f0f\u6709\u5bb3\u5185\u5bb9", "method": "\u63d0\u51faTRR\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1)\u6784\u5efa\u5305\u542b5000\u4e2a\u793a\u4f8b\u7684ReSafe\u6570\u636e\u96c6\uff0c\u9075\u5faa\u601d\u8003-\u53cd\u601d-\u4fee\u8ba2\u8fc7\u7a0b\uff1b2)\u4f7f\u7528ReSafe\u6570\u636e\u96c6\u5fae\u8c03\u76ee\u6807\u6a21\u578b\u4ee5\u521d\u59cb\u5316\u53cd\u601d\u884c\u4e3a\uff1b3)\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5f3a\u5316\u7b56\u7565\u5f15\u5bfc\u7684\u53cd\u601d", "result": "TRR\u663e\u8457\u63d0\u5347\u4e86LVLMs\u7684\u5b89\u5168\u6027\u80fd\uff0c\u5728Qwen2.5-VL-7B\u4e0a\u5c06\u603b\u4f53\u5b89\u5168\u54cd\u5e94\u7387\u4ece42.8%\u63d0\u9ad8\u523087.7%\uff0c\u540c\u65f6\u5728MMMU\u548cMMStar\u7b49\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd", "conclusion": "\u901a\u8fc7\u5229\u7528\u9996\u6b21\u63a8\u7406\u4e2d\u63ed\u793a\u7684\u6076\u610f\u5185\u5bb9\u8fdb\u884c\u53cd\u601d\uff0c\u53ef\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u81ea\u6211\u4fee\u6b63\u5e76\u9632\u6b62\u4e0d\u5b89\u5168\u751f\u6210\uff0cTRR\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86LVLMs\u7684\u5b89\u5168\u5bf9\u9f50\u80fd\u529b"}}
{"id": "2512.06738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06738", "abs": "https://arxiv.org/abs/2512.06738", "authors": ["M Yashwanth", "Sampath Koti", "Arunabh Singh", "Shyam Marjit", "Anirban Chakraborty"], "title": "FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation", "comment": "Accepted to Winter Conference on Applications of Computer Vision (WACV) 2026, Round 1", "summary": "We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.", "AI": {"tldr": "FedSCAl\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u670d\u52a1\u5668-\u5ba2\u6237\u7aef\u5bf9\u9f50\u673a\u5236\u89e3\u51b3\u8054\u90a6\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u95ee\u9898\uff0c\u5728\u5b58\u5728\u663e\u8457\u5ba2\u6237\u7aef\u95f4\u57df\u5dee\u5f02\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4f2a\u6807\u7b7e\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u65e0\u6e90\u57df\u81ea\u9002\u5e94\uff08FFreeDA\uff09\u95ee\u9898\uff0c\u5176\u4e2d\u5ba2\u6237\u7aef\u6301\u6709\u672a\u6807\u8bb0\u6570\u636e\u4e14\u5b58\u5728\u663e\u8457\u7684\u5ba2\u6237\u7aef\u95f4\u57df\u5dee\u5f02\u3002\u8be5\u8bbe\u7f6e\u9650\u5236\u4e86\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u53ea\u80fd\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u670d\u52a1\u5668\u6a21\u578b\uff0c\u65e0\u6cd5\u8bbf\u95ee\u6e90\u6570\u636e\u96c6\uff0c\u800c\u6e90\u57df\u4e0e\u5ba2\u6237\u7aef\u57df\u5206\u5e03\u4e0d\u540c\uff0c\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u5728\u6781\u7aef\u6570\u636e\u5f02\u6784\u6027\u4e0b\u4ea7\u751f\u4e0d\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\u3002", "method": "\u63d0\u51faFedSCAl\u6846\u67b6\uff0c\u91c7\u7528\u670d\u52a1\u5668-\u5ba2\u6237\u7aef\u5bf9\u9f50\uff08SCAl\uff09\u673a\u5236\uff0c\u901a\u8fc7\u5bf9\u9f50\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u6a21\u578b\u7684\u9884\u6d4b\u6765\u6b63\u5219\u5316\u5ba2\u6237\u7aef\u66f4\u65b0\u3002\u8be5\u673a\u5236\u51cf\u8f7b\u4e86\u5ba2\u6237\u7aef\u6f02\u79fb\uff0c\u63d0\u9ad8\u4e86\u4f2a\u6807\u7b7e\u51c6\u786e\u6027\u3002", "result": "\u5728\u57fa\u51c6\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFedSCAl\u5728FFreeDA\u8bbe\u7f6e\u4e0b\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "FedSCAl\u901a\u8fc7\u670d\u52a1\u5668-\u5ba2\u6237\u7aef\u5bf9\u9f50\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u4f2a\u6807\u7b7e\u53ef\u9760\u6027\uff0c\u5728\u5f02\u6784\u57df\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.07198", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07198", "abs": "https://arxiv.org/abs/2512.07198", "authors": ["Xiujie Song", "Qi Jia", "Shota Watanabe", "Xiaoyi Pang", "Ruijie Chen", "Mengyue Wu", "Kenny Q. Zhu"], "title": "Generating Storytelling Images with Rich Chains-of-Reasoning", "comment": null, "summary": "An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Storytelling Image Generation\u4efb\u52a1\uff0c\u5f00\u53d1\u4e86StorytellingPainter\u4e24\u9636\u6bb5\u751f\u6210\u7ba1\u9053\uff0c\u7ed3\u5408LLMs\u7684\u63a8\u7406\u80fd\u529b\u548cT2I\u6a21\u578b\u7684\u89c6\u89c9\u5408\u6210\u80fd\u529b\u6765\u521b\u5efa\u5177\u6709\u4e30\u5bcc\u8bed\u4e49\u8fde\u63a5\u7684\u6545\u4e8b\u6027\u56fe\u50cf\u3002", "motivation": "\u6545\u4e8b\u6027\u56fe\u50cf\u901a\u8fc7\u5448\u73b0\u4e30\u5bcc\u3001\u903b\u8f91\u8fde\u63a5\u7684\u89c6\u89c9\u7ebf\u7d22\u6765\u4f20\u8fbe\u5f15\u4eba\u5165\u80dc\u7684\u6545\u4e8b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u4ef7\u503c\u3002\u4f46\u7531\u4e8e\u5176\u590d\u6742\u7684\u8bed\u4e49\u7279\u6027\uff0c\u8fd9\u7c7b\u56fe\u50cf\u96be\u4ee5\u521b\u5efa\u4e14\u76f8\u5bf9\u7a00\u7f3a\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5982\u4f55\u5229\u7528\u751f\u6210\u5f0fAI\u6a21\u578b\u6765\u521b\u5efa\u8fd9\u7c7b\u56fe\u50cf\u3002", "method": "\u63d0\u51fa\u4e86StorytellingPainter\u4e24\u9636\u6bb5\u7ba1\u9053\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528LLMs\u8fdb\u884c\u521b\u9020\u6027\u63a8\u7406\u751f\u6210\u6545\u4e8b\u63cf\u8ff0\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528T2I\u6a21\u578b\u5c06\u6545\u4e8b\u63cf\u8ff0\u5408\u6210\u4e3a\u89c6\u89c9\u56fe\u50cf\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u5305\u542b\u8bed\u4e49\u590d\u6742\u6027\u8bc4\u4f30\u5668\u3001KNN\u591a\u6837\u6027\u8bc4\u4f30\u5668\u548c\u6545\u4e8b-\u56fe\u50cf\u5bf9\u9f50\u8bc4\u4f30\u5668\u7684\u4e13\u7528\u8bc4\u4f30\u6846\u67b6\u3002\u9488\u5bf9\u5f00\u6e90\u4e0e\u4e13\u6709LLMs\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u63a2\u7d22\u4e86\u5b9a\u5236\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u5f00\u53d1\u4e86Mini-Storytellers\u7cfb\u5217\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u53ef\u884c\u4e14\u6709\u6548\u7684\u3002\u901a\u8fc7\u7ed3\u5408LLMs\u7684\u63a8\u7406\u80fd\u529b\u548cT2I\u6a21\u578b\u7684\u89c6\u89c9\u5408\u6210\u80fd\u529b\uff0c\u80fd\u591f\u6210\u529f\u751f\u6210\u5177\u6709\u4e30\u5bcc\u8bed\u4e49\u8fde\u63a5\u7684\u6545\u4e8b\u6027\u56fe\u50cf\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5b9a\u4e49\u4e86\u6545\u4e8b\u6027\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u6709\u6548\u7684\u751f\u6210\u7ba1\u9053\u548c\u8bc4\u4f30\u6846\u67b6\u3002\u901a\u8fc7\u5229\u7528\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u6545\u4e8b\u6027\u56fe\u50cf\u521b\u5efa\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4e3a\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.07136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07136", "abs": "https://arxiv.org/abs/2512.07136", "authors": ["Siyang Jiang", "Mu Yuan", "Xiang Ji", "Bufang Yang", "Zeyu Liu", "Lilin Xu", "Yang Li", "Yuting He", "Liran Dong", "Wenrui Lu", "Zhenyu Yan", "Xiaofan Jiang", "Wei Gao", "Hongkai Chen", "Guoliang Xing"], "title": "A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning", "comment": null, "summary": "Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.", "AI": {"tldr": "CUHK-X\u662f\u4e00\u4e2a\u7528\u4e8e\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u3001\u7406\u89e3\u548c\u63a8\u7406\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b58,445\u4e2a\u6837\u672c\u300140\u79cd\u52a8\u4f5c\uff0c\u63d0\u4f9b\u6570\u636e\u6807\u7b7e\u548c\u6587\u672c\u63cf\u8ff0\u4e24\u79cd\u6807\u6ce8\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709LVLMs\u5728\u975eRGB\u6a21\u6001\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6df1\u5ea6\u3001IMU\u3001\u6beb\u7c73\u6ce2\u7b49\u975eRGB\u6a21\u6001\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5927\u89c4\u6a21\u7684\u6570\u636e-\u63cf\u8ff0\u914d\u5bf9\u8d44\u6e90\u3002\u73b0\u6709\u7684\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\u901a\u5e38\u53ea\u63d0\u4f9b\u7c97\u7cd9\u7684\u6570\u636e\u6807\u7b7e\u6807\u6ce8\uff0c\u65e0\u6cd5\u6355\u6349\u52a8\u4f5c\u7684\u7ec6\u7c92\u5ea6\u52a8\u6001\u7279\u5f81\uff0c\u9650\u5236\u4e86\u4eba\u7c7b\u52a8\u4f5c\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86CUHK-X\u6570\u636e\u96c6\uff0c\u5305\u542b58,445\u4e2a\u6837\u672c\uff0c\u6db5\u76d630\u540d\u53c2\u4e0e\u8005\u5728\u4e24\u4e2a\u5ba4\u5185\u73af\u5883\u4e2d\u6267\u884c\u768440\u79cd\u52a8\u4f5c\u3002\u4e3a\u4e86\u6539\u5584\u63cf\u8ff0\u7684\u4e00\u81f4\u6027\uff0c\u91c7\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u573a\u666f\u521b\u5efa\u65b9\u6cd5\uff0c\u5229\u7528LLMs\u751f\u6210\u903b\u8f91\u8fde\u8d2f\u7684\u6d3b\u52a8\u5e8f\u5217\uff0c\u7136\u540e\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\u3002\u6570\u636e\u96c6\u5305\u542b\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u516d\u4e2a\u8bc4\u4f30\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728CUHK-X\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u4e3a\uff1a\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b76.52%\uff0c\u4eba\u7c7b\u52a8\u4f5c\u7406\u89e340.76%\uff0c\u4eba\u7c7b\u52a8\u4f5c\u63a8\u740670.25%\u3002\u8be5\u6570\u636e\u96c6\u65e8\u5728\u652f\u6301\u6570\u636e\u5bc6\u96c6\u578b\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u4eba\u7c7b\u6d3b\u52a8\u5206\u6790\u4e2d\u7684\u5e94\u7528\u548c\u53d1\u5c55\u3002", "conclusion": "CUHK-X\u662f\u4e00\u4e2a\u5168\u9762\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4e3a\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u3001\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8d44\u6e90\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u793e\u533a\u5728\u9c81\u68d2\u7684\u591a\u6a21\u6001\u4eba\u7c7b\u6d3b\u52a8\u5206\u6790\u65b9\u9762\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2512.07564", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07564", "abs": "https://arxiv.org/abs/2512.07564", "authors": ["Kassoum Sanogo", "Renzo Ardiccioni"], "title": "Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models", "comment": "24 pages, 3 figures, 2 tables. Training-free self-correction framework for vision-language models. Code and implementation details will be released at: https://github.com/kassoumsanogo1/self-correcting-vlm-re-Attention.git", "summary": "Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u6821\u6b63\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u89c6\u89c9\u91cd\u6ce8\u610f\u673a\u5236\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u5e7b\u89c9\u7387\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u5b9e\u9645\u9519\u8bef\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u6a21\u578b\uff0c\u6210\u672c\u8f83\u9ad8\u4e14\u4e0d\u7075\u6d3b\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u7ea0\u6b63\u8fd9\u4e9b\u5e7b\u89c9\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u514d\u8d39\u7684\u81ea\u6821\u6b63\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u7ef4\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08\u4ee4\u724c\u71b5\u3001\u6ce8\u610f\u529b\u5206\u6563\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u58f0\u660e\u7f6e\u4fe1\u5ea6\uff09\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u88c1\u526a\u673a\u5236\uff0c\u5bf9\u672a\u5145\u5206\u63a2\u7d22\u7684\u56fe\u50cf\u533a\u57df\u8fdb\u884c\u91cd\u65b0\u5173\u6ce8\uff0c\u5b8c\u5168\u4f7f\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u65e0\u9700\u68af\u5ea6\u66f4\u65b0\u3002", "result": "\u5728POPE\u548cMMHAL BENCH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528Qwen2.5-VL-7B\u67b6\u6784\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u5c06\u5e7b\u89c9\u7387\u964d\u4f4e\u4e869.8\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u5bf9\u6297\u6027\u5206\u5272\u4e0a\u7269\u4f53\u5b58\u5728\u51c6\u786e\u6027\u63d0\u9ad8\u4e864.7\u4e2a\u767e\u5206\u70b9\u3002\u5b9a\u6027\u5206\u6790\u8bc1\u5b9e\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u91cd\u6ce8\u610f\u6210\u529f\u5c06\u6821\u6b63\u57fa\u4e8e\u89c6\u89c9\u8bc1\u636e\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bad\u7ec3\u514d\u8d39\u81ea\u6821\u6b63\u6846\u67b6\u6709\u6548\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u89c6\u89c9\u91cd\u6ce8\u610f\u673a\u5236\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u53ef\u4fe1\u8d56\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2512.06750", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06750", "abs": "https://arxiv.org/abs/2512.06750", "authors": ["Weiqi Li", "Xuanyu Zhang", "Bin Chen", "Jingfen Xie", "Yan Wang", "Kexin Zhang", "Junlin Li", "Li Zhang", "Jian Zhang", "Shijie Zhao"], "title": "UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement", "comment": null, "summary": "Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.", "AI": {"tldr": "UARE\u662f\u9996\u4e2a\u7edf\u4e00\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u3001\u4fee\u590d\u548c\u589e\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u534f\u540c\u8bad\u7ec3\u8ba9\u8d28\u91cf\u8bc4\u4f30\u6307\u5bfc\u4fee\u590d\u8fc7\u7a0b", "motivation": "\u867d\u7136\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u548c\u4fee\u590d\u5728\u6982\u5ff5\u4e0a\u7d27\u5bc6\u76f8\u5173\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u5927\u591a\u5c06\u5b83\u4eec\u5206\u5f00\u5904\u7406\u3002\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3-\u751f\u6210\u6a21\u578b\u7684\u8fdb\u5c55\u8868\u660e\u66f4\u5f3a\u7684\u7406\u89e3\u80fd\u529b\u53ef\u4ee5\u63d0\u5347\u751f\u6210\u6027\u80fd\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u5982\u4f55\u7528\u8d28\u91cf\u8bc4\u4f30\u6307\u5bfc\u4fee\u590d\u7684\u7edf\u4e00\u6a21\u578b", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u6e10\u8fdb\u5f0f\u4ece\u5355\u4e00\u7c7b\u578b\u5931\u771f\u6269\u5c55\u5230\u9ad8\u9636\u6df7\u5408\u9000\u5316\uff1b2\uff09\u901a\u8fc7\u4ea4\u9519\u6587\u672c-\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u7edf\u4e00\u5fae\u8c03\uff0c\u5c06\u8d28\u91cf\u8bc4\u4f30\u4fe1\u53f7\u4e0e\u4fee\u590d\u76ee\u6807\u5bf9\u9f50", "result": "\u5728\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u3001\u4fee\u590d\u548c\u589e\u5f3a\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86UARE\u7684\u6709\u6548\u6027", "conclusion": "UARE\u662f\u9996\u4e2a\u7edf\u4e00\u5904\u7406\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u3001\u4fee\u590d\u548c\u589e\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u534f\u540c\u8bad\u7ec3\u5b9e\u73b0\u4e86\u8d28\u91cf\u8bc4\u4f30\u5bf9\u4fee\u590d\u8fc7\u7a0b\u7684\u6307\u5bfc\u4f5c\u7528"}}
{"id": "2512.07186", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07186", "abs": "https://arxiv.org/abs/2512.07186", "authors": ["Zhuoming Liu", "Xiaofeng Gao", "Feiyang Niu", "Qiaozi Gao", "Liu Liu", "Robinson Piramuthu"], "title": "START: Spatial and Textual Learning for Chart Understanding", "comment": "WACV2026 Camera Ready", "summary": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.", "AI": {"tldr": "START\u662f\u4e00\u4e2a\u7528\u4e8e\u56fe\u8868\u7406\u89e3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u6587\u672c\u5b66\u4e60\u589e\u5f3a\u5bf9\u56fe\u8868\u89c6\u89c9\u5e03\u5c40\u548c\u5e95\u5c42\u6570\u636e\u8868\u793a\u7684\u8054\u5408\u7406\u89e3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u56fe\u8868\u7406\u89e3\u5bf9\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u4e0e\u81ea\u7136\u56fe\u50cf\u4e0d\u540c\uff0c\u56fe\u8868\u540c\u65f6\u5305\u542b\u7ed3\u6784\u5316\u89c6\u89c9\u5e03\u5c40\uff08\u7a7a\u95f4\u5c5e\u6027\uff09\u548c\u5e95\u5c42\u6570\u636e\u8868\u793a\uff08\u6587\u672c\u5c5e\u6027\uff09\uff0c\u7406\u89e3\u8fd9\u4e24\u8005\u5bf9\u4e8e\u7cbe\u786e\u7684\u7ec6\u7c92\u5ea6\u56fe\u8868\u63a8\u7406\u662f\u5fc5\u8981\u7684\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u8fd9\u4e24\u4e2a\u65b9\u9762\u3002", "method": "\u63d0\u51faSTART\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u56fe\u8868\u5143\u7d20\u5b9a\u4f4d\uff0c\u589e\u5f3a\u5bf9\u56fe\u8868\u89c6\u89c9\u5e03\u5c40\u7684\u7406\u89e3\uff1b(2) \u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210\uff0c\u5f3a\u5316\u5bf9\u5e95\u5c42\u6570\u636e\u7ec6\u8282\u7684\u638c\u63e1\u3002\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\u521b\u5efaSTART-Dataset\uff0c\u9996\u5148\u5229\u7528MLLM\u5c06\u771f\u5b9e\u56fe\u8868\u56fe\u50cf\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u56fe\u8868\u4ee3\u7801\uff0c\u6062\u590d\u5e95\u5c42\u6570\u636e\u8868\u793a\u540c\u65f6\u4fdd\u6301\u771f\u5b9e\u56fe\u8868\u89c6\u89c9\u5206\u5e03\uff0c\u7136\u540e\u4f7f\u7528LLM\u6f14\u5316\u4ee3\u7801\u4ee5\u786e\u5b9a\u6355\u83b7\u56fe\u8868\u89c6\u89c9\u7ed3\u6784\u7684\u5143\u7d20\u4f4d\u7f6e\u3002", "result": "\u63d0\u51faChart Spatial understanding Benchmark (CS-Bench)\u6765\u8bc4\u4f30\u6a21\u578b\u7406\u89e3\u56fe\u8868\u7a7a\u95f4\u7ed3\u6784\u7684\u80fd\u529b\u3002START\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u90fd\u6bd4\u57fa\u7840\u6a21\u578b\u6709\u6301\u7eed\u63d0\u5347\uff0c\u5e76\u660e\u663e\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7a7a\u95f4\u548c\u6587\u672c\u5b66\u4e60\uff0cSTART\u80fd\u591f\u540c\u65f6\u7406\u89e3\u56fe\u8868\u7684\u89c6\u89c9\u5e03\u5c40\u548c\u5e95\u5c42\u6570\u636e\u8868\u793a\uff0c\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2512.07215", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07215", "abs": "https://arxiv.org/abs/2512.07215", "authors": ["Md Selim Sarowar", "Sungho Kim"], "title": "VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation", "comment": null, "summary": "Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6bd4\u8f83\u4e86\u57fa\u4e8eCLIP\u548cDINOv2\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u624b\u6301\u7269\u4f53\u6293\u53d6\u573a\u666f\u4e2d\u76843D\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\uff0c\u53d1\u73b0CLIP\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u800cDINOv2\u5728\u51e0\u4f55\u7279\u5f81\u63d0\u53d6\u65b9\u9762\u66f4\u80dc\u4e00\u7b79\u3002", "motivation": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u901a\u8fc7\u63d0\u4f9b\u4e30\u5bcc\u7684\u8bed\u4e49\u548c\u51e0\u4f55\u8868\u793a\uff0c\u5df2\u7ecf\u5f7b\u5e95\u6539\u53d8\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u3002\u672c\u7814\u7a76\u65e8\u5728\u5168\u9762\u6bd4\u8f83\u57fa\u4e8eCLIP\u548cDINOv2\u7684\u65b9\u6cd5\u5728\u624b\u6301\u7269\u4f53\u6293\u53d6\u573a\u666f\u4e2d\u76843D\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u6293\u53d6\u5e94\u7528\u63d0\u4f9b\u6a21\u578b\u9009\u62e9\u6307\u5bfc\u3002", "method": "\u5728\u624b\u6301\u7269\u4f53\u6293\u53d6\u573a\u666f\u4e2d\uff0c\u5bf9\u57fa\u4e8eCLIP\u548cDINOv2\u7684\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u7684\u89c6\u89c9\u6bd4\u8f83\u3002\u8bc4\u4f30\u4e24\u79cd\u6a21\u578b\u57286D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u5206\u6790\u5b83\u4eec\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e24\u79cd\u6a21\u578b\u5177\u6709\u4e92\u8865\u4f18\u52bf\uff1a\u57fa\u4e8eCLIP\u7684\u65b9\u6cd5\u901a\u8fc7\u8bed\u8a00\u57fa\u7840\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff1b\u800c\u57fa\u4e8eDINOv2\u7684\u65b9\u6cd5\u5728\u5bc6\u96c6\u51e0\u4f55\u7279\u5f81\u63d0\u53d6\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u5177\u6709\u589e\u5f3a\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "\u8be5\u5206\u6790\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u6293\u53d6\u548c\u62fe\u53d6\u5e94\u7528\u4e2d\u9009\u62e9\u5408\u9002\u7684\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u6839\u636e\u5177\u4f53\u4efb\u52a1\u9700\u6c42\uff08\u8bed\u4e49\u7406\u89e3vs\u51e0\u4f55\u7cbe\u5ea6\uff09\u9009\u62e9CLIP\u6216DINOv2\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.06783", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06783", "abs": "https://arxiv.org/abs/2512.06783", "authors": ["Tobias Leuthold", "Michele Xiloyannis", "Yves Zimmermann"], "title": "Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos", "comment": "16 pages, 5 figures", "summary": "Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5b9e\u65f6\u540e\u5904\u7406\u7b97\u6cd5\uff0c\u878d\u5408BlazePose\u76843D\u548c2D\u4f30\u8ba1\uff0c\u901a\u8fc7\u52a0\u6743\u4f18\u5316\u7ed3\u5408\u9aa8\u9abc\u957f\u5ea6\u7ea6\u675f\u548c\u751f\u7269\u529b\u5b66\u6a21\u578b\uff0c\u63d0\u5347\u59ff\u6001\u4f30\u8ba1\u7684\u89e3\u5256\u5b66\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u5b9e\u65f6\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\uff08\u5982BlazePose\uff09\u7f3a\u4e4f\u89e3\u5256\u5b66\u7ea6\u675f\uff0c\u5728\u7269\u7406\u6cbb\u7597\u7b49\u81ea\u52a8\u6559\u7ec3\u5e94\u7528\u4e2d\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u66f4\u51c6\u786e\u3001\u7b26\u5408\u751f\u7269\u529b\u5b66\u7684\u59ff\u6001\u4f30\u8ba1", "method": "\u91c7\u7528\u52a0\u6743\u4f18\u5316\u540e\u5904\u7406\u7b97\u6cd5\uff0c\u878d\u5408BlazePose\u76843D\u548c2D\u4f30\u8ba1\uff0c\u60e9\u7f5a\u4e0e\u9884\u671f\u9aa8\u9abc\u957f\u5ea6\u548c\u751f\u7269\u529b\u5b66\u6a21\u578b\u7684\u504f\u5dee\uff0c\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u6839\u636e\u4e2a\u4f53\u89e3\u5256\u7ed3\u6784\u7cbe\u70bc\u9aa8\u9abc\u957f\u5ea6\u4f30\u8ba1", "result": "\u5728Physio2.2M\u6570\u636e\u96c6\u4e0a\uff0c3D MPJPE\u964d\u4f4e10.2%\uff0c\u8eab\u4f53\u6bb5\u95f4\u89d2\u5ea6\u8bef\u5dee\u51cf\u5c1116.6%\uff0c\u76f8\u6bd4BlazePose 3D\u4f30\u8ba1\u6709\u660e\u663e\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u3001\u89e3\u5256\u5b66\u4e00\u81f4\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6d88\u8d39\u7ea7\u8bbe\u5907\u7684\u81ea\u52a8\u7269\u7406\u6cbb\u7597\u3001\u533b\u7597\u4fdd\u5065\u548c\u8fd0\u52a8\u6559\u7ec3\u5e94\u7528\uff0c\u540e\u7aef\u8fd0\u884c\u4ec5\u4f7f\u7528\u533f\u540d\u6570\u636e"}}
{"id": "2512.07234", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07234", "abs": "https://arxiv.org/abs/2512.07234", "authors": ["Biao Chen", "Lin Zuo", "Mengmeng Jing", "Kunbin He", "Yuchen Wang"], "title": "Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models", "comment": null, "summary": "Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.", "AI": {"tldr": "\u63d0\u51faDropout Prompt Learning\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u6a21\u6001\u5185\u4e0a\u4e0b\u6587\u548c\u6a21\u6001\u95f4\u5bf9\u9f50\u7684token\u91cd\u8981\u6027\u8bc4\u4f30\uff0c\u4e3a\u6587\u672c\u548c\u89c6\u89c9\u5206\u652f\u7684token\u5e94\u7528\u7075\u6d3bdropout\u6982\u7387\uff0c\u5e76\u7ed3\u5408\u6b8b\u5dee\u71b5\u6b63\u5219\u5316\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "Dropout\u4f5c\u4e3a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6b63\u5219\u5316\u6280\u672f\u80fd\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4f20\u7edfdropout\u65b9\u6cd5\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u4e0d\u591f\u7075\u6d3b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u8003\u8651\u6a21\u6001\u5185\u4e0a\u4e0b\u6587\u548c\u6a21\u6001\u95f4\u5bf9\u9f50\u7684token\u91cd\u8981\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u4f4e\u6837\u672c\u5b66\u4e60\u3001\u957f\u5c3e\u5206\u7c7b\u548c\u5206\u5e03\u5916\u6cdb\u5316\u7b49\u6311\u6218\u6027\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faDropout Prompt Learning\uff1a1\uff09\u5728\u6587\u672c\u548c\u89c6\u89c9\u5206\u652f\u7684token\u4e0a\u5e94\u7528dropout\uff0c\u57fa\u4e8e\u6a21\u6001\u5185\u4e0a\u4e0b\u6587\u548c\u6a21\u6001\u95f4\u5bf9\u9f50\u8bc4\u4f30\u6bcf\u4e2atoken\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u4e0d\u540ctoken\u5206\u914d\u7075\u6d3b\u7684dropout\u6982\u7387\uff1b2\uff09\u63d0\u51fa\u6b8b\u5dee\u71b5\u6b63\u5219\u5316\uff0c\u5728\u4fdd\u6301\u901a\u7528\u77e5\u8bc6\u4f20\u9012\u7684\u8bed\u4e49\u5bf9\u9f50\u540c\u65f6\uff0c\u9f13\u52b1dropout\u5f15\u5165\u7684\u591a\u6837\u5316\u8868\u793a\u3002", "result": "\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6837\u672c\u5b66\u4e60\u3001\u957f\u5c3e\u5206\u7c7b\u548c\u5206\u5e03\u5916\u6cdb\u5316\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u5728\u57fa\u7840\u5230\u65b0\u9896\u6cdb\u5316\u4efb\u52a1\u4e0a\uff0c\u6027\u80fd\u8d85\u8d8a\u6b63\u5219\u5316\u65b9\u6cd5KgCoOp 5.10%\u548cPromptSRC 2.13%\u3002", "conclusion": "Dropout Prompt Learning\u901a\u8fc7\u7075\u6d3b\u7684token\u7ea7dropout\u7b56\u7565\u548c\u6b8b\u5dee\u71b5\u6b63\u5219\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u79cd\u6311\u6218\u6027\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.06802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06802", "abs": "https://arxiv.org/abs/2512.06802", "authors": ["Yutong Wang", "Haiyu Zhang", "Tianfan Xue", "Yu Qiao", "Yaohui Wang", "Chang Xu", "Xinyuan Chen"], "title": "VDOT: Efficient Unified Video Creation via Optimal Transport Distillation", "comment": null, "summary": "The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.", "AI": {"tldr": "VDOT\u662f\u4e00\u4e2a\u9ad8\u6548\u7edf\u4e00\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5e03\u5339\u914d\u84b8\u998f\u548c\u6700\u4f18\u4f20\u8f93\u6280\u672f\u4f18\u5316\uff0c\u4ec5\u97004\u6b65\u63a8\u7406\u5c31\u80fd\u8fbe\u5230\u4f20\u7edf\u65b9\u6cd5100\u6b65\u7684\u6548\u679c\uff0c\u540c\u65f6\u652f\u6301\u591a\u79cd\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u8981\u4e48\u53ea\u80fd\u5904\u7406\u5c11\u6570\u7279\u5b9a\u6761\u4ef6\uff0c\u8981\u4e48\u56e0\u63a8\u7406\u8fc7\u7a0b\u590d\u6742\u5bfc\u81f4\u751f\u6210\u65f6\u95f4\u8fc7\u957f\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u90e8\u7f72\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u65e2\u9ad8\u6548\u53c8\u80fd\u7edf\u4e00\u5904\u7406\u591a\u79cd\u6761\u4ef6\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5339\u914d\u84b8\u998f\u8303\u5f0f\uff0c\u7528\u6700\u4f18\u4f20\u8f93\u8ddd\u79bb\u66ff\u4ee3KL\u6563\u5ea6\u6765\u4f18\u5316\u771f\u5b9e\u4e0e\u751f\u6210\u5206\u6570\u5206\u5e03\u7684\u5dee\u5f02\uff0c\u907f\u514d\u68af\u5ea6\u5d29\u6e83\u95ee\u9898\u3002\u540c\u65f6\u96c6\u6210\u5224\u522b\u5668\u611f\u77e5\u771f\u5b9e\u89c6\u9891\u6570\u636e\uff0c\u5e76\u6784\u5efa\u81ea\u52a8\u5316\u89c6\u9891\u6807\u6ce8\u8fc7\u6ee4\u7ba1\u9053\u652f\u6301\u591a\u4efb\u52a1\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u97004\u6b65\u63a8\u7406\u7684VDOT\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u80fd\u591f\u8d85\u8d8a\u6216\u5339\u914d\u5176\u4ed6\u9700\u8981100\u6b65\u53bb\u566a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7684\u751f\u6210\u901f\u5ea6\u3002", "conclusion": "VDOT\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u6280\u672f\u548c\u5206\u5e03\u5339\u914d\u84b8\u998f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7edf\u4e00\u7684\u89c6\u9891\u751f\u6210\uff0c\u4ec5\u9700\u6781\u5c11\u63a8\u7406\u6b65\u9aa4\u5c31\u80fd\u8fbe\u5230\u9ad8\u8d28\u91cf\u751f\u6210\u6548\u679c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07253", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07253", "abs": "https://arxiv.org/abs/2512.07253", "authors": ["Handing Xu", "Zhenguo Nie", "Tairan Peng", "Huimin Pan", "Xin-Jun Liu"], "title": "DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement", "comment": "18 pages, 8 figures, and 7 tables", "summary": "Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9000\u5316\u611f\u77e5\u7684\u5b9e\u65f6\u5185\u7aa5\u955c\u89c6\u9891\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5e27\u4f20\u64ad\u9000\u5316\u8868\u793a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5b9e\u65f6\u589e\u5f3a", "motivation": "\u5185\u7aa5\u955c\u624b\u672f\u4f9d\u8d56\u672f\u4e2d\u89c6\u9891\uff0c\u4f46\u89c6\u9891\u5e38\u56e0\u5149\u7167\u4e0d\u5747\u3001\u7ec4\u7ec7\u6563\u5c04\u3001\u906e\u6321\u548c\u8fd0\u52a8\u6a21\u7cca\u800c\u8d28\u91cf\u4e0b\u964d\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u91cf\u8fc7\u5927\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528", "method": "\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u63d0\u53d6\u56fe\u50cf\u9000\u5316\u8868\u793a\uff0c\u5f15\u5165\u878d\u5408\u673a\u5236\u7528\u9000\u5316\u8868\u793a\u8c03\u5236\u56fe\u50cf\u7279\u5f81\u6307\u5bfc\u5355\u5e27\u589e\u5f3a\u6a21\u578b\uff0c\u901a\u8fc7\u9000\u5316\u4e0e\u6062\u590d\u56fe\u50cf\u95f4\u7684\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\u8bad\u7ec3", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u6027\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u8fbe\u5230\u4f18\u8d8a\u5e73\u8861\uff0c\u4f18\u4e8e\u591a\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u9000\u5316\u611f\u77e5\u5efa\u6a21\u5bf9\u5b9e\u65f6\u5185\u7aa5\u955c\u89c6\u9891\u589e\u5f3a\u6709\u6548\uff0c\u9690\u5f0f\u5b66\u4e60\u548c\u4f20\u64ad\u9000\u5316\u8868\u793a\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84"}}
{"id": "2512.06818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06818", "abs": "https://arxiv.org/abs/2512.06818", "authors": ["Jan Held", "Sanghyun Son", "Renaud Vandeghen", "Daniel Rebain", "Matheus Gadelha", "Yi Zhou", "Anthony Cioppa", "Ming C. Lin", "Marc Van Droogenbroeck", "Andrea Tagliasacchi"], "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes", "comment": null, "summary": "Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.", "AI": {"tldr": "MeshSplatting\u662f\u4e00\u79cd\u57fa\u4e8e\u7f51\u683c\u7684\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u8054\u5408\u4f18\u5316\u51e0\u4f55\u548c\u5916\u89c2\uff0c\u5c06\u795e\u7ecf\u6e32\u67d3\u4e0e\u4ea4\u4e92\u5f0f3D\u56fe\u5f62\u6865\u63a5\uff0c\u5b9e\u73b0\u5b9e\u65f6\u573a\u666f\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u57fa\u5143\u7684\u6e85\u5c04\u65b9\u6cd5\uff08\u59823D\u9ad8\u65af\u6e85\u5c04\uff09\u867d\u7136\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6e32\u67d3\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u4f46\u5176\u57fa\u4e8e\u70b9\u7684\u8868\u793a\u4e0eAR/VR\u548c\u6e38\u620f\u5f15\u64ce\u4e2d\u57fa\u4e8e\u7f51\u683c\u7684\u6d41\u6c34\u7ebf\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e0e\u73b0\u67093D\u5f15\u64ce\u65e0\u7f1d\u96c6\u6210\u7684\u7f51\u683c\u8868\u793a\u65b9\u6cd5\u3002", "method": "MeshSplatting\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u8054\u5408\u4f18\u5316\u51e0\u4f55\u548c\u5916\u89c2\uff0c\u4f7f\u7528\u53d7\u9650Delaunay\u4e09\u89d2\u5256\u5206\u5f3a\u5236\u8fde\u63a5\u6027\uff0c\u5e76\u7ec6\u5316\u8868\u9762\u4e00\u81f4\u6027\uff0c\u521b\u5efa\u7aef\u5230\u7aef\u5e73\u6ed1\u3001\u89c6\u89c9\u9ad8\u8d28\u91cf\u7684\u7f51\u683c\u3002", "result": "\u5728Mip-NeRF360\u6570\u636e\u96c6\u4e0a\uff0cMeshSplatting\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5MiLo\u63d0\u5347\u4e86+0.69 dB\u7684PSNR\uff0c\u540c\u65f6\u8bad\u7ec3\u901f\u5ea6\u5feb2\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c112\u500d\u3002", "conclusion": "MeshSplatting\u6210\u529f\u6865\u63a5\u4e86\u795e\u7ecf\u6e32\u67d3\u548c\u4ea4\u4e92\u5f0f3D\u56fe\u5f62\uff0c\u4e3a\u5b9e\u65f6\u573a\u666f\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65e0\u7f1d\u7684\u7f51\u683c\u8868\u793a\uff0c\u80fd\u591f\u5728\u5b9e\u65f63D\u5f15\u64ce\u4e2d\u9ad8\u6548\u6e32\u67d3\u3002"}}
{"id": "2512.07328", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07328", "abs": "https://arxiv.org/abs/2512.07328", "authors": ["Ziyang Mai", "Yu-Wing Tai"], "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation", "comment": null, "summary": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.", "AI": {"tldr": "ContextAnyone\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u5f20\u53c2\u8003\u56fe\u50cf\u5b9e\u73b0\u89d2\u8272\u4e00\u81f4\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u53d1\u578b\u3001\u670d\u88c5\u3001\u4f53\u578b\u7b49\u4e0a\u4e0b\u6587\u7279\u5f81\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u4fdd\u6301\u89d2\u8272\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u65e0\u6cd5\u6709\u6548\u4fdd\u7559\u53d1\u578b\u3001\u670d\u88c5\u3001\u4f53\u578b\u7b49\u91cd\u8981\u7684\u4e0a\u4e0b\u6587\u89c6\u89c9\u7ebf\u7d22\uff0c\u8fd9\u5f71\u54cd\u4e86\u89c6\u9891\u7684\u89c6\u89c9\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51faContextAnyone\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u91cd\u5efa\u53c2\u8003\u56fe\u50cf\u548c\u751f\u6210\u65b0\u89c6\u9891\u5e27\uff0c\u4f7f\u6a21\u578b\u80fd\u5145\u5206\u611f\u77e5\u53c2\u8003\u4fe1\u606f\u3002\u91c7\u7528Emphasize-Attention\u6a21\u5757\u9009\u62e9\u6027\u589e\u5f3a\u53c2\u8003\u611f\u77e5\u7279\u5f81\uff0c\u9632\u6b62\u8eab\u4efd\u6f02\u79fb\uff1b\u4f7f\u7528\u53cc\u6307\u5bfc\u635f\u5931\u7ed3\u5408\u6269\u6563\u548c\u53c2\u8003\u91cd\u5efa\u76ee\u6807\uff1b\u63d0\u51faGap-RoPE\u4f4d\u7f6e\u5d4c\u5165\u5206\u79bb\u53c2\u8003\u548c\u89c6\u9891token\u4ee5\u7a33\u5b9a\u65f6\u5e8f\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660eContextAnyone\u5728\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u53c2\u8003\u5230\u89c6\u9891\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u52a8\u4f5c\u548c\u573a\u666f\u4e2d\u751f\u6210\u8fde\u8d2f\u4e14\u4fdd\u6301\u4e0a\u4e0b\u6587\u7279\u5f81\u7684\u89d2\u8272\u89c6\u9891\u3002", "conclusion": "ContextAnyone\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6269\u6563\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u89d2\u8272\u4e00\u81f4\u6027\u7684\u89c6\u9891\u751f\u6210\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u89d2\u8272\u8eab\u4efd\u5b8c\u6574\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2512.06838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06838", "abs": "https://arxiv.org/abs/2512.06838", "authors": ["Jiahao Wang", "Zhongwei Jiang", "Wenchao Sun", "Jiaru Zhong", "Haibao Yu", "Yuner Zhang", "Chenyang Lu", "Chuang Zhang", "Lei He", "Shaobing Xu", "Jianqiang Wang"], "title": "SparseCoop: Cooperative Perception with Kinematic-Grounded Queries", "comment": "Accepted by AAAI 2026", "summary": "Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.", "AI": {"tldr": "SparseCoop\u662f\u4e00\u4e2a\u5b8c\u5168\u7a00\u758f\u7684\u534f\u540c\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e3D\u68c0\u6d4b\u548c\u8ddf\u8e2a\uff0c\u5b8c\u5168\u6452\u5f03\u4e86\u4e2d\u95f4BEV\u8868\u793a\uff0c\u901a\u8fc7\u5b9e\u4f8b\u67e5\u8be2\u3001\u7c97\u5230\u7cbe\u805a\u5408\u548c\u534f\u540c\u5b9e\u4f8b\u53bb\u566a\u5b9e\u73b0\u9ad8\u6548\u534f\u540c\u611f\u77e5\u3002", "motivation": "\u5f53\u524d\u534f\u540c\u611f\u77e5\u65b9\u6cd5\u5b58\u5728\u901a\u4fe1\u6210\u672c\u9ad8\u3001\u7075\u6d3b\u6027\u5dee\u3001\u5bf9\u9f50\u4e0d\u7cbe\u786e\u7b49\u95ee\u9898\u3002\u57fa\u4e8e\u5bc6\u96c6BEV\u7279\u5f81\u5171\u4eab\u7684\u65b9\u6cd5\u9762\u4e34\u4e8c\u6b21\u589e\u957f\u7684\u901a\u4fe1\u6210\u672c\uff0c\u4e14\u7f3a\u4e4f\u8de8\u5f02\u6b65\u6216\u4e0d\u540c\u89c6\u89d2\u7684\u7cbe\u786e\u5bf9\u9f50\u80fd\u529b\u3002\u800c\u7a00\u758f\u67e5\u8be2\u65b9\u6cd5\u5219\u5b58\u5728\u51e0\u4f55\u8868\u793a\u4e0d\u8db3\u3001\u878d\u5408\u7b56\u7565\u6b21\u4f18\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faSparseCoop\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a1) \u57fa\u4e8e\u8fd0\u52a8\u5b66\u7684\u5b9e\u4f8b\u67e5\u8be2\uff0c\u4f7f\u7528\u5305\u542b3D\u51e0\u4f55\u548c\u901f\u5ea6\u7684\u663e\u5f0f\u72b6\u6001\u5411\u91cf\u8fdb\u884c\u7cbe\u786e\u65f6\u7a7a\u5bf9\u9f50\uff1b2) \u7c97\u5230\u7cbe\u805a\u5408\u6a21\u5757\u5b9e\u73b0\u9c81\u68d2\u878d\u5408\uff1b3) \u534f\u540c\u5b9e\u4f8b\u53bb\u566a\u4efb\u52a1\u52a0\u901f\u548c\u7a33\u5b9a\u8bad\u7ec3\u3002\u5b8c\u5168\u6452\u5f03\u4e2d\u95f4BEV\u8868\u793a\u3002", "result": "\u5728V2X-Seq\u548cGriffin\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSparseCoop\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5177\u6709\u5353\u8d8a\u7684\u8ba1\u7b97\u6548\u7387\u3001\u4f4e\u4f20\u8f93\u6210\u672c\u548c\u5f3a\u5927\u7684\u901a\u4fe1\u5ef6\u8fdf\u9c81\u68d2\u6027\u3002", "conclusion": "SparseCoop\u901a\u8fc7\u5b8c\u5168\u7a00\u758f\u7684\u534f\u540c\u611f\u77e5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u901a\u4fe1\u6210\u672c\u3001\u5bf9\u9f50\u7cbe\u5ea6\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u9650\u5236\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u534f\u540c\u611f\u77e5\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07351", "categories": ["cs.CV", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.07351", "abs": "https://arxiv.org/abs/2512.07351", "authors": ["Sayeem Been Zaman", "Wasimul Karim", "Arefin Ittesafun Abian", "Reem E. Mohamed", "Md Rafiqul Islam", "Asif Karim", "Sami Azam"], "title": "DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection", "comment": null, "summary": "The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.", "AI": {"tldr": "DeepAgent\uff1a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u548c\u97f3\u9891\u6a21\u6001\u7684\u4e92\u8865\u5206\u6790\uff0c\u7ed3\u5408\u968f\u673a\u68ee\u6797\u5143\u5206\u7c7b\u5668\u63d0\u5347\u68c0\u6d4b\u6027\u80fd", "motivation": "\u5408\u6210\u5a92\u4f53\u7279\u522b\u662f\u6df1\u5ea6\u4f2a\u9020\u7684\u65e5\u76ca\u666e\u53ca\u7ed9\u6570\u5b57\u5185\u5bb9\u9a8c\u8bc1\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u97f3\u9891\u548c\u89c6\u89c9\u4fe1\u606f\u96c6\u6210\u5728\u5355\u4e00\u6a21\u578b\u4e2d\uff0c\u5bb9\u6613\u53d7\u5230\u6a21\u6001\u4e0d\u5339\u914d\u3001\u566a\u58f0\u548c\u64cd\u7eb5\u7684\u5f71\u54cd\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u6846\u67b6\u6765\u5e94\u5bf9\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faDeepAgent\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u667a\u80fd\u4f53\uff1aAgent-1\u4f7f\u7528\u7b80\u5316\u7684AlexNet-based CNN\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u64cd\u4f5c\u75d5\u8ff9\uff1bAgent-2\u901a\u8fc7\u7ed3\u5408\u58f0\u5b66\u7279\u5f81\u3001Whisper\u97f3\u9891\u8f6c\u5f55\u548cEasyOCR\u5e27\u8bfb\u53d6\u5e8f\u5217\u6765\u68c0\u6d4b\u89c6\u542c\u4e0d\u4e00\u81f4\u6027\u3002\u4e24\u4e2a\u667a\u80fd\u4f53\u7684\u51b3\u7b56\u901a\u8fc7\u968f\u673a\u68ee\u6797\u5143\u5206\u7c7b\u5668\u878d\u5408\uff0c\u5229\u7528\u5404\u81ea\u5b66\u4e60\u7684\u4e0d\u540c\u51b3\u7b56\u8fb9\u754c\u63d0\u5347\u6700\u7ec8\u6027\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff1aAgent-1\u5728Celeb-DF\u548cFakeAVCeleb\u7ec4\u5408\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.35%\u6d4b\u8bd5\u51c6\u786e\u7387\uff1bAgent-2\u5728FakeAVCeleb\u4e0a\u8fbe\u523093.69%\u51c6\u786e\u7387\uff1b\u5143\u5206\u7c7b\u5668\u5728FakeAVCeleb\u4e0a\u8fbe\u523081.56%\u51c6\u786e\u7387\u3002\u8de8\u6570\u636e\u96c6\u9a8c\u8bc1\u4e2d\uff0c\u5728DeepFakeTIMIT\u4e0a\u5143\u5206\u7c7b\u5668\u8fbe\u523097.49%\u6700\u7ec8\u51c6\u786e\u7387\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u5c42\u6b21\u7ed3\u6784\u7684\u878d\u5408\u901a\u8fc7\u7f13\u89e3\u5355\u4e2a\u6a21\u6001\u7684\u5f31\u70b9\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u5904\u7406\u6df1\u5ea6\u4f2a\u9020\u4e2d\u591a\u6837\u5316\u64cd\u7eb5\u7c7b\u578b\u7684\u6709\u6548\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06840", "abs": "https://arxiv.org/abs/2512.06840", "authors": ["Satoshi Hashimoto", "Tatsuya Konishi", "Tomoya Kaichi", "Kazunori Matsumoto", "Mori Kurokawa"], "title": "CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles", "comment": "Accepted to WACV 2026", "summary": "Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the \"incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6301\u7eed\u5b66\u4e60\u548c\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5CADE\uff0c\u901a\u8fc7\u53cc\u751f\u6210\u5668\u548c\u591a\u5224\u522b\u5668\u96c6\u6210\u6765\u89e3\u51b3\u9886\u57df\u504f\u79fb\u548c\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u6570\u636e\u96c6\uff0c\u5ffd\u89c6\u4e86\u6570\u636e\u9886\u57df\u53ef\u80fd\u53d8\u5316\u7684\u95ee\u9898\u3002\u5f53\u6570\u636e\u9886\u57df\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u9700\u8981\u6301\u7eed\u5b66\u4e60\u7684\u89c6\u89d2\uff0c\u5426\u5219\u4ec5\u7528\u65b0\u6570\u636e\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u5bf9\u5148\u524d\u6570\u636e\u7684\u6027\u80fd\u4e0b\u964d\uff08\u9057\u5fd8\uff09\u3002", "method": "\u63d0\u51fa\u4e86CADE\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u53cc\u751f\u6210\u5668\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u548c\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff1b2\uff09\u63d0\u51fa\u591a\u5224\u522b\u5668\u96c6\u6210\u6765\u6355\u6349\u56e0\u9057\u5fd8\u800c\u9057\u6f0f\u7684\u8fc7\u53bb\u573a\u666f\u4e2d\u7684\u5f02\u5e38\u6a21\u5f0f\u3002", "result": "\u5728\u5e38\u89c1\u7684\u591a\u573a\u666f\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\uff08\u5982\u4e0a\u6d77\u79d1\u6280\u548c\u590f\u6d1b\u7279\u5f02\u5e38\u6570\u636e\u96c6\uff09\u4e0a\uff0cCADE\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "CADE\u662f\u9996\u4e2a\u7ed3\u5408\u6301\u7eed\u5b66\u4e60\u548c\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u89c6\u89d2\u7684\u5de5\u4f5c\uff0c\u901a\u8fc7\u53cc\u751f\u6210\u5668\u548c\u591a\u5224\u522b\u5668\u96c6\u6210\u6709\u6548\u89e3\u51b3\u4e86\u9886\u57df\u504f\u79fb\u548c\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u591a\u573a\u666f\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.07360", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07360", "abs": "https://arxiv.org/abs/2512.07360", "authors": ["Qiming Huang", "Hao Ai", "Jianbo Jiao"], "title": "Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation", "comment": "Accepted to WACV2026", "summary": "Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u7684\u7279\u5f81\u6821\u6b63\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u533a\u57df\u90bb\u63a5\u56fe\u6765\u6355\u6349\u5c40\u90e8\u7ed3\u6784\u5173\u7cfb\uff0c\u4ece\u800c\u6539\u5584CLIP\u7279\u5f81\u5728\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u5c40\u90e8\u5224\u522b\u80fd\u529b\uff0c\u51cf\u5c11\u566a\u58f0\u5e76\u63d0\u5347\u4e00\u81f4\u6027\u3002", "motivation": "CLIP\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u5bf9\u9884\u8bad\u7ec3\u4e2d\u4e3b\u8981\u5173\u6ce8\u5168\u5c40\u8bed\u4e49\u5bf9\u9f50\uff0c\u5bfc\u81f4\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u533a\u57df\u4e0e\u6587\u672c\u5173\u8054\u65f6\u6027\u80fd\u4e0d\u4f73\uff0c\u4ea7\u751f\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u7684\u9884\u6d4b\u3002\u8fd9\u6e90\u4e8e\u5bf9\u6bd4\u8bad\u7ec3\u8303\u5f0f\u4ea7\u751f\u7684\u5206\u6563\u504f\u5dee\uff0c\u4ec5\u4f7f\u7528CLIP\u7279\u5f81\u96be\u4ee5\u7f13\u89e3\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u6821\u6b63\u65b9\u6cd5\uff0c\u57fa\u4e8e\u56fe\u50cf\u7684\u4f4e\u7ea7\u7279\u5f81\uff08\u5982\u989c\u8272\u548c\u7eb9\u7406\uff09\u6784\u5efa\u533a\u57df\u90bb\u63a5\u56fe\uff08RAG\uff09\u6765\u6355\u6349\u5c40\u90e8\u7ed3\u6784\u5173\u7cfb\uff0c\u5229\u7528\u8be5\u56fe\u901a\u8fc7\u589e\u5f3a\u5c40\u90e8\u5224\u522b\u80fd\u529b\u6765\u7cbe\u70bcCLIP\u7279\u5f81\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6291\u5236\u5206\u5272\u566a\u58f0\uff0c\u6539\u5584\u533a\u57df\u7ea7\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u56fe\u50cf\u672c\u8eab\u7684\u7ed3\u6784\u5148\u9a8c\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86CLIP\u7279\u5f81\u5728\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u5c40\u90e8\u5224\u522b\u4e0d\u8db3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u8d28\u91cf\u3002"}}
{"id": "2512.06845", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06845", "abs": "https://arxiv.org/abs/2512.06845", "authors": ["Satoshi Hashimoto", "Hitoshi Nishimura", "Yanan Wang", "Mori Kurokawa"], "title": "Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection", "comment": null, "summary": "Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.", "AI": {"tldr": "PA-VAD\uff1a\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u5f02\u5e38\u89c6\u9891\u7684\u751f\u6210\u9a71\u52a8\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u4f2a\u5f02\u5e38\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u6807\u51c6\u5f31\u76d1\u7763\u8bbe\u7f6e\u4e0b\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u5b9e\u9645\u90e8\u7f72\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u771f\u5b9e\u5f02\u5e38\u89c6\u9891\u7a00\u7f3a\u548c\u6536\u96c6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u5f02\u5e38\u89c6\u9891\u7684\u8bad\u7ec3\u65b9\u6cd5", "method": "1) \u5408\u6210\u9636\u6bb5\uff1a\u4f7f\u7528CLIP\u9009\u62e9\u7c7b\u522b\u76f8\u5173\u521d\u59cb\u56fe\u50cf\uff0c\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u6587\u672c\u63d0\u793a\uff0c\u8c03\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u4f2a\u5f02\u5e38\u89c6\u9891\uff1b2) \u8bad\u7ec3\u9636\u6bb5\uff1a\u901a\u8fc7\u57df\u5bf9\u9f50\u6b63\u5219\u5316\u6a21\u5757\u7f13\u89e3\u5408\u6210\u5f02\u5e38\u4e2d\u7684\u8fc7\u5ea6\u65f6\u7a7a\u5e45\u5ea6\u95ee\u9898\uff0c\u7ed3\u5408\u57df\u5bf9\u9f50\u548c\u5185\u5b58\u4f7f\u7528\u611f\u77e5\u66f4\u65b0", "result": "\u5728ShanghaiTech\u4e0a\u8fbe\u523098.2%\uff0c\u5728UCF-Crime\u4e0a\u8fbe\u523082.5%\uff0c\u5206\u522b\u6bd4\u6700\u5f3a\u771f\u5b9e\u5f02\u5e38\u65b9\u6cd5\u9ad8+0.6%\uff0c\u6bd4UVAD SOTA\u65b9\u6cd5\u9ad8+1.9%", "conclusion": "\u65e0\u9700\u6536\u96c6\u771f\u5b9e\u5f02\u5e38\u5373\u53ef\u83b7\u5f97\u9ad8\u7cbe\u5ea6\u5f02\u5e38\u68c0\u6d4b\uff0c\u4e3a\u53ef\u6269\u5c55\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84"}}
{"id": "2512.07415", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07415", "abs": "https://arxiv.org/abs/2512.07415", "authors": ["Gabriele Galatolo", "Mirco Nanni"], "title": "Data-driven Exploration of Mobility Interaction Patterns", "comment": null, "summary": "Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u6316\u6398\u7684\u4e2a\u4f53\u79fb\u52a8\u884c\u4e3a\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u6570\u636e\u4e2d\u76f4\u63a5\u53d1\u73b0\u4e2a\u4f53\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u8bc1\u636e\u548c\u6a21\u5f0f\uff0c\u800c\u4e0d\u662f\u4ece\u9884\u8bbe\u884c\u4e3a\u6a21\u578b\u51fa\u53d1\u3002", "motivation": "\u7406\u89e3\u4e2a\u4f53\u79fb\u52a8\u884c\u4e3a\u53ca\u5176\u5bf9\u5916\u90e8\u73af\u5883\u7684\u53cd\u5e94\u662f\u6a21\u62df\u4eba\u7c7b\u7269\u7406\u52a8\u6001\u7684\u5173\u952e\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4ece\u9884\u8bbe\u7684\u884c\u4e3a\u6a21\u578b\u51fa\u53d1\uff0c\u800c\u672c\u6587\u65e8\u5728\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u6316\u6398\u4e2a\u4f53\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u8bc1\u636e\u548c\u6a21\u5f0f\uff0c\u4e3a\u6539\u8fdb\u73b0\u6709\u6a21\u62df\u6a21\u578b\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002", "method": "\u91c7\u7528\u6570\u636e\u6316\u6398\u89c6\u89d2\uff0c\u4ece\u6570\u636e\u4e2d\u641c\u7d22\u53ef\u80fd\u53cd\u6620\u4e2a\u4f53\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u79fb\u52a8\u4e8b\u4ef6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5bfb\u627e\u590d\u6742\u3001\u6301\u4e45\u7684\u4e8b\u4ef6\u6a21\u5f0f\u548c\u968f\u65f6\u95f4\u6f14\u5316\u7684\u914d\u7f6e\u3002\u5728\u4e24\u4e2a\u771f\u5b9e\u6848\u4f8b\uff08\u6c7d\u8f66\u548c\u884c\u4eba\uff09\u4e0a\u5b9e\u4f8b\u5316\u8be5\u65b9\u6cd5\u3002", "result": "\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5305\u62ec\u6027\u80fd\u3001\u53c2\u6570\u654f\u611f\u6027\u548c\u6837\u672c\u7ed3\u679c\u89e3\u91ca\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u53d1\u73b0\u4e2a\u4f53\u95f4\u79fb\u52a8\u76f8\u4e92\u4f5c\u7528\u7684\u673a\u5236\uff0c\u4e3a\u6539\u8fdb\u73b0\u6709\u6a21\u62df\u6a21\u578b\u63d0\u4f9b\u6f5c\u5728\u5e2e\u52a9\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u80fd\u591f\u4ece\u79fb\u52a8\u6570\u636e\u4e2d\u76f4\u63a5\u53d1\u73b0\u4e2a\u4f53\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u6a21\u5f0f\u548c\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u4eba\u7c7b\u52a8\u6001\u548c\u6539\u5584\u4eba\u7fa4\u6a21\u62df\u3001\u5e94\u6025\u7ba1\u7406\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\u3002"}}
{"id": "2512.06849", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06849", "abs": "https://arxiv.org/abs/2512.06849", "authors": ["Matan Atad", "Alexander W. Marka", "Lisa Steinhelfer", "Anna Curto-Vilalta", "Yannik Leonhardt", "Sarah C. Foreman", "Anna-Sophia Walburga Dietrich", "Robert Graf", "Alexandra S. Gersing", "Bjoern Menze", "Daniel Rueckert", "Jan S. Kirschke", "Hendrik M\u00f6ller"], "title": "Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT", "comment": "In submission", "summary": "Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u690e\u4f53\u7ea7\u522b\u7684\u5065\u5eb7/\u6076\u6027\u6807\u7b7e\uff08\u65e0\u9700\u75c5\u7076\u63a9\u7801\uff09\uff0c\u901a\u8fc7\u6269\u6563\u81ea\u7f16\u7801\u5668\u548c\u50cf\u7d20\u7ea7\u5dee\u5f02\u56fe\u751f\u6210\u5019\u9009\u75c5\u7076\u533a\u57df\uff0c\u518d\u901a\u8fc7Hide-and-Seek Attribution\u6280\u672f\u7b5b\u9009\u51fa\u771f\u6b63\u53cd\u6620\u6076\u6027\u7684\u533a\u57df\uff0c\u5b9e\u73b0\u690e\u4f53\u8f6c\u79fb\u7624\u7684\u51c6\u786e\u5206\u5272\u3002", "motivation": "CT\u4e2d\u690e\u4f53\u8f6c\u79fb\u7624\u7684\u51c6\u786e\u5206\u5272\u5728\u4e34\u5e8a\u4e0a\u5f88\u91cd\u8981\u4f46\u96be\u4ee5\u89c4\u6a21\u5316\uff0c\u56e0\u4e3a\u4f53\u7d20\u7ea7\u6807\u6ce8\u7a00\u7f3a\uff0c\u4e14\u6eb6\u9aa8\u6027\u548c\u6210\u9aa8\u6027\u75c5\u53d8\u5e38\u4e0e\u826f\u6027\u9000\u884c\u6027\u6539\u53d8\u76f8\u4f3c\u3002", "method": "\u7ed3\u5408\u6269\u6563\u81ea\u7f16\u7801\u5668\uff08DAE\uff09\u751f\u6210\u690e\u4f53\u7684\u5206\u7c7b\u5668\u5f15\u5bfc\u5065\u5eb7\u7f16\u8f91\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u5dee\u5f02\u56fe\u63d0\u51fa\u5019\u9009\u75c5\u7076\u533a\u57df\u3002\u5f15\u5165Hide-and-Seek Attribution\u6280\u672f\uff1a\u4f9d\u6b21\u63ed\u793a\u6bcf\u4e2a\u5019\u9009\u533a\u57df\u540c\u65f6\u9690\u85cf\u5176\u4ed6\u533a\u57df\uff0c\u5c06\u7f16\u8f91\u540e\u7684\u56fe\u50cf\u901a\u8fc7DAE\u6295\u5f71\u56de\u6570\u636e\u6d41\u5f62\uff0c\u4f7f\u7528\u6f5c\u5728\u7a7a\u95f4\u5206\u7c7b\u5668\u91cf\u5316\u8be5\u7ec4\u4ef6\u7684\u5b64\u7acb\u6076\u6027\u8d21\u732e\uff0c\u9ad8\u5206\u533a\u57df\u5f62\u6210\u6700\u7ec8\u7684\u6eb6\u9aa8\u6027\u6216\u6210\u9aa8\u6027\u5206\u5272\u3002", "result": "\u5728\u4fdd\u7559\u7684\u653e\u5c04\u79d1\u533b\u751f\u6807\u6ce8\u4e0a\uff0c\u5c3d\u7ba1\u6ca1\u6709\u63a9\u7801\u76d1\u7763\uff0c\u4ecd\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6210\u9aa8\u6027/\u6eb6\u9aa8\u6027\u6027\u80fd\uff08F1: 0.91/0.85; Dice: 0.87/0.78\uff09\uff0c\u8d85\u8fc7\u57fa\u7ebf\u65b9\u6cd5\uff08F1: 0.79/0.67; Dice: 0.74/0.55\uff09\u3002", "conclusion": "\u690e\u4f53\u7ea7\u522b\u6807\u7b7e\u53ef\u4ee5\u8f6c\u5316\u4e3a\u53ef\u9760\u7684\u75c5\u7076\u63a9\u7801\uff0c\u8868\u660e\u751f\u6210\u7f16\u8f91\u7ed3\u5408\u9009\u62e9\u6027\u906e\u6321\u652f\u6301CT\u4e2d\u51c6\u786e\u7684\u5f31\u76d1\u7763\u5206\u5272\u3002"}}
{"id": "2512.07426", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07426", "abs": "https://arxiv.org/abs/2512.07426", "authors": ["Karel Moens", "Matthew B. Blaschko", "Tinne Tuytelaars", "Bart Diricx", "Jonas De Vylder", "Mustafa Yousif"], "title": "When normalization hallucinates: unseen risks in AI-powered whole slide image processing", "comment": "4 pages, accepted for oral presentation at SPIE Medical Imaging, 2026", "summary": "Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.", "AI": {"tldr": "WSI\u5f52\u4e00\u5316\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e0a\u4f1a\u4ea7\u751f\u96be\u4ee5\u5bdf\u89c9\u7684\u5e7b\u89c9\u4f2a\u5f71\uff0c\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u68c0\u6d4b\uff0c\u9700\u8981\u65b0\u7684\u68c0\u6d4b\u6307\u6807\u548c\u66f4\u4e25\u683c\u7684\u9a8c\u8bc1\u6d41\u7a0b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5168\u5207\u7247\u56fe\u50cf\u5f52\u4e00\u5316\u65b9\u6cd5\u503e\u5411\u4e8e\u8f93\u51fa\u5e73\u5747\u5316\u7ed3\u679c\uff0c\u53ef\u80fd\u63a9\u76d6\u8bca\u65ad\u91cd\u8981\u7279\u5f81\uff0c\u66f4\u4e25\u91cd\u7684\u662f\u4f1a\u4ea7\u751f\u89c6\u89c9\u96be\u4ee5\u5bdf\u89c9\u7684\u5e7b\u89c9\u4f2a\u5f71\uff0c\u5bf9\u4e0b\u6e38\u5206\u6790\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u800c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u6bd4\u8f83\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u5f52\u4e00\u5316\u8f93\u51fa\u4e2d\u7684\u5e7b\u89c9\u4f2a\u5f71\u3002\u4f7f\u7528\u8be5\u5ea6\u91cf\u65b9\u6cd5\u7cfb\u7edf\u8bc4\u4f30\u591a\u4e2a\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e0a\u91cd\u65b0\u8bad\u7ec3\u7684\u77e5\u540d\u5f52\u4e00\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e0a\u91cd\u65b0\u8bad\u7ec3\u7684\u6a21\u578b\u663e\u793a\u51fa\u4ee4\u4eba\u62c5\u5fe7\u7684\u5e7b\u89c9\u9891\u7387\uff0c\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u6355\u6349\u5230\u663e\u8457\u7684\u6a21\u578b\u4e0d\u4e00\u81f4\u6027\u548c\u5931\u8d25\u60c5\u51b5\u3002\u65b0\u63d0\u51fa\u7684\u5ea6\u91cf\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63ed\u793a\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u5e7b\u89c9\u98ce\u9669\u662f\u771f\u5b9e\u5b58\u5728\u4e14\u88ab\u4f4e\u4f30\u7684\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u7684\u5f52\u4e00\u5316\u6280\u672f\uff0c\u5e76\u5728\u4e34\u5e8a\u90e8\u7f72\u4e2d\u5b9e\u65bd\u66f4\u4e25\u683c\u7684\u9a8c\u8bc1\u534f\u8bae\u3002"}}
{"id": "2512.06862", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06862", "abs": "https://arxiv.org/abs/2512.06862", "authors": ["Qiancheng Zheng", "Yunhang Shen", "Gen Luo", "Baiyang Song", "Xing Sun", "Xiaoshuai Sun", "Yiyi Zhou", "Rongrong Ji"], "title": "Omni-Referring Image Segmentation", "comment": null, "summary": "In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.", "AI": {"tldr": "\u63d0\u51faOmni-Referring Image Segmentation (OmniRIS)\u65b0\u4efb\u52a1\uff0c\u652f\u6301\u6587\u672c\u6307\u4ee4\u548c\u5e26\u63a9\u7801/\u6846/\u6d82\u9e26\u7684\u53c2\u8003\u56fe\u50cf\u4f5c\u4e3a\u5168\u6a21\u6001\u63d0\u793a\uff0c\u5b9e\u73b0\u9ad8\u5ea6\u6cdb\u5316\u7684\u56fe\u50cf\u5206\u5272", "motivation": "\u73b0\u6709\u5355\u6a21\u6001\u6761\u4ef6\u5206\u5272\u4efb\u52a1\uff08\u5982RIS\u548c\u89c6\u89c9RIS\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u5206\u5272\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u5229\u7528\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u4f18\u52bf", "method": "\u63d0\u51faOmniRIS\u4efb\u52a1\u6846\u67b6\uff0c\u652f\u6301\u6587\u672c\u6307\u4ee4\u548c\u591a\u79cd\u89c6\u89c9\u63d0\u793a\uff08\u63a9\u7801\u3001\u8fb9\u754c\u6846\u3001\u6d82\u9e26\uff09\u4f5c\u4e3a\u5168\u6a21\u6001\u8f93\u5165\uff1b\u6784\u5efaOmniRef\u6570\u636e\u96c6\uff0830,956\u5f20\u56fe\u50cf\uff0c186,939\u4e2a\u5168\u6a21\u6001\u63d0\u793a\uff09\uff1b\u5f00\u53d1OmniSegNet\u57fa\u7ebf\u6a21\u578b\u5904\u7406\u5168\u6a21\u6001\u63d0\u793a\u7f16\u7801\u7b49\u5173\u952e\u6311\u6218", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86OmniSegNet\u80fd\u591f\u6709\u6548\u9075\u5faa\u5168\u6a21\u6001\u6307\u4ee4\uff0c\u5e76\u5c55\u793a\u4e86OmniRIS\u5728\u9ad8\u5ea6\u6cdb\u5316\u56fe\u50cf\u5206\u5272\u65b9\u9762\u7684\u4f18\u8d8a\u6027", "conclusion": "OmniRIS\u901a\u8fc7\u652f\u6301\u6587\u672c\u548c\u89c6\u89c9\u5168\u6a21\u6001\u63d0\u793a\uff0c\u5b9e\u73b0\u4e86\u66f4\u901a\u7528\u3001\u66f4\u7075\u6d3b\u7684\u56fe\u50cf\u5206\u5272\uff0c\u4e3a\u9ad8\u5ea6\u6cdb\u5316\u7684\u56fe\u50cf\u5206\u5272\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2512.06864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06864", "abs": "https://arxiv.org/abs/2512.06864", "authors": ["Kaixuan Lu", "Mehmet Onurcan Kaya", "Dim P. Papadopoulos"], "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training", "comment": "Accepted to WACV 2026. arXiv admin note: substantial text overlap with arXiv:2508.19808", "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\\text{AP}_{50}$ on YouTubeVIS-2019 $\\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.", "AI": {"tldr": "AutoQ-VIS\uff1a\u4e00\u79cd\u901a\u8fc7\u8d28\u91cf\u5f15\u5bfc\u81ea\u8bad\u7ec3\u7684\u65e0\u76d1\u7763\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u5b9e\u73b0\u5408\u6210\u5230\u771f\u5b9e\u89c6\u9891\u7684\u6e10\u8fdb\u9002\u5e94\uff0c\u5728YouTubeVIS-2019\u4e0a\u8fbe\u523052.6 AP50\uff0c\u8d85\u8d8a\u4e4b\u524d\u6700\u4f73\u65b9\u6cd54.4%\u3002", "motivation": "\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u9762\u4e34\u50cf\u7d20\u7ea7\u63a9\u7801\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u6807\u6ce8\u7684\u53cc\u91cd\u6311\u6218\u3002\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u5982VideoCutLER\u867d\u7136\u901a\u8fc7\u5408\u6210\u6570\u636e\u6d88\u9664\u4e86\u5149\u6d41\u4f9d\u8d56\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u5408\u6210\u5230\u771f\u5b9e\u7684\u57df\u5dee\u8ddd\u3002", "method": "\u63d0\u51faAutoQ-VIS\u6846\u67b6\uff0c\u901a\u8fc7\u8d28\u91cf\u5f15\u5bfc\u7684\u81ea\u8bad\u7ec3\u5efa\u7acb\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u81ea\u52a8\u8d28\u91cf\u8bc4\u4f30\u7684\u95ed\u73af\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4ece\u5408\u6210\u5230\u771f\u5b9e\u89c6\u9891\u7684\u6e10\u8fdb\u9002\u5e94\u3002", "result": "\u5728YouTubeVIS-2019\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523052.6 AP50\uff0c\u8d85\u8d8a\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5VideoCutLER 4.4%\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u6807\u6ce8\u3002", "conclusion": "\u8d28\u91cf\u611f\u77e5\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u4e8e\u65e0\u76d1\u7763\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u662f\u53ef\u884c\u7684\uff0cAutoQ-VIS\u901a\u8fc7\u95ed\u73af\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u5408\u6210\u5230\u771f\u5b9e\u7684\u57df\u5dee\u8ddd\u95ee\u9898\u3002"}}
{"id": "2512.06865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06865", "abs": "https://arxiv.org/abs/2512.06865", "authors": ["Xiaosong Jia", "Chenhe Zhang", "Yule Jiang", "Songbur Wong", "Zhiyuan Zhang", "Chen Chen", "Shaofeng Zhang", "Xuanhe Zhou", "Xue Yang", "Junchi Yan", "Yu-Gang Jiang"], "title": "Spatial Retrieval Augmented Autonomous Driving", "comment": "Demo Page: https://spatialretrievalad.github.io/ with open sourced code, dataset, and checkpoints", "summary": "Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall\" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.\n  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.", "AI": {"tldr": "\u63d0\u51fa\u7a7a\u95f4\u68c0\u7d22\u8303\u5f0f\uff0c\u901a\u8fc7\u5f15\u5165\u79bb\u7ebf\u68c0\u7d22\u7684\u5730\u7406\u56fe\u50cf\u4f5c\u4e3a\u989d\u5916\u8f93\u5165\uff0c\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u514b\u670d\u4f20\u7edf\u8f66\u8f7d\u4f20\u611f\u5668\u7684\u89c6\u91ce\u9650\u5236\u548c\u6781\u7aef\u6761\u4ef6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4f9d\u8d56\u8f66\u8f7d\u4f20\u611f\u5668\u8fdb\u884c\u73af\u5883\u611f\u77e5\uff0c\u4f46\u53d7\u9650\u4e8e\u611f\u77e5\u89c6\u91ce\u3001\u906e\u6321\u4ee5\u53ca\u9ed1\u6697\u3001\u96e8\u5929\u7b49\u6781\u7aef\u6761\u4ef6\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4eba\u7c7b\u9a7e\u9a76\u5458\u80fd\u591f\u5728\u80fd\u89c1\u5ea6\u5dee\u7684\u60c5\u51b5\u4e0b\u56de\u5fc6\u9053\u8def\u7ed3\u6784\u3002\u4e3a\u4e86\u8d4b\u4e88\u6a21\u578b\u8fd9\u79cd\"\u56de\u5fc6\"\u80fd\u529b\uff0c\u9700\u8981\u65b0\u7684\u611f\u77e5\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u68c0\u7d22\u8303\u5f0f\uff0c\u4ece\u79bb\u7ebf\u7f13\u5b58\uff08\u5982Google Maps\u6216\u5b58\u50a8\u7684\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\uff09\u4e2d\u68c0\u7d22\u5730\u7406\u56fe\u50cf\u4f5c\u4e3a\u989d\u5916\u8f93\u5165\u3002\u6269\u5c55nuScenes\u6570\u636e\u96c6\uff0c\u901a\u8fc7Google Maps API\u68c0\u7d22\u5730\u7406\u56fe\u50cf\u5e76\u4e0e\u81ea\u8f66\u8f68\u8ff9\u5bf9\u9f50\u3002\u5728\u4e94\u4e2a\u6838\u5fc3\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e0a\u5efa\u7acb\u57fa\u7ebf\u3002", "result": "\u6269\u5c55\u6a21\u6001\u80fd\u591f\u63d0\u5347\u67d0\u4e9b\u4efb\u52a1\u7684\u6027\u80fd\u3002\u5c06\u5f00\u6e90\u6570\u636e\u96c6\u6784\u5efa\u4ee3\u7801\u3001\u6570\u636e\u548c\u57fa\u51c6\uff0c\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u8fd9\u4e00\u65b0\u7684\u81ea\u52a8\u9a7e\u9a76\u8303\u5f0f\u3002", "conclusion": "\u7a7a\u95f4\u68c0\u7d22\u8303\u5f0f\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\"\u56de\u5fc6\"\u80fd\u529b\uff0c\u901a\u8fc7\u5f15\u5165\u79bb\u7ebf\u5730\u7406\u56fe\u50cf\u4f5c\u4e3a\u989d\u5916\u8f93\u5165\uff0c\u80fd\u591f\u514b\u670d\u4f20\u7edf\u8f66\u8f7d\u4f20\u611f\u5668\u7684\u9650\u5236\uff0c\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6269\u5c55\u65b9\u6848\u3002"}}
{"id": "2512.06870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06870", "abs": "https://arxiv.org/abs/2512.06870", "authors": ["Wangkai Li", "Rui Sun", "Zhaoyang Li", "Tianzhu Zhang"], "title": "Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective", "comment": "Accepted by Conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.", "AI": {"tldr": "ECOCSeg\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea0\u9519\u8f93\u51fa\u7801\u7684\u8bed\u4e49\u5206\u5272\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7c7b\u522b\u5206\u89e3\u4e3a\u5c5e\u6027\u5e76\u5904\u7406\u90e8\u5206\u9519\u8bef\u6bd4\u7279\uff0c\u63d0\u9ad8\u4e86\u4f2a\u6807\u7b7e\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5728\u8bed\u4e49\u5206\u5272\u7684\u4f2a\u6807\u7b7e\u5b66\u4e60\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528one-hot\u7f16\u7801\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u4f2a\u6807\u7b7e\uff0c\u8fd9\u4e9b\u9519\u8bef\u4f1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u88ab\u653e\u5927\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faECOCSeg\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u57fa\u4e8eECOC\u7684\u5206\u7c7b\u5668\uff0c\u5c06\u7c7b\u522b\u5206\u89e3\u4e3a\u5c5e\u6027\u5e76\u5904\u7406\u90e8\u5206\u4e0d\u51c6\u786e\u7684\u6bd4\u7279\uff1b2\uff09\u5f00\u53d1\u6bd4\u7279\u7ea7\u6807\u7b7e\u53bb\u566a\u673a\u5236\uff0c\u4e3a\u672a\u6807\u8bb0\u56fe\u50cf\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\u3002", "result": "ECOCSeg\u5728\u591a\u4e2aUDA\u548cSSL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u540e\u5747\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u5206\u5272\u67b6\u6784\u3002", "conclusion": "ECOCSeg\u901a\u8fc7\u7ea0\u9519\u8f93\u51fa\u7801\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u7c7b\u522b\u7f16\u7801\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f2a\u6807\u7b7e\u5b66\u4e60\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5206\u5272\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.06877", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06877", "abs": "https://arxiv.org/abs/2512.06877", "authors": ["Mohammed Q. Alkhatib", "Ali Jamali", "Swalpa Kumar Roy"], "title": "SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification", "comment": "Accepted and presented in ICSPIS", "summary": "Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5377\u79ef\u6df7\u5408\u5668\u8303\u5f0f\u7684\u8f7b\u91cf\u7ea7\u9065\u611f\u573a\u666f\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6df1\u5ea6\u5377\u79ef\u548c\u9010\u70b9\u64cd\u4f5c\u4ea4\u66ff\u8fdb\u884c\u7a7a\u95f4\u4e0e\u901a\u9053\u6df7\u5408\uff0c\u5728AID\u548cEuroSAT\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u826f\u597d\u7cbe\u5ea6\u4e0e\u6548\u7387\u5e73\u8861\u3002", "motivation": "\u9065\u611f\u573a\u666f\u5206\u7c7b\u5bf9\u5730\u7403\u89c2\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709CNN\u548cViT\u6a21\u578b\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u89c6\u89d2\u3001\u65b9\u5411\u548c\u80cc\u666f\u6761\u4ef6\u53d8\u5316\u4e0b\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5377\u79ef\u6df7\u5408\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6df1\u5ea6\u5377\u79ef\u8fdb\u884c\u7a7a\u95f4\u6df7\u5408\uff0c\u901a\u8fc7\u9010\u70b9\u64cd\u4f5c\u8fdb\u884c\u901a\u9053\u6df7\u5408\uff0c\u4ea4\u66ff\u63d0\u53d6\u5c40\u90e8\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4fdd\u6301\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u8f83\u4f4e\u3002", "result": "\u5728AID\u6570\u636e\u96c6\u4e0a\u83b7\u5f9774.7%\u603b\u4f53\u7cbe\u5ea6\u300174.57%\u5e73\u5747\u7cbe\u5ea6\u548c73.79 Kappa\u503c\uff1b\u5728EuroSAT\u6570\u636e\u96c6\u4e0a\u83b7\u5f9793.90%\u603b\u4f53\u7cbe\u5ea6\u300193.93%\u5e73\u5747\u7cbe\u5ea6\u548c93.22 Kappa\u503c\uff0c\u4f18\u4e8e\u73b0\u6709CNN\u548ctransformer\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u5377\u79ef\u6df7\u5408\u5668\u6a21\u578b\u5728\u9065\u611f\u573a\u666f\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06882", "abs": "https://arxiv.org/abs/2512.06882", "authors": ["Yu Zhu", "Naoya Chiba", "Koichi Hashimoto"], "title": "Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion", "comment": "Accepted to BMVC 2025 (Sheffield, UK, Nov 24-27, 2025). Supplementary video and poster available upon request", "summary": "Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u56fe\u50cf\u5f15\u5bfc\u76843D\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u5230\u90e8\u4ef6\u7ea7\u7684\u6e10\u8fdb\u5f0f\u5206\u5272\uff0c\u89e3\u51b3\u5de5\u4e1a\u573a\u666f\u4e2d\u906e\u6321\u548c\u5c3a\u5ea6\u5dee\u5f02\u95ee\u9898\uff0c\u65e0\u9700\u6602\u8d35\u6807\u6ce8\u4e14\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u5de5\u4e1a\u73af\u5883\u4e2d\u5bc6\u96c6\u5e03\u5c40\u548c\u591a\u5c3a\u5ea6\u7269\u4f53\u5bfc\u81f4\u53ef\u97603D\u5206\u5272\u56f0\u96be\uff1a\u4e25\u91cd\u906e\u6321\u524a\u5f31\u51e0\u4f55\u8fb9\u754c\uff0c\u5927\u5c3a\u5ea6\u5dee\u5f02\u4f7f\u7aef\u5230\u7aef\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u6355\u6349\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u6602\u8d35\u6807\u6ce8\uff0c\u8981\u4e48\u5b58\u5728\u8de8\u89c6\u56fe\u8bed\u4e49\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u5206\u5c42\u56fe\u50cf\u5f15\u5bfc3D\u5206\u5272\u6846\u67b6\uff1a1) \u5b9e\u4f8b\u5206\u5272\uff1a\u6e32\u67d3\u4fef\u89c6\u56fe\uff0c\u7528YOLO-World\u63d0\u793aSAM\u751f\u6210\u63a9\u7801\uff0c\u6295\u5f71\u56de3D\u70b9\u4e91\uff1b2) \u90e8\u4ef6\u7ea7\u5206\u5272\uff1a\u5bf9\u6bcf\u4e2a\u5b9e\u4f8b\u6e32\u67d3\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u5728\u5404\u89c6\u56fe\u5e94\u7528\u76f8\u540c2D\u5206\u5272\u548c\u53cd\u6295\u5f71\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u66f4\u65b0\u878d\u5408\u786e\u4fdd\u8de8\u89c6\u56fe\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u5382\u6570\u636e\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u906e\u6321\u548c\u7ed3\u6784\u590d\u6742\u6027\uff0c\u83b7\u5f97\u4e00\u81f4\u9ad8\u7684\u6bcf\u7c7bmIoU\u5206\u6570\u3002\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u989d\u5916\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u6846\u67b6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7a81\u51fa\u4e86\u5176\u9c81\u68d2\u6027\u3001\u6807\u6ce8\u6548\u7387\u548c\u9002\u5e94\u591a\u6837\u53163D\u73af\u5883\u7684\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u56fe\u50cf\u5f15\u5bfc3D\u5206\u5272\u6846\u67b6\u901a\u8fc7\u6e10\u8fdb\u5f0f\u4ece\u5b9e\u4f8b\u5230\u90e8\u4ef6\u7684\u5206\u5272\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u906e\u6321\u548c\u5c3a\u5ea6\u5dee\u5f02\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u8bed\u4e49\u4e00\u81f4\u76843D\u5206\u5272\uff0c\u65e0\u9700\u6602\u8d35\u6807\u6ce8\u3002"}}
{"id": "2512.06886", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06886", "abs": "https://arxiv.org/abs/2512.06886", "authors": ["Wangkai Li", "Rui Sun", "Bohao Liao", "Zhaoyang Li", "Tianzhu Zhang"], "title": "Balanced Learning for Domain Adaptive Semantic Segmentation", "comment": "Accepted by International Conference on Machine Learning (ICML 2025)", "summary": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.", "AI": {"tldr": "BLDA\u63d0\u51fa\u4e86\u4e00\u79cd\u5e73\u8861\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u9884\u6d4blogits\u5206\u5e03\u6765\u8bc6\u522b\u8fc7\u9884\u6d4b\u548c\u6b20\u9884\u6d4b\u7c7b\u522b\uff0c\u4f7f\u7528\u5171\u4eab\u951a\u5206\u5e03\u5bf9\u9f50\u4e0d\u540c\u7c7b\u522b\u7684logits\u5206\u5e03\uff0c\u5e76\u5728\u81ea\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a0\u5165logits\u6821\u6b63\u9879\uff0c\u4ee5\u89e3\u51b3UDA\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u5728\u65e0\u76d1\u7763\u57df\u9002\u5e94\u8bed\u4e49\u5206\u5272\u4e2d\uff0c\u81ea\u8bad\u7ec3\u65b9\u6cd5\u7531\u4e8e\u56fa\u6709\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u57df\u95f4\u6570\u636e/\u6807\u7b7e\u7a7a\u95f4\u5206\u5e03\u504f\u79fb\uff0c\u96be\u4ee5\u5e73\u8861\u5b66\u4e60\u5404\u4e2a\u7c7b\u522b\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u76f4\u63a5\u8bc4\u4f30\u548c\u7f13\u89e3\u7c7b\u522b\u504f\u5dee\u7684\u6709\u6548\u673a\u5236\u3002", "method": "1) \u901a\u8fc7\u5206\u6790\u9884\u6d4blogits\u5206\u5e03\u8bc6\u522b\u8fc7\u9884\u6d4b\u548c\u6b20\u9884\u6d4b\u7c7b\u522b\uff1b2) \u4f7f\u7528\u5171\u4eab\u951a\u5206\u5e03\u8fdb\u884c\u540e\u5904\u7406\u5bf9\u9f50\u4e0d\u540c\u7c7b\u522b\u7684logits\u5206\u5e03\uff1b3) \u5728\u7ebf\u4f30\u8ba1logits\u5206\u5e03\u5e76\u5728\u635f\u5931\u51fd\u6570\u4e2d\u52a0\u5165logits\u6821\u6b63\u9879\uff1b4) \u5229\u7528\u7d2f\u79ef\u5bc6\u5ea6\u4f5c\u4e3a\u57df\u5171\u4eab\u7ed3\u6784\u77e5\u8bc6\u8fde\u63a5\u6e90\u57df\u548c\u76ee\u6807\u57df\u3002", "result": "\u5728\u4e24\u4e2a\u6807\u51c6UDA\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cBLDA\u5728\u96c6\u6210\u5230\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u4e2d\u65f6\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6b20\u9884\u6d4b\u7c7b\u522b\u3002", "conclusion": "BLDA\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u5206\u5e03\u77e5\u8bc6\u5c31\u80fd\u76f4\u63a5\u8bc4\u4f30\u548c\u7f13\u89e3\u7c7b\u522b\u504f\u5dee\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u5b66\u4e60\u7b56\u7565\u663e\u8457\u6539\u5584\u4e86UDA\u8bed\u4e49\u5206\u5272\u4e2d\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2512.06888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06888", "abs": "https://arxiv.org/abs/2512.06888", "authors": ["Liyang Song", "Hardik Bishnoi", "Sai Kumar Reddy Manne", "Sarah Ostadabbas", "Briana J. Taylor", "Michael Wan"], "title": "Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation", "comment": null, "summary": "The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u9996\u4e2a\u53ef\u590d\u73b0\u7684\u5a74\u513f\u547c\u5438\u76d1\u6d4b\u7cfb\u7edf\uff0c\u5305\u542b400\u4e2a\u89c6\u9891\u7684\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u547c\u5438\u4f30\u8ba1\u7b97\u6cd5\uff0c\u586b\u8865\u4e86\u5a74\u513f\u547c\u5438\u76d1\u6d4b\u9886\u57df\u7684\u7a7a\u767d\u3002", "motivation": "\u5a74\u513f\u547c\u5438\u5f02\u5e38\u4e0e\u795e\u7ecf\u53d1\u80b2\u969c\u788d\u548c\u5a74\u513f\u731d\u6b7b\u7efc\u5408\u5f81\u76f8\u5173\uff0c\u4f46\u73b0\u6709\u547c\u5438\u76d1\u6d4b\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u6210\u4eba\uff0c\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u5a74\u513f\u7684\u516c\u5f00\u89c6\u9891\u6570\u636e\u96c6\u548c\u53ef\u590d\u73b0\u7b97\u6cd5\u3002", "method": "1. \u521b\u5efa\u4e86\u5305\u542b400\u4e2a\u89c6\u9891\u7684\u5a74\u513f\u547c\u5438\u6570\u636e\u96c6\uff08AIR-400\uff09\uff0c\u5176\u4e2d275\u4e2a\u4e3a\u65b0\u91c7\u96c6\u7684\u6807\u6ce8\u89c6\u9891\uff1b2. \u5f00\u53d1\u4e86\u57fa\u4e8e\u5a74\u513f\u7279\u5b9a\u611f\u5174\u8da3\u533a\u57df\u68c0\u6d4b\u548c\u65f6\u7a7a\u795e\u7ecf\u5904\u7406\u7684\u7b97\u6cd5\uff0c\u5e76\u5229\u7528\u5149\u6d41\u8f93\u5165\u589e\u5f3a\u6027\u80fd\uff1b3. \u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u5a74\u513f\u547c\u5438\u4f30\u8ba1\u7b97\u6cd5\u6d41\u7a0b\u3002", "result": "1. \u5efa\u7acb\u4e86\u9996\u4e2a\u516c\u5f00\u7684\u5a74\u513f\u547c\u5438\u89c6\u9891\u6570\u636e\u96c6\uff1b2. \u5f00\u53d1\u4e86\u9996\u4e2a\u53ef\u590d\u73b0\u7684\u5a74\u513f\u547c\u5438\u4f30\u8ba1\u7b97\u6cd5\uff1b3. \u4e3a\u57fa\u4e8e\u89c6\u89c9\u7684\u5a74\u513f\u547c\u5438\u4f30\u8ba1\u5efa\u7acb\u4e86\u57fa\u51c6\u6d4b\u8bd5\u6807\u51c6\uff1b4. \u6240\u6709\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u8bad\u7ec3\u6a21\u578b\u90fd\u5df2\u516c\u5f00\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u5a74\u513f\u547c\u5438\u76d1\u6d4b\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u65e9\u671f\u68c0\u6d4b\u548c\u6cbb\u7597\u547c\u5438\u5f02\u5e38\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u9884\u9632\u795e\u7ecf\u53d1\u80b2\u969c\u788d\u548c\u5a74\u513f\u731d\u6b7b\u7efc\u5408\u5f81\u3002"}}
{"id": "2512.06905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06905", "abs": "https://arxiv.org/abs/2512.06905", "authors": ["Zijian Zhou", "Shikun Liu", "Haozhe Liu", "Haonan Qiu", "Zhaochong An", "Weiming Ren", "Zhiheng Liu", "Xiaoke Huang", "Kam Woh Ng", "Tian Xie", "Xiao Han", "Yuren Cong", "Hang Li", "Chuyan Zhu", "Aditya Patel", "Tao Xiang", "Sen He"], "title": "Scaling Zero-Shot Reference-to-Video Generation", "comment": "Website: https://franciszzj.github.io/Saber/", "summary": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.", "AI": {"tldr": "Saber\u662f\u4e00\u4e2a\u65e0\u9700\u53c2\u8003\u56fe\u50cf-\u89c6\u9891-\u6587\u672c\u4e09\u5143\u7ec4\u6570\u636e\u7684\u96f6\u6837\u672c\u53c2\u8003\u5230\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u8bad\u7ec3\u7b56\u7565\u548c\u6ce8\u610f\u529b\u6a21\u578b\u8bbe\u8ba1\u5b9e\u73b0\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u53c2\u8003\u611f\u77e5\u8868\u793a\u3002", "motivation": "\u5f53\u524d\u53c2\u8003\u5230\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u663e\u5f0f\u53c2\u8003\u56fe\u50cf-\u89c6\u9891-\u6587\u672c\u4e09\u5143\u7ec4\u6570\u636e\uff0c\u8fd9\u79cd\u6570\u636e\u6784\u5efa\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faSaber\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u89c6\u9891-\u6587\u672c\u5bf9\u8fdb\u884c\u8bad\u7ec3\uff0c\u91c7\u7528\u63a9\u7801\u8bad\u7ec3\u7b56\u7565\u548c\u5b9a\u5236\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u8bbe\u8ba1\uff0c\u5b66\u4e60\u8eab\u4efd\u4e00\u81f4\u548c\u53c2\u8003\u611f\u77e5\u7684\u8868\u793a\uff0c\u5e76\u96c6\u6210\u63a9\u7801\u589e\u5f3a\u6280\u672f\u51cf\u5c11\u590d\u5236\u7c98\u8d34\u4f2a\u5f71\u3002", "result": "Saber\u5728OpenS2V-Eval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u4f7f\u7528R2V\u6570\u636e\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u4e0d\u540c\u6570\u91cf\u53c2\u8003\u56fe\u50cf\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Saber\u901a\u8fc7\u96f6\u6837\u672c\u6846\u67b6\u89e3\u51b3\u4e86\u53c2\u8003\u5230\u89c6\u9891\u751f\u6210\u7684\u6570\u636e\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u65e0\u9700\u6602\u8d35\u7684\u663e\u5f0f\u4e09\u5143\u7ec4\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u548c\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2512.07652", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07652", "abs": "https://arxiv.org/abs/2512.07652", "authors": ["Hamad Almazrouei", "Mariam Al Nasseri", "Maha Alzaabi"], "title": "An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research", "comment": null, "summary": "Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210YOLOv12 Nano\u3001CNN\u3001PCA\u3001K-Means++\u548cLLM\u6280\u672f\uff0c\u5b9e\u73b0\u6c34\u4e0b\u7269\u4f53\u81ea\u52a8\u68c0\u6d4b\u3001\u5206\u6790\u548c\u62a5\u544a\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u6d77\u6d0b\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6d77\u6d0b\u63a2\u7d22\u9762\u4e34\u6781\u7aef\u6761\u4ef6\u3001\u80fd\u89c1\u5ea6\u4f4e\u3001\u6210\u672c\u9ad8\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u5927\u91cf\u6d77\u6d0b\u533a\u57df\u672a\u88ab\u63a2\u7d22\u3002\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u7cfb\u7edf\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u964d\u4f4e\u4eba\u7c7b\u6f5c\u6c34\u98ce\u9669\uff0c\u63d0\u9ad8\u4efb\u52a1\u6548\u7387\u3002", "method": "\u7cfb\u7edf\u96c6\u6210\u4e86YOLOv12 Nano\u8fdb\u884c\u5b9e\u65f6\u7269\u4f53\u68c0\u6d4b\uff0cResNet50 CNN\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0cPCA\u8fdb\u884c\u964d\u7ef4\uff0cK-Means++\u8fdb\u884c\u805a\u7c7b\u5206\u6790\uff0cGPT-4o Mini LLM\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\u3002\u5728\u5305\u542b55,000\u591a\u5f20\u56fe\u50cf\u7684DeepFish\u548cOzFish\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u8bc4\u4f30\u3002", "result": "\u7cfb\u7edf\u68c0\u6d4b\u6027\u80fd\u8fbe\u5230mAP@0.5\u4e3a0.512\uff0c\u7cbe\u786e\u73870.535\uff0c\u53ec\u56de\u73870.438\u3002PCA\u964d\u7ef4\u4fdd\u7559\u4e8698%\u7684\u65b9\u5dee\uff0cK-Means\u805a\u7c7b\u6210\u529f\u6309\u89c6\u89c9\u7279\u5f81\u5206\u7ec4\u7269\u4f53\uff0cLLM\u80fd\u6709\u6548\u751f\u6210\u5305\u542b\u4f4d\u7f6e\u6570\u636e\u7684\u68c0\u6d4b\u6458\u8981\u3002", "conclusion": "\u8be5\u96c6\u6210\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u7c7b\u6f5c\u6c34\u98ce\u9669\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6548\u7387\uff0c\u589e\u5f3a\u4e86\u6c34\u4e0b\u6570\u636e\u5206\u6790\u7684\u901f\u5ea6\u548c\u6df1\u5ea6\uff0c\u4e3a\u5728\u6311\u6218\u6027\u6d77\u6d0b\u73af\u5883\u4e2d\u8fdb\u884c\u66f4\u6709\u6548\u7684\u79d1\u5b66\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.07674", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07674", "abs": "https://arxiv.org/abs/2512.07674", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Virginia Fernandez", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations", "comment": null, "summary": "Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.", "AI": {"tldr": "DIST-CLIP\u662f\u4e00\u4e2a\u7528\u4e8eMRI\u56fe\u50cf\u6807\u51c6\u5316\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u89e3\u5256\u5185\u5bb9\u548c\u56fe\u50cf\u5bf9\u6bd4\u5ea6\uff0c\u4f7f\u7528CLIP\u7f16\u7801\u5668\u63d0\u53d6\u5bf9\u6bd4\u5ea6\u8868\u793a\uff0c\u5e76\u5229\u7528\u81ea\u9002\u5e94\u98ce\u683c\u8f6c\u79fb\u6a21\u5757\u5b9e\u73b0\u7075\u6d3b\u7684\u56fe\u50cf\u6807\u51c6\u5316\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4e34\u5e8a\u6cdb\u5316\u80fd\u529b\u53d7\u5230\u6570\u636e\u5f02\u8d28\u6027\u7684\u4e25\u91cd\u9650\u5236\u3002\u7279\u522b\u662f\u5728\u78c1\u5171\u632f\u6210\u50cf\u4e2d\uff0c\u626b\u63cf\u4eea\u786c\u4ef6\u5dee\u5f02\u3001\u91c7\u96c6\u534f\u8bae\u591a\u6837\u6027\u548c\u5e8f\u5217\u53c2\u6570\u53d8\u5316\u5f15\u5165\u4e86\u663e\u8457\u7684\u57df\u504f\u79fb\uff0c\u63a9\u76d6\u4e86\u6f5c\u5728\u7684\u751f\u7269\u4fe1\u53f7\u3002\u73b0\u6709\u7684\u56fe\u50cf\u6807\u51c6\u5316\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\u9700\u8981\u76ee\u6807\u56fe\u50cf\uff0c\u800c\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\u4f9d\u8d56\u8fc7\u4e8e\u7b80\u5316\u7684\u6807\u7b7e\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u7684\u91c7\u96c6\u7ec6\u8282\uff0c\u4e14\u901a\u5e38\u5c40\u9650\u4e8e\u53d8\u5f02\u6027\u6709\u9650\u7684\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faDIST-CLIP\uff08\u57fa\u4e8eCLIP\u6307\u5bfc\u7684\u89e3\u8026\u98ce\u683c\u8f6c\u79fb\uff09\u6846\u67b6\uff0c\u660e\u786e\u89e3\u8026\u89e3\u5256\u5185\u5bb9\u548c\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u3002\u4f7f\u7528\u9884\u8bad\u7ec3\u7684CLIP\u7f16\u7801\u5668\u63d0\u53d6\u5bf9\u6bd4\u5ea6\u8868\u793a\uff0c\u7136\u540e\u901a\u8fc7\u65b0\u9896\u7684\u81ea\u9002\u5e94\u98ce\u683c\u8f6c\u79fb\u6a21\u5757\u5c06\u8fd9\u4e9b\u5bf9\u6bd4\u5ea6\u5d4c\u5165\u6574\u5408\u5230\u89e3\u5256\u5185\u5bb9\u4e2d\u3002\u8be5\u6846\u67b6\u53ef\u4ee5\u7075\u6d3b\u5730\u4f7f\u7528\u76ee\u6807\u56fe\u50cf\u6216DICOM\u5143\u6570\u636e\u8fdb\u884c\u6307\u5bfc\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30DIST-CLIP\uff0c\u7ed3\u679c\u663e\u793a\u5728\u98ce\u683c\u8f6c\u6362\u4fdd\u771f\u5ea6\u548c\u89e3\u5256\u7ed3\u6784\u4fdd\u6301\u65b9\u9762\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002\u8be5\u6846\u67b6\u4e3a\u98ce\u683c\u8f6c\u79fb\u548cMRI\u6570\u636e\u6807\u51c6\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "DIST-CLIP\u901a\u8fc7\u89e3\u8026\u89e3\u5256\u5185\u5bb9\u548c\u56fe\u50cf\u5bf9\u6bd4\u5ea6\uff0c\u7ed3\u5408CLIP\u7f16\u7801\u5668\u548c\u81ea\u9002\u5e94\u98ce\u683c\u8f6c\u79fb\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u7075\u6d3b\u7684MRI\u6807\u51c6\u5316\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5f02\u8d28\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u98ce\u683c\u8f6c\u6362\u7684\u4fdd\u771f\u5ea6\u548c\u89e3\u5256\u7ed3\u6784\u7684\u4fdd\u6301\u80fd\u529b\u3002"}}
{"id": "2512.06981", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06981", "abs": "https://arxiv.org/abs/2512.06981", "authors": ["Yuemin Wang", "Ian Stavness"], "title": "Selective Masking based Self-Supervised Learning for Image Semantic Segmentation", "comment": null, "summary": "This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bed\u4e49\u5206\u5272\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63a9\u7801\u56fe\u50cf\u91cd\u5efa\u4f5c\u4e3a\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u76f8\u6bd4\u968f\u673a\u63a9\u7801\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u5927\u591a\u6570\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u9884\u8bad\u7ec3\u65b9\u6cd5\u4f7f\u7528\u968f\u673a\u63a9\u7801\u589e\u5f3a\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u6709\u6548\u7684\u9009\u62e9\u6027\u63a9\u7801\u65b9\u6cd5\uff0c\u5229\u7528\u5df2\u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\u6765\u6539\u8fdb\u9884\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u63a9\u7801\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u6b65\u9aa4\u9009\u62e9\u91cd\u5efa\u635f\u5931\u6700\u9ad8\u7684\u56fe\u50cf\u5757\u8fdb\u884c\u63a9\u7801\uff0c\u800c\u4e0d\u662f\u968f\u673a\u63a9\u7801\u3002\u8fd9\u79cd\u65b9\u6cd5\u5229\u7528\u5df2\u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\u6765\u6307\u5bfc\u63a9\u7801\u9009\u62e9\u3002", "result": "\u5728\u4e24\u4e2a\u901a\u7528\u6570\u636e\u96c6\uff08Pascal VOC\u548cCityscapes\uff09\u548c\u4e24\u4e2a\u6742\u8349\u5206\u5272\u6570\u636e\u96c6\uff08Nassar 2020\u548cSugarbeets 2016\uff09\u4e0a\uff0c\u9009\u62e9\u6027\u63a9\u7801\u65b9\u6cd5\u6bd4\u4f20\u7edf\u968f\u673a\u63a9\u7801\u65b9\u6cd5\u548c\u76d1\u7763ImageNet\u9884\u8bad\u7ec3\u5728\u4e0b\u6e38\u5206\u5272\u7cbe\u5ea6\u4e0a\u5206\u522b\u63d0\u9ad8\u4e862.9%\u548c2.5%\u3002\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6700\u4f4e\u6027\u80fd\u7c7b\u522b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u9009\u62e9\u6027\u63a9\u7801\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\u4e3a\u6539\u8fdb\u7aef\u5230\u7aef\u8bed\u4e49\u5206\u5272\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u6709\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u6709\u9650\u6a21\u578b\u5bb9\u91cf\u4ee5\u6ee1\u8db3\u63a8\u7406\u901f\u5ea6\u548c\u8ba1\u7b97\u8d44\u6e90\u8981\u6c42\u7684\u573a\u666f\u3002\u4f7f\u7528\u76f8\u540c\u7684\u9884\u8bad\u7ec3\u548c\u4e0b\u6e38\u6570\u636e\u96c6\u80fd\u5728\u4f4e\u9884\u7b97\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4e2d\u83b7\u5f97\u6700\u4f73\u7ed3\u679c\u3002"}}
{"id": "2512.07702", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07702", "abs": "https://arxiv.org/abs/2512.07702", "authors": ["Sangha Park", "Eunji Kim", "Yeongtak Oh", "Jooyoung Choi", "Sungroh Yoon"], "title": "Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment", "comment": "WACV 2026", "summary": "Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.", "AI": {"tldr": "NPC\u901a\u8fc7\u81ea\u52a8\u8bc6\u522b\u548c\u5e94\u7528\u8d1f\u9762\u63d0\u793a\u6765\u6291\u5236\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u975e\u9884\u671f\u5185\u5bb9\uff0c\u4ece\u800c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u7cbe\u786e\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5bf9\u4e8e\u5177\u6709\u4e30\u5bcc\u7ec4\u5408\u7ed3\u6784\u6216\u60f3\u8c61\u6027\u5143\u7d20\u7684\u63d0\u793a\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u63d0\u793a\u65f6\u5f80\u5f80\u4f1a\u4ea7\u751f\u4e0e\u6587\u672c\u63cf\u8ff0\u4e0d\u7b26\u7684\u975e\u9884\u671f\u5185\u5bb9\u3002", "method": "\u63d0\u51faNPC\uff08Negative Prompting for Image Correction\uff09\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u901a\u8fc7\u5206\u6790\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5f0f\u6765\u89e3\u91ca\u76ee\u6807\u8d1f\u9762\u63d0\u793a\uff08\u4e0e\u5bf9\u9f50\u9519\u8bef\u76f4\u63a5\u76f8\u5173\uff09\u548c\u975e\u76ee\u6807\u8d1f\u9762\u63d0\u793a\uff08\u4e0e\u63d0\u793a\u65e0\u5173\u4f46\u5728\u751f\u6210\u56fe\u50cf\u4e2d\u5b58\u5728\u7684\u6807\u8bb0\uff09\u5982\u4f55\u589e\u5f3a\u5bf9\u9f50\u3002\u91c7\u7528\u9a8c\u8bc1\u5668-\u5b57\u5e55\u5668-\u63d0\u8bae\u5668\u6846\u67b6\u751f\u6210\u5019\u9009\u8d1f\u9762\u63d0\u793a\uff0c\u5e76\u4f7f\u7528\u663e\u8457\u6587\u672c\u7a7a\u95f4\u8bc4\u5206\u8fdb\u884c\u6392\u5e8f\uff0c\u5b9e\u73b0\u65e0\u9700\u989d\u5916\u56fe\u50cf\u5408\u6210\u7684\u6709\u6548\u9009\u62e9\u3002", "result": "\u5728GenEval++\u548cImagine-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNPC\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff1a\u5728GenEval++\u4e0a\u8fbe\u52300.571 vs 0.371\uff0c\u5728Imagine-Bench\u4e0a\u83b7\u5f97\u6700\u4f73\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6307\u5bfc\u6a21\u578b\u4e0d\u751f\u6210\u4ec0\u4e48\u5185\u5bb9\uff0cNPC\u4e3a\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u3001\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u590d\u6742\u63d0\u793a\u4e0b\u7684\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2512.07037", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07037", "abs": "https://arxiv.org/abs/2512.07037", "authors": ["Josep M. Rocafort", "Shaolin Su", "Javier Vazquez-Corral", "Alexandra Gomez-Villa"], "title": "Evaluating and Preserving High-level Fidelity in Super-Resolution", "comment": null, "summary": "Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u8861\u91cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u9ad8\u5c42\u8bed\u4e49\u4fdd\u771f\u5ea6\u4f5c\u4e3a\u8865\u5145\u8bc4\u4ef7\u6807\u51c6\uff0c\u6784\u5efa\u9996\u4e2a\u5e26\u4fdd\u771f\u5ea6\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u5206\u6790\u73b0\u6709\u6307\u6807\u76f8\u5173\u6027\uff0c\u5e76\u5c55\u793a\u901a\u8fc7\u4fdd\u771f\u5ea6\u53cd\u9988\u5fae\u8c03\u53ef\u540c\u65f6\u63d0\u5347\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u89c6\u89c9\u8d28\u91cf\u9ad8\u7684\u56fe\u50cf\uff0c\u4f46\u6709\u65f6\u4f1a\u4ea7\u751f\u5e7b\u89c9\u6548\u5e94\u6539\u53d8\u56fe\u50cf\u5185\u5bb9\u3002\u8fd9\u79cd\u9ad8\u5c42\u8bed\u4e49\u53d8\u5316\u5bb9\u6613\u88ab\u4eba\u7c7b\u8bc6\u522b\uff0c\u4f46\u73b0\u6709\u4f4e\u5c42\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u672a\u80fd\u5f88\u597d\u8861\u91cf\u3002\u9700\u8981\u5efa\u7acb\u9ad8\u5c42\u4fdd\u771f\u5ea6\u6d4b\u91cf\u4f5c\u4e3a\u8865\u5145\u6807\u51c6\uff0c\u4ee5\u8bc4\u4f30\u751f\u6210\u5f0f\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "method": "1) \u6784\u5efa\u9996\u4e2a\u5e26\u4fdd\u771f\u5ea6\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30SOTA\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u9ad8\u5c42\u4fdd\u771f\u5ea6\u8868\u73b0\uff1b2) \u5206\u6790\u73b0\u6709\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0e\u4fdd\u771f\u5ea6\u6d4b\u91cf\u7684\u76f8\u5173\u6027\uff1b3) \u5c55\u793a\u57fa\u7840\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5904\u7406\u9ad8\u5c42\u8bed\u4e49\u4efb\u52a1\uff1b4) \u57fa\u4e8e\u4fdd\u771f\u5ea6\u53cd\u9988\u5fae\u8c03\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u3002", "result": "1) \u5efa\u7acb\u4e86\u9ad8\u5c42\u4fdd\u771f\u5ea6\u6d4b\u91cf\u6807\u51c6\uff1b2) \u521b\u5efa\u4e86\u9996\u4e2a\u5e26\u4fdd\u771f\u5ea6\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff1b3) \u53d1\u73b0\u73b0\u6709\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0e\u9ad8\u5c42\u4fdd\u771f\u5ea6\u76f8\u5173\u6027\u4e0d\u8db3\uff1b4) \u57fa\u7840\u6a21\u578b\u80fd\u66f4\u597d\u5904\u7406\u9ad8\u5c42\u8bed\u4e49\u4efb\u52a1\uff1b5) \u901a\u8fc7\u4fdd\u771f\u5ea6\u53cd\u9988\u5fae\u8c03\u53ef\u540c\u65f6\u63d0\u5347\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u3002", "conclusion": "\u9ad8\u5c42\u4fdd\u771f\u5ea6\u6d4b\u91cf\u662f\u8bc4\u4f30\u751f\u6210\u5f0f\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u53ef\u9760\u6027\u7684\u91cd\u8981\u8865\u5145\u6807\u51c6\u3002\u63d0\u51fa\u7684\u4fdd\u771f\u5ea6\u6807\u51c6\u5728\u6a21\u578b\u8bc4\u4f30\u548c\u4f18\u5316\u4e2d\u5177\u6709\u6f5c\u5728\u4ef7\u503c\uff0c\u901a\u8fc7\u4fdd\u771f\u5ea6\u53cd\u9988\u5fae\u8c03\u53ef\u540c\u65f6\u6539\u5584\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u3002"}}
{"id": "2512.07730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07730", "abs": "https://arxiv.org/abs/2512.07730", "authors": ["Sangha Park", "Seungryong Yoo", "Jisoo Mok", "Sungroh Yoon"], "title": "SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination", "comment": "WACV 2026", "summary": "Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\\%p improvement in CHAIR\\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.", "AI": {"tldr": "SAVE\u6846\u67b6\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u7279\u5f81\u5f15\u5bfc\u6765\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8c61\u5e7b\u89c9\u95ee\u9898", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u8bed\u8a00\u5148\u9a8c\u548c\u89c6\u89c9\u4fe1\u606f\u4e22\u5931\u5bfc\u81f4\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u5f71\u54cd", "method": "\u63d0\u51faSAVE\u6846\u67b6\uff0c\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u7279\u5f81\u5f15\u5bfc\u6a21\u578b\u3002\u901a\u8fc7\u4e8c\u5143\u7269\u4f53\u5b58\u5728\u95ee\u7b54\u63a2\u9488\u8bc6\u522b\u6700\u80fd\u53cd\u6620\u6a21\u578b\u89c6\u89c9\u4fe1\u606f\u5904\u7406\u7684SAE\u7279\u5f81\uff08\u79f0\u4e3a\u89c6\u89c9\u7406\u89e3\u7279\u5f81\uff09\uff0c\u7136\u540e\u6cbf\u7740\u8fd9\u4e9b\u7279\u5f81\u5f15\u5bfc\u6a21\u578b", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728CHAIR_S\u4e0a\u63d0\u534710\u4e2a\u767e\u5206\u70b9\uff0c\u5728POPE\u548cMMHal-Bench\u4e0a\u83b7\u5f97\u4e00\u81f4\u589e\u76ca\u3002\u8de8\u591a\u4e2a\u6a21\u578b\u548c\u5c42\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027", "conclusion": "SAVE\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u5f15\u5bfc\u6709\u6548\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7269\u4f53\u5e7b\u89c9\uff0c\u589e\u5f3a\u89c6\u89c9\u57fa\u7840\u7406\u89e3\uff0c\u6291\u5236\u4e0d\u786e\u5b9a\u7269\u4f53\u6807\u8bb0\u751f\u6210\u5e76\u589e\u52a0\u5bf9\u56fe\u50cf\u6807\u8bb0\u7684\u5173\u6ce8"}}
{"id": "2512.07052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07052", "abs": "https://arxiv.org/abs/2512.07052", "authors": ["Hoang-Nhat Tran", "Francesco Di Sario", "Gabriele Spadaro", "Giuseppe Valenzise", "Enzo Tartaglione"], "title": "RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting", "comment": null, "summary": "Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u76843D\u9ad8\u65af\u6cfc\u6e85\u538b\u7f29\u65b9\u6848\uff0c\u652f\u6301\u5728\u9884\u5b9a\u4e49\u8fb9\u754c\u5185\u4efb\u610f\u901f\u7387\u63d2\u503c\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u5e26\u5bbd\u548c\u8bbe\u5907\u9650\u5236", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u867d\u7136\u80fd\u5b9e\u73b0\u5b9e\u65f6\u903c\u771f\u6e32\u67d3\uff0c\u4f46\u5b58\u5728\u5185\u5b58\u9700\u6c42\u5927\u3001\u8bad\u7ec3\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u53ea\u80fd\u5728\u56fa\u5b9a\u901f\u7387\u4e0b\u5de5\u4f5c\uff0c\u65e0\u6cd5\u9002\u5e94\u53d8\u5316\u7684\u5e26\u5bbd\u548c\u8bbe\u5907\u7ea6\u675f", "method": "\u63d0\u51fa\u7075\u6d3b\u76843DGS\u538b\u7f29\u65b9\u6848\uff0c\u652f\u6301\u5728\u9884\u5b9a\u4e49\u8fb9\u754c\u5185\u4efb\u610f\u901f\u7387\u63d2\u503c\uff0c\u8ba1\u7b97\u8f7b\u91cf\u4e14\u65e0\u9700\u4e3a\u4e0d\u540c\u901f\u7387\u91cd\u65b0\u8bad\u7ec3", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u538b\u7f29\uff0c\u540c\u65f6\u63d0\u4f9b\u52a8\u6001\u901f\u7387\u63a7\u5236\uff0c\u5728\u5e7f\u6cdb\u7684\u8fd0\u884c\u70b9\u4e0a\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u5408\u5b9e\u9645\u6c89\u6d78\u5f0f\u5e94\u7528\u90e8\u7f72\uff0c\u4ee3\u7801\u5c06\u5728\u5de5\u4f5c\u88ab\u63a5\u53d7\u540e\u5f00\u6e90\u63d0\u4f9b"}}
{"id": "2512.07821", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07821", "abs": "https://arxiv.org/abs/2512.07821", "authors": ["Shaoheng Fang", "Hanwen Jiang", "Yunpeng Bai", "Niloy J. Mitra", "Qixing Huang"], "title": "WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling", "comment": null, "summary": "Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.", "AI": {"tldr": "WorldReel\u662f\u4e00\u4e2a4D\u89c6\u9891\u751f\u6210\u5668\uff0c\u901a\u8fc7\u8054\u5408\u751f\u6210RGB\u5e27\u548c4D\u573a\u666f\u8868\u793a\uff08\u70b9\u4e91\u3001\u76f8\u673a\u8f68\u8ff9\u3001\u5bc6\u96c6\u5149\u6d41\uff09\uff0c\u5b9e\u73b0\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u5728\u52a8\u6001\u573a\u666f\u548c\u79fb\u52a8\u76f8\u673a\u4e0b\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u5668\u867d\u7136\u89c6\u89c9\u6548\u679c\u903c\u771f\uff0c\u4f46\u57283D\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u4e0d\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u52a8\u6001\u5185\u5bb9\u548c\u76f8\u673a\u79fb\u52a8\u4e0b\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u539f\u751f\u4fdd\u63014D\u4e00\u81f4\u6027\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51faWorldReel\u7cfb\u7edf\uff0c\u8054\u5408\u751f\u6210RGB\u5e27\u548c4D\u573a\u666f\u8868\u793a\uff08\u70b9\u4e91\u3001\u76f8\u673a\u8f68\u8ff9\u3001\u5bc6\u96c6\u5149\u6d41\uff09\u3002\u91c7\u7528\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff1a\u5408\u6210\u6570\u636e\u63d0\u4f9b\u7cbe\u786e\u76844D\u76d1\u7763\uff08\u51e0\u4f55\u3001\u8fd0\u52a8\u3001\u76f8\u673a\uff09\uff0c\u771f\u5b9e\u89c6\u9891\u63d0\u4f9b\u89c6\u89c9\u591a\u6837\u6027\u548c\u771f\u5b9e\u611f\u3002", "result": "WorldReel\u5728\u52a8\u6001\u573a\u666f\u548c\u79fb\u52a8\u76f8\u673a\u4e0b\u7684\u89c6\u9891\u751f\u6210\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u51e0\u4f55\u4e00\u81f4\u6027\u3001\u8fd0\u52a8\u8fde\u8d2f\u6027\u6307\u6807\uff0c\u5e76\u51cf\u5c11\u4e86\u89c6\u89d2-\u65f6\u95f4\u4f2a\u5f71\u3002\u80fd\u591f\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u89c6\u9891\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "conclusion": "WorldReel\u5c06\u89c6\u9891\u751f\u6210\u63a8\u54114D\u4e00\u81f4\u7684\u4e16\u754c\u5efa\u6a21\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u5355\u4e00\u7a33\u5b9a\u7684\u65f6\u7a7a\u8868\u793a\u8fdb\u884c\u6e32\u67d3\u3001\u4ea4\u4e92\u548c\u573a\u666f\u63a8\u7406\uff0c\u4e3a\u6784\u5efa\u4e00\u81f4\u7684\u65f6\u7a7a\u573a\u666f\u8868\u793a\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2512.07829", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07829", "abs": "https://arxiv.org/abs/2512.07829", "authors": ["Yuan Gao", "Chen Chen", "Tianrong Chen", "Jiatao Gu"], "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation", "comment": null, "summary": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.", "AI": {"tldr": "FAE\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8868\u5f81\u9002\u914d\u5230\u9002\u5408\u751f\u6210\u7684\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u4ec5\u9700\u5355\u4e2a\u6ce8\u610f\u529b\u5c42\u5373\u53ef\u5b9e\u73b0\uff0c\u540c\u65f6\u4fdd\u7559\u91cd\u5efa\u548c\u7406\u89e3\u6240\u9700\u7684\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u7406\u89e3\u5bfc\u5411\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u5f81\u9002\u914d\u5230\u751f\u6210\u53cb\u597d\u7684\u6f5c\u5728\u7a7a\u95f4\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u7406\u89e3\u7279\u5f81\u9700\u8981\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u6765\u6355\u6349\u591a\u6837\u5316\u5047\u8bbe\uff0c\u800c\u751f\u6210\u6a21\u578b\u9700\u8981\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u6765\u5fe0\u5b9e\u4fdd\u7559\u6ce8\u5165\u7684\u566a\u58f0\uff0c\u8fd9\u79cd\u4e0d\u5339\u914d\u5bfc\u81f4\u5148\u524d\u5de5\u4f5c\u9700\u8981\u590d\u6742\u7684\u67b6\u6784\u548c\u76ee\u6807\u3002", "method": "FAE\u6846\u67b6\u901a\u8fc7\u8026\u5408\u4e24\u4e2a\u72ec\u7acb\u7684\u6df1\u5ea6\u89e3\u7801\u5668\u6765\u9002\u914d\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u5f81\uff1a\u4e00\u4e2a\u89e3\u7801\u5668\u8bad\u7ec3\u7528\u4e8e\u91cd\u5efa\u539f\u59cb\u7279\u5f81\u7a7a\u95f4\uff0c\u7b2c\u4e8c\u4e2a\u89e3\u7801\u5668\u5c06\u91cd\u5efa\u7279\u5f81\u4f5c\u4e3a\u8f93\u5165\u8fdb\u884c\u56fe\u50cf\u751f\u6210\u3002\u8be5\u6846\u67b6\u901a\u7528\u6027\u5f3a\uff0c\u53ef\u4e0e\u591a\u79cd\u81ea\u76d1\u7763\u7f16\u7801\u5668\uff08\u5982DINO\u3001SigLIP\uff09\u7ed3\u5408\uff0c\u5e76\u9002\u7528\u4e8e\u6269\u6563\u6a21\u578b\u548c\u5f52\u4e00\u5316\u6d41\u4e24\u79cd\u751f\u6210\u6a21\u578b\u5bb6\u65cf\u3002", "result": "\u5728\u7c7b\u522b\u6761\u4ef6\u548c\u6587\u672c\u5230\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFAE\u8868\u73b0\u51fa\u8272\u3002\u5728ImageNet 256x256\u4e0a\uff0c\u5e26CFG\u7684\u6269\u6563\u6a21\u578b\u8fbe\u5230\u63a5\u8fd1SOTA\u7684FID 1.29\uff08800\u8f6e\uff09\u548c1.70\uff0880\u8f6e\uff09\uff1b\u4e0d\u5e26CFG\u65f6\u8fbe\u5230SOTA\u7684FID 1.48\uff08800\u8f6e\uff09\u548c2.08\uff0880\u8f6e\uff09\uff0c\u5c55\u793a\u4e86\u9ad8\u8d28\u91cf\u548c\u5feb\u901f\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "FAE\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u5730\u5c06\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u5f81\u9002\u914d\u5230\u751f\u6210\u53cb\u597d\u7684\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u4ec5\u9700\u6700\u5c0f\u67b6\u6784\u4fee\u6539\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u7406\u89e3\u548c\u91cd\u5efa\u80fd\u529b\u3002"}}
{"id": "2512.07833", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07833", "abs": "https://arxiv.org/abs/2512.07833", "authors": ["Thao Nguyen", "Sicheng Mo", "Krishna Kumar Singh", "Yilin Wang", "Jing Shi", "Nicholas Kolkin", "Eli Shechtman", "Yong Jae Lee", "Yuheng Li"], "title": "Relational Visual Similarity", "comment": "Project page, data, and code: https://thaoshibe.github.io/relsim", "summary": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5173\u7cfb\u76f8\u4f3c\u6027\u7684\u6982\u5ff5\uff0c\u5f00\u53d1\u4e86\u9996\u4e2a\u80fd\u591f\u6d4b\u91cf\u56fe\u50cf\u95f4\u5173\u7cfb\u76f8\u4f3c\u6027\u7684\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u89c6\u89c9\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\u5728\u6355\u6349\u4eba\u7c7b\u5173\u7cfb\u611f\u77e5\u80fd\u529b\u65b9\u9762\u7684\u91cd\u5927\u7f3a\u9677\u3002", "motivation": "\u4eba\u7c7b\u4e0d\u4ec5\u80fd\u611f\u77e5\u5c5e\u6027\u76f8\u4f3c\u6027\uff08\u5982\u989c\u8272\u3001\u5f62\u72b6\uff09\uff0c\u8fd8\u80fd\u611f\u77e5\u5173\u7cfb\u76f8\u4f3c\u6027\uff08\u5982\u5730\u7403\u548c\u6843\u5b50\u7684\u5206\u5c42\u7ed3\u6784\u76f8\u4f3c\uff09\u3002\u7136\u800c\uff0c\u5f53\u524d\u5e7f\u6cdb\u4f7f\u7528\u7684\u89c6\u89c9\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff08\u5982LPIPS\u3001CLIP\u3001DINO\uff09\u90fd\u53ea\u5173\u6ce8\u611f\u77e5\u5c5e\u6027\u76f8\u4f3c\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u611f\u77e5\u5230\u7684\u4e30\u5bcc\u4e14\u5e38\u5e38\u4ee4\u4eba\u60ca\u8bb6\u7684\u5173\u7cfb\u76f8\u4f3c\u6027\u3002\u8fd9\u63ed\u793a\u4e86\u89c6\u89c9\u8ba1\u7b97\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u7a7a\u767d\u3002", "method": "1. \u9996\u5148\u5c06\u5173\u7cfb\u56fe\u50cf\u76f8\u4f3c\u6027\u5f62\u5f0f\u5316\u4e3a\u53ef\u6d4b\u91cf\u7684\u95ee\u9898\uff1a\u5f53\u4e24\u5e45\u56fe\u50cf\u7684\u5185\u90e8\u5173\u7cfb\u6216\u89c6\u89c9\u5143\u7d20\u4e4b\u95f4\u7684\u529f\u80fd\u5bf9\u5e94\u65f6\uff0c\u5373\u4f7f\u5b83\u4eec\u7684\u89c6\u89c9\u5c5e\u6027\u4e0d\u540c\uff0c\u5b83\u4eec\u4e5f\u5177\u6709\u5173\u7cfb\u76f8\u4f3c\u6027\u3002\n2. \u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b11.4\u4e07\u5f20\u56fe\u50cf-\u6807\u9898\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u6807\u9898\u7ecf\u8fc7\u533f\u540d\u5316\u5904\u7406\uff0c\u63cf\u8ff0\u573a\u666f\u7684\u5e95\u5c42\u5173\u7cfb\u903b\u8f91\u800c\u975e\u8868\u9762\u5185\u5bb9\u3002\n3. \u4f7f\u7528\u8fd9\u4e2a\u6570\u636e\u96c6\u5bf9\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6d4b\u91cf\u56fe\u50cf\u4e4b\u95f4\u7684\u5173\u7cfb\u76f8\u4f3c\u6027\u3002", "result": "\u5f00\u53d1\u51fa\u4e86\u9996\u4e2a\u80fd\u591f\u6d4b\u91cf\u56fe\u50cf\u5173\u7cfb\u76f8\u4f3c\u6027\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6839\u636e\u56fe\u50cf\u7684\u5e95\u5c42\u5173\u7cfb\u7ed3\u6784\u800c\u975e\u53ef\u89c1\u5916\u89c2\u6765\u8fde\u63a5\u56fe\u50cf\u3002\u7814\u7a76\u8868\u660e\uff0c\u867d\u7136\u5173\u7cfb\u76f8\u4f3c\u6027\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6709\u8bb8\u591a\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u7684\u56fe\u50cf\u76f8\u4f3c\u6027\u6a21\u578b\u90fd\u65e0\u6cd5\u6355\u6349\u5230\u5b83\u3002", "conclusion": "\u5173\u7cfb\u76f8\u4f3c\u6027\u662f\u89c6\u89c9\u8ba1\u7b97\u4e2d\u4e00\u4e2a\u88ab\u5ffd\u89c6\u4f46\u81f3\u5173\u91cd\u8981\u7684\u7ef4\u5ea6\u3002\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u9996\u4e2a\u80fd\u591f\u6d4b\u91cf\u5173\u7cfb\u76f8\u4f3c\u6027\u7684\u6a21\u578b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u89c6\u89c9\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\u7684\u91cd\u5927\u7a7a\u767d\uff0c\u4e3a\u7406\u89e3\u4eba\u7c7b\u5982\u4f55\u611f\u77e5\u548c\u8bc6\u522b\u56fe\u50cf\u95f4\u7684\u6df1\u5c42\u5173\u7cfb\u7ed3\u6784\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2512.07076", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07076", "abs": "https://arxiv.org/abs/2512.07076", "authors": ["Chen-Yang Wang", "Gepeng Ji", "Song Shao", "Ming-Ming Cheng", "Deng-Ping Fan"], "title": "Context-measure: Contextualizing Metric for Camouflage", "comment": "Technical Report", "summary": "Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.", "AI": {"tldr": "\u63d0\u51faContext-measure\uff1a\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u50cf\u7d20\u611f\u77e5\u76f8\u5173\u6846\u67b6\u7684\u4e0a\u4e0b\u6587\u5316\u8bc4\u4f30\u8303\u5f0f\uff0c\u7528\u4e8e\u8bc4\u4f30\u4f2a\u88c5\u7269\u4f53\u5206\u5272\uff0c\u6bd4\u73b0\u6709\u4e0a\u4e0b\u6587\u65e0\u5173\u6307\u6807\u66f4\u53ef\u9760", "motivation": "\u5f53\u524d\u4f2a\u88c5\u573a\u666f\u7684\u8bc4\u4f30\u6307\u6807\u5ffd\u89c6\u4e86\u4e0a\u4e0b\u6587\u4f9d\u8d56\u8fd9\u4e00\u5173\u952e\u56e0\u7d20\uff0c\u8fd9\u4e9b\u6307\u6807\u539f\u672c\u662f\u4e3a\u8bc4\u4f30\u4e00\u822c\u6216\u663e\u8457\u7269\u4f53\u8bbe\u8ba1\u7684\uff0c\u5047\u8bbe\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4e0d\u76f8\u5173\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4f2a\u88c5\u573a\u666f\u7684\u7279\u6027", "method": "\u63d0\u51faContext-measure\u8bc4\u4f30\u8303\u5f0f\uff0c\u57fa\u4e8e\u6982\u7387\u50cf\u7d20\u611f\u77e5\u76f8\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u50cf\u7d20\u7ea7\u4f2a\u88c5\u91cf\u5316\uff0c\u4f7f\u8bc4\u4f30\u66f4\u7b26\u5408\u4eba\u7c7b\u611f\u77e5", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4f2a\u88c5\u7269\u4f53\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cContext-measure\u6bd4\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u65e0\u5173\u6307\u6807\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u7ed3\u679c", "conclusion": "Context-measure\u53ef\u4ee5\u4e3a\u6d89\u53ca\u4f2a\u88c5\u6a21\u5f0f\u7684\u5404\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\uff08\u5982\u519c\u4e1a\u3001\u5de5\u4e1a\u548c\u533b\u7597\u573a\u666f\uff09\u63d0\u4f9b\u57fa\u7840\u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2512.07107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07107", "abs": "https://arxiv.org/abs/2512.07107", "authors": ["Jaeyoon Lee", "Hojoon Jung", "Sungtae Hwang", "Jihyong Oh", "Jongwon Choi"], "title": "COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision", "comment": "Project page: https://vilab-cau.github.io/COREA/", "summary": "We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.", "AI": {"tldr": "COREA\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u8054\u5408\u5b66\u4e60\u53ef\u91cd\u7167\u660e\u76843D\u9ad8\u65af\u548cSDF\uff0c\u5b9e\u73b0\u7cbe\u786e\u51e0\u4f55\u91cd\u5efa\u548c\u771f\u5b9e\u91cd\u7167\u660e\uff0c\u901a\u8fc73D\u52303D\u5bf9\u9f50\u7b56\u7565\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u51e0\u4f55\u7c97\u7cd9\u548cBRDF-\u5149\u7167\u5206\u89e3\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\u867d\u7136\u6269\u5c55\u5230\u7f51\u683c\u91cd\u5efa\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\uff0c\u4f46\u5176\u51e0\u4f55\u4ecd\u4ece2D\u6e32\u67d3\u5b66\u4e60\uff0c\u5bfc\u81f4\u8868\u9762\u7c97\u7cd9\u4e14BRDF-\u5149\u7167\u5206\u89e3\u4e0d\u53ef\u9760\u3002\u9700\u8981\u76f4\u63a5\u57283D\u7a7a\u95f4\u4e2d\u5b66\u4e60\u51e0\u4f55\u4fe1\u53f7\u4ee5\u83b7\u5f97\u66f4\u7cbe\u786e\u7684\u51e0\u4f55\u91cd\u5efa\u548c\u7a33\u5b9a\u7684\u5149\u7167\u5206\u89e3\u3002", "method": "\u63d0\u51fa\u7c97\u5230\u7ec6\u7684\u53cc\u54113D\u52303D\u5bf9\u9f50\u7b56\u7565\uff1a\u6df1\u5ea6\u63d0\u4f9b\u7c97\u5bf9\u9f50\uff0c\u6df1\u5ea6\u68af\u5ea6\u548c\u6cd5\u7ebf\u7ec6\u5316\u7cbe\u7ec6\u7ed3\u6784\uff1b\u5f15\u5165\u5bc6\u5ea6\u63a7\u5236\u673a\u5236\u7a33\u5b9a\u9ad8\u65af\u589e\u957f\uff1b\u8054\u5408\u5b66\u4e60\u53ef\u91cd\u7167\u660e3D\u9ad8\u65af\u548cSDF\u8868\u793a\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOREA\u5728\u65b0\u89c6\u89d2\u5408\u6210\u3001\u7f51\u683c\u91cd\u5efa\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\u65b9\u9762\u5747\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u7684\u5e73\u8861\u3002", "conclusion": "COREA\u662f\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc73D\u52303D\u5bf9\u9f50\u76f4\u63a5\u5b66\u4e60\u51e0\u4f55\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u51e0\u4f55\u91cd\u5efa\u548c\u771f\u5b9e\u91cd\u7167\u660e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u51e0\u4f55\u7cbe\u5ea6\u548cBRDF-\u5149\u7167\u5206\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.07126", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07126", "abs": "https://arxiv.org/abs/2512.07126", "authors": ["Shengjie Lu", "Zhibin Wan", "Jiejie Liu", "Quan Zhang", "Mingjie Sun"], "title": "Training-free Clothing Region of Interest Self-correction for Virtual Try-On", "comment": "16 pages, 8 figures", "summary": "VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\uff0c\u901a\u8fc7\u80fd\u91cf\u51fd\u6570\u7ea6\u675f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u751f\u6210\u7ed3\u679c\u66f4\u7b26\u5408\u76ee\u6807\u670d\u88c5\u7ec6\u8282\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807VTID\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u5728\u751f\u6210\u670d\u88c5\u7ed3\u679c\u4e0e\u76ee\u6807\u670d\u88c5\u4e4b\u95f4\u5b58\u5728\u6a21\u5f0f\u3001\u7eb9\u7406\u548c\u8fb9\u754c\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u4e14\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u4ec5\u5173\u6ce8\u56fe\u50cf\u771f\u5b9e\u611f\u800c\u5ffd\u7565\u4e0e\u76ee\u6807\u5143\u7d20\u7684\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u80fd\u91cf\u51fd\u6570\u7ea6\u675f\u751f\u6210\u8fc7\u7a0b\u4e2d\u63d0\u53d6\u7684\u6ce8\u610f\u529b\u56fe\uff0c\u4f7f\u6ce8\u610f\u529b\u66f4\u96c6\u4e2d\u4e8e\u670d\u88c5\u611f\u5174\u8da3\u533a\u57df\uff0c\u4ece\u800c\u5f71\u54cd\u751f\u6210\u7ed3\u679c\u66f4\u7b26\u5408\u76ee\u6807\u670d\u88c5\u7ec6\u8282\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807VTID\u3002", "result": "\u5728VITON-HD\u548cDressCode\u6570\u636e\u96c6\u4e0a\uff0cLPIPS\u3001FID\u3001KID\u548cVTID\u6307\u6807\u5206\u522b\u63d0\u53471.4%\u30012.3%\u300112.3%\u548c5.8%\u3002\u5728\u670d\u88c5\u66f4\u6362\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0cLTCC\u3001PRCC\u3001VC-Clothes\u6570\u636e\u96c6\u7684Rank-1\u6307\u6807\u5206\u522b\u63d0\u53472.5%\u30011.1%\u548c1.6%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u7ea6\u675f\u65b9\u6cd5\u548cVTID\u8bc4\u4f30\u6307\u6807\u6709\u6548\u63d0\u5347\u4e86\u865a\u62df\u8bd5\u7a7f\u7684\u8d28\u91cf\u548c\u8bc4\u4f30\u5168\u9762\u6027\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.07128", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07128", "abs": "https://arxiv.org/abs/2512.07128", "authors": ["Chau Truong", "Hieu Ta Quang", "Dung D. Le"], "title": "MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP", "comment": null, "summary": "Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.", "AI": {"tldr": "MulCLIP\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u591a\u7ea7\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u5bf9\u6bd4\u5bf9\u9f50\u3001\u5c40\u90e8\u7279\u5f81\u6821\u51c6\u548c\u5b50\u6807\u9898\u805a\u5408\u8865\u4e01\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86CLIP\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u63cf\u8ff0\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u90e8\u7f72\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u7406\u89e3\u80fd\u529b\u3002", "motivation": "CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u77ed\u6587\u672c\u5bf9\u9f50\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9762\u5bf9\u957f\u6587\u672c\u8be6\u7ec6\u63cf\u8ff0\u65f6\u6548\u679c\u4e0d\u4f73\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u533a\u57df\u5efa\u8bae\u4fe1\u606f\u6765\u6620\u5c04\u89c6\u89c9\u533a\u57df\u548c\u957f\u6587\u672c\u53e5\u5b50\uff0c\u4f46\u90e8\u7f72\u6210\u672c\u8f83\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5904\u7406\u957f\u6587\u672c\u7ed3\u6784\uff0c\u53c8\u80fd\u4fdd\u6301\u90e8\u7f72\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u4fdd\u6301\u56fe\u50cf\u4e0e\u6458\u8981\u53ca\u957f\u6807\u9898\u7684\u5168\u5c40\u5bf9\u6bd4\u5bf9\u9f50\uff0c\u6269\u5c55\u4f4d\u7f6e\u5d4c\u5165\u4ee5\u652f\u6301\u66f4\u957f\u6587\u672c\u5e8f\u5217\uff1b2. \u63d0\u51fa\u5c40\u90e8\u6821\u51c6\u7279\u5f81\u4e0a\u7684\u4ee4\u724c\u91cd\u5efa\u5bf9\u9f50\uff0c\u589e\u5f3a\u5355\u8bcd\u548c\u56fe\u50cf\u8865\u4e01\u4e4b\u95f4\u7684\u8bed\u4e49\u8fde\u63a5\uff1b3. \u63d0\u51fa\u5b50\u6807\u9898\u805a\u5408\u8865\u4e01\u5bf9\u9f50\uff0c\u81ea\u52a8\u63d0\u53d6\u548c\u805a\u5408\u6bcf\u4e2a\u5b50\u6807\u9898\u7684\u4e0a\u4e0b\u6587\u4e30\u5bcc\u8865\u4e01\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMulCLIP\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u5176\u591a\u5c3a\u5ea6\u5bf9\u9f50\u662f\u9a71\u52a8\u6bd4\u533a\u57df\u5efa\u8bae\u8f85\u52a9\u65b9\u6cd5\u66f4\u597d\u7ec6\u7c92\u5ea6\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u7279\u522b\u9002\u5408\u591a\u6837\u5316\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "conclusion": "MulCLIP\u901a\u8fc7\u521b\u65b0\u7684\u591a\u7ea7\u5bf9\u9f50\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86CLIP\u6a21\u578b\u5904\u7406\u957f\u6587\u672c\u63cf\u8ff0\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u90e8\u7f72\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07155", "abs": "https://arxiv.org/abs/2512.07155", "authors": ["Dahyeon Kye", "Jeahun Sung", "MinKyu Jeon", "Jihyong Oh"], "title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics", "comment": "Please visit our project page at https://cmlab-korea.github.io/CHIMERA/", "summary": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.", "AI": {"tldr": "CHIMERA\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7f13\u5b58\u53cd\u8f6c\u5f15\u5bfc\u7684\u53bb\u566a\u8fc7\u7a0b\u5b9e\u73b0\u56fe\u50cf\u53d8\u5f62\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u5e73\u6ed1\u8fc7\u6e21\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5728\u5b9e\u73b0\u5e73\u6ed1\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u56fe\u50cf\u53d8\u5f62\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u7f3a\u4e4f\u81ea\u9002\u5e94\u7ed3\u6784\u548c\u8bed\u4e49\u5bf9\u9f50\uff0c\u5f80\u5f80\u4ea7\u751f\u7a81\u5140\u7684\u8fc7\u6e21\u6216\u8fc7\u9971\u548c\u7684\u5916\u89c2\u3002", "method": "\u63d0\u51faCHIMERA\u6846\u67b6\uff0c\u5c06\u53d8\u5f62\u95ee\u9898\u6784\u5efa\u4e3a\u7f13\u5b58\u53cd\u8f6c\u5f15\u5bfc\u7684\u53bb\u566a\u8fc7\u7a0b\u3002\u91c7\u7528\u81ea\u9002\u5e94\u7f13\u5b58\u6ce8\u5165\uff08ACI\uff09\u5728DDIM\u53cd\u8f6c\u671f\u95f4\u7f13\u5b58\u8f93\u5165\u7279\u5f81\u5e76\u5728\u53bb\u566a\u65f6\u81ea\u9002\u5e94\u91cd\u65b0\u6ce8\u5165\uff0c\u5b9e\u73b0\u7a7a\u95f4\u548c\u8bed\u4e49\u5bf9\u9f50\uff1b\u4f7f\u7528\u8bed\u4e49\u951a\u70b9\u63d0\u793a\uff08SAP\uff09\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5171\u4eab\u951a\u70b9\u63d0\u793a\u4f5c\u4e3a\u8bed\u4e49\u6865\u6881\uff1b\u5f15\u5165\u5168\u5c40-\u5c40\u90e8\u4e00\u81f4\u6027\u8bc4\u5206\uff08GLCS\uff09\u4f5c\u4e3a\u53d8\u5f62\u5bfc\u5411\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cCHIMERA\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u3001\u8bed\u4e49\u66f4\u4e00\u81f4\u7684\u8fc7\u6e21\uff0c\u5728\u56fe\u50cf\u53d8\u5f62\u4efb\u52a1\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "CHIMERA\u901a\u8fc7\u521b\u65b0\u7684\u7f13\u5b58\u53cd\u8f6c\u5f15\u5bfc\u53bb\u566a\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7279\u5f81\u6ce8\u5165\u548c\u8bed\u4e49\u951a\u70b9\u63d0\u793a\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u50cf\u53d8\u5f62\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u5e73\u6ed1\u8fc7\u6e21\u95ee\u9898\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u53d8\u5f62\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07171", "abs": "https://arxiv.org/abs/2512.07171", "authors": ["Shravan Venkatraman", "Rakesh Raj Madavan", "Pavan Kumar S", "Muthu Subash Kavitha"], "title": "TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration", "comment": "21 pages, 11 figures, 5 tables", "summary": "Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\\underline{t}$wo stage $\\underline{i}$nverse $\\underline{d}$egradation $\\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.", "AI": {"tldr": "TIDE\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6c34\u4e0b\u56fe\u50cf\u590d\u539f\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u5148\u9a8c\u5206\u89e3\u6765\u663e\u5f0f\u5efa\u6a21\u9000\u5316\u7279\u5f81\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u590d\u539f\uff0c\u5c06\u6c34\u4e0b\u9000\u5316\u5206\u89e3\u4e3a\u56db\u4e2a\u5173\u952e\u56e0\u7d20\u5e76\u8bbe\u8ba1\u4e13\u95e8\u7684\u590d\u539f\u4e13\u5bb6\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u590d\u539f\u5bf9\u6d77\u6d0b\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u6574\u4e2a\u56fe\u50cf\u4e0a\u5e94\u7528\u7edf\u4e00\u7684\u590d\u539f\u7b56\u7565\uff0c\u96be\u4ee5\u5904\u7406\u7a7a\u95f4\u53d8\u5316\u4e14\u540c\u65f6\u53d1\u751f\u7684\u591a\u79cd\u9000\u5316\u95ee\u9898\u3002", "method": "TIDE\u91c7\u7528\u4e24\u9636\u6bb5\u9006\u9000\u5316\u4f30\u8ba1\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5c06\u6c34\u4e0b\u9000\u5316\u5206\u89e3\u4e3a\u989c\u8272\u5931\u771f\u3001\u96fe\u973e\u3001\u7ec6\u8282\u635f\u5931\u548c\u566a\u58f0\u56db\u4e2a\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u8bbe\u8ba1\u4e13\u95e8\u7684\u590d\u539f\u4e13\u5bb6\u751f\u6210\u591a\u4e2a\u5047\u8bbe\uff1b\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u5c40\u90e8\u9000\u5316\u6a21\u5f0f\u81ea\u9002\u5e94\u878d\u5408\u8fd9\u4e9b\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u6e10\u8fdb\u7ec6\u5316\u9636\u6bb5\u6821\u6b63\u6b8b\u7559\u4f2a\u5f71\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u548c\u6311\u6218\u6027\u7684\u6d51\u6d4a\u6c34\u57df\u6761\u4ef6\u4e0b\uff0cTIDE\u5728\u57fa\u4e8e\u53c2\u8003\u7684\u4fdd\u771f\u5ea6\u6307\u6807\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u540c\u65f6\u5728\u975e\u53c2\u8003\u611f\u77e5\u8d28\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u989c\u8272\u6821\u6b63\u548c\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "TIDE\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u9000\u5316\u7279\u5f81\u548c\u4e13\u95e8\u7684\u5148\u9a8c\u5206\u89e3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0b\u56fe\u50cf\u590d\u539f\u4e2d\u7a7a\u95f4\u53d8\u5316\u7684\u591a\u91cd\u9000\u5316\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u6c34\u4e0b\u73af\u5883\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u590d\u539f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07197", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07197", "abs": "https://arxiv.org/abs/2512.07197", "authors": ["Seokhyun Youn", "Soohyun Lee", "Geonho Kim", "Weeyoung Kwon", "Sung-Ho Bae", "Jihyong Oh"], "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting", "comment": "The first three authors contributed equally to this work. The last two authors are co-corresponding authors. Please visit our project page at https://cmlab-korea.github.io/Awesome-Efficient-GS/", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u603b\u7ed3\u4e86\u9ad8\u65483D\u548c4D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5c06\u5176\u5206\u4e3a\u53c2\u6570\u538b\u7f29\u548c\u7ed3\u6784\u538b\u7f29\u4e24\u5927\u65b9\u5411\uff0c\u5e76\u5206\u6790\u4e86\u65b9\u6cd5\u8d8b\u52bf\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u867d\u7136\u80fd\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u4fdd\u771f3D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u4f46\u5176\u5b58\u50a8\u548c\u6e32\u67d3\u6570\u767e\u4e07\u9ad8\u65af\u5206\u5e03\u9700\u8981\u5de8\u5927\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u7279\u522b\u662f\u57284D\u52a8\u6001\u573a\u666f\u4e2d\u95ee\u9898\u66f4\u52a0\u4e25\u91cd\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u6765\u51cf\u5c11\u5197\u4f59\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u5c06\u73b0\u6709\u9ad8\u65483D\u548c4D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u7cfb\u7edf\u6027\u5730\u5206\u4e3a\u4e24\u5927\u65b9\u5411\uff1a1) \u53c2\u6570\u538b\u7f29 - \u51cf\u5c11\u6bcf\u4e2a\u9ad8\u65af\u5206\u5e03\u7684\u53c2\u6570\u6570\u91cf\uff1b2) \u7ed3\u6784\u538b\u7f29 - \u51cf\u5c11\u9ad8\u65af\u5206\u5e03\u7684\u603b\u6570\u91cf\u3002\u5bf9\u6bcf\u4e2a\u7c7b\u522b\u603b\u7ed3\u4e86\u6838\u5fc3\u601d\u60f3\u548c\u65b9\u6cd5\u8d8b\u52bf\u3002", "result": "\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u7684\u7edf\u4e00\u6982\u8ff0\uff0c\u6db5\u76d6\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u4ee3\u8868\u6027\u57fa\u51c6\u6bd4\u8f83\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6280\u672f\u8def\u7ebf\u56fe\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5f53\u524d\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u7d27\u51d1\u4e14\u5b9e\u65f6\u7684\u9759\u6001\u548c\u52a8\u60013D\u573a\u666f\u8868\u793a\u7684\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u3002"}}
{"id": "2512.07203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07203", "abs": "https://arxiv.org/abs/2512.07203", "authors": ["Xuhui Zheng", "Kang An", "Ziliang Wang", "Yuhang Wang", "Faqiang Qian", "Yichao Wu"], "title": "MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning", "comment": "7 pages, 1 figures", "summary": "Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.", "AI": {"tldr": "MMRPT\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a9\u7801\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u89c6\u89c9\u57fa\u7840\u800c\u975e\u6587\u672c\u6a21\u4eff\uff0c\u589e\u5f3a\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u53d7\u9650\u4e8e\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u63cf\u8ff0\u6027\u504f\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u66f4\u503e\u5411\u4e8e\u4f9d\u8d56\u8868\u9762\u8bed\u8a00\u7ebf\u7d22\u800c\u975e\u771f\u6b63\u7684\u89c6\u89c9\u7406\u89e3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5f3a\u5316\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMMRPT\u6846\u67b6\uff1a1)\u901a\u8fc7\u89c6\u89c9token\u6ce8\u610f\u529b\u4f30\u8ba1\u53e5\u5b50\u7ea7\u89c6\u89c9\u4f9d\u8d56\u6027\uff0c\u63a9\u7801\u9ad8\u5ea6\u4f9d\u8d56\u89c6\u89c9\u7684\u6587\u672c\u7247\u6bb5\uff1b2)\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u5230\u9884\u8bad\u7ec3\u4e2d\uff0c\u4f7f\u7528\u8bed\u4e49-\u89c6\u89c9\u5956\u52b1\u5f15\u5bfc\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u57fa\u7840\u7684\u63a8\u7406\u91cd\u5efa\uff1b3)\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4fe1\u53f7\u5956\u52b1\u89c6\u89c9\u57fa\u7840\u800c\u975e\u6587\u672c\u6a21\u4eff\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a1)\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u4e00\u81f4\u7684\u96f6\u6837\u672c\u6027\u80fd\u63d0\u5347\uff1b2)\u5728\u6709\u76d1\u7763\u5fae\u8c03\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff1b3)\u8bc1\u660e\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a9\u7801\u63a8\u7406\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u6cdb\u5316\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u3002", "conclusion": "MMRPT\u901a\u8fc7\u5c06\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u6574\u5408\u5230\u591a\u6a21\u6001\u5927\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\uff0c\u6210\u529f\u5f3a\u5316\u4e86\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u4e2d\u7684\u63cf\u8ff0\u6027\u504f\u5dee\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2512.07206", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07206", "abs": "https://arxiv.org/abs/2512.07206", "authors": ["Boyang Pan", "Zeyu Zhang", "Hongyu Meng", "Bin Cui", "Yingying Zhang", "Wenli Hou", "Junhao Li", "Langdi Zhong", "Xiaoxiao Chen", "Xiaoyu Xu", "Changjin Zuo", "Chao Cheng", "Nan-Jie Gong"], "title": "AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT", "comment": null, "summary": "Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u540d\u4e3aAutoLugano\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u80fd\u591f\u4eceFDG-PET/CT\u626b\u63cf\u4e2d\u81ea\u52a8\u8fdb\u884c\u6dcb\u5df4\u7624\u5206\u7c7b\uff0c\u5305\u62ec\u75c5\u7076\u5206\u5272\u3001\u89e3\u5256\u5b9a\u4f4d\u548cLugano\u5206\u671f", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5168\u81ea\u52a8\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u4ece\u57fa\u7ebfFDG-PET/CT\u626b\u63cf\u4e2d\u81ea\u52a8\u5b8c\u6210\u6dcb\u5df4\u7624\u5206\u7c7b\uff0c\u5305\u62ec\u75c5\u7076\u5206\u5272\u3001\u89e3\u5256\u5b9a\u4f4d\u548cLugano\u5206\u671f\uff0c\u4ee5\u8f85\u52a9\u4e34\u5e8a\u51b3\u7b56", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u987a\u5e8f\u6a21\u5757\uff1a1\uff09\u57fa\u4e8e3D nnU-Net\u7684\u89e3\u5256\u611f\u77e5\u75c5\u7076\u5206\u5272\uff1b2\uff09\u4f7f\u7528TotalSegmentator\u5de5\u5177\u5305\u8fdb\u884c\u57fa\u4e8e\u56fe\u8c31\u7684\u89e3\u5256\u5b9a\u4f4d\uff1b3\uff09\u5c06\u53d7\u7d2f\u533a\u57df\u7a7a\u95f4\u5206\u5e03\u8f6c\u6362\u4e3aLugano\u5206\u671f\u548c\u6cbb\u7597\u5206\u7ec4\u7684\u81ea\u52a8\u5316\u5206\u671f\u6a21\u5757", "result": "\u5728\u5916\u90e8\u9a8c\u8bc1\u96c6\u4e0a\uff0c\u533a\u57df\u53d7\u7d2f\u68c0\u6d4b\u51c6\u786e\u738788.31%\uff0c\u654f\u611f\u602774.47%\uff0c\u7279\u5f02\u602794.21%\uff0cF1\u5206\u657080.80%\uff1b\u6cbb\u7597\u5206\u5c42\uff08\u5c40\u9650\u671fvs.\u8fdb\u5c55\u671f\uff09\u51c6\u786e\u738785.07%\uff0c\u7279\u5f02\u602790.48%\uff0c\u654f\u611f\u602782.61%", "conclusion": "AutoLugano\u662f\u9996\u4e2a\u5168\u81ea\u52a8\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u80fd\u5c06\u5355\u6b21FDG-PET/CT\u626b\u63cf\u8f6c\u6362\u4e3a\u5b8c\u6574\u7684Lugano\u5206\u671f\uff0c\u5728\u521d\u59cb\u5206\u671f\u3001\u6cbb\u7597\u5206\u5c42\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u65b9\u9762\u5177\u6709\u5f3a\u5927\u6f5c\u529b"}}
{"id": "2512.07229", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07229", "abs": "https://arxiv.org/abs/2512.07229", "authors": ["Fang Zhou", "Zhiqiang Chen", "Martin Pavlovski", "Yizhong Zhang"], "title": "ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery", "comment": "Accepted to the Main Track of the 28th European Conference on Artificial Intelligence (ECAI 2025). To appear in the proceedings published by IOS Press (DOI: 10.3233/FAIA413)", "summary": "Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.", "AI": {"tldr": "ReLKD\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u9690\u5f0f\u7684\u7c7b\u95f4\u5173\u7cfb\u6765\u589e\u5f3a\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u4e2d\u65b0\u9896\u7c7b\u522b\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u76ee\u6807\u7c92\u5ea6\u6a21\u5757\u3001\u7c97\u7c92\u5ea6\u6a21\u5757\u548c\u84b8\u998f\u6a21\u5757\u3002", "motivation": "\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0(GCD)\u9762\u4e34\u5728\u53ea\u6709\u5df2\u77e5\u7c7b\u522b\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u5bf9\u5305\u542b\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u522b\u7684\u672a\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u7684\u6311\u6218\u3002\u5148\u524d\u7684\u7814\u7a76\u901a\u5e38\u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u7c7b\u522b\uff0c\u5ffd\u7565\u4e86\u56fa\u6709\u7684\u7c7b\u95f4\u5173\u7cfb\u3002\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u76f4\u63a5\u83b7\u53d6\u8fd9\u4e9b\u7c7b\u95f4\u5173\u7cfb\u5177\u6709\u663e\u8457\u6311\u6218\u6027\u3002", "method": "ReLKD\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a1)\u76ee\u6807\u7c92\u5ea6\u6a21\u5757\u7528\u4e8e\u5b66\u4e60\u5224\u522b\u6027\u8868\u793a\uff1b2)\u7c97\u7c92\u5ea6\u6a21\u5757\u7528\u4e8e\u6355\u83b7\u5c42\u6b21\u5316\u7684\u7c7b\u95f4\u5173\u7cfb\uff1b3)\u84b8\u998f\u6a21\u5757\u7528\u4e8e\u5c06\u77e5\u8bc6\u4ece\u7c97\u7c92\u5ea6\u6a21\u5757\u8f6c\u79fb\u5230\u76ee\u6807\u7c92\u5ea6\u6a21\u5757\uff0c\u4ee5\u4f18\u5316\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86ReLKD\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ReLKD\u901a\u8fc7\u6709\u6548\u5229\u7528\u9690\u5f0f\u7c7b\u95f4\u5173\u7cfb\u5e76\u5c06\u5176\u77e5\u8bc6\u8f6c\u79fb\u5230\u76ee\u6807\u7c92\u5ea6\u8868\u793a\u5b66\u4e60\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u4e2d\u65b0\u9896\u7c7b\u522b\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2512.07230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07230", "abs": "https://arxiv.org/abs/2512.07230", "authors": ["Abhinav Raundhal", "Gaurav Behera", "P J Narayanan", "Ravi Kiran Sarvadevabhatla", "Makarand Tapaswi"], "title": "STRinGS: Selective Text Refinement in Gaussian Splatting", "comment": "Accepted to WACV 2026. Project Page, see https://STRinGS-official.github.io", "summary": "Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.", "AI": {"tldr": "STRinGS\u662f\u4e00\u4e2a\u9488\u5bf93D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u7684\u6587\u672c\u611f\u77e5\u9009\u62e9\u6027\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u522b\u5904\u7406\u6587\u672c\u548c\u975e\u6587\u672c\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u53ef\u8bfb\u6027\uff0c\u5e76\u5f15\u5165OCR\u5b57\u7b26\u9519\u8bef\u7387\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6587\u672c\u4f5c\u4e3a\u6807\u5fd7\u3001\u6807\u7b7e\u6216\u6307\u4ee4\u627f\u8f7d\u91cd\u8981\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4f46\u73b0\u67093D\u8868\u793a\u65b9\u6cd5\u59823DGS\u96be\u4ee5\u4fdd\u7559\u7ec6\u7c92\u5ea6\u6587\u672c\u7ec6\u8282\uff0c\u6587\u672c\u91cd\u5efa\u4e2d\u7684\u5fae\u5c0f\u9519\u8bef\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u8bed\u4e49\u635f\u5931\u3002", "method": "\u63d0\u51faSTRinGS\u6846\u67b6\uff0c\u5c06\u6587\u672c\u548c\u975e\u6587\u672c\u533a\u57df\u5206\u5f00\u5904\u7406\uff1a\u5148\u4f18\u5316\u6587\u672c\u533a\u57df\uff0c\u7136\u540e\u4e0e\u975e\u6587\u672c\u533a\u57df\u5408\u5e76\u8fdb\u884c\u5168\u573a\u666f\u4f18\u5316\uff0c\u786e\u4fdd\u6587\u672c\u533a\u57df\u4fdd\u6301\u6e05\u6670\u53ef\u8bfb\u3002", "result": "STRinGS\u5728\u4ec57K\u6b21\u8fed\u4ee3\u4e0b\uff0c\u76f8\u6bd43DGS\u5728\u6587\u672c\u53ef\u8bfb\u6027\u4e0a\u83b7\u5f9763.6%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u80fd\u591f\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u6587\u672c\u914d\u7f6e\uff0c\u751f\u6210\u6e05\u6670\u53ef\u8bfb\u7684\u6587\u672c\u3002", "conclusion": "STRinGS\u65b9\u6cd5\u548c\u914d\u5957\u6570\u636e\u96c6STRinGS-360\u5171\u540c\u63a8\u52a8\u4e86\u6587\u672c\u4e30\u5bcc\u73af\u5883\u4e2d3D\u573a\u666f\u7406\u89e3\u7684\u8fb9\u754c\uff0c\u4e3a\u66f4\u9c81\u68d2\u7684\u6587\u672c\u611f\u77e5\u91cd\u5efa\u65b9\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.07237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07237", "abs": "https://arxiv.org/abs/2512.07237", "authors": ["Cheng Zhang", "Boying Li", "Meng Wei", "Yan-Pei Cao", "Camilo Cruz Gambardella", "Dinh Phung", "Jianfei Cai"], "title": "Unified Camera Positional Encoding for Controlled Video Generation", "comment": "Code: https://github.com/chengzhag/UCPE", "summary": "Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UCPE\uff08\u7edf\u4e00\u76f8\u673a\u4f4d\u7f6e\u7f16\u7801\uff09\uff0c\u4e00\u79cd\u51e0\u4f55\u4e00\u81f4\u7684\u76f8\u673a\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f8\u5bf9\u5149\u7ebf\u7f16\u7801\u548c\u7edd\u5bf9\u65b9\u5411\u7f16\u7801\uff0c\u5728\u89c6\u9891\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u76f8\u673a\u63a7\u5236\u80fd\u529b\uff0c\u4ec5\u9700\u589e\u52a0\u4e0d\u52301%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "motivation": "\u73b0\u6709\u76f8\u673a\u7f16\u7801\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u7b80\u5316\u7684\u9488\u5b54\u6a21\u578b\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u591a\u6837\u5316\u76f8\u673a\u5185\u53c2\u548c\u955c\u5934\u7578\u53d8\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u76f8\u673a\u8868\u793a\u65b9\u6cd5\u6765\u66f4\u597d\u5730\u652f\u63013D\u611f\u77e5\u3001\u89c6\u9891\u751f\u6210\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u4e2d\u7684\u76f8\u673a\u51e0\u4f55\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u76f8\u5bf9\u5149\u7ebf\u7f16\u7801\uff08Relative Ray Encoding\uff09\u6765\u7edf\u4e00\u8868\u793a\u5b8c\u6574\u7684\u76f8\u673a\u4fe1\u606f\uff086\u81ea\u7531\u5ea6\u4f4d\u59ff\u3001\u5185\u53c2\u548c\u955c\u5934\u7578\u53d8\uff09\uff0c\u5e76\u8bc6\u522b\u51fa\u4fef\u4ef0\u89d2\u548c\u6eda\u8f6c\u89d2\u4f5c\u4e3a\u7edd\u5bf9\u65b9\u5411\u7f16\u7801\u7684\u6709\u6548\u7ec4\u4ef6\u3002\u8fd9\u4e9b\u8bbe\u8ba1\u7ec4\u5408\u6210UCPE\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u6ce8\u610f\u529b\u9002\u914d\u5668\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u6269\u6563Transformer\u4e2d\u3002", "result": "UCPE\u5728\u76f8\u673a\u53ef\u63a7\u7684\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u76f8\u673a\u63a7\u5236\u80fd\u529b\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002\u6784\u5efa\u4e86\u8986\u76d6\u5e7f\u6cdb\u76f8\u673a\u8fd0\u52a8\u548c\u955c\u5934\u7c7b\u578b\u7684\u5927\u578b\u89c6\u9891\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86UCPE\u7684\u6709\u6548\u6027\u3002", "conclusion": "UCPE\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u7684\u76f8\u673a\u8868\u793a\u65b9\u6cd5\uff0c\u5728\u76f8\u673a\u53ef\u63a7\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5177\u6709\u5728\u591a\u89c6\u89d2\u3001\u89c6\u9891\u548c3D\u4efb\u52a1\u4e2d\u4f5c\u4e3aTransformer\u901a\u7528\u76f8\u673a\u8868\u793a\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.07245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07245", "abs": "https://arxiv.org/abs/2512.07245", "authors": ["Toshinori Yamauchi", "Hiroshi Kera", "Kazuhiko Kawamoto"], "title": "Zero-Shot Textual Explanations via Translating Decision-Critical Features", "comment": "11+6 pages, 8 figures, 4 tables", "summary": "Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.", "AI": {"tldr": "TEXTER\u662f\u4e00\u79cd\u65b0\u7684\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u5668\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u9694\u79bb\u51b3\u7b56\u5173\u952e\u7279\u5f81\u5e76\u6620\u5c04\u5230CLIP\u7279\u5f81\u7a7a\u95f4\u6765\u751f\u6210\u66f4\u5fe0\u5b9e\u3001\u53ef\u89e3\u91ca\u7684\u6587\u672c\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u89e3\u91ca\u65b9\u6cd5\u4ec5\u5bf9\u9f50\u5168\u5c40\u56fe\u50cf\u7279\u5f81\u4e0e\u8bed\u8a00\uff0c\u63cf\u8ff0\u53ef\u89c1\u5185\u5bb9\u800c\u975e\u9a71\u52a8\u9884\u6d4b\u7684\u5173\u952e\u56e0\u7d20\u3002\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u751f\u6210\u63cf\u8ff0\uff0c\u4f46\u975e\u9488\u5bf9\u5206\u7c7b\u5668\u7279\u5b9a\u63a8\u7406\u8bbe\u8ba1\u3002", "method": "TEXTER\u9996\u5148\u8bc6\u522b\u5bf9\u9884\u6d4b\u6709\u8d21\u732e\u7684\u795e\u7ecf\u5143\u5e76\u5f3a\u8c03\u8fd9\u4e9b\u795e\u7ecf\u5143\u7f16\u7801\u7684\u51b3\u7b56\u5173\u952e\u7279\u5f81\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u5f3a\u8c03\u7684\u7279\u5f81\u6620\u5c04\u5230CLIP\u7279\u5f81\u7a7a\u95f4\u4ee5\u68c0\u7d22\u53cd\u6620\u6a21\u578b\u63a8\u7406\u7684\u6587\u672c\u89e3\u91ca\u3002\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8fdb\u4e00\u6b65\u63d0\u5347Transformer\u67b6\u6784\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTEXTER\u6bd4\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u66f4\u5fe0\u5b9e\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\u3002", "conclusion": "\u901a\u8fc7\u9694\u79bb\u51b3\u7b56\u5173\u952e\u7279\u5f81\u518d\u8fdb\u884c\u5bf9\u9f50\uff0cTEXTER\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u751f\u6210\u66f4\u51c6\u786e\u53cd\u6620\u5206\u7c7b\u5668\u63a8\u7406\u8fc7\u7a0b\u7684\u6587\u672c\u89e3\u91ca\u3002"}}
{"id": "2512.07247", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07247", "abs": "https://arxiv.org/abs/2512.07247", "authors": ["Ziming Hong", "Tianyu Huang", "Runnan Chen", "Shanshan Ye", "Mingming Gong", "Bo Han", "Tongliang Liu"], "title": "AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing", "comment": "40 pages, 34 figures, 18 tables", "summary": "Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.", "AI": {"tldr": "AdLift\u662f\u9996\u4e2a\u9488\u5bf93D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u7f16\u8f91\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e25\u683c\u6709\u754c\u76842D\u5bf9\u6297\u6270\u52a8\u63d0\u5347\u52303D\u9ad8\u65af\u8868\u793a\u4e2d\uff0c\u9632\u6b62\u4efb\u610f\u89c6\u89d2\u548c\u7ef4\u5ea6\u7684\u6307\u4ee4\u9a71\u52a8\u7f16\u8f91\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u9a71\u52a8\u76843DGS\u7f16\u8f91\u6280\u672f\u867d\u7136\u63a8\u52a8\u4e863D\u5185\u5bb9\u521b\u4f5c\uff0c\u4f46\u4e5f\u4f7f3D\u8d44\u4ea7\u9762\u4e34\u672a\u7ecf\u6388\u6743\u7f16\u8f91\u548c\u6076\u610f\u7be1\u6539\u7684\u98ce\u9669\u3002\u73b0\u6709\u9488\u5bf92D\u56fe\u50cf\u7684\u5bf9\u6297\u6270\u52a8\u4fdd\u62a4\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e3DGS\uff0c\u4e3b\u8981\u9762\u4e34\u89c6\u89d2\u6cdb\u5316\u4fdd\u62a4\u4ee5\u53ca\u4fdd\u62a4\u80fd\u529b\u4e0e\u4e0d\u53ef\u89c1\u6027\u5e73\u8861\u4e24\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faAdLift\u65b9\u6cd5\uff1a1\uff09\u5c06\u4e25\u683c\u6709\u754c\u76842D\u5bf9\u6297\u6270\u52a8\u63d0\u5347\u52303D\u9ad8\u65af\u8868\u793a\u7684\u4fdd\u62a4\u673a\u5236\u4e2d\uff1b2\uff09\u4f7f\u7528\u5b9a\u5236\u7684Lifted PGD\u8fdb\u884c\u6e10\u8fdb\u4f18\u5316\uff0c\u5728\u8bad\u7ec3\u89c6\u89d2\u4e0a\u8fdb\u884c\u68af\u5ea6\u622a\u65ad\u548c\u56fe\u50cf\u5230\u9ad8\u65af\u62df\u5408\u7684\u4ea4\u66ff\u4f18\u5316\uff1b3\uff09\u901a\u8fc7\u68af\u5ea6\u622a\u65ad\u9650\u5236\u7f16\u8f91\u6a21\u578b\u7684\u68af\u5ea6\u4f20\u64ad\uff0c\u5e94\u7528\u6295\u5f71\u68af\u5ea6\u4e25\u683c\u7ea6\u675f\u56fe\u50cf\u7ea7\u6270\u52a8\uff1b4\uff09\u5c06\u6270\u52a8\u901a\u8fc7\u56fe\u50cf\u5230\u9ad8\u65af\u62df\u5408\u64cd\u4f5c\u53cd\u5411\u4f20\u64ad\u5230\u4fdd\u62a4\u9ad8\u65af\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAdLift\u80fd\u6709\u6548\u4fdd\u62a43DGS\u8d44\u4ea7\u514d\u53d7\u6700\u5148\u8fdb\u7684\u6307\u4ee4\u9a71\u52a82D\u56fe\u50cf\u548c3DGS\u7f16\u8f91\u653b\u51fb\uff0c\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u63d0\u4f9b\u4e00\u81f4\u7684\u5bf9\u6297\u6027\u4fdd\u62a4\u6027\u80fd\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u65b0\u89c6\u89d2\u3002", "conclusion": "AdLift\u662f\u9996\u4e2a\u9488\u5bf93DGS\u7684\u7f16\u8f91\u4fdd\u62a4\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89d2\u6cdb\u5316\u4fdd\u62a4\u548c\u4fdd\u62a4\u80fd\u529b-\u4e0d\u53ef\u89c1\u6027\u5e73\u8861\u7684\u6311\u6218\uff0c\u4e3a3DGS\u8d44\u4ea7\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2512.07269", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07269", "abs": "https://arxiv.org/abs/2512.07269", "authors": ["Mike Diessner", "Yannick Tarant"], "title": "A graph generation pipeline for critical infrastructures based on heuristics, images and depth data", "comment": null, "summary": "Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6444\u5f71\u6d4b\u91cf\u7684\u56fe\u751f\u6210\u7ba1\u9053\uff0c\u4f7f\u7528\u7acb\u4f53\u76f8\u673a\u83b7\u53d6RGB\u56fe\u50cf\u548c\u6df1\u5ea6\u6570\u636e\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u548c\u5b9e\u4f8b\u5206\u5272\uff0c\u7ed3\u5408\u542f\u53d1\u5f0f\u89c4\u5219\u63a8\u65ad\u5173\u7cfb\uff0c\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u521b\u5efa\u865a\u62df\u8868\u793a", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6fc0\u5149\u626b\u63cf\u76843D\u70b9\u4e91\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u65b9\u6cd5\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u521b\u5efa\u865a\u62df\u8868\u793a\u4ee5\u8fdb\u884c\u4eff\u771f\u548c\u6570\u5b57\u5b6a\u751f", "method": "\u57fa\u4e8e\u6444\u5f71\u6d4b\u91cf\u7684\u56fe\u751f\u6210\u7ba1\u9053\uff1a\u4f7f\u7528\u7acb\u4f53\u76f8\u673a\u83b7\u53d6RGB\u56fe\u50cf\u548c\u6df1\u5ea6\u6570\u636e\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u548c\u5b9e\u4f8b\u5206\u5272\uff0c\u7ed3\u5408\u7528\u6237\u5b9a\u4e49\u7684\u542f\u53d1\u5f0f\u89c4\u5219\u63a8\u65ad\u5bf9\u8c61\u95f4\u5173\u7cfb", "result": "\u5728\u4e24\u4e2a\u6db2\u538b\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u56fe\u63a5\u8fd1\u771f\u5b9e\u60c5\u51b5\uff0c\u5177\u6709\u7075\u6d3b\u6027\u53ef\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u5b9a\u5236\uff0c\u900f\u660e\u5ea6\u9ad8\u9002\u5408\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u9ad8\u98ce\u9669\u51b3\u7b56", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6444\u5f71\u6d4b\u91cf\u7684\u56fe\u751f\u6210\u7ba1\u9053\u662f\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u521b\u5efa\u51c6\u786e\u7684\u865a\u62df\u8868\u793a\uff0c\u9002\u7528\u4e8e\u6570\u5b57\u5b6a\u751f\u548c\u4eff\u771f\u5e94\u7528"}}
{"id": "2512.07273", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07273", "abs": "https://arxiv.org/abs/2512.07273", "authors": ["Zhi Rao", "Yucheng Zhou", "Benjia Zhou", "Yiqing Huang", "Sergio Escalera", "Jun Wan"], "title": "RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation", "comment": null, "summary": "Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.", "AI": {"tldr": "\u63d0\u51faRVLF\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u9aa8\u67b6\u8fd0\u52a8\u4e0e\u89c6\u89c9\u7279\u5f81\u89e3\u51b3\u624b\u8bed\u8868\u5f81\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u5f15\u5165GRPO\u4f18\u5316\u7b56\u7565\u6539\u5584\u53e5\u5b50\u7ea7\u8bed\u4e49\u5bf9\u9f50\uff0c\u5728\u65e0\u6ce8\u91ca\u624b\u8bed\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65e0\u6ce8\u91ca\u624b\u8bed\u7ffb\u8bd1\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u624b\u8bed\u8868\u5f81\u4e0d\u8db3\uff0c\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u89c6\u89c9\u7ebf\u7d22\uff1b2\uff09\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5b58\u5728\u53e5\u5b50\u7ea7\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u5f3a\u5316\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6RVLF\uff1a1\uff09\u6784\u5efa\u4e13\u95e8\u7684\u624b\u8bed\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u878d\u5408\u9aa8\u67b6\u8fd0\u52a8\u7ebf\u7d22\u548cDINOv2\u63d0\u53d6\u7684\u89c6\u89c9\u7279\u5f81\uff1b2\uff09\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u83b7\u5f97SLT-SFT\u57fa\u7ebf\u6a21\u578b\uff1b3\uff09\u5f15\u5165GRPO\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408BLEU\u548cROUGE\u5956\u52b1\u51fd\u6570\u5fae\u8c03\u6a21\u578b\uff0c\u5f97\u5230SLT-GRPO\u6a21\u578b\u3002", "result": "\u5728CSL-Daily\u3001PHOENIX-2014T\u3001How2Sign\u548cOpenASL\u6570\u636e\u96c6\u4e0a\uff0cBLEU-4\u5206\u6570\u5206\u522b\u63d0\u5347+5.1\u3001+1.11\u3001+1.4\u548c+1.61\uff0c\u65e0\u9700\u5916\u90e8\u5927\u89c4\u6a21\u624b\u8bed\u6570\u636e\u96c6\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u6539\u5584\u7ffb\u8bd1\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "RVLF\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u624b\u8bed\u8868\u5f81\u4e0d\u8db3\u548c\u53e5\u5b50\u7ea7\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\uff0c\u9996\u6b21\u5c06GRPO\u5f15\u5165\u624b\u8bed\u7ffb\u8bd1\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u663e\u8457\u63d0\u5347\u65e0\u6ce8\u91ca\u624b\u8bed\u7ffb\u8bd1\u6027\u80fd\uff0c\u4e3a\u624b\u8bed\u7ffb\u8bd1\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.07276", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07276", "abs": "https://arxiv.org/abs/2512.07276", "authors": ["Mai Tsujimoto", "Junjue Wang", "Weihao Xuan", "Naoto Yokoya"], "title": "Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery", "comment": "Accepted to WACV 2026. Camera-ready-based version with minor edits for readability (no change in the contents)", "summary": "Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.", "AI": {"tldr": "Geo3DVQA\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528RGB\u9065\u611f\u56fe\u50cf\uff0c\u5305\u542b11\u4e07\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u6db5\u76d616\u4e2a\u4efb\u52a1\u7c7b\u522b\u548c3\u4e2a\u590d\u6742\u5ea6\u7ea7\u522b\u3002\u73b0\u6709VLMs\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d3D\u5730\u7406\u7a7a\u95f4\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4e13\u4e1a\u4f20\u611f\u5668\uff08\u5982LiDAR\u548c\u591a\u5149\u8c31\uff09\uff0c\u9650\u5236\u4e86\u5168\u7403\u53ef\u8bbf\u95ee\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6574\u5408\u591a\u4e2a3D\u7ebf\u7d22\u3001\u5904\u7406\u591a\u6837\u5316\u67e5\u8be2\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002\u9700\u8981\u5f00\u53d1\u57fa\u4e8eRGB\u56fe\u50cf\u76843D\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGeo3DVQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528RGB\u9065\u611f\u56fe\u50cf\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u5ea6\u611f\u77e53D\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002\u57fa\u51c6\u5305\u542b11\u4e07\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u6db5\u76d616\u4e2a\u4efb\u52a1\u7c7b\u522b\uff0c\u5206\u4e3a\u4e09\u4e2a\u590d\u6742\u5ea6\u7ea7\u522b\uff1a\u5355\u7279\u5f81\u63a8\u7406\u3001\u591a\u7279\u5f81\u63a8\u7406\u548c\u5e94\u7528\u7ea7\u7a7a\u95f4\u5206\u6790\u3002", "result": "\u8bc4\u4f30\u4e8610\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793aRGB\u52303D\u63a8\u7406\u5177\u6709\u6311\u6218\u6027\uff1aGPT-4o\u51c6\u786e\u738728.6%\uff0cGemini-2.5-Flash\u51c6\u786e\u738733.0%\u3002\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u7684Qwen2.5-VL-7B\u6a21\u578b\u8fbe\u523049.6%\u51c6\u786e\u7387\uff0c\u63d0\u5347\u4e8624.8\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "Geo3DVQA\u63ed\u793a\u4e86\u5f53\u524dVLMs\u57283D\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u9886\u57df\u9002\u5e94\u7684\u6709\u6548\u6027\u3002\u8be5\u57fa\u51c6\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u8bbf\u95ee\u548c\u5168\u9762\u76843D\u5730\u7406\u7a7a\u95f4\u5206\u6790\u8bbe\u7acb\u4e86\u65b0\u7684\u6311\u6218\u524d\u6cbf\u3002"}}
{"id": "2512.07338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07338", "abs": "https://arxiv.org/abs/2512.07338", "authors": ["Lu\u00eds Marnoto", "Alexandre Bernardino", "Bruno Martins"], "title": "Generalized Referring Expression Segmentation on Aerial Photos", "comment": "Submitted to IEEE J-STARS", "summary": "Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .", "AI": {"tldr": "Aerial-D\u662f\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u822a\u7a7a\u56fe\u50cf\u6307\u4ee3\u8868\u8fbe\u5206\u5272\u6570\u636e\u96c6\uff0c\u5305\u542b37,288\u5f20\u56fe\u50cf\u548c1,522,523\u4e2a\u6307\u4ee3\u8868\u8fbe\uff0c\u6db5\u76d6259,709\u4e2a\u6807\u6ce8\u76ee\u6807\uff0c\u8986\u76d621\u4e2a\u7c7b\u522b\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u5168\u81ea\u52a8\u6d41\u6c34\u7ebf\u6784\u5efa\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u8868\u8fbe\u751f\u6210\u548cLLM\u589e\u5f3a\uff0c\u5e76\u6a21\u62df\u4e86\u5386\u53f2\u6210\u50cf\u6761\u4ef6\u3002\u4f7f\u7528RSRefSeg\u67b6\u6784\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u5f53\u4ee3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728\u5386\u53f2\u822a\u7a7a\u7167\u7247\u7684\u9000\u5316\u6761\u4ef6\u4e0b\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002", "motivation": "\u822a\u7a7a\u56fe\u50cf\uff08\u5982\u65e0\u4eba\u673a\u62cd\u6444\u7684\u73b0\u4ee3\u822a\u7a7a\u7167\u7247\u3001\u5386\u53f2\u822a\u7a7a\u6863\u6848\u3001\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u7b49\uff09\u5728\u6307\u4ee3\u8868\u8fbe\u5206\u5272\u4efb\u52a1\u4e2d\u9762\u4e34\u72ec\u7279\u6311\u6218\uff1a\u7a7a\u95f4\u5206\u8fa8\u7387\u5dee\u5f02\u5927\u3001\u8272\u5f69\u4f7f\u7528\u4e0d\u4e00\u81f4\u3001\u76ee\u6807\u53ef\u80fd\u53ea\u6709\u51e0\u4e2a\u50cf\u7d20\u3001\u573a\u666f\u4e2d\u7269\u4f53\u5bc6\u5ea6\u9ad8\u4e14\u5b58\u5728\u90e8\u5206\u906e\u6321\u3002\u73b0\u6709\u6570\u636e\u96c6\u65e0\u6cd5\u5145\u5206\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u822a\u7a7a\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u6307\u4ee3\u8868\u8fbe\u5206\u5272\u6570\u636e\u96c6\u3002", "method": "1. \u6784\u5efaAerial-D\u6570\u636e\u96c6\uff1a\u5305\u542b37,288\u5f20\u56fe\u50cf\u30011,522,523\u4e2a\u6307\u4ee3\u8868\u8fbe\u3001259,709\u4e2a\u6807\u6ce8\u76ee\u6807\uff0c\u8986\u76d621\u4e2a\u7c7b\u522b\uff08\u4ece\u8f66\u8f86\u3001\u57fa\u7840\u8bbe\u65bd\u5230\u571f\u5730\u8986\u76d6\u7c7b\u578b\uff09\u30022. \u91c7\u7528\u5168\u81ea\u52a8\u6d41\u6c34\u7ebf\uff1a\u7ed3\u5408\u7cfb\u7edf\u6027\u57fa\u4e8e\u89c4\u5219\u7684\u8868\u8fbe\u751f\u6210\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u589e\u5f3a\u7a0b\u5e8f\uff0c\u4e30\u5bcc\u8bed\u8a00\u591a\u6837\u6027\u548c\u89c6\u89c9\u7ec6\u8282\u5173\u6ce8\u30023. \u4f7f\u7528\u8fc7\u6ee4\u5668\u6a21\u62df\u5386\u53f2\u6210\u50cf\u6761\u4ef6\u30024. \u91c7\u7528RSRefSeg\u67b6\u6784\uff0c\u5728Aerial-D\u548c\u5148\u524d\u822a\u7a7a\u6570\u636e\u96c6\u4e0a\u8054\u5408\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9e\u73b0\u6587\u672c\u9a71\u52a8\u7684\u7edf\u4e00\u5b9e\u4f8b\u548c\u8bed\u4e49\u5206\u5272\u3002", "result": "\u8054\u5408\u8bad\u7ec3\u5728\u5f53\u4ee3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5355\u8272\u3001\u68d5\u8910\u8272\u548c\u9897\u7c92\u72b6\u9000\u5316\u6761\u4ef6\u4e0b\uff08\u8fd9\u4e9b\u662f\u6863\u6848\u822a\u7a7a\u6444\u5f71\u4e2d\u5e38\u89c1\u7684\u9000\u5316\uff09\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u51c6\u786e\u6027\u3002\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u6a21\u578b\u548c\u5b8c\u6574\u8f6f\u4ef6\u6d41\u6c34\u7ebf\u5df2\u516c\u5f00\u63d0\u4f9b\u3002", "conclusion": "Aerial-D\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u822a\u7a7a\u56fe\u50cf\u6307\u4ee3\u8868\u8fbe\u5206\u5272\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5168\u81ea\u52a8\u6d41\u6c34\u7ebf\u6784\u5efa\u5e76\u5305\u542bLLM\u589e\u5f3a\u3002\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u4e0d\u4ec5\u5728\u5f53\u4ee3\u822a\u7a7a\u56fe\u50cf\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u8fd8\u80fd\u6709\u6548\u5904\u7406\u5386\u53f2\u822a\u7a7a\u7167\u7247\u7684\u5404\u79cd\u9000\u5316\u6761\u4ef6\uff0c\u4e3a\u822a\u7a7a\u56fe\u50cf\u7406\u89e3\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2512.07345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07345", "abs": "https://arxiv.org/abs/2512.07345", "authors": ["Shilong Jin", "Haoran Duan", "Litao Hua", "Wentao Huang", "Yuan Zhou"], "title": "Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting", "comment": "15 pages, 8 figures, 5 tables, 2 algorithms, Accepted by AAAI 2026", "summary": "Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.", "AI": {"tldr": "TD-Attn\u662f\u4e00\u4e2a\u89e3\u51b3T2I\u6269\u6563\u6a21\u578b\u4e2d\u5148\u9a8c\u89c6\u89d2\u504f\u7f6e\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc73D\u611f\u77e5\u6ce8\u610f\u529b\u5f15\u5bfc\u548c\u5206\u5c42\u6ce8\u610f\u529b\u8c03\u5236\u6765\u63d0\u5347\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u53ef\u4f5c\u4e3a\u901a\u7528\u63d2\u4ef6\u589e\u5f3a3D\u4efb\u52a1\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u4ece\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u84b8\u998f\u76843D\u4efb\u52a1\u5b58\u5728\u5148\u9a8c\u89c6\u89d2\u504f\u7f6e\u95ee\u9898\uff0c\u5bfc\u81f4\u4e0d\u540c\u89c6\u89d2\u95f4\u5916\u89c2\u51b2\u7a81\uff0c\u8fd9\u662f\u56e0\u4e3a\u4e3b\u9898\u8bcd\u5728\u4ea4\u53c9\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u4f18\u5148\u6fc0\u6d3b\u5148\u9a8c\u89c6\u89d2\u7279\u5f81\uff0c\u800c\u5ffd\u7565\u76ee\u6807\u89c6\u89d2\u6761\u4ef6\u3002", "method": "\u63d0\u51faTD-Attn\u6846\u67b6\uff1a1) 3D-AAG\u6a21\u5757\u6784\u5efa\u89c6\u89d2\u4e00\u81f4\u76843D\u6ce8\u610f\u529b\u9ad8\u65af\u5206\u5e03\uff0c\u5f3a\u5236\u7a7a\u95f4\u4e00\u81f4\u6027\uff1b2) HAM\u6a21\u5757\u4f7f\u7528\u8bed\u4e49\u5f15\u5bfc\u6811\u548c\u8bed\u4e49\u54cd\u5e94\u5206\u6790\u5668\u5b9a\u4f4d\u548c\u8c03\u5236\u5bf9\u89c6\u89d2\u6761\u4ef6\u654f\u611f\u7684CA\u5c42\uff0c\u652f\u6301\u6784\u5efa\u66f4\u4e00\u81f4\u76843D\u6ce8\u610f\u529b\u9ad8\u65af\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTD-Attn\u53ef\u4f5c\u4e3a\u901a\u7528\u63d2\u4ef6\uff0c\u663e\u8457\u63d0\u53473D\u4efb\u52a1\u4e2d\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5e76\u652f\u6301\u53ef\u63a7\u7cbe\u786e\u76843D\u7f16\u8f91\u3002", "conclusion": "TD-Attn\u901a\u8fc7\u6570\u5b66\u5206\u6790\u63ed\u793a\u4e86T2I\u6a21\u578b\u5148\u9a8c\u89c6\u89d2\u504f\u7f6e\u7684\u6839\u6e90\uff0c\u5e76\u63d0\u51fa\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u4e0d\u4f9d\u8d56\u5927\u91cf3D\u8bad\u7ec3\u6570\u636e\u76843D\u4efb\u52a1\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u7a81\u7834\u3002"}}
{"id": "2512.07348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07348", "abs": "https://arxiv.org/abs/2512.07348", "authors": ["Xinyu Wei", "Kangrui Cen", "Hongyang Wei", "Zhen Guo", "Bairui Li", "Zeqing Wang", "Jinrui Zhang", "Lei Zhang"], "title": "MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition", "comment": "Project Page: https://MICo-150K.github.io/", "summary": "In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u591a\u56fe\u50cf\u7ec4\u5408(MICo)\u4efb\u52a1\uff0c\u6784\u5efa\u4e86MICo-150K\u6570\u636e\u96c6\u3001MICo-Bench\u57fa\u51c6\u6d4b\u8bd5\u548cQwen-MICo\u57fa\u7ebf\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u591a\u53c2\u8003\u56fe\u50cf\u751f\u6210\u4e2d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u56fe\u50cf\u7ec4\u5408\u4efb\u52a1\u5728\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u969c\u788d\u662f\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u4e2a\u53c2\u8003\u8f93\u5165\u65f6\u96be\u4ee5\u4fdd\u6301\u56fe\u50cf\u7684\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\u3002", "method": "1) \u7cfb\u7edf\u7814\u7a76MICo\u4efb\u52a1\uff0c\u5206\u4e3a7\u7c7b\u4ee3\u8868\u6027\u4efb\u52a1\uff1b2) \u6536\u96c6\u9ad8\u8d28\u91cf\u6e90\u56fe\u50cf\u5e76\u6784\u5efa\u591a\u6837\u5316MICo\u63d0\u793a\uff1b3) \u5229\u7528\u4e13\u6709\u6a21\u578b\u5408\u6210\u5927\u91cf\u5e73\u8861\u7684\u590d\u5408\u56fe\u50cf\uff1b4) \u4eba\u5de5\u7b5b\u9009\u548c\u7cbe\u70bc\uff0c\u6784\u5efaMICo-150K\u6570\u636e\u96c6\uff1b5) \u521b\u5efa\u5206\u89e3\u4e0e\u91cd\u7ec4\u5b50\u96c6\uff1b6) \u6784\u5efaMICo-Bench\u57fa\u51c6\u6d4b\u8bd5\uff1b7) \u63d0\u51faWeighted-Ref-VIEScore\u8bc4\u4f30\u6307\u6807\uff1b8) \u5728MICo-150K\u4e0a\u5fae\u8c03\u591a\u4e2a\u6a21\u578b\u3002", "result": "MICo-150K\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u591a\u56fe\u50cf\u7ec4\u5408\u80fd\u529b\uff0c\u57fa\u7ebf\u6a21\u578bQwen-MICo\u57283\u56fe\u50cf\u7ec4\u5408\u4efb\u52a1\u4e2d\u8fbe\u5230Qwen-Image-2509\u6c34\u5e73\uff0c\u540c\u65f6\u652f\u6301\u4efb\u610f\u591a\u56fe\u50cf\u8f93\u5165\u3002\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u57fa\u7ebf\u6a21\u578b\u4e3aMICo\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u57fa\u51c6\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u4e3a\u591a\u56fe\u50cf\u7ec4\u5408\u4efb\u52a1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5904\u7406\u591a\u53c2\u8003\u8f93\u5165\u7684\u80fd\u529b\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.07381", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07381", "abs": "https://arxiv.org/abs/2512.07381", "authors": ["Shuohan Tao", "Boyao Zhou", "Hanzhang Tu", "Yuwang Wang", "Yebin Liu"], "title": "Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects", "comment": null, "summary": "3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.", "AI": {"tldr": "Tessellation GS\uff1a\u4e00\u79cd\u57fa\u4e8e\u7f51\u683c\u9762\u7684\u7ed3\u6784\u53162D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5355\u76f8\u673a\u91cd\u5efa\u52a8\u6001\u573a\u666f\uff0c\u901a\u8fc7\u7ea6\u675f2D\u9ad8\u65af\u5230\u5c40\u90e8\u533a\u57df\u5e76\u4f7f\u7528\u5c42\u6b21\u795e\u7ecf\u7279\u5f81\u63a8\u65ad\u5c5e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u89d2\u548c\u52a8\u6001\u573a\u666f\u7684\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf3D\u9ad8\u65af\u6cfc\u6e85\uff08GS\uff09\u65b9\u6cd5\u5728\u89c6\u89d2\u5916\u63a8\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u89c6\u89d2\u548c\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u96be\u4ee5\u4ece\u5355\u9759\u6001\u76f8\u673a\u91cd\u5efa\u4e00\u822c\u52a8\u6001\u7269\u4f53\u3002", "method": "\u63d0\u51faTessellation GS\u65b9\u6cd5\uff1a1\uff09\u5c062D\u9ad8\u65af\u7ea6\u675f\u5728\u7f51\u683c\u9762\u7684\u5c40\u90e8\u533a\u57df\uff1b2\uff09\u901a\u8fc7\u7f51\u683c\u9762\u4e0a\u7684\u5c42\u6b21\u795e\u7ecf\u7279\u5f81\u63a8\u65ad\u9ad8\u65af\u5c5e\u6027\uff1b3\uff09\u4f7f\u7528\u7ec6\u8282\u611f\u77e5\u635f\u5931\u51fd\u6570\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u9762\u7ec6\u5206\u7b56\u7565\u6307\u5bfc\u9ad8\u65af\u7ec6\u5206\uff1b4\uff09\u5229\u7528\u91cd\u5efa\u57fa\u7840\u6a21\u578b\u7684\u5148\u9a8c\u521d\u59cb\u5316\u9ad8\u65af\u53d8\u5f62\u3002", "result": "\u5728\u8868\u89c2\u548c\u7f51\u683c\u91cd\u5efa\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u4e4b\u524dSOTA\u65b9\u6cd5\uff0cLPIPS\u964d\u4f4e29.1%\uff0cChamfer\u8ddd\u79bb\u51cf\u5c1149.2%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "Tessellation GS\u901a\u8fc7\u7ed3\u6784\u5316\u7ea6\u675f\u548c\u81ea\u9002\u5e94\u7ec6\u5206\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfGS\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4ece\u5355\u76f8\u673a\u5bf9\u4e00\u822c\u52a8\u6001\u7269\u4f53\u7684\u9c81\u68d2\u91cd\u5efa\u3002"}}
{"id": "2512.07383", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07383", "abs": "https://arxiv.org/abs/2512.07383", "authors": ["Deepika SN Vemuri", "Gautham Bellamkonda", "Aditya Pola", "Vineeth N Balasubramanian"], "title": "LogicCBMs: Logic-Enhanced Concept-Based Learning", "comment": "18 pages, 19 figures, WACV 2026", "summary": "Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLogicCBM\uff0c\u901a\u8fc7\u53ef\u5fae\u903b\u8f91\u8fd0\u7b97\u589e\u5f3a\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff0c\u8d85\u8d8a\u7b80\u5355\u7684\u7ebf\u6027\u6982\u5ff5\u7ec4\u5408\uff0c\u63d0\u9ad8\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u548c\u51c6\u786e\u6027", "motivation": "\u4f20\u7edf\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u867d\u7136\u63d0\u4f9b\u8bed\u4e49\u62bd\u8c61\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u4ec5\u901a\u8fc7\u7ebf\u6027\u7ec4\u5408\u6982\u5ff5\u6765\u751f\u6210\u9884\u6d4b\uff0c\u8fd9\u79cd\u7ebf\u6027\u7ec4\u5408\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u548c\u5bf9\u6982\u5ff5\u95f4\u590d\u6742\u5173\u7cfb\u7684\u6355\u6349", "method": "\u63d0\u51faLogicCBM\uff0c\u5f15\u5165\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u903b\u8f91\u6a21\u5757\uff0c\u901a\u8fc7\u53ef\u5fae\u903b\u8f91\u8fd0\u7b97\u8fde\u63a5\u4eceCBMs\u5b66\u4e60\u5230\u7684\u6982\u5ff5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8d85\u8d8a\u7b80\u5355\u7684\u52a0\u6743\u6982\u5ff5\u7ec4\u5408\uff0c\u5229\u7528\u5404\u79cd\u903b\u8f91\u8fd0\u7b97\u751f\u6210\u6700\u7ec8\u9884\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u7aef\u5230\u7aef\u53ef\u5b66\u4e60\u6027", "result": "\u5728\u77e5\u540d\u57fa\u51c6\u6d4b\u8bd5\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cLogicCBM\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u6267\u884c\u6709\u6548\u7684\u5e72\u9884\uff0c\u5e76\u4e14\u4fdd\u6301\u9ad8\u5ea6\u53ef\u89e3\u91ca\u6027", "conclusion": "\u901a\u8fc7\u547d\u9898\u903b\u8f91\u589e\u5f3a\u6982\u5ff5\u5b66\u4e60\u6a21\u578b\u662f\u6709\u6548\u7684\uff0cLogicCBM\u4e0d\u4ec5\u80fd\u591f\u6355\u6349\u6982\u5ff5\u95f4\u5173\u7cfb\uff0c\u8fd8\u80fd\u63d0\u9ad8\u6a21\u578b\u5728\u903b\u8f91\u8fd0\u7b97\u65b9\u9762\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u6269\u5c55"}}
{"id": "2512.07385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07385", "abs": "https://arxiv.org/abs/2512.07385", "authors": ["Chunhui Zhang", "Li Liu", "Zhipeng Zhang", "Yong Wang", "Hao Wen", "Xi Zhou", "Shiming Ge", "Yanfeng Wang"], "title": "How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline", "comment": "https://github.com/983632847/Awesome-Multimodal-Object-Tracking", "summary": "Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684\u65e0\u4eba\u673a\u53cd\u65e0\u4eba\u673a\u8ddf\u8e2a\u4efb\u52a1UAV-Anti-UAV\uff0c\u6784\u5efa\u767e\u4e07\u7ea7\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eMamba\u7684MambaSTS\u65b9\u6cd5\u8fdb\u884c\u7a7a\u95f4-\u65f6\u95f4-\u8bed\u4e49\u96c6\u6210\u5b66\u4e60", "motivation": "\u5f53\u524d\u53cd\u65e0\u4eba\u673a\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u56fa\u5b9a\u5730\u9762\u6444\u50cf\u5934\u91c7\u96c6\u7684RGB\u3001\u7ea2\u5916\u6216RGB-IR\u89c6\u9891\uff0c\u7f3a\u4e4f\u4ece\u79fb\u52a8\u65e0\u4eba\u673a\u5e73\u53f0\u8ddf\u8e2a\u76ee\u6807\u65e0\u4eba\u673a\u7684\u7814\u7a76\u3002\u65e0\u4eba\u673a\u53cd\u65e0\u4eba\u673a\u8ddf\u8e2a\u9762\u4e34\u53cc\u91cd\u52a8\u6001\u5e72\u6270\u7684\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMambaSTS\u65b9\u6cd5\uff1a\u4f7f\u7528Mamba\u6a21\u578b\u5b66\u4e60\u5168\u5c40\u8bed\u4e49\u7279\u5f81\uff0cTransformer\u5b66\u4e60\u7a7a\u95f4\u7279\u5f81\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u65f6\u95f4\u4ee4\u724c\u4f20\u64ad\u673a\u5236\u5efa\u7acb\u89c6\u9891\u7ea7\u957f\u671f\u4e0a\u4e0b\u6587\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b1,810\u4e2a\u89c6\u9891\u7684\u767e\u4e07\u7ea7\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u89c6\u9891\u90fd\u6709\u4eba\u5de5\u6807\u6ce8\u7684\u8fb9\u754c\u6846\u3001\u8bed\u8a00\u63d0\u793a\u548c15\u4e2a\u8ddf\u8e2a\u5c5e\u6027\u3002\u572850\u79cd\u73b0\u4ee3\u6df1\u5ea6\u8ddf\u8e2a\u7b97\u6cd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUAV-Anti-UAV\u9886\u57df\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u63d0\u51fa\u4e86\u65b0\u7684\u65e0\u4eba\u673a\u53cd\u65e0\u4eba\u673a\u8ddf\u8e2a\u4efb\u52a1\u548c\u6570\u636e\u96c6\uff0c\u5c55\u793a\u4e86\u8be5\u4efb\u52a1\u7684\u6311\u6218\u6027\uff0c\u63d0\u51fa\u7684MambaSTS\u65b9\u6cd5\u4e3a\u8fd9\u4e00\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u57fa\u7ebf\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.07394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07394", "abs": "https://arxiv.org/abs/2512.07394", "authors": ["Zhifan Zhu", "Siddhant Bansal", "Shashank Tripathi", "Dima Damen"], "title": "Reconstructing Objects along Hand Interaction Timelines in Egocentric Video", "comment": "webpage: https://zhifanzhu.github.io/objects-along-hit", "summary": "We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faROHIT\u4efb\u52a1\uff0c\u901a\u8fc7\u624b\u4ea4\u4e92\u65f6\u95f4\u7ebf(HIT)\u91cd\u5efa\u7269\u4f53\u59ff\u6001\uff0c\u5229\u7528\u7ea6\u675f\u4f18\u5316\u4f20\u64ad(COP)\u6846\u67b6\u5728\u7a33\u5b9a\u6293\u63e1\u573a\u666f\u4e2d\u63d0\u5347\u91cd\u5efa\u6548\u679c", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u52a8\u6001\u624b\u7269\u4ea4\u4e92\u573a\u666f\u4e2d\u51c6\u786e\u91cd\u5efa\u7269\u4f53\u59ff\u6001\uff0c\u7279\u522b\u662f\u5728\u624b\u7a33\u5b9a\u6293\u63e1\u7269\u4f53\u65f6\uff0c\u7269\u4f53\u76f8\u5bf9\u4e8e\u624b\u7684\u59ff\u6001\u4fdd\u6301\u6052\u5b9a\uff0c\u8fd9\u4e3a\u65e03D\u771f\u503c\u6807\u6ce8\u63d0\u4f9b\u4e86\u673a\u4f1a", "method": "\u5b9a\u4e49\u624b\u4ea4\u4e92\u65f6\u95f4\u7ebf(HIT)\u63cf\u8ff0\u7269\u4f53\u4ece\u9759\u6b62\u5230\u88ab\u6293\u63e1\u3001\u4f7f\u7528\u3001\u91ca\u653e\u7684\u8fc7\u7a0b\uff1b\u63d0\u51fa\u7ea6\u675f\u4f18\u5316\u4f20\u64ad(COP)\u6846\u67b6\uff0c\u5229\u7528\u7a33\u5b9a\u6293\u63e1\u671f\u95f4\u7684\u59ff\u6001\u7ea6\u675f\u5728\u65f6\u95f4\u7ebf\u4e0a\u4f20\u64ad\u7269\u4f53\u59ff\u6001", "result": "\u5728HOT3D\u548cEPIC-Kitchens\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cCOP\u6846\u67b6\u5c06\u7a33\u5b9a\u6293\u63e1\u91cd\u5efa\u63d0\u53476.2-11.3%\uff0cHIT\u91cd\u5efa\u63d0\u5347\u8fbe24.5%\uff0c\u4ec5\u4f7f\u75282D\u6295\u5f71\u8bef\u5dee\u8bc4\u4f30\u800c\u65e03D\u771f\u503c", "conclusion": "ROHIT\u4efb\u52a1\u548cCOP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u624b\u7269\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u7269\u4f53\u91cd\u5efa\u95ee\u9898\uff0c\u5229\u7528\u7a33\u5b9a\u6293\u63e1\u7684\u7ea6\u675f\u5b9e\u73b0\u4e86\u65e03D\u771f\u503c\u7684\u59ff\u6001\u4f20\u64ad\u548c\u4f18\u5316"}}
{"id": "2512.07410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07410", "abs": "https://arxiv.org/abs/2512.07410", "authors": ["Bin Li", "Ruichi Zhang", "Han Liang", "Jingyan Zhang", "Juze Zhang", "Xin Chen", "Lan Xu", "Jingyi Yu", "Jingya Wang"], "title": "InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs", "comment": "Project page: https://binlee26.github.io/InterAgent-Page", "summary": "Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.", "AI": {"tldr": "InterAgent\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u7684\u6587\u672c\u9a71\u52a8\u7269\u7406\u591a\u667a\u80fd\u4f53\u4eba\u5f62\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u6269\u6563\u53d8\u6362\u5668\u548c\u591a\u6d41\u5757\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u7269\u7406\u5408\u7406\u3001\u8bed\u4e49\u51c6\u786e\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u667a\u80fd\u4f53\u573a\u666f\uff0c\u5ffd\u89c6\u4e86\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u5fc5\u9700\u7684\u7269\u7406\u5408\u7406\u4e92\u52a8\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u590d\u6742\u793e\u4ea4\u534f\u8c03\u884c\u4e3a\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u81ea\u56de\u5f52\u6269\u6563\u53d8\u6362\u5668\uff0c\u91c7\u7528\u591a\u6d41\u5757\u8bbe\u8ba1\u5206\u79bb\u672c\u4f53\u611f\u77e5\u3001\u5916\u90e8\u611f\u77e5\u548c\u52a8\u4f5c\u4ee5\u907f\u514d\u8de8\u6a21\u6001\u5e72\u6270\uff1b\u5f15\u5165\u4ea4\u4e92\u56fe\u5916\u90e8\u611f\u77e5\u8868\u793a\u6355\u6349\u7ec6\u7c92\u5ea6\u5173\u8282\u95f4\u7a7a\u95f4\u4f9d\u8d56\uff1b\u8bbe\u8ba1\u7a00\u758f\u8fb9\u57fa\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u526a\u679d\u5197\u4f59\u8fde\u63a5\u5e76\u5f3a\u8c03\u5173\u952e\u667a\u80fd\u4f53\u95f4\u7a7a\u95f4\u5173\u7cfb\u3002", "result": "InterAgent\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u80fd\u591f\u4ec5\u4ece\u6587\u672c\u63d0\u793a\u751f\u6210\u8fde\u8d2f\u3001\u7269\u7406\u5408\u7406\u4e14\u8bed\u4e49\u51c6\u786e\u7684\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u3002", "conclusion": "InterAgent\u6210\u529f\u586b\u8865\u4e86\u591a\u667a\u80fd\u4f53\u4eba\u5f62\u63a7\u5236\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u6587\u672c\u9a71\u52a8\u7684\u7269\u7406\u5408\u7406\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u63d0\u4f9b\u4e86\u9996\u4e2a\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2512.07469", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07469", "abs": "https://arxiv.org/abs/2512.07469", "authors": ["Xiangpeng Yang", "Ji Xie", "Yiyuan Yang", "Yan Huang", "Min Xu", "Qiang Wu"], "title": "Unified Video Editing with Temporal Reasoner", "comment": "Project Page: https://videocof.github.io/", "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "AI": {"tldr": "VideoCoF\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e27\u94fe\u63a8\u7406\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u7f16\u8f91\u533a\u57df\u6f5c\u5728\u8868\u793a\u4f5c\u4e3a\u663e\u5f0f\u63a8\u7406\u6b65\u9aa4\uff0c\u5b9e\u73b0\u65e0\u9700\u63a9\u7801\u7684\u7cbe\u786e\u6307\u4ee4\u5230\u533a\u57df\u5bf9\u9f50\u548c\u7ec6\u7c92\u5ea6\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u9762\u4e34\u5173\u952e\u6743\u8861\uff1a\u4e13\u5bb6\u6a21\u578b\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u5148\u9a8c\uff08\u5982\u63a9\u7801\uff09\u96be\u4ee5\u7edf\u4e00\uff1b\u7edf\u4e00\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\u5b66\u4e60\u6a21\u578b\u65e0\u9700\u63a9\u7801\u4f46\u7f3a\u4e4f\u663e\u5f0f\u7a7a\u95f4\u7ebf\u7d22\uff0c\u5bfc\u81f4\u6307\u4ee4\u5230\u533a\u57df\u6620\u5c04\u4e0d\u7cbe\u786e\u548c\u5b9a\u4f4d\u80fd\u529b\u5f31\u3002", "method": "\u63d0\u51faVideoCoF\u6846\u67b6\uff0c\u91c7\u7528\"\u89c2\u5bdf-\u63a8\u7406-\u7f16\u8f91\"\u6d41\u7a0b\uff0c\u5f3a\u5236\u89c6\u9891\u6269\u6563\u6a21\u578b\u5148\u9884\u6d4b\u63a8\u7406\u6807\u8bb0\uff08\u7f16\u8f91\u533a\u57df\u6f5c\u5728\u8868\u793a\uff09\uff0c\u518d\u751f\u6210\u76ee\u6807\u89c6\u9891\u6807\u8bb0\u3002\u5f15\u5165RoPE\u5bf9\u9f50\u7b56\u7565\u5229\u7528\u63a8\u7406\u6807\u8bb0\u786e\u4fdd\u8fd0\u52a8\u5bf9\u9f50\u5e76\u652f\u6301\u8d85\u51fa\u8bad\u7ec3\u65f6\u957f\u7684\u957f\u5ea6\u5916\u63a8\u3002", "result": "\u4ec5\u4f7f\u75285\u4e07\u4e2a\u89c6\u9891\u5bf9\u7684\u6700\u5c0f\u6570\u636e\u6210\u672c\uff0c\u5728VideoCoF-Bench\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "conclusion": "VideoCoF\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u6b65\u9aa4\u89e3\u51b3\u4e86\u89c6\u9891\u7f16\u8f91\u4e2d\u7cbe\u786e\u6027\u4e0e\u7edf\u4e00\u6027\u7684\u51b2\u7a81\uff0c\u65e0\u9700\u7528\u6237\u63d0\u4f9b\u63a9\u7801\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u7684\u6307\u4ee4\u5230\u533a\u57df\u5bf9\u9f50\u548c\u7ec6\u7c92\u5ea6\u7f16\u8f91\uff0c\u540c\u65f6\u652f\u6301\u8fd0\u52a8\u5bf9\u9f50\u548c\u957f\u5ea6\u5916\u63a8\u3002"}}
{"id": "2512.07480", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07480", "abs": "https://arxiv.org/abs/2512.07480", "authors": ["Naifu Xue", "Zhaoyang Jia", "Jiahao Li", "Bin Li", "Zihan Zheng", "Yuan Zhang", "Yan Lu"], "title": "Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance", "comment": null, "summary": "While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.", "AI": {"tldr": "S2VC\u662f\u4e00\u79cd\u57fa\u4e8e\u5355\u6b65\u6269\u6563\u7684\u89c6\u9891\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u6761\u4ef6\u7f16\u7801\u6846\u67b6\u548c\u9ad8\u6548\u5355\u6b65\u6269\u6563\u751f\u6210\u5668\uff0c\u5728\u4f4e\u7801\u7387\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\uff0c\u540c\u65f6\u964d\u4f4e\u91c7\u6837\u590d\u6742\u5ea6\u3002", "motivation": "\u4f20\u7edf\u548c\u795e\u7ecf\u89c6\u9891\u7f16\u89e3\u7801\u5668\u5728\u4f4e\u7801\u7387\u4e0b\u63d0\u5347\u611f\u77e5\u8d28\u91cf\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u53d7\u9650\u4e8e\u751f\u6210\u80fd\u529b\u5bfc\u81f4\u4f2a\u5f71\uff0c\u8981\u4e48\u4f9d\u8d56\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4f46\u91c7\u6837\u590d\u6742\u5ea6\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u9ad8\u611f\u77e5\u8d28\u91cf\u53c8\u80fd\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faS2VC\u5355\u6b65\u6269\u6563\u89c6\u9891\u7f16\u89e3\u7801\u5668\uff1a1) \u7ed3\u5408\u6761\u4ef6\u7f16\u7801\u6846\u67b6\u4e0e\u9ad8\u6548\u5355\u6b65\u6269\u6563\u751f\u6210\u5668\uff1b2) \u5f15\u5165\u4e0a\u4e0b\u6587\u8bed\u4e49\u6307\u5bfc\uff0c\u4ece\u7f13\u51b2\u7279\u5f81\u4e2d\u63d0\u53d6\u5e27\u81ea\u9002\u5e94\u8bed\u4e49\uff0c\u66ff\u4ee3\u6587\u672c\u63cf\u8ff0\uff1b3) \u5728\u6269\u6563U-Net\u4e2d\u52a0\u5165\u65f6\u95f4\u4e00\u81f4\u6027\u6307\u5bfc\uff0c\u786e\u4fdd\u5e27\u95f4\u65f6\u5e8f\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eS2VC\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u611f\u77e5\u65b9\u6cd5\u5e73\u5747\u8282\u770152.73%\u7684\u7801\u7387\uff0c\u8bc1\u660e\u4e86\u5355\u6b65\u6269\u6563\u5728\u9ad8\u6548\u9ad8\u8d28\u91cf\u89c6\u9891\u538b\u7f29\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "S2VC\u901a\u8fc7\u5355\u6b65\u6269\u6563\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u4f4e\u7801\u7387\u89c6\u9891\u538b\u7f29\u4e2d\u611f\u77e5\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u9ad8\u8d28\u91cf\u89c6\u9891\u538b\u7f29\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07500", "abs": "https://arxiv.org/abs/2512.07500", "authors": ["Penghui Liu", "Jiangshan Wang", "Yutong Shen", "Shanhui Mo", "Chenyang Qi", "Yue Ma"], "title": "MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer", "comment": null, "summary": "Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.", "AI": {"tldr": "MultiMotion\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u5bf9\u8c61\u89c6\u9891\u8fd0\u52a8\u8f6c\u79fb\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7Mask-aware Attention Motion Flow\u548cRectPC\u6c42\u89e3\u5668\u89e3\u51b3DiT\u67b6\u6784\u4e2d\u7684\u8fd0\u52a8\u7ea0\u7f20\u95ee\u9898\uff0c\u5e76\u5728\u9996\u4e2aDiT\u591a\u5bf9\u8c61\u8fd0\u52a8\u8f6c\u79fb\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u67b6\u6784\u5728\u591a\u5bf9\u8c61\u89c6\u9891\u8fd0\u52a8\u8f6c\u79fb\u4e2d\u9762\u4e34\u8fd0\u52a8\u7ea0\u7f20\u548c\u7f3a\u4e4f\u5bf9\u8c61\u7ea7\u63a7\u5236\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u660e\u786e\u89e3\u8026\u548c\u63a7\u5236\u591a\u4e2a\u5bf9\u8c61\u8fd0\u52a8\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86MultiMotion\u6846\u67b6\uff0c\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a1) Mask-aware Attention Motion Flow (AMF)\uff0c\u5229\u7528SAM2\u63a9\u7801\u5728DiT\u6d41\u7a0b\u4e2d\u663e\u5f0f\u89e3\u8026\u548c\u63a7\u5236\u591a\u4e2a\u5bf9\u8c61\u7684\u8fd0\u52a8\u7279\u5f81\uff1b2) RectPC\uff0c\u4e00\u79cd\u9ad8\u9636\u9884\u6d4b\u5668-\u6821\u6b63\u5668\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u91c7\u6837\uff0c\u7279\u522b\u9002\u7528\u4e8e\u591a\u5b9e\u4f53\u751f\u6210\u3002", "result": "\u6784\u5efa\u4e86\u9996\u4e2a\u4e13\u95e8\u7528\u4e8eDiT\u591a\u5bf9\u8c61\u8fd0\u52a8\u8f6c\u79fb\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0cMultiMotion\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u7cbe\u786e\u3001\u8bed\u4e49\u5bf9\u9f50\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u591a\u5bf9\u8c61\u8fd0\u52a8\u8f6c\u79fb\uff0c\u4fdd\u6301\u4e86DiT\u7684\u9ad8\u8d28\u91cf\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MultiMotion\u901a\u8fc7AMF\u548cRectPC\u6210\u529f\u89e3\u51b3\u4e86DiT\u5728\u591a\u5bf9\u8c61\u89c6\u9891\u8fd0\u52a8\u8f6c\u79fb\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u591a\u5bf9\u8c61\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4ee3\u7801\u5df2\u5728\u8865\u5145\u6750\u6599\u4e2d\u63d0\u4f9b\u3002"}}
{"id": "2512.07503", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07503", "abs": "https://arxiv.org/abs/2512.07503", "authors": ["Yao Teng", "Zhihuan Jiang", "Han Shi", "Xian Liu", "Xuefei Ning", "Guohao Dai", "Yu Wang", "Zhenguo Li", "Xihui Liu"], "title": "SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation", "comment": null, "summary": "Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\\times$ to $3\\times$ inference latency reduction and $2\\times$ to $7\\times$ step compression, while preserving visual quality with no observable degradation.", "AI": {"tldr": "SJD++\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6982\u7387\u5e76\u884c\u89e3\u7801\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u4ee4\u724c\u9884\u6d4b\u548c\u8349\u7a3f\u91cd\u7528\u673a\u5236\uff0c\u5c06\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e2-3\u500d\uff0c\u6b65\u9aa4\u538b\u7f292-7\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u81ea\u56de\u5f52\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u56e0\u4e3a\u9700\u8981\u6570\u767e\u5230\u6570\u5343\u6b21\u987a\u5e8f\u524d\u5411\u4f20\u9012\u8fdb\u884c\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u3002\u9700\u8981\u52a0\u901f\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faSpeculative Jacobi Decoding++ (SJD++)\u7b97\u6cd5\uff0c\u7ed3\u5408Jacobi\u89e3\u7801\u7684\u8fed\u4ee3\u591a\u4ee4\u724c\u9884\u6d4b\u673a\u5236\u548c\u63a8\u6d4b\u91c7\u6837\u7684\u6982\u7387\u8349\u7a3f\u9a8c\u8bc1\u673a\u5236\uff0c\u5e76\u91cd\u7528\u9ad8\u7f6e\u4fe1\u5ea6\u8349\u7a3f\u4ee4\u724c\u800c\u975e\u5168\u90e8\u91cd\u65b0\u91c7\u6837\u3002", "result": "\u5728\u591a\u4e2a\u4ee3\u8868\u6027\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e0a\u5b9e\u9a8c\uff0cSJD++\u5b9e\u73b0\u4e862-3\u500d\u7684\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u548c2-7\u500d\u7684\u6b65\u9aa4\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u65e0\u53ef\u89c1\u9000\u5316\u3002", "conclusion": "SJD++\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u52a0\u901f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07504", "abs": "https://arxiv.org/abs/2512.07504", "authors": ["Ryota Okumura", "Kaede Shiohara", "Toshihiko Yamasaki"], "title": "ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points", "comment": "Accepted to WACV 2026, 8 pages, supplementary included. Dataset and code: https://github.com/RyotaOkumura/ControlVP", "summary": "Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .", "AI": {"tldr": "ControlVP\uff1a\u901a\u8fc7\u7528\u6237\u5f15\u5bfc\u4fee\u6b63\u6587\u672c\u751f\u6210\u56fe\u50cf\u4e2d\u6d88\u5931\u70b9\u4e0d\u4e00\u81f4\u95ee\u9898\u7684\u6846\u67b6\uff0c\u63d0\u5347\u51e0\u4f55\u4e00\u81f4\u6027", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u867d\u7136\u89c6\u89c9\u8d28\u91cf\u51fa\u8272\uff0c\u4f46\u7ecf\u5e38\u5b58\u5728\u51e0\u4f55\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u7279\u522b\u662f\u6d88\u5931\u70b9\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u5e73\u884c\u7ebf\u6295\u5f71\u57282D\u7a7a\u95f4\u4e2d\u65e0\u6cd5\u6b63\u786e\u6536\u655b\uff0c\u7834\u574f\u4e86\u573a\u666f\u7684\u7ed3\u6784\u771f\u5b9e\u611f\uff0c\u5c24\u5176\u5728\u5efa\u7b51\u573a\u666f\u4e2d\u66f4\u4e3a\u660e\u663e", "method": "\u63d0\u51faControlVP\u6846\u67b6\uff0c\u6269\u5c55\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5efa\u7b51\u8f6e\u5ed3\u63d0\u4f9b\u7ed3\u6784\u5f15\u5bfc\uff0c\u5e76\u5f15\u5165\u51e0\u4f55\u7ea6\u675f\u6765\u660e\u786e\u4fc3\u8fdb\u56fe\u50cf\u8fb9\u7f18\u4e0e\u900f\u89c6\u7ebf\u7d22\u7684\u5bf9\u9f50", "result": "\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u51c6\u786e\u7a7a\u95f4\u7ed3\u6784\u7684\u5e94\u7528\uff0c\u5982\u56fe\u50cf\u52303D\u91cd\u5efa", "conclusion": "ControlVP\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u6d88\u5931\u70b9\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7ed3\u6784\u771f\u5b9e\u611f\uff0c\u4e3a\u9700\u8981\u7cbe\u786e\u51e0\u4f55\u7ed3\u6784\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177"}}
{"id": "2512.07514", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07514", "abs": "https://arxiv.org/abs/2512.07514", "authors": ["Junkai Lin", "Hang Long", "Huipeng Guo", "Jielei Zhang", "JiaYi Yang", "Tianle Guo", "Yang Yang", "Jianwen Li", "Wenxiao Zhang", "Matthias Nie\u00dfner", "Wei Yang"], "title": "MeshRipple: Structured Autoregressive Generation of Artist-Meshes", "comment": null, "summary": "Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.", "AI": {"tldr": "MeshRipple\u901a\u8fc7\u524d\u6cbf\u611f\u77e5\u7684BFS\u6807\u8bb0\u5316\u3001\u6269\u5c55\u9884\u6d4b\u7b56\u7565\u548c\u7a00\u758f\u6ce8\u610f\u529b\u5168\u5c40\u5185\u5b58\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u7f51\u683c\u751f\u6210\u4e2d\u957f\u8ddd\u79bb\u51e0\u4f55\u4f9d\u8d56\u65ad\u88c2\u7684\u95ee\u9898\uff0c\u751f\u6210\u5177\u6709\u9ad8\u8868\u9762\u4fdd\u771f\u5ea6\u548c\u62d3\u6251\u5b8c\u6574\u6027\u7684\u7f51\u683c\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u56de\u5f52\u7f51\u683c\u751f\u6210\u5668\u7531\u4e8e\u5185\u5b58\u9650\u5236\uff0c\u5c06\u9762\u5e8f\u5217\u5316\u4e3a\u7247\u6bb5\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u63a8\u7406\uff0c\u4f46\u8fd9\u79cd\u4e0d\u5339\u914d\u7834\u574f\u4e86\u957f\u8ddd\u79bb\u51e0\u4f55\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u751f\u6210\u7f51\u683c\u4e2d\u51fa\u73b0\u5b54\u6d1e\u548c\u788e\u7247\u5316\u7ec4\u4ef6\u3002", "method": "MeshRipple\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u524d\u6cbf\u611f\u77e5\u7684BFS\u6807\u8bb0\u5316\uff0c\u4f7f\u751f\u6210\u987a\u5e8f\u4e0e\u8868\u9762\u62d3\u6251\u5bf9\u9f50\uff1b2\uff09\u6269\u5c55\u9884\u6d4b\u7b56\u7565\uff0c\u4fdd\u6301\u8fde\u8d2f\u3001\u8fde\u63a5\u7684\u8868\u9762\u589e\u957f\uff1b3\uff09\u7a00\u758f\u6ce8\u610f\u529b\u5168\u5c40\u5185\u5b58\uff0c\u63d0\u4f9b\u6709\u6548\u65e0\u754c\u7684\u611f\u53d7\u91ce\u6765\u89e3\u51b3\u957f\u8ddd\u79bb\u62d3\u6251\u4f9d\u8d56\u3002", "result": "MeshRipple\u80fd\u591f\u751f\u6210\u5177\u6709\u9ad8\u8868\u9762\u4fdd\u771f\u5ea6\u548c\u62d3\u6251\u5b8c\u6574\u6027\u7684\u7f51\u683c\uff0c\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u8fd1\u671f\u5f3a\u5927\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MeshRipple\u901a\u8fc7\u5176\u96c6\u6210\u8bbe\u8ba1\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u7f51\u683c\u751f\u6210\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u8fde\u8d2f\u7684\u7f51\u683c\u751f\u6210\uff0c\u4fdd\u6301\u4e86\u957f\u8ddd\u79bb\u51e0\u4f55\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2512.07580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07580", "abs": "https://arxiv.org/abs/2512.07580", "authors": ["Yahong Wang", "Juncheng Wu", "Zhangkai Ni", "Longzhen Yang", "Yihang Liu", "Chengmei Yang", "Ying Wen", "Xianfeng Tang", "Hui Liu", "Yuyin Zhou", "Lianghua He"], "title": "All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs", "comment": null, "summary": "Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by \"vanishing token information\", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as \"information horizon\", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u6df1\u5c42\u89c6\u89c9\u4ee4\u724c\u4fe1\u606f\u6d88\u5931\u73b0\u8c61\uff0c\u63d0\u51fa\"\u4fe1\u606f\u5730\u5e73\u7ebf\"\u6982\u5ff5\uff0c\u8bc1\u660e\u6df1\u5c42\u968f\u673a\u526a\u679d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7ed3\u5408\u968f\u673a\u526a\u679d\u7684DivPrune\u5728Qwen-2.5-VL-7B\u4e0a\u526a\u679d50%\u4ee4\u724c\u4ecd\u4fdd\u630196.9%\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u6570\u767e\u4e2a\u89c6\u89c9\u4ee4\u724c\u8868\u793a\u56fe\u50cf\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u73b0\u6709\u8bad\u7ec3\u65e0\u5173\u526a\u679d\u65b9\u6cd5\u5728\u6df1\u5c42\uff08\u598220\u5c42\u540e\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u4f5c\u8005\u5047\u8bbe\u8fd9\u662f\u7531\u4e8e\"\u4ee4\u724c\u4fe1\u606f\u6d88\u5931\"\u73b0\u8c61\u5bfc\u81f4\u3002", "method": "\u63d0\u51fa\u91cf\u5316\u4ee4\u724c\u4fe1\u606f\u542b\u91cf\u7684\u65b9\u6cd5\uff1a\u901a\u8fc7\u79fb\u9664\u4ee4\u724c\u540e\u6a21\u578b\u8f93\u51fa\u6982\u7387\u7684\u53d8\u5316\u6765\u8861\u91cf\u3002\u5206\u6790\u53d1\u73b0\"\u4fe1\u606f\u5730\u5e73\u7ebf\"\u73b0\u8c61\uff0c\u57fa\u4e8e\u6b64\u63d0\u51fa\u5728\u6df1\u5c42\u4f7f\u7528\u7b80\u5355\u968f\u673a\u526a\u679d\uff0c\u5e76\u5c06\u968f\u673a\u526a\u679d\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u73b0\u8c61\uff1a1\uff09\u89c6\u89c9\u4ee4\u724c\u4fe1\u606f\u968f\u5c42\u6df1\u9010\u6e10\u5747\u5300\u5316\u5e76\u5728\u4e2d\u95f4\u5c42\u6d88\u5931\uff1b2\uff09\u4fe1\u606f\u5730\u5e73\u7ebf\u4f4d\u7f6e\u56e0\u4efb\u52a1\u800c\u5f02\uff08OCR\u6bd4VQA\u66f4\u6df1\uff09\uff1b3\uff09\u5730\u5e73\u7ebf\u4e0e\u6a21\u578b\u80fd\u529b\u76f8\u5173\u3002\u7ed3\u5408\u968f\u673a\u526a\u679d\u7684DivPrune\u5728Qwen-2.5-VL-7B\u4e0a\u526a\u679d50%\u4ee4\u724c\u4ecd\u4fdd\u630196.9%\u6027\u80fd\u3002", "conclusion": "\u6df1\u5c42\u89c6\u89c9\u4ee4\u724c\u4fe1\u606f\u6d88\u5931\u5bfc\u81f4\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u5931\u6548\uff0c\u7b80\u5355\u968f\u673a\u526a\u679d\u5728\u6df1\u5c42\u66f4\u6709\u6548\u3002\u7ed3\u5408\u968f\u673a\u526a\u679d\u80fd\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\uff0c\u4e3a\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.07584", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07584", "abs": "https://arxiv.org/abs/2512.07584", "authors": ["Meituan LongCat Team", "Hanghang Ma", "Haoxian Tan", "Jiale Huang", "Junqiang Wu", "Jun-Yan He", "Lishuai Gao", "Songlin Xiao", "Xiaoming Wei", "Xiaoqi Ma", "Xunliang Cai", "Yayong Guan", "Jie Hu"], "title": "LongCat-Image Technical Report", "comment": null, "summary": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.", "AI": {"tldr": "LongCat-Image\u662f\u4e00\u4e2a\u5f00\u521b\u6027\u7684\u5f00\u6e90\u53cc\u8bed\u56fe\u50cf\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u5728\u6587\u672c\u6e32\u67d3\u3001\u771f\u5b9e\u611f\u3001\u90e8\u7f72\u6548\u7387\u548c\u5f00\u53d1\u8005\u53ef\u8bbf\u95ee\u6027\u65b9\u9762\u89e3\u51b3\u5f53\u524d\u4e3b\u6d41\u6a21\u578b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u7b56\u7565\u548c\u7d27\u51d1\u67b6\u6784\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u4e3b\u6d41\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u591a\u8bed\u8a00\u6587\u672c\u6e32\u67d3\u3001\u771f\u5b9e\u611f\u3001\u90e8\u7f72\u6548\u7387\u548c\u5f00\u53d1\u8005\u53ef\u8bbf\u95ee\u6027\u65b9\u9762\u7684\u6838\u5fc3\u6311\u6218\uff0c\u7279\u522b\u662f\u4e2d\u6587\u6587\u672c\u6e32\u67d3\u7684\u884c\u4e1a\u6807\u51c6\u95ee\u9898\u3002", "method": "1) \u5728\u9884\u8bad\u7ec3\u3001\u4e2d\u671f\u8bad\u7ec3\u548cSFT\u9636\u6bb5\u91c7\u7528\u4e25\u683c\u7684\u6570\u636e\u7b5b\u9009\u7b56\u7565\uff1b2) \u5728\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u534f\u8c03\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u6a21\u578b\uff1b3) \u91c7\u7528\u7d27\u51d1\u76846B\u53c2\u6570\u6269\u6563\u6a21\u578b\u67b6\u6784\uff0c\u8fdc\u5c0f\u4e8e\u5e38\u89c1\u768420B+ MoE\u67b6\u6784\uff1b4) \u5efa\u7acb\u5b8c\u6574\u7684\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u3002", "result": "1) \u5728\u6587\u672c\u6e32\u67d3\u80fd\u529b\u548c\u771f\u5b9e\u611f\u65b9\u9762\u8fbe\u5230\u65b0\u7684SOTA\u6c34\u5e73\uff1b2) \u5728\u4e2d\u6587\u5b57\u7b26\u6e32\u67d3\u65b9\u9762\u5efa\u7acb\u65b0\u7684\u884c\u4e1a\u6807\u51c6\uff0c\u652f\u6301\u590d\u6742\u7f55\u89c1\u5b57\u7b26\uff0c\u5728\u8986\u76d6\u7387\u548c\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u5f00\u6e90\u548c\u5546\u4e1a\u65b9\u6848\uff1b3) \u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\uff0cVRAM\u4f7f\u7528\u6700\u5c0f\u5316\uff0c\u63a8\u7406\u901f\u5ea6\u5feb\uff1b4) \u5728\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e0a\u4e5f\u8fbe\u5230SOTA\u7ed3\u679c\u3002", "conclusion": "LongCat-Image\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u7b56\u7565\u548c\u7d27\u51d1\u67b6\u6784\uff0c\u5728\u591a\u8bed\u8a00\u56fe\u50cf\u751f\u6210\u7279\u522b\u662f\u4e2d\u6587\u6587\u672c\u6e32\u67d3\u65b9\u9762\u53d6\u5f97\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u5176\u5b8c\u6574\u7684\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u5c06\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u5f3a\u5927\u652f\u6301\uff0c\u63a8\u52a8\u89c6\u89c9\u5185\u5bb9\u521b\u4f5c\u7684\u524d\u6cbf\u53d1\u5c55\u3002"}}
{"id": "2512.07596", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07596", "abs": "https://arxiv.org/abs/2512.07596", "authors": ["Wenzhen Dong", "Jieming Yu", "Yiming Huang", "Hongqiu Wang", "Lei Zhu", "Albert C. S. Chung", "Hongliang Ren", "Long Bai"], "title": "More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery", "comment": "Technical Report", "summary": "The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.", "AI": {"tldr": "SAM 3\u5728\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u4e2d\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff1a\u76f8\u6bd4SAM\u548cSAM 2\uff0c\u5728\u96f6\u6837\u672c\u5206\u5272\u30013D\u611f\u77e5\u548c\u89c6\u9891\u8ddf\u8e2a\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u8bed\u8a00\u63d0\u793a\u5728\u624b\u672f\u9886\u57df\u8868\u73b0\u6b20\u4f73\uff0c\u590d\u6742\u52a8\u6001\u573a\u666f\u4ecd\u6709\u5c40\u9650\u3002", "motivation": "\u8bc4\u4f30SAM 3\u5728\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5176\u65b0\u5f15\u5165\u7684\u8bed\u8a00\u63d0\u793a\u5206\u5272\u548c\u589e\u5f3a\u76843D\u611f\u77e5\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5728\u52a8\u6001\u624b\u672f\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u5728MICCAI EndoVis 2017\u548c2018\u57fa\u51c6\u4e0a\u8fdb\u884c\u7efc\u5408\u6d4b\u8bd5\uff0c\u8bc4\u4f30SAM 3\u7684\u96f6\u6837\u672c\u5206\u5272\uff08\u70b9\u3001\u8fb9\u754c\u6846\u3001\u8bed\u8a00\u63d0\u793a\uff09\u548c\u89c6\u9891\u8ddf\u8e2a\u80fd\u529b\uff1b\u540c\u65f6\u5728SCARED\u3001StereoMIS\u548cEndoNeRF\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u5176\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c3D\u91cd\u5efa\u80fd\u529b\u3002", "result": "SAM 3\u5728\u7a7a\u95f4\u63d0\u793a\u4e0b\u7684\u56fe\u50cf\u548c\u89c6\u9891\u5206\u5272\u660e\u663e\u4f18\u4e8eSAM\u548cSAM 2\uff1b\u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c3D\u5668\u68b0\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff1b\u4f46\u8bed\u8a00\u63d0\u793a\u5728\u624b\u672f\u9886\u57df\u8868\u73b0\u4e0d\u7406\u60f3\uff0c\u590d\u6742\u52a8\u6001\u624b\u672f\u573a\u666f\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "SAM 3\u5728\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u8fdb\u6b65\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u5206\u5272\u548c3D\u611f\u77e5\u65b9\u9762\uff0c\u4f46\u8bed\u8a00\u63d0\u793a\u9700\u8981\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\uff0c\u590d\u6742\u52a8\u6001\u573a\u666f\u7684\u5904\u7406\u80fd\u529b\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2512.07599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07599", "abs": "https://arxiv.org/abs/2512.07599", "authors": ["Hanshi Wang", "Zijian Cai", "Jin Gao", "Yiwei Zhang", "Weiming Hu", "Ke Wang", "Zhipeng Zhang"], "title": "Online Segment Any 3D Thing as Instance Tracking", "comment": "NeurIPS 2025, Code is at https://github.com/AutoLab-SAI-SJTU/AutoSeg3D", "summary": "Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.", "AI": {"tldr": "AutoSeg3D\u5c06\u5728\u7ebf3D\u5206\u5272\u91cd\u6784\u4e3a\u5b9e\u4f8b\u8ddf\u8e2a\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u8c61\u67e5\u8be2\u5b9e\u73b0\u65f6\u7a7a\u4fe1\u606f\u4f20\u64ad\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u67e5\u8be2\u76843D\u5206\u5272\u65b9\u6cd5\u5ffd\u89c6\u4e86\u65f6\u95f4\u7ef4\u5ea6\u8fd9\u4e00\u5173\u952e\u56e0\u7d20\uff0c\u800c\u611f\u77e5\u672c\u8d28\u4e0a\u662f\u52a8\u6001\u8fc7\u7a0b\u3002\u673a\u5668\u4eba\u89c6\u89d2\u53d8\u5316\u5e38\u5bfc\u81f4\u7269\u4f53\u90e8\u5206\u53ef\u89c1\uff0c\u9700\u8981\u8d85\u8d8a\u77ac\u65f6\u4e0d\u5b8c\u6574\u89c6\u56fe\u7684\u5168\u9762\u7269\u4f53\u7406\u89e3\u3002", "method": "1) \u5c06\u5728\u7ebf3D\u5206\u5272\u91cd\u6784\u4e3a\u5b9e\u4f8b\u8ddf\u8e2a\u95ee\u9898\uff1b2) \u5229\u7528\u5bf9\u8c61\u67e5\u8be2\u8fdb\u884c\u65f6\u95f4\u4fe1\u606f\u4f20\u64ad\uff1a\u957f\u671f\u5b9e\u4f8b\u5173\u8054\u4fc3\u8fdb\u7279\u5f81\u548c\u7269\u4f53\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u77ed\u671f\u5b9e\u4f8b\u66f4\u65b0\u4e30\u5bcc\u5373\u65f6\u89c2\u6d4b\uff1b3) \u5f15\u5165\u7a7a\u95f4\u4e00\u81f4\u6027\u5b66\u4e60\u7f13\u89e3VFMs\u7684\u788e\u7247\u5316\u95ee\u9898\u3002", "result": "\u5728ScanNet200\u4e0a\u8d85\u8d8aESAM 2.8 AP\uff0c\u5728ScanNet\u3001SceneNN\u548c3RScan\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u5efa\u7acb\u4e86\u65b0\u7684SOTA\u3002", "conclusion": "\u901a\u8fc7\u5c063D\u5206\u5272\u91cd\u6784\u4e3a\u5b9e\u4f8b\u8ddf\u8e2a\u95ee\u9898\uff0c\u5229\u7528\u7a00\u758f\u5bf9\u8c61\u67e5\u8be2\u5b9e\u73b0\u65f6\u7a7a\u4fe1\u606f\u4ea4\u6362\u548c\u4e00\u81f4\u6027\u5b66\u4e60\uff0c\u4e0d\u4ec5\u589e\u5f3a\u4e86\u7a7a\u95f4\u7406\u89e3\uff0c\u8fd8\u907f\u514d\u4e86\u5bc6\u96c6\u65f6\u95f4\u70b9\u4e91\u4ea4\u4e92\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5177\u8eab\u667a\u80fd\u4f53\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2512.07606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07606", "abs": "https://arxiv.org/abs/2512.07606", "authors": ["Jingna Qiu", "Frauke Wilm", "Mathias \u00d6ttl", "Jonas Utz", "Maja Schlereth", "Moritz Schillinger", "Marc Aubreville", "Katharina Breininger"], "title": "Decomposition Sampling for Efficient Region Annotations in Active Learning", "comment": null, "summary": "Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.", "AI": {"tldr": "DECOMP\u662f\u4e00\u79cd\u65b0\u7684\u4e3b\u52a8\u5b66\u4e60\u91c7\u6837\u7b56\u7565\uff0c\u4e13\u95e8\u9488\u5bf9\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u7c7b\u522b\u7279\u5b9a\u7ec4\u4ef6\u5e76\u91c7\u6837\u6bcf\u4e2a\u7c7b\u522b\u7684\u533a\u57df\uff0c\u63d0\u9ad8\u6807\u6ce8\u591a\u6837\u6027\uff0c\u7279\u522b\u5173\u6ce8\u56f0\u96be\u7c7b\u522b\u3002", "motivation": "\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u533b\u5b66\u5f71\u50cf\u5206\u5272\uff09\u7684\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u65f6\u95f4\u5bc6\u96c6\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5185\u5b58\u6210\u672c\u9ad8\u3001\u533a\u57df\u9009\u62e9\u4e0d\u76f8\u5173\u3001\u8fc7\u5ea6\u4f9d\u8d56\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u3002", "method": "DECOMP\u4f7f\u7528\u4f2a\u6807\u7b7e\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u7c7b\u522b\u7279\u5b9a\u7ec4\u4ef6\uff0c\u4ece\u6bcf\u4e2a\u7c7b\u522b\u4e2d\u91c7\u6837\u533a\u57df\uff0c\u5e76\u7ed3\u5408\u7c7b\u522b\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u6307\u5bfc\u91c7\u6837\u8fc7\u7a0b\uff0c\u786e\u4fdd\u56f0\u96be\u7c7b\u522b\u83b7\u5f97\u66f4\u591a\u6807\u6ce8\u3002", "result": "\u5728ROI\u5206\u7c7b\u30012D\u5206\u5272\u548c3D\u5206\u5272\u4efb\u52a1\u4e2d\uff0cDECOMP\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u66f4\u597d\u5730\u91c7\u6837\u5c11\u6570\u7c7b\u522b\u533a\u57df\u5e76\u63d0\u5347\u8fd9\u4e9b\u56f0\u96be\u7c7b\u522b\u7684\u6027\u80fd\u3002", "conclusion": "DECOMP\u901a\u8fc7\u5206\u89e3\u91c7\u6837\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5bc6\u96c6\u9884\u6d4b\u4e3b\u52a8\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6807\u6ce8\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u56f0\u96be\u7c7b\u522b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2512.07628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07628", "abs": "https://arxiv.org/abs/2512.07628", "authors": ["Zhiqi Li", "Wenhuan Li", "Tengfei Wang", "Zhenwei Wang", "Junta Wu", "Haoyuan Wang", "Yunhan Yang", "Zehuan Huang", "Yang Li", "Peidong Liu", "Chunchao Guo"], "title": "MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation", "comment": null, "summary": "Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA", "AI": {"tldr": "MoCA\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u7ec4\u5408\u5f0f3D\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u7ec4\u4ef6\u8def\u7531\u548c\u672a\u9009\u7ec4\u4ef6\u538b\u7f29\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u5168\u5c40\u6ce8\u610f\u529b\u8ba1\u7b97\u590d\u6742\u5ea6\u968f\u7ec4\u4ef6\u6570\u91cf\u589e\u52a0\u800c\u6025\u5267\u4e0a\u5347\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u90e8\u5206\u76843D\u751f\u6210\u65b9\u6cd5\u5728\u589e\u52a0\u7ec4\u4ef6\u6570\u91cf\u65f6\uff0c\u7531\u4e8e\u4e8c\u6b21\u65b9\u5168\u5c40\u6ce8\u610f\u529b\u6210\u672c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7ec4\u5408\u5f0f3D\u751f\u6210\u65b9\u6cd5\u3002", "method": "MoCA\u91c7\u7528\u4e24\u79cd\u5173\u952e\u8bbe\u8ba1\uff1a1) \u57fa\u4e8e\u91cd\u8981\u6027\u7684\u7ec4\u4ef6\u8def\u7531\uff0c\u9009\u62e9top-k\u76f8\u5173\u7ec4\u4ef6\u8fdb\u884c\u7a00\u758f\u5168\u5c40\u6ce8\u610f\u529b\u8ba1\u7b97\uff1b2) \u4e0d\u91cd\u8981\u7ec4\u4ef6\u538b\u7f29\uff0c\u5728\u964d\u4f4e\u5168\u5c40\u6ce8\u610f\u529b\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u7559\u672a\u9009\u7ec4\u4ef6\u7684\u4e0a\u4e0b\u6587\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMoCA\u5728\u7ec4\u5408\u5f0f\u7269\u4f53\u548c\u573a\u666f\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7ec6\u7c92\u5ea6\u7684\u7ec4\u5408\u5f0f3D\u8d44\u4ea7\u521b\u5efa\u3002", "conclusion": "MoCA\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u7ec4\u5408\u5f0f3D\u751f\u6210\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u7ec4\u4ef6\u7ec4\u5408\u76843D\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07651", "abs": "https://arxiv.org/abs/2512.07651", "authors": ["Yuanye Liu", "Hanxiao Zhang", "Nannan Shi", "Yuxin Shi", "Arif Mahmood", "Murtaza Taj", "Xiahai Zhuang"], "title": "Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method", "comment": null, "summary": "Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.", "AI": {"tldr": "LiQA\u6570\u636e\u96c6\u4e3a\u809d\u810f\u7ea4\u7ef4\u5316\u5206\u671f\u63d0\u4f9b\u591a\u4e2d\u5fc3MRI\u57fa\u51c6\uff0c\u5305\u542b440\u4f8b\u60a3\u8005\u6570\u636e\uff0c\u652f\u6301\u5206\u5272\u548c\u5206\u671f\u4efb\u52a1\uff0c\u6311\u6218\u8d5b\u6700\u4f73\u65b9\u6cd5\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u548c\u591a\u89c6\u56fe\u5171\u8bc6\u7b56\u7565\u3002", "motivation": "\u809d\u810f\u7ea4\u7ef4\u5316\u662f\u5168\u7403\u91cd\u5927\u5065\u5eb7\u8d1f\u62c5\uff0c\u9700\u8981\u51c6\u786e\u5206\u671f\u4ee5\u8fdb\u884c\u6709\u6548\u4e34\u5e8a\u7ba1\u7406\u3002\u73b0\u6709\u7b97\u6cd5\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\uff08\u5982\u57df\u504f\u79fb\u3001\u6a21\u6001\u7f3a\u5931\u3001\u7a7a\u95f4\u9519\u4f4d\uff09\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u5efa\u7acbLiQA\u6570\u636e\u96c6\uff08440\u4f8b\u60a3\u8005\u591a\u671f\u76f8\u591a\u4e2d\u5fc3MRI\uff09\uff0c\u6311\u6218\u8d5b\u6700\u4f73\u65b9\u6cd5\u91c7\u7528\uff1a1\uff09\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u5916\u90e8\u6570\u636e\u8fdb\u884c\u9c81\u68d2\u5206\u5272\uff1b2\uff09\u591a\u89c6\u56fe\u5171\u8bc6\u65b9\u6cd5\u7ed3\u5408CAM\u6b63\u5219\u5316\u8fdb\u884c\u5206\u671f\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u5229\u7528\u591a\u6e90\u6570\u636e\u548c\u89e3\u5256\u7ea6\u675f\u80fd\u663e\u8457\u589e\u5f3a\u6a21\u578b\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002\u8be5\u6570\u636e\u96c6\u4e3a\u809d\u810f\u5206\u5272\u548c\u7ea4\u7ef4\u5316\u5206\u671f\u7b97\u6cd5\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u3002", "conclusion": "LiQA\u6570\u636e\u96c6\u548c\u6311\u6218\u8d5b\u65b9\u6cd5\u4e3a\u809d\u810f\u7ea4\u7ef4\u5316\u5206\u671f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u57fa\u51c6\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u591a\u6e90\u6570\u636e\u6574\u5408\u548c\u89e3\u5256\u7ea6\u675f\u80fd\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.07661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07661", "abs": "https://arxiv.org/abs/2512.07661", "authors": ["Shiaho Li", "Naisheng Ye", "Tianyu Li", "Kashyap Chitta", "Tuo An", "Peng Su", "Boyang Wang", "Haiou Liu", "Chen Lv", "Hongyang Li"], "title": "Optimization-Guided Diffusion for Interactive Scene Generation", "comment": null, "summary": "Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.", "AI": {"tldr": "OMEGA\u662f\u4e00\u4e2a\u4f18\u5316\u5f15\u5bfc\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u5728\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e2d\u589e\u5f3a\u7269\u7406\u5408\u7406\u6027\u548c\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u751f\u6210\u5b89\u5168\u5173\u952e\u7684\u5bf9\u6297\u573a\u666f\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8bc4\u4f30\u9700\u8981\u771f\u5b9e\u591a\u6837\u7684\u591a\u667a\u80fd\u4f53\u9a7e\u9a76\u573a\u666f\uff0c\u4f46\u73b0\u6709\u9a7e\u9a76\u6570\u636e\u96c6\u4e2d\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u7a00\u7f3a\u4e14\u4ee3\u8868\u6027\u4e0d\u8db3\u3002\u73b0\u6709\u6570\u636e\u9a71\u52a8\u573a\u666f\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u53ef\u63a7\u6027\u6216\u4ea7\u751f\u8fdd\u53cd\u7269\u7406/\u793e\u4f1a\u7ea6\u675f\u7684\u6837\u672c\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faOMEGA\u6846\u67b6\uff1a1\uff09\u5728\u6269\u6563\u6a21\u578b\u7684\u53cd\u5411\u6269\u6563\u6b65\u9aa4\u4e2d\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u91cd\u65b0\u951a\u5b9a\uff0c\u5f15\u5bfc\u751f\u6210\u7269\u7406\u5408\u7406\u4e14\u884c\u4e3a\u4e00\u81f4\u7684\u8f68\u8ff9\uff1b2\uff09\u5c06\u81ea\u6211\u8f66\u8f86-\u653b\u51fb\u8005\u4ea4\u4e92\u5efa\u6a21\u4e3a\u5206\u5e03\u7a7a\u95f4\u7684\u535a\u5f08\u8bba\u4f18\u5316\uff0c\u8fd1\u4f3c\u7eb3\u4ec0\u5747\u8861\u4ee5\u751f\u6210\u771f\u5b9e\u7684\u5b89\u5168\u5173\u952e\u5bf9\u6297\u573a\u666f\u3002", "result": "\u5728nuPlan\u548cWaymo\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1aOMEGA\u5c06\u7269\u7406\u548c\u884c\u4e3a\u6709\u6548\u573a\u666f\u7684\u6bd4\u4f8b\u4ece32.35%\u63d0\u5347\u523072.27%\uff08\u81ea\u7531\u63a2\u7d22\u80fd\u529b\uff09\uff0c\u4ece11%\u63d0\u5347\u523080%\uff08\u53ef\u63a7\u6027\u751f\u6210\uff09\uff1b\u80fd\u751f\u62105\u500d\u591a\u7684\u8fd1\u78b0\u649e\u5e27\uff08\u78b0\u649e\u65f6\u95f4\u5c0f\u4e8e3\u79d2\uff09\u540c\u65f6\u4fdd\u6301\u573a\u666f\u771f\u5b9e\u6027\u3002", "conclusion": "OMEGA\u901a\u8fc7\u4f18\u5316\u5f15\u5bfc\u7684\u6269\u6563\u91c7\u6837\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u751f\u6210\u7684\u73b0\u5b9e\u6027\u3001\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5b89\u5168\u5173\u952e\u573a\u666f\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2512.07668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07668", "abs": "https://arxiv.org/abs/2512.07668", "authors": ["Ronan John", "Aditya Kesari", "Vincenzo DiMatteo", "Kristin Dana"], "title": "EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset", "comment": null, "summary": "We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .", "AI": {"tldr": "\u63d0\u51fa\u4e86EgoCampus\u6570\u636e\u96c6\u548cEgoCampusNet\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u6237\u5916\u6821\u56ed\u73af\u5883\u4e2d\u884c\u4eba\u5bfc\u822a\u65f6\u7684\u89c6\u89c9\u6ce8\u610f\u529b", "motivation": "\u73b0\u6709\u5927\u591a\u6570\u81ea\u6211\u4e2d\u5fc3\u6570\u636e\u96c6\u4e13\u6ce8\u4e8e\u5ba4\u5185\u4efb\u52a1\u6216\u7f3a\u5c11\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u6237\u5916\u5bfc\u822a\u73af\u5883\u4e2d\u884c\u4eba\u89c6\u89c9\u6ce8\u610f\u529b\u7684\u7814\u7a76", "method": "\u4f7f\u7528Meta\u7684Project Aria\u773c\u955c\u6536\u96c6\u6570\u636e\uff0c\u5305\u542b\u773c\u52a8\u8ffd\u8e2a\u3001RGB\u76f8\u673a\u3001\u60ef\u6027\u4f20\u611f\u5668\u548cGPS\uff1b\u5f00\u53d1\u4e86EgoCampusNet\u65b9\u6cd5\u6765\u9884\u6d4b\u5bfc\u822a\u884c\u4eba\u7684\u773c\u52a8\u6ce8\u89c6\u70b9", "result": "\u521b\u5efa\u4e86EgoCampus\u6570\u636e\u96c6\uff0c\u5305\u542b25\u6761\u72ec\u7279\u6237\u5916\u8def\u5f84\u30016\u516c\u91cc\u8ddd\u79bb\u300180\u591a\u540d\u4e0d\u540c\u884c\u4eba\u7684\u773c\u52a8\u6807\u6ce8\u89c6\u9891\uff1b\u63d0\u4f9b\u4e86\u7814\u7a76\u771f\u5b9e\u4e16\u754c\u6ce8\u610f\u529b\u7684\u65b0\u8d44\u6e90", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7814\u7a76\u771f\u5b9e\u4e16\u754c\u6ce8\u610f\u529b\u63d0\u4f9b\u4e86\u65b0\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u5bfc\u822a\u773c\u52a8\u9884\u6d4b\u6a21\u578b\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u8d44\u6e90"}}
{"id": "2512.07698", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07698", "abs": "https://arxiv.org/abs/2512.07698", "authors": ["Arslan Artykov", "Corentin Sautier", "Vincent Lepetit"], "title": "sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only", "comment": null, "summary": "Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/", "AI": {"tldr": "\u9996\u4e2a\u4ece\u81ea\u7531\u79fb\u52a8\u5355\u76ee\u89c6\u9891\u4e2d\u8054\u5408\u9884\u6d4b\u90e8\u4ef6\u5206\u5272\u548c\u5173\u8282\u53c2\u6570\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4ec5\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5c31\u80fd\u6cdb\u5316\u5230\u771f\u5b9e\u7269\u4f53", "motivation": "\u7406\u89e3\u94f0\u63a5\u7269\u4f53\u5bf9\u673a\u5668\u4eba\u548c\u6570\u5b57\u5b6a\u751f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u591a\u89c6\u89d2\u7cfb\u7edf\u3001\u7269\u4f53\u626b\u63cf\u6216\u9759\u6001\u76f8\u673a\uff0c\u7f3a\u4e4f\u4ece\u81ea\u7531\u79fb\u52a8\u5355\u76ee\u89c6\u9891\u4e2d\u6062\u590d\u90e8\u4ef6\u5206\u5272\u548c\u5173\u8282\u53c2\u6570\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4ece\u81ea\u7531\u79fb\u52a8\u5355\u76ee\u89c6\u9891\u4e2d\u8054\u5408\u9884\u6d4b\u90e8\u4ef6\u5206\u5272\u548c\u5173\u8282\u53c2\u6570\uff0c\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3", "result": "\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5c55\u73b0\u51fa\u5bf9\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u94f0\u63a5\u7269\u4f53\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u76f4\u63a5\u5904\u7406\u968f\u610f\u5f55\u5236\u7684\u89c6\u9891\uff0c\u9002\u5408\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5e94\u7528\uff0c\u4e3a\u94f0\u63a5\u7269\u4f53\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9e\u7528\u9014\u5f84"}}
{"id": "2512.07712", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07712", "abs": "https://arxiv.org/abs/2512.07712", "authors": ["Sayak Dutta", "Harish Katti", "Shashikant Verma", "Shanmuganathan Raman"], "title": "UnCageNet: Tracking and Pose Estimation of Caged Animal", "comment": "9 pages, 2 figures, 2 tables. Accepted to the Indian Conference on Computer Vision, Graphics, and Image Processing (ICVGIP 2025), Mandi, India", "summary": "Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u9636\u6bb5\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u901a\u8fc7\u7b3c\u5b50\u5206\u5272\u3001\u4fee\u590d\u548c\u8bc4\u4f30\uff0c\u89e3\u51b3\u52a8\u7269\u8ffd\u8e2a\u548c\u59ff\u6001\u4f30\u8ba1\u5728\u7b3c\u5b50\u906e\u6321\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898", "motivation": "\u73b0\u6709\u52a8\u7269\u8ffd\u8e2a\u548c\u59ff\u6001\u4f30\u8ba1\u7cfb\u7edf\uff08\u5982STEP\u3001ViTPose\uff09\u5728\u5904\u7406\u5e26\u6709\u7b3c\u5b50\u7ed3\u6784\u548c\u7cfb\u7edf\u6027\u906e\u6321\u7684\u56fe\u50cf\u89c6\u9891\u65f6\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u7b3c\u5b50\u906e\u6321\u5bf9\u7b97\u6cd5\u6027\u80fd\u7684\u5f71\u54cd", "method": "\u4e09\u9636\u6bb5\u9884\u5904\u7406\u6d41\u7a0b\uff1a1) \u4f7f\u7528Gabor\u589e\u5f3a\u7684ResNet-UNet\u67b6\u6784\u8fdb\u884c\u7b3c\u5b50\u5206\u5272\uff0c\u914d\u590772\u4e2a\u65b9\u5411\u53ef\u8c03\u6ee4\u6ce2\u5668\uff1b2) \u4f7f\u7528CRFill\u8fdb\u884c\u5185\u5bb9\u611f\u77e5\u7684\u7b3c\u5b50\u4fee\u590d\uff1b3) \u5728\u4fee\u590d\u540e\u7684\u65e0\u7b3c\u5e27\u4e0a\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\u548c\u8ffd\u8e2a\u8bc4\u4f30", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u901a\u8fc7\u8be5\u6d41\u7a0b\u53bb\u9664\u7b3c\u5b50\u906e\u6321\u540e\uff0c\u59ff\u6001\u4f30\u8ba1\u548c\u8ffd\u8e2a\u6027\u80fd\u53ef\u8fbe\u5230\u4e0e\u65e0\u906e\u6321\u73af\u5883\u76f8\u5f53\u7684\u6c34\u5e73\uff0c\u5173\u952e\u70b9\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8f68\u8ff9\u4e00\u81f4\u6027\u5747\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u63d0\u51fa\u7684\u4e09\u9636\u6bb5\u9884\u5904\u7406\u6d41\u7a0b\u80fd\u6709\u6548\u89e3\u51b3\u7b3c\u5b50\u906e\u6321\u5bf9\u52a8\u7269\u8ffd\u8e2a\u548c\u59ff\u6001\u4f30\u8ba1\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u4f7f\u7b97\u6cd5\u5728\u590d\u6742\u906e\u6321\u73af\u5883\u4e0b\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u6027\u80fd"}}
{"id": "2512.07720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07720", "abs": "https://arxiv.org/abs/2512.07720", "authors": ["Fan Yang", "Heyuan Li", "Peihao Li", "Weihao Yuan", "Lingteng Qiu", "Chaoyue Song", "Cheng Chen", "Yisheng He", "Shifeng Zhang", "Xiaoguang Han", "Steven Hoi", "Guosheng Lin"], "title": "ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation", "comment": null, "summary": "Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u54083D\u91cd\u5efa\u6a21\u578b\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ece\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e0a\u534a\u8eab3D\u865a\u62df\u5f62\u8c61\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7eb9\u7406\u6a21\u7cca\u3001\u8fd0\u52a8\u50f5\u786c\u548c\u7ed3\u6784\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3D\u865a\u62df\u5f62\u8c61\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e24\u65b9\u9762\u7684\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u5927\u91cd\u5efa\u6a21\u578b\u7684\u65b9\u6cd5\u867d\u7136\u5feb\u901f\u4e14\u80fd\u4ea7\u751f\u7a33\u5b9a\u8eab\u4f53\u7ed3\u6784\uff0c\u4f46\u5b58\u5728\u7eb9\u7406\u6a21\u7cca\u548c\u8fd0\u52a8\u50f5\u786c\u7684\u95ee\u9898\uff1b\u800c\u751f\u6210\u5f0f\u89c6\u9891\u6a21\u578b\u867d\u7136\u80fd\u5408\u6210\u903c\u771f\u52a8\u6001\u7ed3\u679c\uff0c\u4f46\u7ecf\u5e38\u51fa\u73b0\u8eab\u4f53\u7ed3\u6784\u9519\u8bef\u548c\u8eab\u4efd\u6f02\u79fb\u7b49\u4e0d\u7a33\u5b9a\u884c\u4e3a\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u4f7f\u75283D\u91cd\u5efa\u6a21\u578b\u63d0\u4f9b\u7a33\u5065\u7684\u7ed3\u6784\u548c\u5916\u89c2\u5148\u9a8c\uff0c\u7136\u540e\u5f15\u5bfc\u4e00\u4e2a\u5b9e\u65f6\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6e32\u67d3\u3002\u8fd9\u79cd\u65b9\u6cd5\u7ed3\u5408\u4e86\u51e0\u4f55\u7a33\u5b9a\u6027\u548c\u751f\u6210\u80fd\u529b\uff0c\u80fd\u591f\u5408\u6210\u9ad8\u9891\u3001\u903c\u771f\u7684\u7ec6\u8282\u548c\u6d41\u7545\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u4f2a\u5f71\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u76f8\u6bd4\u9886\u5148\u65b9\u6cd5\u6709\u5b9e\u8d28\u6027\u63d0\u5347\u3002\u80fd\u591f\u6709\u6548\u51cf\u5c11\u7eb9\u7406\u6a21\u7cca\u548c\u8fd0\u52a8\u50f5\u786c\uff0c\u540c\u65f6\u9632\u6b62\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4e2d\u5e38\u89c1\u7684\u7ed3\u6784\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u5c063D\u91cd\u5efa\u7684\u51e0\u4f55\u7a33\u5b9a\u6027\u4e0e\u89c6\u9891\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5177\u6709\u903c\u771f\u5916\u89c2\u548c\u52a8\u6001\u3001\u65f6\u95f4\u4e00\u81f4\u8fd0\u52a8\u7684\u9ad8\u4fdd\u771f\u6570\u5b57\u865a\u62df\u5f62\u8c61\uff0c\u4e3a\u6e38\u620f\u548c\u865a\u62df\u73b0\u5b9e\u7b49\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u7a33\u5065\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07733", "abs": "https://arxiv.org/abs/2512.07733", "authors": ["Meng Cao", "Xingyu Li", "Xue Liu", "Ian Reid", "Xiaodan Liang"], "title": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "comment": null, "summary": "Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.", "AI": {"tldr": "SpatialDreamer\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u3001\u89c6\u89c9\u60f3\u8c61\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u6765\u89e3\u51b3MLLMs\u5728\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4f7f\u7528GeoPO\u65b9\u6cd5\u89e3\u51b3\u957f\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u7684\u5956\u52b1\u76d1\u7763\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u5fc3\u7406\u6a21\u62df\u7684\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u4f9d\u8d56\u88ab\u52a8\u89c2\u5bdf\u7a7a\u95f4\u6570\u636e\uff0c\u7f3a\u4e4f\u4e3b\u52a8\u7684\u5fc3\u7406\u610f\u8c61\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faSpatialDreamer\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u4e3b\u52a8\u63a2\u7d22\u3001\u4e16\u754c\u6a21\u578b\u7684\u89c6\u89c9\u60f3\u8c61\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u95ed\u73af\u8fc7\u7a0b\u3002\u4e3a\u89e3\u51b3\u957f\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u7684\u7ec6\u7c92\u5ea6\u5956\u52b1\u76d1\u7763\u95ee\u9898\uff0c\u63d0\u51fa\u51e0\u4f55\u7b56\u7565\u4f18\u5316\uff08GeoPO\uff09\uff0c\u5305\u542b\u6811\u7ed3\u6784\u91c7\u6837\u548c\u5177\u6709\u51e0\u4f55\u4e00\u81f4\u6027\u7ea6\u675f\u7684\u6b65\u9aa4\u7ea7\u5956\u52b1\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u6807\u5fd7\u7740MLLMs\u5728\u7c7b\u4eba\u4e3b\u52a8\u7a7a\u95f4\u5fc3\u7406\u6a21\u62df\u65b9\u9762\u7684\u5173\u952e\u8fdb\u5c55\u3002", "conclusion": "SpatialDreamer\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u4e3b\u52a8\u5fc3\u7406\u6a21\u62df\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2512.07738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07738", "abs": "https://arxiv.org/abs/2512.07738", "authors": ["Dengjia Zhang", "Charles Weng", "Katherine Guerrerio", "Yi Lu", "Kenton Murray", "Alexander Martin", "Reno Kriz", "Benjamin Van Durme"], "title": "HLTCOE Evaluation Team at TREC 2025: VQA Track", "comment": "7 pages, 1 figure", "summary": "The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.", "AI": {"tldr": "HLTCOE\u56e2\u961f\u5728TREC VQA\u7684\u7b54\u6848\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5217\u8868\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u6392\u5e8f\u5019\u9009\u7b54\u6848\u6765\u63d0\u9ad8\u8bed\u4e49\u7cbe\u5ea6\u548c\u6392\u5e8f\u4e00\u81f4\u6027\u3002", "motivation": "\u65e8\u5728\u6539\u8fdb\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u7b54\u6848\u751f\u6210\u7684\u8bed\u4e49\u7cbe\u5ea6\u548c\u6392\u5e8f\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u65f6\u95f4\u63a8\u7406\u548c\u8bed\u4e49\u6d88\u6b67\u7684\u95ee\u9898\u4e0a\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u57fa\u7840\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210\u591a\u4e2a\u5019\u9009\u7b54\u6848\uff1b2) \u4f7f\u7528\u65b0\u9896\u7684Masked Pointer Cross-Entropy Loss with Rank Weights\u8bad\u7ec3\u7684\u91cd\u6392\u5e8f\u6a21\u578b\u5bf9\u5019\u9009\u7b54\u6848\u8fdb\u884c\u6392\u5e8f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6392\u5e8f\u7a33\u5b9a\u6027\u65b9\u9762\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u65f6\u95f4\u63a8\u7406\u548c\u8bed\u4e49\u6d88\u6b67\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u901a\u8fc7\u5c06\u751f\u6210\u5f0f\u5efa\u6a21\u4e0e\u5224\u522b\u5f0f\u6392\u5e8f\u76f8\u7ed3\u5408\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u8fde\u8d2f\u3001\u7ec6\u7c92\u5ea6\u7684\u7b54\u6848\u5217\u8868\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684\u5217\u8868\u5f0f\u4f18\u5316\u3002"}}
{"id": "2512.07747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07747", "abs": "https://arxiv.org/abs/2512.07747", "authors": ["Shihao Zhao", "Yitong Chen", "Zeyinzi Jiang", "Bojia Zi", "Shaozhe Hao", "Yu Liu", "Chaojie Mao", "Kwan-Yee K. Wong"], "title": "Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation", "comment": null, "summary": "Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.", "AI": {"tldr": "Unison\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u7684\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6848\uff0c\u4ec5\u970050\u4e07\u8bad\u7ec3\u6837\u672c\u548c50GPU\u5c0f\u65f6\uff0c\u80fd\u81ea\u52a8\u89e3\u6790\u7528\u6237\u610f\u56fe\u548c\u4efb\u52a1\u53c2\u6570\uff0c\u8986\u76d6\u591a\u79cd\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u591a\u6a21\u6001\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a\u81ea\u56de\u5f52\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e24\u9636\u6bb5\u65b9\u6cd5\u4efb\u52a1\u8986\u76d6\u6709\u9650\u4e14\u751f\u6210\u8d28\u91cf\u5dee\u3002\u4e24\u8005\u90fd\u7f3a\u4e4f\u81ea\u52a8\u89e3\u6790\u8f93\u5165\u5143\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u9700\u8981\u624b\u52a8\u914d\u7f6e\u53c2\u6570\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6848\uff0c\u8fde\u63a5\u9884\u8bad\u7ec3\u7684\u7406\u89e3\u548c\u751f\u6210\u6a21\u578b\u8fdb\u884c\u5bf9\u9f50\u5fae\u8c03\u3002\u6a21\u578b\u80fd\u81ea\u52a8\u89e3\u6790\u7528\u6237\u610f\u56fe\u3001\u786e\u5b9a\u4efb\u52a1\u7c7b\u578b\u3001\u63d0\u53d6\u6240\u9700\u5143\u4fe1\u606f\uff0c\u5b9e\u73b0\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u5168\u81ea\u52a8\u5316\u591a\u6a21\u6001\u4efb\u52a1\u5904\u7406\u3002", "result": "\u5728\u4ec550\u4e07\u8bad\u7ec3\u6837\u672c\u548c50GPU\u5c0f\u65f6\u7684\u6781\u4f4e\u6210\u672c\u4e0b\uff0c\u6a21\u578b\u80fd\u51c6\u786e\u81ea\u52a8\u8bc6\u522b\u4efb\u52a1\u548c\u63d0\u53d6\u76f8\u5173\u53c2\u6570\uff0c\u5728\u591a\u79cd\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "Unison\u8bc1\u660e\u4e86\u5728\u4f4e\u6210\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u81ea\u52a8\u4efb\u52a1\u89e3\u6790\u548c\u53c2\u6570\u63d0\u53d6\u5b9e\u73b0\u4e86\u5168\u81ea\u52a8\u5316\u7684\u591a\u6a21\u6001\u4efb\u52a1\u5904\u7406\u3002"}}
{"id": "2512.07756", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07756", "abs": "https://arxiv.org/abs/2512.07756", "authors": ["Mayank Anand", "Ujair Alam", "Surya Prakash", "Priya Shukla", "Gora Chand Nandi", "Domenec Puig"], "title": "UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction", "comment": null, "summary": "Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.", "AI": {"tldr": "UltrasODM\u662f\u4e00\u4e2a\u53cc\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u6bcf\u5e27\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u3001\u663e\u8457\u6027\u8bca\u65ad\u548c\u53ef\u64cd\u4f5c\u63d0\u793a\u6765\u8f85\u52a9\u8d85\u58f0\u533b\u5e08\u91c7\u96c6\u56fe\u50cf\uff0c\u51cf\u5c11\u91cd\u5efa\u8bef\u5dee\uff0c\u63d0\u9ad8\u4e34\u5e8a\u53ef\u9760\u6027\u3002", "motivation": "\u4e34\u5e8a\u8d85\u58f0\u91c7\u96c6\u9ad8\u5ea6\u4f9d\u8d56\u64cd\u4f5c\u8005\uff0c\u5feb\u901f\u7684\u63a2\u5934\u8fd0\u52a8\u548c\u4eae\u5ea6\u6ce2\u52a8\u5e38\u5bfc\u81f4\u91cd\u5efa\u8bef\u5dee\uff0c\u964d\u4f4e\u4fe1\u4efb\u5ea6\u548c\u4e34\u5e8a\u6548\u7528\u3002\u9700\u8981\u4e00\u79cd\u80fd\u8f85\u52a9\u8d85\u58f0\u533b\u5e08\u3001\u63d0\u9ad8\u91cd\u5efa\u53ef\u9760\u6027\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51faUltrasODM\u53cc\u6d41\u6846\u67b6\uff1a1)\u5bf9\u6bd4\u6392\u5e8f\u6a21\u5757\u6309\u8fd0\u52a8\u76f8\u4f3c\u6027\u5206\u7ec4\u5e27\uff1b2)\u5149\u6d41\u6d41\u4e0eDual-Mamba\u65f6\u5e8f\u6a21\u5757\u878d\u5408\uff0c\u7528\u4e8e\u7a33\u5065\u76846-DoF\u59ff\u6001\u4f30\u8ba1\uff1b3)\u4eba\u673a\u4ea4\u4e92\u5c42\u7ed3\u5408\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u3001\u4e34\u5e8a\u6821\u51c6\u9608\u503c\u548c\u663e\u8457\u6027\u56fe\u3002\u5f53\u4e0d\u786e\u5b9a\u6027\u8d85\u8fc7\u9608\u503c\u65f6\uff0c\u7cfb\u7edf\u53d1\u51fa\u975e\u4fb5\u5165\u6027\u8b66\u62a5\uff0c\u5efa\u8bae\u7ea0\u6b63\u63aa\u65bd\u3002", "result": "\u5728\u4e34\u5e8a\u81ea\u7531\u624b\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4UltrasOM\uff0cUltrasODM\u5c06\u6f02\u79fb\u51cf\u5c1115.2%\uff0c\u8ddd\u79bb\u8bef\u5dee\u51cf\u5c1112.1%\uff0cHausdorff\u8ddd\u79bb\u51cf\u5c1110.1%\uff0c\u540c\u65f6\u4ea7\u751f\u6bcf\u5e27\u4e0d\u786e\u5b9a\u6027\u548c\u663e\u8457\u6027\u8f93\u51fa\u3002", "conclusion": "UltrasODM\u901a\u8fc7\u5f3a\u8c03\u900f\u660e\u5ea6\u548c\u4e34\u5e8a\u533b\u5e08\u53cd\u9988\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u53ef\u9760\u6027\uff0c\u652f\u6301\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u4fe1\u7684\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2512.07760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07760", "abs": "https://arxiv.org/abs/2512.07760", "authors": ["Menglin Wang", "Xiaojin Gong", "Jiachen Li", "Genlin Ji"], "title": "Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification", "comment": "Accepted to AAAI 2026", "summary": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u6001\u611f\u77e5Jaccard\u8ddd\u79bb\u7f13\u89e3\u6a21\u6001\u5dee\u5f02\u5e26\u6765\u7684\u8ddd\u79bb\u504f\u5dee\uff0c\u5e76\u7ed3\u5408\"\u5206\u5272-\u5bf9\u6bd4\"\u7b56\u7565\u5b66\u4e60\u6a21\u6001\u4e0d\u53d8\u7279\u5f81\u8868\u793a\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u65e0\u76d1\u7763\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u7531\u4e8e\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u6a21\u6001\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u5f02\uff0c\u96be\u4ee5\u4f30\u8ba1\u53ef\u9760\u7684\u8de8\u6a21\u6001\u5173\u8054\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u6700\u4f18\u4f20\u8f93\u6765\u5173\u8054\u6a21\u6001\u5185\u805a\u7c7b\uff0c\u4f46\u5bb9\u6613\u4f20\u64ad\u5c40\u90e8\u805a\u7c7b\u9519\u8bef\uff0c\u4e14\u5ffd\u7565\u4e86\u5168\u5c40\u5b9e\u4f8b\u7ea7\u5173\u7cfb\u3002", "method": "1. \u63d0\u51fa\u6a21\u6001\u611f\u77e5Jaccard\u8ddd\u79bb\u6765\u7f13\u89e3\u7531\u6a21\u6001\u5dee\u5f02\u5f15\u8d77\u7684\u8ddd\u79bb\u504f\u5dee\uff0c\u901a\u8fc7\u5168\u5c40\u805a\u7c7b\u4f30\u8ba1\u66f4\u53ef\u9760\u7684\u8de8\u6a21\u6001\u5173\u8054\uff1b2. \u8bbe\u8ba1\"\u5206\u5272-\u5bf9\u6bd4\"\u7b56\u7565\u83b7\u53d6\u6a21\u6001\u7279\u5b9a\u7684\u5168\u5c40\u539f\u578b\uff0c\u5728\u5168\u5c40\u5173\u8054\u6307\u5bfc\u4e0b\u663e\u5f0f\u5bf9\u9f50\u8fd9\u4e9b\u539f\u578b\uff0c\u5b9e\u73b0\u6a21\u6001\u4e0d\u53d8\u4e14ID\u53ef\u533a\u5206\u7684\u8868\u793a\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6VI-ReID\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6316\u6398\u548c\u5173\u6ce8\u53ef\u89c1\u5149-\u7ea2\u5916\u6a21\u6001\u504f\u5dee\uff0c\u4ece\u504f\u5dee\u7f13\u89e3\u7684\u5168\u5c40\u5173\u8054\u548c\u6a21\u6001\u4e0d\u53d8\u8868\u793a\u5b66\u4e60\u4e24\u4e2a\u65b9\u9762\u89e3\u51b3\u8de8\u6a21\u6001\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u867d\u7136\u6982\u5ff5\u7b80\u5355\u4f46\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2512.07778", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07778", "abs": "https://arxiv.org/abs/2512.07778", "authors": ["Sen Ye", "Jianning Pei", "Mengde Xu", "Shuyang Gu", "Chunyu Wang", "Liwei Wang", "Han Hu"], "title": "Distribution Matching Variational AutoEncoder", "comment": null, "summary": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \\textbf{Distribution-Matching VAE} (\\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.", "AI": {"tldr": "DMVAE\u901a\u8fc7\u5206\u5e03\u5339\u914d\u7ea6\u675f\u663e\u5f0f\u5bf9\u9f50\u7f16\u7801\u5668\u6f5c\u5728\u5206\u5e03\u4e0e\u4efb\u610f\u53c2\u8003\u5206\u5e03\uff0c\u8d85\u8d8a\u4f20\u7edfVAE\u7684\u9ad8\u65af\u5148\u9a8c\uff0c\u53d1\u73b0SSL\u884d\u751f\u5206\u5e03\u80fd\u5e73\u8861\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u5efa\u6a21\u6548\u7387", "motivation": "\u73b0\u6709\u89c6\u89c9\u751f\u6210\u6a21\u578b\uff08\u5982VAE\u548c\u57fa\u7840\u6a21\u578b\u5bf9\u9f50\u7f16\u7801\u5668\uff09\u9690\u5f0f\u7ea6\u675f\u6f5c\u5728\u7a7a\u95f4\u800c\u672a\u663e\u5f0f\u5851\u9020\u5176\u5206\u5e03\uff0c\u4e0d\u6e05\u695a\u54ea\u79cd\u5206\u5e03\u6700\u9002\u5408\u5efa\u6a21\u3002\u9700\u8981\u660e\u786e\u7814\u7a76\u54ea\u79cd\u6f5c\u5728\u5206\u5e03\u7ed3\u6784\u66f4\u6709\u5229\u4e8e\u5efa\u6a21\u3002", "method": "\u63d0\u51faDistribution-Matching VAE (DMVAE)\uff0c\u901a\u8fc7\u5206\u5e03\u5339\u914d\u7ea6\u675f\u663e\u5f0f\u5bf9\u9f50\u7f16\u7801\u5668\u7684\u6f5c\u5728\u5206\u5e03\u4e0e\u4efb\u610f\u53c2\u8003\u5206\u5e03\u3002\u8fd9\u8d85\u8d8a\u4e86\u4f20\u7edfVAE\u7684\u9ad8\u65af\u5148\u9a8c\uff0c\u53ef\u4ee5\u5bf9\u9f50\u81ea\u76d1\u7763\u7279\u5f81\u3001\u6269\u6563\u566a\u58f0\u6216\u5176\u4ed6\u5148\u9a8c\u5206\u5e03\u3002", "result": "\u53d1\u73b0SSL\u884d\u751f\u5206\u5e03\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u5efa\u6a21\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u826f\u597d\u5e73\u8861\uff0c\u5728ImageNet\u4e0a\u4ec5\u752864\u4e2a\u8bad\u7ec3\u5468\u671f\u5c31\u8fbe\u5230gFID=3.2\u3002\u5206\u5e03\u7ea7\u5bf9\u9f50\u6bd4\u56fa\u5b9a\u5148\u9a8c\u66f4\u5173\u952e\u3002", "conclusion": "\u9009\u62e9\u5408\u9002\u7684\u6f5c\u5728\u5206\u5e03\u7ed3\u6784\uff08\u901a\u8fc7\u5206\u5e03\u7ea7\u5bf9\u9f50\u5b9e\u73b0\uff09\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u56fa\u5b9a\u5148\u9a8c\uff0c\u662f\u5f25\u5408\u6613\u4e8e\u5efa\u6a21\u7684\u6f5c\u5728\u7a7a\u95f4\u548c\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210\u4e4b\u95f4\u5dee\u8ddd\u7684\u5173\u952e\u3002"}}
{"id": "2512.07802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07802", "abs": "https://arxiv.org/abs/2512.07802", "authors": ["Zhaochong An", "Menglin Jia", "Haonan Qiu", "Zijian Zhou", "Xiaoke Huang", "Zhiheng Liu", "Weiming Ren", "Kumara Kahatapitiya", "Ding Liu", "Sen He", "Chenyang Zhang", "Tao Xiang", "Fanny Yang", "Serge Belongie", "Tian Xie"], "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory", "comment": "Project Page: https://zhaochongan.github.io/projects/OneStory", "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.", "AI": {"tldr": "OneStory\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u91cd\u6784\u4e3a\"\u4e0b\u4e00\u955c\u5934\u751f\u6210\"\u4efb\u52a1\uff0c\u4f7f\u7528\u5168\u5c40\u7d27\u51d1\u7684\u8de8\u955c\u5934\u4e0a\u4e0b\u6587\u5efa\u6a21\u6765\u751f\u6210\u8fde\u8d2f\u7684\u957f\u7bc7\u53d9\u4e8b\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u957f\u8ddd\u79bb\u8de8\u955c\u5934\u4e0a\u4e0b\u6587\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u6709\u9650\u7684\u65f6\u95f4\u7a97\u53e3\u6216\u5355\u5173\u952e\u5e27\u6761\u4ef6\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u53d9\u4e8b\u4e0b\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5168\u5c40\u5efa\u6a21\u8de8\u955c\u5934\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u6765\u751f\u6210\u4e00\u81f4\u4e14\u53ef\u6269\u5c55\u7684\u53d9\u4e8b\u89c6\u9891\u3002", "method": "1) \u5c06\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u91cd\u6784\u4e3a\u4e0b\u4e00\u955c\u5934\u751f\u6210\u4efb\u52a1\uff0c\u5b9e\u73b0\u81ea\u56de\u5f52\u955c\u5934\u5408\u6210\uff1b2) \u5f15\u5165\u5e27\u9009\u62e9\u6a21\u5757\uff0c\u57fa\u4e8e\u5148\u524d\u955c\u5934\u7684\u4fe1\u606f\u5e27\u6784\u5efa\u8bed\u4e49\u76f8\u5173\u7684\u5168\u5c40\u8bb0\u5fc6\uff1b3) \u8bbe\u8ba1\u81ea\u9002\u5e94\u6761\u4ef6\u5668\uff0c\u6267\u884c\u91cd\u8981\u6027\u5f15\u5bfc\u7684\u8865\u4e01\u5316\u4ee5\u751f\u6210\u7d27\u51d1\u4e0a\u4e0b\u6587\u8fdb\u884c\u76f4\u63a5\u6761\u4ef6\u5316\uff1b4) \u6784\u5efa\u9ad8\u8d28\u91cf\u591a\u955c\u5934\u6570\u636e\u96c6\u5e76\u8bbe\u8ba1\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u81ea\u5efa\u768460K\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u9884\u8bad\u7ec3\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u540e\uff0cOneStory\u5728\u6587\u672c\u548c\u56fe\u50cf\u6761\u4ef6\u8bbe\u7f6e\u4e0b\uff0c\u5728\u591a\u6837\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0c\u80fd\u591f\u751f\u6210\u53ef\u63a7\u4e14\u6c89\u6d78\u5f0f\u7684\u957f\u7bc7\u89c6\u9891\u53d9\u4e8b\u3002", "conclusion": "OneStory\u901a\u8fc7\u5168\u5c40\u7d27\u51d1\u7684\u8de8\u955c\u5934\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u4e2d\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8fde\u8d2f\u4e14\u53ef\u6269\u5c55\u7684\u53d9\u4e8b\u89c6\u9891\u751f\u6210\uff0c\u4e3a\u957f\u7bc7\u89c6\u9891\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07806", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07806", "abs": "https://arxiv.org/abs/2512.07806", "authors": ["Gyeongjin Kang", "Seungkwon Yang", "Seungtae Nam", "Younggeun Lee", "Jungwoo Kim", "Eunbyung Park"], "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader", "comment": "Project page: see https://gynjn.github.io/MVP/", "summary": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.", "AI": {"tldr": "MVP\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u89c6\u89d2Transformer\u67b6\u6784\uff0c\u80fd\u591f\u4ece\u6570\u5341\u5230\u6570\u767e\u5f20\u56fe\u50cf\u4e2d\u5355\u6b21\u524d\u5411\u4f20\u64ad\u91cd\u5efa\u5927\u578b3D\u573a\u666f\uff0c\u901a\u8fc7\u5c40\u90e8\u5230\u5168\u5c40\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u7cbe\u7ec6\u5230\u7c97\u7cd9\u7684\u8868\u793a\u805a\u5408\u5b9e\u73b0\u9ad8\u6548\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u5f53\u524d3D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u5728\u5904\u7406\u5927\u91cf\u8f93\u5165\u56fe\u50cf\u548c\u5927\u578b\u590d\u6742\u573a\u666f\u65f6\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5904\u7406\u6570\u5341\u5230\u6570\u767e\u5f20\u56fe\u50cf\u5e76\u9ad8\u6548\u91cd\u5efa\u5927\u578b3D\u573a\u666f\u7684\u67b6\u6784\u3002", "method": "\u63d0\u51faMulti-view Pyramid Transformer (MVP)\uff0c\u57fa\u4e8e\u4e24\u4e2a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff1a1) \u5c40\u90e8\u5230\u5168\u5c40\u7684\u89c6\u89d2\u95f4\u5c42\u6b21\u7ed3\u6784\uff0c\u4ece\u5c40\u90e8\u89c6\u56fe\u9010\u6b65\u6269\u5c55\u5230\u7ec4\u89c6\u56fe\u518d\u5230\u5b8c\u6574\u573a\u666f\uff1b2) \u7cbe\u7ec6\u5230\u7c97\u7cd9\u7684\u89c6\u89d2\u5185\u5c42\u6b21\u7ed3\u6784\uff0c\u4ece\u8be6\u7ec6\u7a7a\u95f4\u8868\u793a\u9010\u6b65\u805a\u5408\u6210\u7d27\u51d1\u7684\u4fe1\u606f\u5bc6\u96c6token\u3002\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u4f5c\u4e3a\u5e95\u5c423D\u8868\u793a\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5f53\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u7ed3\u5408\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6cdb\u5316\u91cd\u5efa\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u9002\u5e94\u5e7f\u6cdb\u7684\u89c6\u89d2\u914d\u7f6e\u3002", "conclusion": "MVP\u901a\u8fc7\u53cc\u5c42\u6b21\u7ed3\u6784\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u8868\u793a\u4e30\u5bcc\u6027\u7684\u5e73\u8861\uff0c\u4e3a\u5927\u89c4\u6a213D\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u5927\u91cf\u8f93\u5165\u56fe\u50cf\u7684\u80fd\u529b\u3002"}}
{"id": "2512.07807", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.07807", "abs": "https://arxiv.org/abs/2512.07807", "authors": ["Shai Krakovsky", "Gal Fiebelman", "Sagie Benaim", "Hadar Averbuch-Elor"], "title": "Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes", "comment": "Accepted to SIGGRAPH Asia 2025. Project webpage: https://tau-vailab.github.io/Lang3D-XL", "summary": "Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57283D\u9ad8\u65af\u8868\u793a\u4e2d\u5d4c\u5165\u8bed\u8a00\u573a\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6781\u4f4e\u7ef4\u8bed\u4e49\u74f6\u9888\u7279\u5f81\u548c\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f16\u7801\u5668\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u7279\u5f81\u5bf9\u9f50\u548c\u6548\u7387\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5728HolyScenes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c06\u8bed\u8a00\u573a\u5d4c\u51653D\u8868\u793a\u53ef\u4ee5\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u7a7a\u95f4\u73af\u5883\u8bed\u4e49\u7406\u89e3\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u548c\u7f16\u8f91\u573a\u666f\uff0c\u4f46\u73b0\u6709\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u4e92\u8054\u7f51\u6570\u636e\u65f6\u9762\u4e34\u8bed\u4e49\u7279\u5f81\u9519\u4f4d\u548c\u6548\u7387\u4f4e\u4e0b\u7684\u6311\u6218\u3002", "method": "1) \u57283D\u9ad8\u65af\u8868\u793a\u4e2d\u5f15\u5165\u6781\u4f4e\u7ef4\u8bed\u4e49\u74f6\u9888\u7279\u5f81\uff0c\u901a\u8fc7\u6e32\u67d3\u548c\u591a\u5206\u8fa8\u7387\u57fa\u4e8e\u7279\u5f81\u7684\u54c8\u5e0c\u7f16\u7801\u5668\u5904\u7406\uff1b2) \u63d0\u51fa\u8870\u51cf\u4e0b\u91c7\u6837\u5668\u6a21\u5757\u548c\u591a\u79cd\u6b63\u5219\u5316\u65b9\u6cd5\u89e3\u51b32D\u7279\u5f81\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\u3002", "result": "\u5728HolyScenes\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u884c\u65f6\u6548\u7387\u548cGPU\u5185\u5b58\u4f7f\u7528\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u8bed\u8a00\u573a\u5d4c\u5165\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u66f4\u76f4\u89c2\u7684\u4eba\u673a\u4ea4\u4e92\u548c\u4e30\u5bcc\u7684\u7a7a\u95f4\u8bed\u4e49\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.07826", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07826", "abs": "https://arxiv.org/abs/2512.07826", "authors": ["Haoyang He", "Jie Wang", "Jiangning Zhang", "Zhucun Xue", "Xingyuan Bu", "Qiangpeng Yang", "Shilei Wen", "Lei Xie"], "title": "OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing", "comment": "38 pages", "summary": "The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86OpenVE-3M\uff0c\u4e00\u4e2a\u7528\u4e8e\u6307\u4ee4\u5f0f\u89c6\u9891\u7f16\u8f91\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u5305\u542b\u7a7a\u95f4\u5bf9\u9f50\u548c\u975e\u7a7a\u95f4\u5bf9\u9f50\u4e24\u7c7b\u7f16\u8f91\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u6d41\u6c34\u7ebf\u751f\u6210\u3002\u540c\u65f6\u6784\u5efa\u4e86OpenVE-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bad\u7ec3\u4e86OpenVE-Edit\u6a21\u578b\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u6570\u636e\u96c6\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u4e0d\u65ad\u63d0\u5347\uff0c\u4f46\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u5f0f\u89c6\u9891\u7f16\u8f91\u6570\u636e\u96c6\u4ecd\u7136\u7a00\u7f3a\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u5f00\u6e90\u3001\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u5f0f\u89c6\u9891\u7f16\u8f91\u6570\u636e\u96c6\u3002", "method": "1. \u6784\u5efaOpenVE-3M\u6570\u636e\u96c6\uff1a\u5305\u542b\u7a7a\u95f4\u5bf9\u9f50\u7f16\u8f91\uff08\u5168\u5c40\u98ce\u683c\u3001\u80cc\u666f\u66f4\u6362\u3001\u5c40\u90e8\u66f4\u6539\u3001\u5c40\u90e8\u79fb\u9664\u3001\u5c40\u90e8\u6dfb\u52a0\u3001\u5b57\u5e55\u7f16\u8f91\uff09\u548c\u975e\u7a7a\u95f4\u5bf9\u9f50\u7f16\u8f91\uff08\u6444\u50cf\u673a\u591a\u955c\u5934\u7f16\u8f91\u548c\u521b\u610f\u7f16\u8f91\uff09\u4e24\u5927\u7c7b\u30022. \u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u6d41\u6c34\u7ebf\u751f\u6210\u6570\u636e\u5e76\u8fdb\u884c\u4e25\u683c\u7684\u8d28\u91cf\u8fc7\u6ee4\u30023. \u6784\u5efaOpenVE-Bench\u57fa\u51c6\u6d4b\u8bd5\uff1a\u5305\u542b431\u4e2a\u89c6\u9891\u7f16\u8f91\u5bf9\uff0c\u6db5\u76d6\u591a\u6837\u5316\u7684\u7f16\u8f91\u4efb\u52a1\uff0c\u91c7\u7528\u4e09\u4e2a\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\u7684\u5173\u952e\u6307\u6807\u30024. \u8bad\u7ec3OpenVE-Edit\u6a21\u578b\uff1a\u4e00\u4e2a50\u4ebf\u53c2\u6570\u7684\u6a21\u578b\uff0c\u5728OpenVE-3M\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "1. OpenVE-3M\u5728\u89c4\u6a21\u3001\u7f16\u8f91\u7c7b\u578b\u591a\u6837\u6027\u3001\u6307\u4ee4\u957f\u5ea6\u548c\u6574\u4f53\u8d28\u91cf\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5f00\u6e90\u6570\u636e\u96c6\u30022. OpenVE-Edit\u6a21\u578b\u5728OpenVE-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5305\u62ec140\u4ebf\u53c2\u6570\u57fa\u7ebf\u6a21\u578b\u5728\u5185\u7684\u6240\u6709\u5148\u524d\u5f00\u6e90\u6a21\u578b\u30023. \u8be5\u6a21\u578b\u5c55\u793a\u4e86\u663e\u8457\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u5f0f\u89c6\u9891\u7f16\u8f91\u6570\u636e\u96c6OpenVE-3M\u3001\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5OpenVE-Bench\u4ee5\u53ca\u9ad8\u6548\u7684OpenVE-Edit\u6a21\u578b\uff0c\u4e3a\u6307\u4ee4\u5f0f\u89c6\u9891\u7f16\u8f91\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8d44\u6e90\u548c\u5de5\u5177\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u6570\u636e\u96c6\u7a7a\u767d\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\u3002"}}
{"id": "2512.07831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07831", "abs": "https://arxiv.org/abs/2512.07831", "authors": ["Jiehui Huang", "Yuechen Zhang", "Xu He", "Yuan Gao", "Zhi Cen", "Bin Xia", "Yan Zhou", "Xin Tao", "Pengfei Wan", "Jiaya Jia"], "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation", "comment": "Project Website https://jackailab.github.io/Projects/UnityVideo", "summary": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo", "AI": {"tldr": "UnityVideo\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u5206\u5272\u63a9\u7801\u3001\u4eba\u4f53\u9aa8\u67b6\u3001DensePose\u3001\u5149\u6d41\u548c\u6df1\u5ea6\u56fe\u7b49\u591a\u79cd\u6a21\u6001\uff0c\u5b9e\u73b0\u4e16\u754c\u611f\u77e5\u7684\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u53d7\u9650\u4e8e\u5355\u6a21\u6001\u6761\u4ef6\u7ea6\u675f\uff0c\u7f3a\u4e4f\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u6a21\u6001\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u4e16\u754c\u7684\u6574\u4f53\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u52a8\u6001\u52a0\u566a\u7edf\u4e00\u5f02\u6784\u8bad\u7ec3\u8303\u5f0f\uff1b2\uff09\u5e26\u4e0a\u4e0b\u6587\u5b66\u4e60\u5668\u7684\u6a21\u6001\u5207\u6362\u5668\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u53c2\u6570\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u7edf\u4e00\u5904\u7406\u3002\u6784\u5efa\u4e86130\u4e07\u6837\u672c\u7684\u5927\u89c4\u6a21\u7edf\u4e00\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\uff0cUnityVideo\u52a0\u901f\u4e86\u6536\u655b\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u672a\u89c1\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u89c6\u9891\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u548c\u7269\u7406\u4e16\u754c\u7ea6\u675f\u5bf9\u9f50\u3002", "conclusion": "UnityVideo\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6a21\u6001\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u4e16\u754c\u611f\u77e5\u7684\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07834", "abs": "https://arxiv.org/abs/2512.07834", "authors": ["Yi-Chuan Huang", "Jiewen Chan", "Hao-Jen Chien", "Yu-Lun Liu"], "title": "Voxify3D: Pixel Art Meets Volumetric Rendering", "comment": "Project page: https://yichuanh.github.io/Voxify-3D/", "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/", "AI": {"tldr": "Voxify3D\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u53ef\u5fae\u5206\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u50cf\u7d20\u827a\u672f\u76d1\u7763\u3001\u57fa\u4e8e\u8865\u4e01\u7684CLIP\u5bf9\u9f50\u548c\u8c03\u8272\u677f\u7ea6\u675f\u7684Gumbel-Softmax\u91cf\u5316\uff0c\u5b9e\u73b0\u4ece3D\u7f51\u683c\u5230\u4f53\u7d20\u827a\u672f\u7684\u81ea\u52a8\u5316\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u51e0\u4f55\u62bd\u8c61\u3001\u8bed\u4e49\u4fdd\u6301\u548c\u79bb\u6563\u989c\u8272\u4e00\u81f4\u6027\u7684\u51b2\u7a81\u8981\u6c42\u3002", "motivation": "\u4f53\u7d20\u827a\u672f\u5728\u6e38\u620f\u548c\u6570\u5b57\u5a92\u4f53\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4ece3D\u7f51\u683c\u81ea\u52a8\u751f\u6210\u9762\u4e34\u6311\u6218\uff1a\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8fc7\u5ea6\u7b80\u5316\u51e0\u4f55\u7ed3\u6784\uff0c\u8981\u4e48\u65e0\u6cd5\u5b9e\u73b0\u50cf\u7d20\u7ea7\u7cbe\u786e\u3001\u8c03\u8272\u677f\u7ea6\u675f\u7684\u4f53\u7d20\u827a\u672f\u7f8e\u5b66\u3002\u9700\u8981\u89e3\u51b3\u51e0\u4f55\u62bd\u8c61\u3001\u8bed\u4e49\u4fdd\u6301\u548c\u79bb\u6563\u989c\u8272\u4e00\u81f4\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\u8981\u6c42\u3002", "method": "\u63d0\u51faVoxify3D\u4e24\u9636\u6bb5\u53ef\u5fae\u5206\u6846\u67b6\uff1a1) \u6b63\u4ea4\u50cf\u7d20\u827a\u672f\u76d1\u7763\u6d88\u9664\u900f\u89c6\u7578\u53d8\uff0c\u5b9e\u73b0\u4f53\u7d20-\u50cf\u7d20\u7cbe\u786e\u5bf9\u9f50\uff1b2) \u57fa\u4e8e\u8865\u4e01\u7684CLIP\u5bf9\u9f50\u5728\u4e0d\u540c\u79bb\u6563\u5316\u7ea7\u522b\u4fdd\u6301\u8bed\u4e49\uff1b3) \u8c03\u8272\u677f\u7ea6\u675f\u7684Gumbel-Softmax\u91cf\u5316\uff0c\u5728\u79bb\u6563\u989c\u8272\u7a7a\u95f4\u8fdb\u884c\u53ef\u5fae\u5206\u4f18\u5316\uff0c\u652f\u6301\u53ef\u63a7\u8c03\u8272\u677f\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4f18\u8d8a\u6027\u80fd\uff1aCLIP-IQA\u5f97\u520637.12\uff0c\u7528\u6237\u504f\u597d\u738777.90%\uff0c\u5728\u591a\u6837\u5316\u89d2\u8272\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u652f\u6301\u53ef\u63a7\u62bd\u8c61\uff082-8\u79cd\u989c\u8272\uff0c20x-50x\u5206\u8fa8\u7387\uff09\u3002", "conclusion": "Voxify3D\u901a\u8fc7\u534f\u540c\u6574\u5408\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u6781\u7aef\u79bb\u6563\u5316\u4e0b\u7684\u8bed\u4e49\u4fdd\u6301\u3001\u901a\u8fc7\u4f53\u79ef\u6e32\u67d3\u5b9e\u73b0\u50cf\u7d20\u827a\u672f\u7f8e\u5b66\u4ee5\u53ca\u7aef\u5230\u7aef\u79bb\u6563\u4f18\u5316\u7b49\u57fa\u672c\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u4f53\u7d20\u827a\u672f\u751f\u6210\u3002"}}
