<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 224]
- [cs.AI](#cs.AI) [Total: 53]
- [cs.CL](#cs.CL) [Total: 75]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Psychological stress during Examination and its estimation by handwriting in answer script](https://arxiv.org/abs/2511.11633)
*Abhijeet Kumar,Chetan Agarwal,Pronoy B. Neogi,Mayank Goswami*

Main category: cs.CV

TL;DR: 该研究结合笔迹学和人工智能，通过分析学生手写考试卷来量化心理压力水平，使用OCR和基于transformer的情感分析模型，提供超越传统评分系统的数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 传统评分系统无法深入了解学生在考试期间的认知和情绪状态，需要一种能够量化心理压力的创新方法。

Method: 集成高分辨率图像处理、TrOCR和使用RoBERTa模型的情感熵融合来生成压力指数，采用五模型投票机制和无监督异常检测确保鲁棒性。

Result: 开发了一个在学术取证领域的创新框架，能够有效量化学生的心理压力水平。

Conclusion: 该方法为评估学生考试期间的心理状态提供了新的技术途径，在学术取证领域具有重要应用价值。

Abstract: This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.

</details>


### [2] [Real-time pothole detection with onboard sensors and camera on vehicles](https://arxiv.org/abs/2511.11643)
*Aswath Muthuselvam,Jeevak Raj S,Mohanaprasad K*

Main category: cs.CV

TL;DR: 使用车载传感器和SVM分类器实时检测道路坑洞，在2公里路段上实现98.1%的准确率


<details>
  <summary>Details</summary>
Motivation: 道路状况对日常通勤至关重要，随着车辆数量增加，需要频繁评估道路状况以确保交通顺畅，小裂缝可能发展成大坑洞

Method: 利用车载传感器收集数据，采用SVM分类器进行坑洞检测

Result: 在2公里路段（包含26个坑洞）上实现了98.1%的检测准确率

Conclusion: 该方法能够有效实时检测道路坑洞，为大规模坑洞管理提供有用数据

Abstract: Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes

</details>


### [3] [AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2511.11662)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: AGENet是一种新颖的医学图像分割框架，通过边缘感知的测地距离学习整合空间关系，在有限标注数据下实现精确的边界分割。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要大量标注数据，这成为临床应用的瓶颈。现有少样本分割方法在医学图像精确边界分割方面表现不佳，特别是当解剖结构相似且缺乏足够空间上下文时。

Method: 提出AGENet框架，包含三个主要组件：(1)边缘感知测地距离学习模块，通过迭代快速行进细化来尊重解剖边界；(2)自适应原型提取，通过空间加权聚合捕获全局结构和局部边界细节；(3)自适应参数学习，自动适应不同器官特征。

Result: 在多个医学影像数据集上的广泛实验表明，该方法优于现有最先进方法，显著减少边界误差，同时保持计算效率。

Conclusion: AGENet在有限标注数据下实现了精确的医学图像分割，特别适合需要精确分割的临床应用场景。

Abstract: Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.

</details>


### [4] [Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement](https://arxiv.org/abs/2511.11702)
*Lian He,Meng Liu,Qilang Ye,Yu Zhou,Xiang Deng,Gangyi Ding*

Main category: cs.CV

TL;DR: TASA是一个新颖的3D场景级可供性分割框架，通过联合利用2D语义线索和3D几何推理，在粗到细的方式下实现高效准确的可供性检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注对象级可供性或仅将2D预测提升到3D，忽略了点云中的丰富几何结构信息且计算成本高，需要解决语义推理和空间定位的挑战。

Method: TASA框架包含任务感知的2D可供性检测模块和3D可供性细化模块，前者从语言和视觉输入识别可操作点以指导任务相关视图选择，后者集成2D语义先验与局部3D几何信息。

Result: 在SceneFun3D数据集上的实验表明，TASA在场景级可供性分割的准确性和效率方面显著优于基线方法。

Conclusion: TASA通过几何优化的方法有效解决了3D场景级可供性分割的挑战，在保持高精度的同时提升了计算效率。

Abstract: Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

</details>


### [5] [LE-CapsNet: A Light and Enhanced Capsule Network](https://arxiv.org/abs/2511.11708)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 提出了LE-CapsNet，一种轻量级、增强且更准确的CapsNet变体，在CIFAR-10数据集上达到76.73%准确率，推理速度比CapsNet快4倍，在AffNIST数据集上达到94.3%准确率。


<details>
  <summary>Details</summary>
Motivation: CapsNet虽然具有检测重叠类别图像和变换图像的优势，但速度慢、资源消耗大、参数多且准确率低于CNN。

Method: 开发了LE-CapsNet作为CapsNet的轻量级增强版本，使用380万权重。

Result: 在CIFAR-10数据集上获得76.73%准确率，推理速度比CapsNet快4倍；在AffNIST数据集上达到94.3%准确率，优于CapsNet的90.52%。

Conclusion: LE-CapsNet在保持CapsNet优势的同时，显著提升了速度和准确率，对仿射变换图像具有更强的鲁棒性。

Abstract: Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).

</details>


### [6] [Target-Balanced Score Distillation](https://arxiv.org/abs/2511.11710)
*Zhou Xu,Qi Wang,Yuxiao Yang,Luyuan Zhang,Zhang Liang,Yang Li*

Main category: cs.CV

TL;DR: 本文提出Target-Balanced Score Distillation (TBSD)方法，通过多目标优化和自适应策略解决Score Distillation Sampling (SDS)在3D资产生成中的纹理-形状权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统SDS方法存在过饱和和过平滑问题，其变体虽然引入负提示但面临纹理优化有限或纹理增益伴随形状失真的关键权衡。研究发现这种权衡根本上由负提示的使用决定。

Method: 提出TBSD方法，将生成建模为多目标优化问题，引入自适应策略来平衡纹理真实性和几何形状准确性。

Result: 大量实验表明，TBSD显著优于现有最先进方法，能够生成具有高保真纹理和几何精确形状的3D资产。

Conclusion: TBSD通过系统分析和自适应多目标优化，有效解决了SDS方法在3D生成中的纹理-形状权衡问题，实现了更好的生成质量。

Abstract: Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.

</details>


### [7] [Exposing DeepFakes via Hyperspectral Domain Mapping](https://arxiv.org/abs/2511.11732)
*Aditya Mehta,Swarnim Chaudhary,Pratik Narang,Jagat Sesh Challa*

Main category: cs.CV

TL;DR: HSI-Detect是一个两阶段检测管道，通过将RGB图像重建为31通道高光谱图像，在高光谱域进行Deepfake检测，相比RGB域检测方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现代生成和扩散模型产生的逼真图像可能误导人类感知和自动化检测系统。大多数检测方法仅在RGB空间操作，只能分析三个光谱通道，限制了检测能力。

Method: 提出HSI-Detect两阶段管道：首先从标准RGB输入重建31通道高光谱图像，然后在高光谱域进行检测。通过扩展输入表示到更密集的光谱带，放大RGB域中弱或不可见的操作伪影。

Result: 在FaceForensics++数据集上的评估显示，HSI-Detect相比仅使用RGB的基线方法取得了持续改进，特别是在特定频率带中效果显著。

Conclusion: 光谱域映射在Deepfake检测中具有广阔前景，高光谱分析能够增强对生成图像伪影的检测能力。

Abstract: Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.

</details>


### [8] [Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition](https://arxiv.org/abs/2511.11754)
*Stanislav Selitskiy*

Main category: cs.CV

TL;DR: 提出了一种新型的隐式稀疏Transformer架构——Batch Transformers，通过对重要维度（主成分）进行注意力机制，显著减小编码器-解码器架构中的瓶颈大小，并在人脸识别任务中测试了合成图像生成效果。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer对序列或批次的全部维度进行注意力计算，导致计算复杂度高。本文旨在通过关注重要维度来减少编码器-解码器架构的瓶颈大小，提高效率。

Method: 提出Batch Transformers架构，采用隐式稀疏风格，仅对重要维度（主成分）实施注意力机制，实现特征选择，从而减小模型瓶颈。

Result: 在化妆和遮挡数据集的人脸识别任务中测试了合成图像生成，证明该方法能够增加有限原始数据集的变异性。

Conclusion: Batch Transformers通过关注重要维度的注意力机制，有效减小了模型瓶颈，并在人脸识别任务中提升了数据增强效果。

Abstract: A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike "traditional" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the "important" dimensions (primary components) is implemented. In such a way, the "important" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.

</details>


### [9] [Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing](https://arxiv.org/abs/2511.11780)
*Hossein Mohebbi,Mohammed Abdulrahman,Yanting Miao,Pascal Poupart,Suraj Kothawade*

Main category: cs.CV

TL;DR: Image-POSER是一个基于强化学习的框架，通过协调多个预训练文本到图像和图像到图像专家模型，动态分解长提示任务，并使用视觉语言模型进行结构化反馈监督，实现更好的图像生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型在处理长而复杂的提示时表现不佳，缺乏可靠执行创意工作流程中典型的长组合提示的能力。

Method: 采用反射式强化学习框架，将图像合成和编辑建模为马尔可夫决策过程，动态协调多个预训练专家模型，通过视觉语言模型提供结构化反馈监督每个步骤的对齐。

Result: 实验表明Image-POSER在行业标准和自定义基准测试中，在对齐度、保真度和美学方面均优于基线模型（包括前沿模型），并在人工评估中持续获得偏好。

Conclusion: 强化学习可以赋予AI系统自主分解、重新排序和组合视觉模型的能力，朝着通用视觉助手的方向发展。

Abstract: Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.

</details>


### [10] [SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction](https://arxiv.org/abs/2511.11824)
*Zhongping Dong,Pengyang Yu,Shuangjian Li,Liming Chen,Mohand Tahar Kechadi*

Main category: cs.CV

TL;DR: SOTFormer是一个统一的端到端框架，集成了目标检测、跟踪和短期轨迹预测，通过地面真值引导记忆和锚点损失实现稳定的身份传播，在遮挡、尺度变化和时间漂移下表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决在遮挡、尺度变化和时间漂移下，单目标跟踪和短期运动预测的挑战，这些因素破坏了实时感知所需的时间一致性。

Method: 采用最小常数记忆时间变换器，通过地面真值引导记忆和锚点损失稳定初始化，单个轻量级时间注意力层跨帧优化嵌入，实现实时推理和固定GPU内存使用。

Result: 在Mini-LaSOT（20%）基准测试中，SOTFormer达到76.3 AUC和53.7 FPS（AMP，4.3 GB VRAM），在快速运动、尺度变化和遮挡情况下优于TrackFormer和MOTRv2等变换器基线。

Conclusion: SOTFormer通过统一的端到端框架，在保持实时性能的同时，显著提升了在复杂场景下的跟踪和预测精度。

Abstract: Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.

</details>


### [11] [MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning](https://arxiv.org/abs/2511.11837)
*Fatemeh Elhambakhsh,Gaurav Ameta,Aditi Roy,Hyunwoong Ko*

Main category: cs.CV

TL;DR: MP-GFormer是一个3D几何感知的动态图变换器，通过将演化的3D几何表示集成到动态图学习中，用于预测加工操作序列，相比现有方法在主要和子操作预测准确率上分别提升了24%和36%。


<details>
  <summary>Details</summary>
Motivation: 现有动态图学习方法在加工工艺规划中虽然能捕捉时空依赖关系，但未能融入零件的三维几何信息，缺乏领域感知能力，这限制了加工操作序列预测的准确性。

Method: 提出MP-GFormer方法，利用StereoLithography表面网格表示每次加工操作后零件的3D几何，通过注意力机制将演化的3D几何表示集成到动态图学习中。

Result: 在合成数据集上的评估显示，该方法在主要操作和子操作预测准确率上分别比最先进方法提升了24%和36%。

Conclusion: MP-GFormer通过集成3D几何信息显著提升了加工操作序列预测的准确性，证明了3D几何感知在加工工艺规划中的重要性。

Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.

</details>


### [12] [Defending Unauthorized Model Merging via Dual-Stage Weight Protection](https://arxiv.org/abs/2511.11851)
*Wei-Jia Chen,Min-Yen Tsai,Cheng-Yi Lee,Chia-Mu Yu*

Main category: cs.CV

TL;DR: MergeGuard是一个双阶段权重保护框架，通过重新分布任务相关信息和注入结构化扰动来破坏模型合并兼容性，同时保持原始模型性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型和开放存储库的快速扩散使得模型合并成为方便但有风险的做法，未经授权的模型合并侵犯知识产权并破坏模型所有权和问责制。

Method: 第一阶段通过L2正则化优化重新分布任务相关信息，第二阶段注入结构化扰动以错位任务子空间，破坏损失景观中的曲率兼容性。

Result: 在视觉和语言模型上的广泛实验表明，MergeGuard将合并模型准确率降低高达90%，而受保护模型性能损失小于1.5%。

Conclusion: MergeGuard有效防止未经授权的模型合并，同时保持受保护模型的功能完整性，为解决模型知识产权保护问题提供了可行方案。

Abstract: The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.

</details>


### [13] [Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)](https://arxiv.org/abs/2511.11882)
*Simon Durand,Samuel Foucher,Alexandre Delplanque,Joëlle Taillon,Jérôme Théau*

Main category: cs.CV

TL;DR: 本研究探讨了使用合成图像(SI)补充有限训练数据以提高麝牛检测性能的方法，在零样本和少样本设置下验证了SI对深度学习目标检测模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统野生动物调查方法资源密集且受限于后勤挑战，而深度学习目标检测模型在稀疏分布物种(如麝牛)上因训练数据有限而效果不佳。

Method: 比较了仅使用真实图像的基线模型与5个零样本和5个少样本模型，这些模型在训练集中逐步加入更多合成图像。

Result: 零样本模型中，添加SI提高了检测性能，但超过基线模型训练数据集100%后出现收益递减；少样本模型中，结合真实和SI图像获得了更好的召回率和略高的准确率。

Conclusion: 合成图像在数据稀缺时能有效训练准确的目标检测模型，为监测稀有或难以接近物种提供了重要视角，可在没有真实数据时启动模型并随时间推移逐步完善。

Abstract: Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.

</details>


### [14] [Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation](https://arxiv.org/abs/2511.11890)
*Camila Machado de Araujo,Egon P. B. S. Borges,Ricardo Marcelo Canteiro Grangeiro,Allan Pinto*

Main category: cs.CV

TL;DR: 本文介绍了Harpia，一个基于CUDA的处理库，用于支持大规模3D数据集的可扩展、交互式分割工作流程，解决了高分辨率体积成像技术生成的大数据集处理挑战。


<details>
  <summary>Details</summary>
Motivation: 高分辨率体积成像技术（如X射线断层扫描和先进显微镜）生成的数据集越来越大，现有工具在处理、分割和交互探索方面效率不足。

Method: 通过Harpia库实现严格的内存控制、原生分块执行，以及一套GPU加速的过滤、注释和量化工具，支持在超过单个GPU内存容量的数据集上可靠运行。

Result: 实验结果显示，与NVIDIA cuCIM和scikit-image等广泛使用的框架相比，在处理速度、内存效率和可扩展性方面有显著改进。

Conclusion: 该系统具有交互式人机界面和高效的GPU资源管理，特别适合在共享HPC基础设施中进行协作科学成像工作流程。

Abstract: High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.

</details>


### [15] [Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks](https://arxiv.org/abs/2511.11898)
*Arnav Singhvi,Vasiliki Bikia,Asad Aali,Akshay Chaudhari,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本文提出使用DSPy框架进行自动化提示优化，在医学视觉语言系统中显著提升性能，相比零样本提示基线中位相对改进达53%，最大改进达300%-3400%。


<details>
  <summary>Details</summary>
Motivation: 视觉语言基础模型在医学基准测试中表现不佳，传统方法如微调需要大量领域数据和计算资源，手动提示工程难以泛化且不易部署，因此需要探索不依赖人工设计提示的自动化优化方法。

Method: 采用DSPy框架进行结构化自动提示优化，在放射学、胃肠病学和皮肤病学等五个医学成像任务中实现提示管道，评估了10个开源视觉语言模型和四种提示优化技术。

Result: 优化后的管道相比零样本提示基线实现了53%的中位相对改进，在零样本性能较低的任务上最大改进达300%-3400%。

Conclusion: 自动化提示优化在医学AI系统中具有巨大潜力，显著提升了临床图像解释的准确性，减少了对提示设计的依赖，使临床医生能专注于患者护理和临床决策。

Abstract: Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.

</details>


### [16] [Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models](https://arxiv.org/abs/2511.11910)
*Siyou Li,Huanan Wu,Juexi Shao,Yinghao Ma,Yujian Gan,Yihao Luo,Yuwei Wang,Dong Nie,Lu Wang,Wengqing Wu,Le Zhang,Massimo Poesio,Juntao Yu*

Main category: cs.CV

TL;DR: QTSplus是一种轻量级视觉令牌选择模块，通过动态选择与文本查询最相关的视觉证据来解决长视频理解中的令牌爆炸问题，显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在长视频理解中面临的视觉令牌数量线性增长导致的注意力成本、内存和延迟爆炸问题。

Method: 使用交叉注意力对视觉令牌评分，基于查询复杂度预测保留预算，通过可微分直通估计器选择Top-n令牌，并利用小型重编码器保持时间顺序。

Result: 在Qwen2.5-VL中集成QTSplus，视觉流压缩达89%，端到端延迟降低28%，在八个长视频理解基准测试中保持接近原始模型的准确率，在TempCompass方向准确率提升20.5点。

Conclusion: QTSplus是扩展MLLMs到真实世界长视频场景的有效通用机制，能够保留任务相关证据同时显著降低计算成本。

Abstract: Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.
  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \textbf{89\%} and reduces end-to-end latency by \textbf{28\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \textbf{+20.5} and \textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.
  We will make all code, data, and trained models' weights publicly available.

</details>


### [17] [From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing](https://arxiv.org/abs/2511.11944)
*Ling Wang,Yunfan Lu,Wenzong Ma,Huizai Yao,Pengteng Li,Hui Xiong*

Main category: cs.CV

TL;DR: 首次使用事件相机进行图像去雾，通过事件引导的扩散模型将高动态范围的事件信息传输到RGB图像中，在浓雾条件下实现清晰成像。


<details>
  <summary>Details</summary>
Motivation: 传统RGB图像去雾方法受限于动态范围，容易丢失结构和光照细节。事件相机具有更高的动态范围和微秒级延迟，更适合处理雾霾场景。

Method: 提出事件引导的扩散模型，设计事件引导模块将稀疏的高动态范围事件特征映射到扩散潜在空间，为生成过程提供精确的结构指导。

Result: 在两个基准测试和自建的重雾无人机数据集上取得了最先进的结果。

Conclusion: 事件相机结合扩散模型能够有效解决雾霾条件下的图像去雾问题，显著提升图像质量和结构保持能力。

Abstract: Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.

</details>


### [18] [Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs](https://arxiv.org/abs/2511.11959)
*Leonardi Melo,Luís Gustavo,Dimmy Magalhães,Lucciani Vieira,Mauro Araújo*

Main category: cs.CV

TL;DR: 本研究比较了三种基于U-Net的架构在巴西考古遗址岩画岩刻语义分割中的性能，发现结合残差块和门控注意力机制的Attention-Residual BEGL-UNet表现最佳，Dice Score达到0.710。


<details>
  <summary>Details</summary>
Motivation: 开发有效的深度学习方法来数字化保存和保护考古遗产，特别是岩画岩刻的语义分割。

Method: 比较三种U-Net变体架构：(1) 使用边界增强高斯损失函数的BEGL-UNet；(2) 结合残差块和门控注意力机制的Attention-Residual BEGL-UNet；(3) 使用基于卷积块注意力模块的空间通道注意力模块的Spatial Channel Attention BEGL-UNet。所有实现都采用结合二元交叉熵和高斯边缘增强的BEGL损失函数，使用5折交叉验证在巴西Poço da Bebidinha考古复合体图像上进行实验。

Result: Attention-Residual BEGL-UNet表现最佳，Dice Score为0.710，验证损失为0.067，召回率最高为0.854。Spatial Channel Attention BEGL-UNet获得可比较性能，DSC为0.707，召回率为0.857。基线BEGL-UNet的DSC为0.690。注意力机制相比基线带来2.5-2.9%的Dice Score提升。

Conclusion: 注意力机制在考古遗产数字保护中具有显著效果，特别是结合残差块和门控注意力机制的架构能够有效提升岩画岩刻语义分割的性能。

Abstract: This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Poço da Bebidinha Archaeological Complex, Piauí, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.

</details>


### [19] [BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups](https://arxiv.org/abs/2511.11989)
*Songsong Zhang,Chuanqi Tang,Hongguang Zhang,Guijian Tang,Minglong Li,Xueqiong Li,Shaowu Yang,Yuanxi Peng,Wenjing Yang,Jing Zhao*

Main category: cs.CV

TL;DR: 提出了一种突破面部特写限制的身份保持个性化生成方法，通过双线推理、身份自适应融合和身份聚合前置模块，实现身份保真度和场景语义创作的协同优化。


<details>
  <summary>Details</summary>
Motivation: 现有身份保持个性化生成方法过度强调面部区域，导致输出被面部特写主导，存在视觉叙事性弱和复杂文本提示下语义一致性差的问题，核心限制在于身份特征嵌入削弱了生成模型的语义表达能力。

Method: 设计双线推理管道实现身份-语义分离，提出身份自适应融合策略延迟身份-语义融合到噪声预测阶段，引入身份聚合前置模块聚合身份信息并替换随机初始化。

Result: 实验验证该方法在超越面部特写的身份保持个性化生成任务中实现稳定有效性能，无需手动遮罩或微调即可高效生成。

Conclusion: 该方法作为即插即用组件可快速部署到现有框架中，解决对面部特写的过度依赖，促进电影级角色-场景创作，为相关领域提供更丰富的个性化生成能力。

Abstract: Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.

</details>


### [20] [Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis](https://arxiv.org/abs/2511.12018)
*Shounak Ray Chaudhuri,Arash Jahangiri,Christopher Paolini*

Main category: cs.CV

TL;DR: 提出了一种基于多摄像头计算机视觉的实时交通安全评估框架，通过计算后侵占时间(PET)来识别信号交叉口的高风险区域，并在边缘设备上实现实时处理。


<details>
  <summary>Details</summary>
Motivation: 传统基于事故的交通安全研究受限于数据稀疏性和延迟问题，需要开发实时、高分辨率的评估方法来改善交叉口安全。

Method: 使用四个同步摄像头提供连续视觉覆盖，在NVIDIA Jetson AGX Xavier设备上使用YOLOv11分割进行车辆检测，通过单应性矩阵将检测到的车辆多边形转换为统一的鸟瞰图，并开发了像素级PET算法进行精细危险可视化。

Result: 框架能够在边缘设备上以平均2.68 FPS的速度识别高风险区域，提供亚秒级精度和800x800像素对数热图，实现3.3平方厘米的精确度。

Conclusion: 验证了基于分散式视觉的PET分析在智能交通系统中的可行性，为高分辨率、实时和可扩展的交叉口安全评估提供了可复制的方法论。

Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.

</details>


### [21] [LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension](https://arxiv.org/abs/2511.12020)
*Xianglong Shi,Silin Cheng,Sirui Zhao,Yunhan Jiang,Enhong Chen,Yang Liu,Sebastien Ourselin*

Main category: cs.CV

TL;DR: 本文提出了弱监督广义指代表达理解任务（WGREC），解决了现有WREC方法只能处理一对一映射的局限性。作者开发了LIHE框架，通过参考解耦和参考物定位两个阶段，结合双曲-欧几里得混合相似度模块，有效处理零个或多个目标的表达。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督指代表达理解方法受限于一对一映射假设，无法处理现实场景中对应零个或多个目标的表达。需要开发更实用的WGREC范式来应对这些挑战。

Method: 提出LIHE框架，包含两个阶段：参考解耦阶段预测目标数量并分解复杂表达为简单子表达；参考物定位阶段使用HEMix混合相似度模块，结合欧几里得几何的精确对齐能力和双曲几何的层次建模优势。

Result: 在gRefCOCO和Ref-ZOM数据集上建立了首个有效的弱监督WGREC基线，HEMix在标准REC基准上实现了一致改进，IoU@0.5提升高达2.5%。

Conclusion: LIHE框架成功解决了弱监督广义指代表达理解任务，通过混合几何方法有效防止语义坍塌，为处理可变数量参考物的表达提供了有效解决方案。

Abstract: Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.

</details>


### [22] [Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark](https://arxiv.org/abs/2511.12026)
*Rulin Zhou,Wenlong He,An Wang,Jianhang Zhang,Xuanhui Zeng,Xi Zhang,Chaowei Zhu,Haijun Hu,Hongliang Ren*

Main category: cs.CV

TL;DR: VL-SurgPT是首个大规模多模态手术点跟踪数据集，结合视觉跟踪与文本描述，包含908个手术视频片段，涵盖组织和器械跟踪，并提出TG-SurgPT文本引导跟踪方法。


<details>
  <summary>Details</summary>
Motivation: 现有手术跟踪数据集缺乏语义上下文，无法理解跟踪失败机制，在烟雾遮挡、镜面反射和组织变形等复杂视觉条件下跟踪精度受限。

Method: 建立包含754个组织跟踪视频和154个器械跟踪视频的多模态数据集，提出TG-SurgPT方法，利用语义描述在视觉挑战条件下提升跟踪鲁棒性。

Result: 实验结果表明，整合点状态信息显著提高了跟踪精度和可靠性，特别是在传统纯视觉方法难以应对的恶劣视觉场景中。

Conclusion: 通过连接视觉和语言模态，VL-SurgPT能够开发上下文感知的跟踪系统，在挑战性术中条件下保持性能，对推进计算机辅助手术应用至关重要。

Abstract: Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.

</details>


### [23] [VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation](https://arxiv.org/abs/2511.12030)
*Jun Zhou,Chi Xu,Kaifeng Tang,Yuting Ge,Tingrui Guo,Li Cheng*

Main category: cs.CV

TL;DR: 提出了一种结合视觉和物理线索的联合框架，用于从单张RGB图像估计手和物体的3D姿态，解决了现有方法违反物理约束的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖视觉线索，常产生违反物理约束的结果，而基于物理推理的方法通常依赖后优化或不可微物理引擎，影响视觉一致性和端到端可训练性。

Method: 通过两个关键思想实现：1）联合视觉-物理线索学习，训练模型提取2D视觉线索和3D物理线索；2）候选姿态聚合，利用视觉和物理预测聚合多个扩散生成的候选姿态。

Result: 大量实验表明，该方法在姿态精度和物理合理性方面显著优于现有最先进方法。

Conclusion: 提出的框架成功整合了视觉和物理线索，实现了视觉一致且物理合理的手-物体姿态估计。

Abstract: Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.

</details>


### [24] [Improved Masked Image Generation with Knowledge-Augmented Token Representations](https://arxiv.org/abs/2511.12032)
*Guotao Liang,Baoquan Zhang,Zhiyuan Wen,Zihao Han,Yunming Ye*

Main category: cs.CV

TL;DR: KA-MIG是一个知识增强的掩码图像生成框架，通过引入三种token级语义依赖知识图（共现图、语义相似度图、位置-token不兼容图）来增强模型捕获语义依赖的能力，提高生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有掩码图像生成方法仅依赖模型自身学习视觉token序列的语义依赖，但由于单个token缺乏明确语义且序列较长，直接从数据中学习这种依赖关系具有挑战性。

Method: 提出KA-MIG框架，构建三种先验知识图（两个正图和一个负图），设计图感知编码器学习token和位置感知表示，并通过轻量级融合机制将这些增强表示集成到现有MIG方法中。

Result: 实验结果表明，该方法在ImageNet上的类条件图像生成任务中优于现有MIG方法。

Conclusion: 通过引入token级语义依赖的先验知识，KA-MIG有效增强了模型捕获语义依赖的能力，从而提高了生成质量。

Abstract: Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.

</details>


### [25] [Calibrated Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2511.12034)
*Xiaohao Liu,Xiaobo Xia,Jiaheng Wei,Shuo Yang,Xiu Su,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出CalMRL方法，通过表示级插补解决多模态学习中模态缺失导致的锚点偏移问题，使现有方法能够利用不完整模态数据。


<details>
  <summary>Details</summary>
Motivation: 现有多模态表示学习方法需要所有模态都完整存在，无法有效利用普遍存在的缺失模态数据集。从锚点偏移的理论视角分析，模态缺失会导致观测模态与局部锚点对齐，偏离最优锚点。

Method: 提出CalMRL方法，利用模态先验和内在联系在表示级对缺失模态进行建模插补。采用双步学习方法和共享潜在变量后验分布的闭式解解决优化困境。

Result: 理论验证了CalMRL能够缓解锚点偏移并保证收敛。实验表明该方法优于现有方法，为吸收缺失模态数据提供了新的灵活性。

Conclusion: CalMRL通过校准不完整对齐，有效解决了多模态学习中模态缺失的问题，扩展了现有方法的适用性。

Abstract: Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.

</details>


### [26] [SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images](https://arxiv.org/abs/2511.12040)
*Xinyuan Hu,Changyue Shi,Chuxiao Yang,Minghao Chen,Jiajun Ding,Tao Wei,Chen Wei,Zhou Yu,Min Tan*

Main category: cs.CV

TL;DR: SRSplat是一种前馈式3D重建框架，能从少量低分辨率图像重建高分辨率3D场景，通过结合外部参考图像和内部纹理线索来补偿纹理信息缺失。


<details>
  <summary>Details</summary>
Motivation: 现有方法从稀疏低分辨率图像进行3D重建时往往无法恢复精细纹理细节，这源于低分辨率输入中高频信息的固有缺失。

Method: 首先使用多模态大语言模型和扩散模型为每个场景构建特定参考图库，然后通过参考引导特征增强模块对齐融合低分辨率输入和参考图像特征，最后使用纹理感知密度控制自适应调整高斯密度。

Result: 在RealEstate10K、ACID和DTU等多个数据集上的广泛实验表明，SRSplat优于现有方法，并展现出强大的跨数据集和跨分辨率泛化能力。

Conclusion: SRSplat通过有效结合外部参考信息和内部纹理线索，成功解决了从稀疏低分辨率图像重建高质量3D场景的挑战。

Abstract: Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.

</details>


### [27] [FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification](https://arxiv.org/abs/2511.12044)
*Cheng-Chang Tsai,Kai-Wen Cheng,Chun-Shien Lu*

Main category: cs.CV

TL;DR: 提出FedSDA方法，通过扩散模型和染色分离技术对齐联邦学习中非IID组织病理图像的染色分布，缓解分布偏移问题，同时避免隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中非IID数据（特别是组织病理图像）的特征分布偏移问题严重阻碍了模型性能，需要从数据分布角度解决这一挑战。

Method: 基于扩散模型拟合数据分布，利用染色分离提取关键特征，提出FedSDA方法在FL框架中对齐各客户端的染色分布与目标分布。

Result: 实验结果表明FedSDA能有效提升基线方法性能，在缓解客户端间分布偏移方面优于其他从数据分布角度处理非IID问题的方法。

Conclusion: FedSDA为计算病理学社区提供了有价值且实用的见解，能够有效解决联邦学习中非IID组织病理图像的分布对齐问题。

Abstract: Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.

</details>


### [28] [DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging](https://arxiv.org/abs/2511.12047)
*Huimin Cheng,Xiaowei Yu,Shushan Wu,Luyang Fang,Chao Cao,Jing Zhang,Tianming Liu,Dajiang Zhu,Wenxuan Zhong,Ping Ma*

Main category: cs.CV

TL;DR: DCMM-Transformer是一种用于医学图像分析的新型ViT架构，通过将度修正混合成员模型作为自注意力中的加性偏置来整合解剖学分组结构，解决了现有方法在可微性、训练稳定性和复杂社区结构建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 医学图像中存在潜在解剖学分组（如器官、组织和病理区域），但标准ViT无法利用这些结构。现有方法如SBM-Transformer通过随机二值掩码整合结构，但存在不可微性、训练不稳定和无法建模复杂社区结构的问题。

Method: 提出DCMM-Transformer架构，将度修正混合成员模型作为自注意力中的加性偏置引入，以完全可微和可解释的方式整合社区结构和度异质性，避免了乘性掩码和二值采样。

Result: 在包括脑部、胸部、乳腺和眼部等多种医学成像数据集上的综合实验表明，该方法具有优越的性能和泛化能力。

Conclusion: 学习到的分组结构和结构化注意力调制通过产生解剖学上有意义且语义连贯的注意力图，显著增强了可解释性。

Abstract: Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.

</details>


### [29] [DeiTFake: Deepfake Detection Model using DeiT Multi-Stage Training](https://arxiv.org/abs/2511.12048)
*Saksham Kumar,Ashish Singh,Srinivasarao Thota,Sunil Kumar Singh,Chandan Kumar*

Main category: cs.CV

TL;DR: 提出DeiTFake方法，基于DeiT变换器和两阶段渐进训练策略，通过知识蒸馏捕获深度伪造的细微伪影，在OpenForensics数据集上达到99.22%准确率和0.9997 AUROC。


<details>
  <summary>Details</summary>
Motivation: 深度伪造对数字媒体完整性构成重大威胁，需要开发更有效的检测方法来应对这一挑战。

Method: 使用DeiT变换器架构和两阶段渐进训练策略：第一阶段使用标准增强的迁移学习，第二阶段使用高级仿射和深度伪造特定增强进行微调。

Result: 在OpenForensics数据集（190,335张图像）上，第一阶段达到98.71%准确率，第二阶段达到99.22%准确率和0.9997 AUROC，优于最新的OpenForensics基线。

Conclusion: DeiTFake方法通过知识蒸馏和渐进训练策略有效提升了面部深度伪造检测性能，为实际应用提供了实用基准。

Abstract: Deepfakes are major threats to the integrity of digital media. We propose DeiTFake, a DeiT-based transformer and a novel two-stage progressive training strategy with increasing augmentation complexity. The approach applies an initial transfer-learning phase with standard augmentations followed by a fine-tuning phase using advanced affine and deepfake-specific augmentations. DeiT's knowledge distillation model captures subtle manipulation artifacts, increasing robustness of the detection model. Trained on the OpenForensics dataset (190,335 images), DeiTFake achieves 98.71\% accuracy after stage one and 99.22\% accuracy with an AUROC of 0.9997, after stage two, outperforming the latest OpenForensics baselines. We analyze augmentation impact and training schedules, and provide practical benchmarks for facial deepfake detection.

</details>


### [30] [UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization](https://arxiv.org/abs/2511.12054)
*Cuiqun Chen,Qi Chen,Bin Yang,Xingyi Zhang*

Main category: cs.CV

TL;DR: UniABG是一个新颖的双阶段无监督跨视角地理定位框架，通过对抗性视角桥接和图基对应校准来解决无监督方法中的伪标签噪声问题，在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 监督方法依赖大量成对标注限制了可扩展性，而无监督方法由于跨视角域差距导致伪标签噪声严重，需要开发更有效的无监督跨视角地理定位方法。

Method: 提出双阶段框架：1) 视角感知对抗桥接(VAAB)建模视角不变特征增强伪标签鲁棒性；2) 异构图过滤校准(HGFC)通过构建双跨视角结构图来精化跨视角关联。

Result: 在University-1652数据集上卫星→无人机AP提升+10.63%，在SUES-200数据集上提升+16.73%，甚至超越了监督基线方法。

Conclusion: UniABG通过结合对抗学习和图结构建模，有效解决了无监督跨视角地理定位中的伪标签噪声问题，实现了优异的性能表现。

Abstract: Cross-view geo-localization (CVGL) matches query images ($\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\rightarrow$ Drone AP by +10.63\% on University-1652 and +16.73\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG

</details>


### [31] [PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling](https://arxiv.org/abs/2511.12056)
*Sijie Wang,Qiang Wang,Shaohuai Shi*

Main category: cs.CV

TL;DR: PipeDiT是一个加速视频生成的流水线框架，通过三个创新技术解决扩散变换器模型推理速度慢和内存消耗高的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器模型在视频生成方面表现出色，但实际部署受到推理速度慢和内存消耗高的限制，需要优化方案。

Method: 1. 为序列并行设计流水线算法PipeSP；2. 提出DeDiVAE将扩散模块和VAE模块解耦到不同GPU组；3. 提出注意力协同处理方法Aco优化GPU资源利用。

Result: 在8-GPU系统上测试，PipeDiT在OpenSoraPlan和HunyuanVideo框架上实现了1.06倍到4.02倍的加速效果。

Conclusion: PipeDiT框架通过创新的流水线技术有效提升了视频生成模型的推理效率，具有实际部署价值。

Abstract: Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.

</details>


### [32] [MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity](https://arxiv.org/abs/2511.12061)
*Zhichen Lai,Hua Lu,Huan Li,Jialiang Li,Christian S. Jensen*

Main category: cs.CV

TL;DR: MovSemCL是一个用于轨迹相似性计算的运动语义对比学习框架，通过将GPS轨迹转换为运动语义特征并分块处理，使用内外块注意力编码局部和全局模式，结合曲率引导的数据增强策略，在减少计算成本的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于学习的轨迹相似性计算方法存在的三个关键问题：轨迹语义和层次结构建模不足、点级编码计算成本高、以及使用物理上不可行的数据增强方法扭曲轨迹语义。

Method: 提出MovSemCL框架：1）将原始GPS轨迹转换为运动语义特征并分块；2）使用内外块注意力机制编码局部和全局轨迹模式；3）采用曲率引导的数据增强策略，保留信息丰富的轨迹段（如转弯和交叉口）并屏蔽冗余段。

Result: 在真实世界数据集上的实验表明，MovSemCL在相似性搜索任务中平均排名接近理想值1，在启发式近似任务中性能提升高达20.3%，同时推理延迟降低高达43.4%。

Conclusion: MovSemCL通过有效的运动语义建模、层次表示和物理合理的数据增强，显著提升了轨迹相似性计算的性能，同时降低了计算成本。

Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.

</details>


### [33] [DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal](https://arxiv.org/abs/2511.12066)
*Jialang Lu,Shuning Sun,Pu Wang,Chen Wu,Feng Gao,Lina Gong,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: DCA-LUT是首个基于深度学习的紫色边缘去除框架，通过色度感知坐标变换模块分离紫色边缘到专用维度，并使用5D查找表进行高效颜色校正。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖昂贵的光学硬件和手工特征提取，忽视了数据驱动方法。紫色边缘是由镜头色差引起的RGB通道空间错位问题，需要更智能的解决方案。

Method: 提出色度感知坐标变换模块学习图像自适应颜色空间，将紫色边缘分离到专用维度；使用5D查找表进行非线性颜色映射；构建大规模合成数据集PF-Synth进行训练。

Result: 在合成和真实数据集上的广泛实验表明，该方法在紫色边缘去除方面达到了最先进的性能。

Conclusion: DCA-LUT框架通过深度学习方法有效解决了紫色边缘问题，无需依赖昂贵的硬件解决方案，在图像质量恢复方面表现出色。

Abstract: Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.

</details>


### [34] [Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound](https://arxiv.org/abs/2511.12077)
*Dengming Zhang,Weitao You,Jingxiong Li,Weishen Lin,Wenda Shi,Xue Zhao,Heda Zuo,Junxian Wu,Lingyun Sun*

Main category: cs.CV

TL;DR: VAEmotionLLM是一个两阶段框架，通过有限的音频预训练教视觉语言模型具备听觉能力，并增强跨模态情感理解。第一阶段使用视觉引导的音频对齐，第二阶段通过轻量级跨模态情感适配器注入情感敏感残差和应用情感监督。


<details>
  <summary>Details</summary>
Motivation: 当前音频-视觉语言模型通常需要大规模音频预训练来赋予视觉语言模型听觉能力，这限制了可扩展性。同时，大多数先前工作是人类中心或单模态的，忽视了艺术作品有意表达的情感。

Method: 两阶段框架：第一阶段VG-Align通过对齐同步音频-视频剪辑的共享LLM的下一个标记分布，将冻结的视觉路径蒸馏到新的音频路径；第二阶段EmoAdapter包含情感增强器和情感监督器，注入情感敏感残差并应用情感监督。

Result: 在ArtEmoBenchmark上实现了最先进的结果，优于音频、视觉和音频-视觉基线。消融研究表明所提出的组件是互补的。

Conclusion: VAEmotionLLM通过有限的音频预训练成功教视觉语言模型具备听觉能力，并显著增强了跨模态情感理解，在艺术中心情感基准上表现优异。

Abstract: Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.

</details>


### [35] [Point Cloud Quantization through Multimodal Prompting for 3D Understanding](https://arxiv.org/abs/2511.12079)
*Hongxuan Li,Wencheng Zhu,Huiying Xu,Xinzhong Zhu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出一种基于多模态提示的向量量化框架，通过文本嵌入作为原型先验，结合紧凑性和分离正则化，实现点云数据的几何和语义信息联合编码。


<details>
  <summary>Details</summary>
Motivation: 当前基于可训练向量或聚类质心的原型方法在代表性和可解释性方面存在不足，需要更强大的码本设计来提升向量量化在多模态模型中的效果。

Method: 利用预训练模型的文本嵌入作为视觉语义原型先验，通过多模态提示进行自适应精炼，构建双约束量化空间，并采用Gumbel-Softmax松弛实现可微离散化。

Result: 在ModelNet40和ScanObjectNN数据集上的广泛实验表明，该方法具有优越的有效性。

Conclusion: 所提出的多模态提示驱动量化框架能够有效缓解视觉-语言语义差距，生成同时编码几何和语义信息的混合表示，显著提升点云分析性能。

Abstract: Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.

</details>


### [36] [Supervised Multilabel Image Classification Using Residual Networks with Probabilistic Reasoning](https://arxiv.org/abs/2511.12082)
*Lokender Singh,Saksham Kumar,Chandan Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种基于改进ResNet-101架构和概率推理的多标签图像分类方法，在COCO-2014数据集上实现了0.794 mAP的优异性能，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 多标签图像分类在计算机视觉应用中具有重要价值，但传统方法难以有效处理标签间的依赖关系和不确定性。本文旨在通过集成概率推理来应对多标签场景中的挑战。

Method: 使用改进的ResNet-101架构，通过概率推理模拟标签依赖关系和不确定性，提升多标签图像分类的预测准确性。

Result: 在COCO-2014数据集上实现了0.794 mAP的优异性能，超越了ResNet-SRN（0.771）和Vision Transformer基线（0.785），在精度-召回率等指标上表现突出。

Conclusion: 将概率推理集成到深度学习模型中能够有效解决多标签场景的挑战，该方法在多标签图像分类任务中达到了先进水平。

Abstract: Multilabel image categorization has drawn interest recently because of its numerous computer vision applications. The proposed work introduces a novel method for classifying multilabel images using the COCO-2014 dataset and a modified ResNet-101 architecture. By simulating label dependencies and uncertainties, the approach uses probabilistic reasoning to improve prediction accuracy. Extensive tests show that the model outperforms earlier techniques and approaches to state-of-the-art outcomes in multilabel categorization. The work also thoroughly assesses the model's performance using metrics like precision-recall score and achieves 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785). The novelty of the work lies in integrating probabilistic reasoning into deep learning models to effectively address the challenges presented by multilabel scenarios.

</details>


### [37] [SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving](https://arxiv.org/abs/2511.12084)
*Ji-Ping Jin,Chen-Bin Feng,Rui Fan,Chi-Man Vong*

Main category: cs.CV

TL;DR: 提出SemanticStitch深度学习框架，通过引入语义先验信息来解决图像拼接中的前景物体连续性问题，显著提升拼接质量。


<details>
  <summary>Details</summary>
Motivation: 传统图像拼接方法因拍摄角度、位置差异和物体移动导致错位和视觉不一致，且传统接缝裁剪方法忽略语义信息，破坏前景连续性。

Method: 开发基于深度学习的SemanticStitch框架，包含强调显著物体语义完整性的新型损失函数，并构建两个专用真实世界数据集进行评估。

Result: 实验结果显示相比传统技术有显著改进，为实际应用提供有力支持。

Conclusion: SemanticStitch通过语义先验信息有效保持前景物体完整性，增强视觉连贯性，在图像拼接任务中表现优异。

Abstract: Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.

</details>


### [38] [Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning](https://arxiv.org/abs/2511.12090)
*Shengqin Jiang,Tianqi Kong,Yuankai Qi,Haokui Zhang,Lina Yao,Quan Z. Sheng,Qingshan Liu,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种分层分组提示调优方法，通过层分组共享提示和根提示生成子提示，减少层间独立更新，缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的持续学习方法在每个层独立添加任务特定提示，虽然灵活性高但容易导致某些层不必要更新，增加灾难性遗忘风险。

Method: 分层分组提示调优：1）同组层共享相似提示，保持预训练模型特征关系；2）使用单一根提示生成各层组子提示，增强协同性。

Result: 在四个基准测试上的广泛实验表明，该方法相比多个最先进方法取得了优越性能。

Conclusion: 所提出的分层分组提示调优方法通过增强模型稳定性和减少层间独立性，有效缓解了灾难性遗忘问题。

Abstract: Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.

</details>


### [39] [Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks](https://arxiv.org/abs/2511.12097)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Xudong Jiang,Dacheng Tao*

Main category: cs.CV

TL;DR: SpikeNM是首个面向SNN的半结构化N:M剪枝框架，通过M路基对数参数化和可微分top-k采样器线性化复杂度，结合神经科学启发的资格蒸馏方法，在保持精度的同时实现硬件友好的稀疏模式。


<details>
  <summary>Details</summary>
Motivation: 解决SNN深度架构参数膨胀和计算成本高的问题，现有非结构化剪枝难以硬件加速，结构化剪枝灵活性差且精度下降，需要一种兼顾硬件友好性和精度的半结构化剪枝方法。

Method: 采用M路基对数参数化和可微分top-k采样器，将每块复杂度从指数级降低到线性级；提出资格蒸馏方法，将时间累积信用转换为块级软目标，减少采样方差。

Result: 在2:4稀疏度下，SpikeNM在主流数据集上保持甚至提升精度，同时产生硬件友好的稀疏模式，与内在脉冲稀疏性互补。

Conclusion: SpikeNM成功实现了SNN的半结构化剪枝，在保持精度的同时提供了硬件友好的稀疏模式，为SNN的边缘部署提供了有效解决方案。

Abstract: Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \textbf{SpikeNM}, the first SNN-oriented \emph{semi-structured} \(N{:}M\) pruning framework that learns sparse SNNs \emph{from scratch}, enforcing \emph{at most \(N\)} non-zeros per \(M\)-weight block. To avoid the combinatorial space complexity \(\sum_{k=1}^{N}\binom{M}{k}\) growing exponentially with \(M\), SpikeNM adopts an \(M\)-way basis-logit parameterization with a differentiable top-\(k\) sampler, \emph{linearizing} per-block complexity to \(\mathcal O(M)\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \(2{:}4\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.

</details>


### [40] [DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT](https://arxiv.org/abs/2511.12098)
*Xianhao Zhou,Jianghao Wu,Ku Zhao,Jinlong He,Huangxuan Zhao,Lei Chen,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: 提出DGCF框架，结合冻结自监督DINOv3 Transformer和可训练CNN编码器-解码器，通过交叉融合模块平衡局部外观和上下文表示，在SynthRAD2023盆腔数据集上实现最先进的MRI→CT和CBCT→CT转换性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型缺乏全局语义理解，而Transformer在小规模医学数据集上容易过拟合，需要平衡局部特征和全局语义表示。

Method: DGCF框架：冻结DINOv3 Transformer + 可训练CNN编码器-解码器 + 可学习交叉融合模块 + 多级DINOv3感知损失函数。

Result: 在SynthRAD2023盆腔数据集上，DGCF在MS-SSIM、PSNR和基于分割的指标上均达到最先进水平，适用于MRI→CT和CBCT→CT转换任务。

Conclusion: 这是首个将DINOv3表示用于医学图像转换的工作，展示了自监督Transformer指导在语义感知CT合成中的潜力。

Abstract: Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\rightarrow$CT and CBCT$\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at https://github.com/HiLab-git/DGCF.

</details>


### [41] [Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models](https://arxiv.org/abs/2511.12099)
*Tianle Cheng,Zeyan Zhang,Kaifeng Gao,Jun Xiao*

Main category: cs.CV

TL;DR: 本文提出Adaptive Begin-of-Video Tokens (ada-BOV)方法，用于改进自回归视频扩散模型，解决现有方法在生成长视频时的一致性和动态质量不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成长视频时面临两个主要挑战：基于分块扩展的方法存在去噪延迟和错误累积问题，而流去噪方法在一致性和运动动态方面表现不佳。需要一种能够保持全局一致性同时提升局部动态质量的方法。

Method: 提出自适应视频开始令牌(ada-BOV)，通过类似自适应层归一化的调制机制自适应吸收去噪的前帧；提出流去噪细化策略，将采样轨迹长度与注意力窗口大小约束解耦；提出扰动增强训练噪声调度，平衡收敛速度与模型鲁棒性。

Result: 大量实验表明，该方法在多个指标上取得了令人信服的定性和定量结果，显著提升了长视频生成的一致性和动态质量。

Conclusion: ada-BOV方法有效解决了自回归视频扩散模型在生成长视频时的关键挑战，在保持全局一致性的同时显著提升了局部动态质量，为高质量长视频生成提供了有效解决方案。

Abstract: Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.

</details>


### [42] [Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation](https://arxiv.org/abs/2511.12100)
*Yannan Chen,Ruoyu Chen,Bin Zeng,Wei Wang,Shiming Liu,Qunli Zhang,Zheng Hu,Laiyuan Wang,Yaowei Wang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出了SS-CA方法，通过反事实解释增强训练，解决视觉模型依赖有限充分原因导致对分布偏移敏感的问题，提高模型泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型训练中，模型往往仅依赖有限的充分原因进行预测，导致对分布偏移或关键特征缺失敏感。虽然归因方法能准确识别关键区域，但遮蔽这些区域创建反事实时模型会误分类，而人类仍能识别，表明模型学习到的依赖关系可能不够因果充分。

Method: 基于子集选择的LIMA归因方法开发Counterfactual LIMA，识别最小空间区域集合，其移除能选择性改变模型预测。利用这些归因，提出数据增强策略，将识别区域替换为自然背景，并在增强样本和原始样本上联合训练模型，以缓解不完全因果学习。

Result: 在多个ImageNet变体上的广泛实验表明，SS-CA提高了在分布内测试数据的泛化能力，并在分布外基准（如ImageNet-R和ImageNet-S）上获得优越性能。在包括噪声在内的扰动下，使用SS-CA训练的模型也表现出增强的泛化能力。

Conclusion: 该方法有效利用可解释性洞察来纠正模型缺陷，提高性能和鲁棒性，证明将反事实解释整合到训练过程中可以改善模型的因果学习能力。

Abstract: In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.

</details>


### [43] [BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation](https://arxiv.org/abs/2511.12103)
*Sayad Ibna Azad,Md. Atiqur Rahman*

Main category: cs.CV

TL;DR: BdSL-SPOTER是一个基于姿态的transformer框架，用于准确高效地识别孟加拉手语，在BdSLW60基准测试中达到97.92%的Top-1验证准确率，比基线提升22.82%，同时保持低计算成本。


<details>
  <summary>Details</summary>
Motivation: 为孟加拉手语开发一个准确且高效的手语识别框架，解决低资源区域手语识别问题，并为其他低资源区域手语提供可扩展模型。

Method: 扩展SPOTER范式，包含文化特定的预处理、紧凑的四层transformer编码器（优化可学习位置编码），并采用课程学习来增强泛化能力和加速收敛。

Result: 在BdSLW60基准测试中达到97.92%的Top-1验证准确率，比Bi-LSTM基线提升22.82%，同时参数数量更少、FLOPs更低、FPS更高。

Conclusion: BdSL-SPOTER为现实世界的无障碍应用提供了实用框架，并可作为其他低资源区域手语的可扩展模型。

Abstract: We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a 22.82% improvement over the Bi-LSTM baseline, all while keeping computational costs low. With its reduced number of parameters, lower FLOPs, and higher FPS, BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.

</details>


### [44] [TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery](https://arxiv.org/abs/2511.12104)
*Tammy Glazer,Gilles Q. Hacheme,Akram Zaytar,Luana Marotti,Amy Michaels,Girmaw Abebe Tadesse,Kevin White,Rahul Dodhia,Andrew Zolli,Inbal Becker-Reshef,Juan M. Lavista Ferres,Caleb Robinson*

Main category: cs.CV

TL;DR: TEMPO是一个全球性的时间分辨数据集，通过深度学习模型从高分辨率卫星图像中提取建筑密度和高度信息，提供从2018年第一季度到2025年第二季度的季度性全球建筑地图。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够大规模监测全球建筑发展模式和气候影响的工具，为全球韧性和适应工作提供支持。

Method: 使用现有的建筑足迹和高度数据与季度PlanetScope卫星图像配对，训练多任务深度学习模型，以37.6米/像素的分辨率预测建筑密度和高度。

Result: 模型在不同手动标记子集上达到85%至88%的F1分数，时间稳定性高，五年趋势一致性得分为0.96，能够以较低计算成本捕获季度建筑变化。

Conclusion: TEMPO数据集能够高效地监测全球建筑变化，为大规模发展模式和气候影响监测提供了可行解决方案。

Abstract: We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.

</details>


### [45] [Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection](https://arxiv.org/abs/2511.12107)
*Tianxiang Zhang,Peipeng Yu,Zhihua Xia,Longchen Dai,Xiaoyu Zhou,Hui Gao*

Main category: cs.CV

TL;DR: 提出了DFF-Adapter方法，通过轻量级多头部LoRA模块适配DINOv2，同时处理真实性检测和细粒度伪造方法分类，仅需3.5M可训练参数即可达到或超越现有复杂方法的检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于DINOv2的深度伪造检测方法将其视为通用二元分类，忽略了不同伪造方法产生的独特伪影特征。

Method: 在DINOv2的每个transformer块中集成轻量级多头部LoRA模块，引入共享分支将细粒度伪造方法线索传播到真实性检测头，实现多任务协同优化。

Result: 仅使用3.5M可训练参数，检测精度达到或超越了当前复杂的先进方法。

Conclusion: DFF-Adapter通过参数高效的方式，利用细粒度伪造方法分类增强伪影敏感性，显著提升了深度伪造检测性能。

Abstract: The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.

</details>


### [46] [MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images](https://arxiv.org/abs/2511.12110)
*Qinyue Tong,Ziqian Lu,Jun Liu,Rui Zuo,Zheming Lu*

Main category: cs.CV

TL;DR: MEMR-Seg是一个新的多轮实体级医学推理分割任务，通过构建MR-MedSeg数据集和提出MediRound基线模型来解决传统医学分割方法缺乏多轮交互推理能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法大多任务特定且缺乏交互性，基于文本提示的方法虽然增强了用户驱动和推理能力，但仍局限于单轮对话，无法进行多轮推理。

Method: 构建了包含17.7万轮多轮医学分割对话的MR-MedSeg数据集，提出MediRound基线模型，并在推理阶段引入轻量级但有效的判断与校正机制来缓解多轮分割中的错误传播问题。

Result: 实验结果表明，该方法有效解决了MEMR-Seg任务，并优于传统的医学参考分割方法。

Conclusion: MEMR-Seg任务和MediRound模型为医学图像分割提供了多轮交互推理能力，通过判断与校正机制缓解了错误传播问题，在医学分割领域取得了显著进展。

Abstract: Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.

</details>


### [47] [RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving](https://arxiv.org/abs/2511.12117)
*Ruiqi Cheng,Huijun Di,Jian Li,Feng Liu,Wei Liang*

Main category: cs.CV

TL;DR: RadarMP是一种使用连续两帧毫米波雷达回波信号的3D场景运动感知方法，通过统一架构联合建模雷达目标检测和运动估计任务，在恶劣天气条件下实现精确的运动感知。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达因其全天候工作能力和独特感知特性成为自动驾驶系统的重要组成部分，但稀疏和嘈杂的雷达点往往导致运动感知不精确，在光学传感器性能下降时限制自动驾驶车辆的感知能力。

Method: 提出RadarMP方法，在统一架构中联合建模雷达目标检测和运动估计，设计基于多普勒频移和回波强度的自监督损失函数，无需显式标注即可监督空间和运动一致性。

Result: 在公共数据集上的广泛实验表明，RadarMP在不同天气和光照条件下实现了可靠的运动感知，优于基于雷达的解耦运动感知流程。

Conclusion: RadarMP提升了全场景自动驾驶系统的感知能力，为恶劣天气条件下的自动驾驶提供了可靠的3D场景运动感知解决方案。

Abstract: Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.

</details>


### [48] [OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description](https://arxiv.org/abs/2511.12131)
*Quanxing Xu,Ling Zhou,Feifei Zhang,Jinyu Tian,Rubing Huang*

Main category: cs.CV

TL;DR: 提出OAD-Promoter方法，通过缓解语言偏见和提升领域迁移鲁棒性来增强基于LLM的视觉问答系统，在少样本和零样本场景下取得新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在视觉问答中处理知识密集型问题时存在语言偏见和领域外泛化能力不足的问题，限制了其可靠性和泛化性能。

Method: 提出OAD-Promoter方法，包含三个组件：对象集中示例生成模块生成全局和区域视觉信息，记忆知识辅助模块检索相关知识处理领域外样本，OAD提示整合前两个模块输出优化LLM推理。

Result: 实验表明OAD-Promoter显著提升了基于LLM的视觉问答方法在少样本和零样本设置下的性能，达到了新的最优结果。

Conclusion: OAD-Promoter通过缓解语言偏见和增强领域迁移鲁棒性，有效提升了LLM在视觉问答任务中的表现，特别是在少样本和零样本场景下。

Abstract: Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.

</details>


### [49] [MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering](https://arxiv.org/abs/2511.12142)
*Seokwon Song,Minsu Park,Gunhee Kim*

Main category: cs.CV

TL;DR: MAVIS是首个评估多模态来源归因系统的基准，包含15.7万视觉问答实例，开发了信息量、可靠性和流畅性的自动评估指标，发现多模态RAG比单模态RAG生成更信息丰富和流畅的答案，但在图像文档的可靠性方面较弱。


<details>
  <summary>Details</summary>
Motivation: 现有来源归因工作主要关注纯文本场景，忽视了多模态的重要作用，需要开发能够理解视觉问题用户意图、检索多模态证据并生成带引用的长文本答案的系统。

Method: 构建包含15.7万视觉问答实例的数据集，每个答案都标注了引用多模态文档的事实级引用，开发了信息量、可靠性和流畅性的细粒度自动评估指标。

Result: 发现多模态RAG比单模态RAG生成更信息丰富和流畅的答案，但在图像文档的可靠性方面较弱；不同提示方法在信息量和可靠性之间存在权衡；提出缓解图像文档解释中的上下文偏差是未来研究的关键方向。

Conclusion: MAVIS基准为评估多模态来源归因系统提供了重要工具，揭示了多模态RAG在可靠性方面的挑战，并指出了缓解上下文偏差的未来研究方向。

Abstract: Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at https://github.com/seokwon99/MAVIS

</details>


### [50] [FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing](https://arxiv.org/abs/2511.12151)
*Kaixiang Yang,Boyang Shen,Xin Li,Yuchen Dai,Yuxuan Luo,Yueran Ma,Wei Fang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: FIA-Edit是一个基于频率交互注意力的免反演图像编辑框架，通过频率表示交互模块和特征注入模块实现高保真度的文本引导图像编辑，在计算效率和编辑质量方面优于现有方法，并首次扩展到医学图像应用。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的免反演方法虽然效率高，但由于缺乏有效的源信息整合，常常导致背景保留不佳、空间不一致和过度编辑等问题。

Method: 提出FIA-Edit框架，包含两个关键组件：频率表示交互模块（在自注意力中交换源和目标特征的频率分量）和特征注入模块（在交叉注意力中显式整合源侧查询、键、值和文本嵌入）。

Result: 在RTX 4090上每张512*512图像仅需约6秒，在视觉质量、背景保真度和可控性方面持续优于现有方法，并成功应用于医学图像出血合成，显著提升下游出血分类性能。

Conclusion: FIA-Edit实现了高效高保真的文本引导图像编辑，为医学数据增强开辟了新机会，在临床应用中具有重要价值。

Abstract: Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.

</details>


### [51] [Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function](https://arxiv.org/abs/2511.12162)
*Shuo Yin,Zhiyuan Yin,Yuqing Hou,Rui Liu,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的中心重分配哈希（CRH）框架，通过动态重分配预设码本中的哈希中心来联合优化哈希函数，避免了传统两阶段方法的复杂性和性能损失。


<details>
  <summary>Details</summary>
Motivation: 现有的基于哈希中心的深度哈希方法虽然比成对或三元组方法更高效，但随机中心初始化忽略了类间语义关系，而两阶段方法虽然能缓解这个问题，但引入了额外的复杂性和计算开销，且由于阶段间差异导致性能次优。

Method: 提出CRH框架，动态重分配预设码本中的哈希中心，同时联合优化哈希函数，无需显式的中心优化阶段；采用多头机制增强哈希中心的表示能力，捕获更丰富的语义结构。

Result: 在三个基准数据集上的广泛实验表明，CRH能够学习到具有语义意义的哈希中心，并在检索任务中优于最先进的深度哈希方法。

Conclusion: CRH通过端到端的动态中心重分配机制，有效整合了语义关系到学习过程中，避免了传统两阶段方法的局限性，在哈希检索任务中取得了优越性能。

Abstract: Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.

</details>


### [52] [Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective](https://arxiv.org/abs/2511.12170)
*Wang Luo,Di Wu,Hengyuan Na,Yinlin Zhu,Miao Hu,Guocong Quan*

Main category: cs.CV

TL;DR: 本文提出了一种新的点云补全范式Completion-by-Correction，通过从预训练图像到3D模型生成拓扑完整的形状先验，然后在特征空间进行校正，而不是传统的补全-修复方法。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态点云补全方法采用Completion-by-Inpainting范式，经常导致结构不一致和拓扑伪影，因为几何和语义约束有限。

Method: 提出了PGNet多阶段框架：进行双特征编码以支撑生成先验，合成粗粒度但结构对齐的支架，通过分层校正逐步细化几何细节。

Result: 在ShapeNetViPC数据集上的实验显示，PGNet在平均Chamfer距离上优于最先进基线方法23.5%，F-score提高7.1%。

Conclusion: Completion-by-Correction范式将补全从无约束合成转变为引导式细化，能够实现结构一致且与观测对齐的重建。

Abstract: Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).

</details>


### [53] [MixAR: Mixture Autoregressive Image Generation](https://arxiv.org/abs/2511.12181)
*Jinyuan Hu,Jiayou Zhang,Shaobo Cui,Kun Zhang,Guangyi Chen*

Main category: cs.CV

TL;DR: MixAR是一个新颖的自回归图像生成框架，通过混合离散和连续表示来解决连续空间中自回归建模的挑战，利用离散标记作为先验指导来提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统自回归方法使用离散标记会丢失细粒度信息，而连续空间建模虽然质量更高但面临空间巨大且无结构的挑战，需要找到平衡方案。

Method: 提出MixAR框架，采用因子化公式，利用离散标记作为连续自回归预测的先验指导。探索了多种离散-连续混合策略：自注意力(DC-SA)、交叉注意力(DC-CA)和简单的DC-Mix方法。还提出了训练-推理混合(TI-Mix)来弥合训练和生成分布之间的差距。

Result: 实验表明DC-Mix策略在计算效率和生成保真度之间取得了良好平衡，TI-Mix带来了持续改进。

Conclusion: MixAR通过混合离散和连续表示成功解决了连续自回归建模的挑战，在保持计算效率的同时显著提升了生成质量。

Abstract: Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.

</details>


### [54] [Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System](https://arxiv.org/abs/2511.12196)
*Aditi Bhalla,Christian Hellert,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: 提出了一种两阶段跨视角、跨模态的无监督域自适应框架，用于解决驾驶员活动识别中的视角变化和域偏移问题，显著提升了模型在真实部署中的性能。


<details>
  <summary>Details</summary>
Motivation: 驾驶员分心是交通事故的主要原因，现有深度学习方法在真实部署中面临两个关键挑战：摄像机视角变化（跨视角）和域偏移（如传感器模态或环境变化）。现有方法通常单独处理跨视角泛化或无监督域自适应，缺乏联合解决方案。

Method: 采用两阶段框架：第一阶段使用对比学习在多视角数据上学习视角不变和动作区分性特征；第二阶段使用信息瓶颈损失进行跨模态域自适应，无需新域的标注数据。使用Video Swin和MViT等视频变换器在Drive&Act多模态数据集上进行评估。

Result: 联合框架相比基于监督对比学习的跨视角方法，在RGB视频数据上的top-1准确率提升了近50%；相比仅使用无监督域自适应的方法，性能提升了高达5%。

Conclusion: 该框架能够有效解决驾驶员活动识别中的跨视角和跨模态挑战，为模型在不同车辆配置中的鲁棒和可扩展部署提供了解决方案。

Abstract: Driver distraction remains a leading cause of road traffic accidents, contributing to thousands of fatalities annually across the globe. While deep learning-based driver activity recognition methods have shown promise in detecting such distractions, their effectiveness in real-world deployments is hindered by two critical challenges: variations in camera viewpoints (cross-view) and domain shifts such as change in sensor modality or environment. Existing methods typically address either cross-view generalization or unsupervised domain adaptation in isolation, leaving a gap in the robust and scalable deployment of models across diverse vehicle configurations. In this work, we propose a novel two-phase cross-view, cross-modal unsupervised domain adaptation framework that addresses these challenges jointly on real-time driver monitoring data. In the first phase, we learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. In the second phase, we perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain. We evaluate our approach using state-of-the art video transformers (Video Swin, MViT) and multi modal driver activity dataset called Drive&Act, demonstrating that our joint framework improves top-1 accuracy on RGB video data by almost 50% compared to a supervised contrastive learning-based cross-view method, and outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.

</details>


### [55] [OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs](https://arxiv.org/abs/2511.12201)
*Feng Chen,Yefei He,Shaoxuan He,Yuanyu He,Jing Liu,Lequan Lin,Akide Liu,Zhaoyang Li,Jiyuan Zhang,Zhenbang Sun,Bohan Zhuang,Qi Wu*

Main category: cs.CV

TL;DR: OmniSparse是一个训练感知的细粒度稀疏注意力框架，用于长视频多模态大语言模型，在训练和推理中实现动态token预算分配，达到与全注意力相当的性能，同时获得2.7倍预填充加速和2.4倍解码内存减少。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法主要针对推理时加速，但存在训练-推理差距，且缺乏跨查询、键值对和注意力头的细粒度token选择能力，导致性能次优和加速效果有限。

Method: 包含三个自适应互补机制：1) 通过懒-活跃分类的查询选择；2) 基于最平坦头的动态预算分配的KV选择；3) 根据头级解码查询模式选择性获取视觉KV缓存的KV缓存精简。

Result: 实验结果显示OmniSparse在保持全注意力性能的同时，实现了预填充阶段2.7倍加速和解码阶段2.4倍内存减少。

Conclusion: OmniSparse框架有效解决了训练-推理差距问题，通过细粒度的动态token预算分配机制，在长视频MLLMs中实现了性能与效率的良好平衡。

Abstract: Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.

</details>


### [56] [LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image](https://arxiv.org/abs/2511.12202)
*Zhuojiang Cai,Yiheng Zhang,Meitong Guo,Mingdao Wang,Yuwang Wang*

Main category: cs.CV

TL;DR: LSS3D是一种高质量图像到3D生成方法，通过可学习空间位移参数解决多视图不一致性和非正面输入视角问题，在几何细节和纹理质量方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的多视图扩散3D生成方法存在形状和纹理不对齐问题，导致几何细节不完整和纹理重影，且对倾斜视角输入鲁棒性差。

Method: 为每个视图分配可学习的空间位移参数，通过重建网格引导调整各视图实现空间一致性，并加入输入视图作为额外约束增强对非正面视角的鲁棒性。

Result: 在广泛的实验中，该方法在几何和纹理评估指标上始终取得领先结果，特别是在更灵活的输入视角下表现优异。

Conclusion: LSS3D通过可学习空间位移机制有效解决了多视图不一致性问题，提供了高质量的3D生成结果，并为社区提供了全面的定量评估流程。

Abstract: Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.

</details>


### [57] [GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction](https://arxiv.org/abs/2511.12204)
*Jiaqi Wu,Yaosen Chen,Shuyuan Zhu*

Main category: cs.CV

TL;DR: 提出了一种几何引导的多视图扩散模型，通过提取多视图几何信息和调整几何特征强度，生成跨视图一致且细节丰富的图像。


<details>
  <summary>Details</summary>
Motivation: 多视图图像生成在3D重建、虚拟现实和增强现实等领域具有重要应用价值。现有方法在保持跨视图一致性和生成高分辨率输出方面面临显著计算挑战。

Method: 设计多视图几何信息提取模块，利用深度图、法线图和前景分割掩码构建共享几何结构；开发解耦的几何增强注意力机制；应用自适应学习策略；采用迭代细化过程；提出动态几何信息强度调整机制。

Result: 该方法能够生成跨视图一致且细节丰富的图像，提高了图像质量和细节保持能力。

Conclusion: 所提出的几何引导多视图扩散模型有效解决了多视图图像生成中的一致性和细节问题，为相关应用提供了高质量的解决方案。

Abstract: Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: https://github.com/SobeyMIL/GeoMVD.com.

</details>


### [58] [A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR](https://arxiv.org/abs/2511.12206)
*Nishant Vasantkumar Hegde,Aditi Agarwal,Minal Moharir*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的自动化交通违规检测系统，使用YOLOv8进行目标检测和EasyOCR进行车牌识别，能够检测头盔违规、摩托车后视镜缺失等交通违规行为，并提取车辆注册号码。


<details>
  <summary>Details</summary>
Motivation: 道路安全是全球关键问题，手动执行头盔法规和车辆安全标准资源密集且不一致，需要自动化解决方案来提高执法效率和道路安全。

Method: 系统采用YOLOv8进行目标检测和EasyOCR进行车牌识别，使用自定义标注图像数据集（经过数据增强处理），通过Streamlit界面实现实时监控和违规记录，并采用先进的图像预处理技术提升车牌识别性能。

Result: 模型整体精度达到0.9147，召回率为0.886，平均精度(mAP@50)为0.843，mAP@50 95为0.503，表明在严格IoU阈值下仍具有强大的检测能力。

Conclusion: 这项工作展示了一个实用有效的自动化交通规则执法解决方案，并讨论了实际部署的考虑因素。

Abstract: Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.

</details>


### [59] [Mixture of States: Routing Token-Level Dynamics for Multimodal Generation](https://arxiv.org/abs/2511.12207)
*Haozhe Liu,Ding Liu,Mingchen Zhuge,Zijian Zhou,Tian Xie,Sen He,Yukang Yang,Shuming Liu,Yuren Cong,Jiadong Guo,Hongyu Xu,Ke Xu,Kam-Woh Ng,Juan C. Pérez,Juan-Manuel~Pérez-Rúa,Tao Xiang,Wei Liu,Shikun Liu,Jürgen Schmidhuber*

Main category: cs.CV

TL;DR: MoS是一种新颖的多模态扩散模型融合范式，通过可学习的token-wise路由器实现模态间的灵活状态交互，在文本到图像生成和编辑任务中达到最先进效果，参数量仅为3B-5B但性能优于4倍大的模型。


<details>
  <summary>Details</summary>
Motivation: 开发一种计算效率高的多模态扩散模型融合方法，通过灵活的模态交互来提升性能，同时减少参数量和计算开销。

Method: 使用可学习的token-wise路由器，根据去噪时间步和输入创建模态间隐藏状态的交互，通过稀疏选择top-k隐藏状态和ε-greedy训练策略实现高效特征选择。

Result: 在文本到图像生成和编辑任务中取得最先进结果，仅用3B-5B参数就匹配或超越参数量4倍大的模型，计算开销最小。

Conclusion: MoS为扩展多模态扩散模型提供了一个灵活且计算效率高的范式，在保持高性能的同时显著减少了模型复杂度。

Abstract: We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $ε$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.

</details>


### [60] [FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention](https://arxiv.org/abs/2511.12215)
*Peng Zhang,Zhihui Lai,Wenting Chen,Xu Wu,Heng Kong*

Main category: cs.CV

TL;DR: FaNe是一个语义增强的医学视觉-语言预训练框架，通过语义感知的正样本挖掘、文本条件稀疏注意力池化和硬负样本感知对比损失，解决了现有方法中由语义相似文本引起的假阴性和细粒度跨模态对齐不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言预训练方法存在语义相似文本引起的假阴性问题和细粒度跨模态对齐不足的局限性。

Method: 提出语义感知的正样本挖掘策略、文本条件稀疏注意力池化模块和硬负样本感知对比损失。

Result: 在五个下游医学影像基准测试中，FaNe在图像分类、目标检测和语义分割任务上均达到了最先进的性能。

Conclusion: FaNe框架通过解决假阴性和增强细粒度对齐，有效提升了医学图像理解能力。

Abstract: Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.

</details>


### [61] [Suppressing VLM Hallucinations with Spectral Representation Filtering](https://arxiv.org/abs/2511.12220)
*Ameen Ali,Tamim Zoabi,Lior Wolf*

Main category: cs.CV

TL;DR: SRF是一种无需训练的后处理方法，通过分析模型表示中的协方差结构来抑制视觉语言模型的幻觉现象，无需修改架构或增加推理开销。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型由于过度依赖语言先验和跨模态对齐不精确，经常产生描述图像中不存在对象的幻觉。

Method: 通过特征协方差矩阵的特征分解识别幻觉模式，使用软谱滤波器在深层vLLM层的前馈投影权重中衰减这些模式，均衡特征方差同时保持语义保真度。

Result: 在LLaVA-1.5、MiniGPT-4和mPLUG-Owl2等模型上，SRF在MSCOCO、POPE-VQA等基准测试中持续降低幻觉率，达到最先进的忠实度而不降低描述质量。

Conclusion: SRF是一种轻量级、无需训练的后处理方法，能有效抑制视觉语言模型的幻觉，且不影响推理效率。

Abstract: Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.

</details>


### [62] [MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Classification and Detection](https://arxiv.org/abs/2412.15925)
*Andrea Moglia,Elia Clement Nastasio,Luca Mainardi,Pietro Cerveri*

Main category: cs.CV

TL;DR: MiniGPT-Pancreas是一个基于MiniGPT-v2的多模态大语言模型，通过级联微调实现胰腺检测、肿瘤分类和肿瘤检测，为临床医生提供胰腺癌诊断支持。


<details>
  <summary>Details</summary>
Motivation: 胰腺放射影像学成像具有挑战性，因为胰腺体积小、边界模糊，且在不同患者中形状和位置存在变异性。

Method: 使用NIH、MSD和AbdomenCT-1k数据集，通过级联微调MiniGPT-v2模型，结合问题和CT扫描的多模态提示，实现胰腺检测、肿瘤分类和肿瘤检测。

Result: 在NIH和MSD数据集上胰腺检测IoU分别为0.595和0.550；MSD数据集上胰腺癌分类准确率0.876、精确率0.874、召回率0.878；多器官检测中肝脏IoU 0.8399、肾脏0.722、脾脏0.705、胰腺0.497；胰腺肿瘤检测IoU为0.168。

Conclusion: MiniGPT-Pancreas为临床医生分类胰腺肿瘤图像提供了有前景的解决方案，未来需要改进检测任务性能，特别是胰腺肿瘤检测。

Abstract: Problem: Pancreas radiological imaging is challenging due to the small size, blurred boundaries, and variability of shape and position of the organ among patients. Goal: In this work we present MiniGPT-Pancreas, a Multimodal Large Language Model (MLLM), as an interactive chatbot to support clinicians in pancreas cancer diagnosis by integrating visual and textual information. Methods: MiniGPT-v2, a general-purpose MLLM, was fine-tuned in a cascaded way for pancreas detection, tumor classification, and tumor detection with multimodal prompts combining questions and computed tomography scans from the National Institute of Health (NIH), and Medical Segmentation Decathlon (MSD) datasets. The AbdomenCT-1k dataset was used to detect the liver, spleen, kidney, and pancreas. Results: MiniGPT-Pancreas achieved an Intersection over Union (IoU) of 0.595 and 0.550 for the detection of pancreas on NIH and MSD datasets, respectively. For the pancreas cancer classification task on the MSD dataset, accuracy, precision, and recall were 0.876, 0.874, and 0.878, respectively. When evaluating MiniGPT-Pancreas on the AbdomenCT-1k dataset for multi-organ detection, the IoU was 0.8399 for the liver, 0.722 for the kidney, 0.705 for the spleen, and 0.497 for the pancreas. For the pancreas tumor detection task, the IoU score was 0.168 on the MSD dataset. Conclusions: MiniGPT-Pancreas represents a promising solution to support clinicians in the classification of pancreas images with pancreas tumors. Future research is needed to improve the score on the detection task, especially for pancreas tumors.

</details>


### [63] [Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233)
*Dongdong Zhao,Qiben Xu,Ranxin Fang,Baogang Song*

Main category: cs.CV

TL;DR: DHMI是首个针对深度哈希的扩散式模型反演框架，能够在黑盒设置下成功重建高分辨率、高质量的图像，揭示了深度哈希系统的严重隐私风险。


<details>
  <summary>Details</summary>
Motivation: 深度哈希虽然提高了检索效率，但带来了严重的隐私风险，特别是从哈希码重建原始训练数据可能导致生物特征伪造和隐私泄露。然而针对深度哈希的模型反演攻击尚未被探索。

Method: 提出DHMI框架：首先对辅助数据集聚类得到语义哈希中心作为替代锚点；然后引入替代引导的去噪优化方法，使用融合分类一致性和哈希邻近度的新攻击指标动态选择候选样本；最后通过替代模型集群指导候选样本的细化。

Result: 在多个数据集上的实验表明，DHMI即使在最具挑战性的黑盒设置下也能成功重建高分辨率、高质量图像，在黑盒场景中优于现有最先进的模型反演攻击方法。

Conclusion: DHMI证实了深度哈希系统存在关键的隐私风险，同时展示了其在实际应用中的有效性。

Abstract: Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.

</details>


### [64] [Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets](https://arxiv.org/abs/2511.12255)
*Huy M. Le,Dat Tien Nguyen,Phuc Binh Nguyen,Gia-Bao Le-Tran,Phu Truong Thien,Cuong Dinh,Minh Nguyen,Nga Nguyen,Thuy T. N. Nguyen,Huy Gia Ngo,Tan Nhat Nguyen,Binh T. Nguyen,Monojit Choudhury*

Main category: cs.CV

TL;DR: Fusionista2.0是一个优化的视频检索系统，通过重新设计核心模块和用户界面，显著提升了检索速度和用户体验，在VBS挑战中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了满足Video Browser Showdown（VBS）在严格时间限制下提供准确结果的需求，需要开发一个高效、用户友好的视频检索系统。

Method: 重新设计核心模块：使用ffmpeg进行快速关键帧提取，Vintern-1B-v3.5进行多语言OCR，faster-whisper进行实时语音识别，轻量级视觉语言模型进行问答。同时改进了用户界面的响应性和工作流程。

Result: 检索时间减少了75%，准确率和用户满意度均有所提升，证明系统在大规模视频搜索中具有竞争力。

Conclusion: Fusionista2.0是一个高效、用户友好的视频检索系统，在速度和准确性方面都有显著改进，适合大规模视频搜索应用。

Abstract: The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.

</details>


### [65] [Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment](https://arxiv.org/abs/2511.12256)
*Tolga Demiroglu,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: 提出基于MedSigLIP的提示条件框架，通过FiLM和多尺度池化注入文本先验，在LDCTIQA2023数据集上取得优异性能，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发数据高效学习和快速适应的医学图像质量评估方法，通过文本提示将临床意图注入到图像特征中。

Method: 使用提示条件框架，通过FiLM和多尺度池化注入文本先验，结合全局、局部和纹理感知池化，采用轻量级MLP融合和成对排序损失训练。

Result: 在LDCTIQA2023数据集上，使用1000张训练图像，获得PLCC=0.9575、SROCC=0.9561、KROCC=0.8301，超越已发表的最佳挑战提交结果。

Conclusion: 提示引导方法在医学图像质量评估中表现出色，验证了文本条件特征调制的有效性。

Abstract: We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.

</details>


### [66] [A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation](https://arxiv.org/abs/2511.12259)
*Puzhen Wu,Hexin Dong,Yi Lin,Yihao Ding,Yifan Peng*

Main category: cs.CV

TL;DR: 提出了一种新颖的双阶段疾病感知框架用于胸部X光报告生成，通过疾病感知语义标记和视觉语言对齐来提升临床准确性和语言质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉表示中缺乏足够的疾病感知能力，视觉语言对齐不足，导致忽略关键病理特征且难以生成临床准确的报告。

Method: 第一阶段学习疾病感知语义标记并通过对比学习对齐视觉语言表示；第二阶段引入疾病视觉注意力融合模块和双模态相似性检索机制。

Result: 在多个基准数据集上的实验表明，该框架在胸部X光报告生成中实现了最先进的性能，临床准确性和语言质量显著提升。

Conclusion: 该疾病感知框架有效解决了现有方法的局限性，能够生成更准确和临床相关的放射学报告。

Abstract: Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.

</details>


### [67] [CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2511.12263)
*Jingyao Li,Jingyun Wang,Molin Tan,Haochen Wang,Cilin Yan,Likun Shi,Jiayin Cai,Xiaolong Jiang,Yao Hu*

Main category: cs.CV

TL;DR: CrossVid是首个专门评估多模态大语言模型在跨视频推理中时空推理能力的基准，包含4个高级维度和10个具体任务，提供5331个视频和9015个问题对，实验显示现有模型在跨视频推理方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准主要关注单视频分析，无法评估多模态大语言模型同时推理多个视频的能力，需要构建专门评估跨视频推理能力的基准。

Method: 构建CrossVid基准，包含广泛的层次化任务、5331个视频和9015个挑战性问题对，涵盖单选、多选和开放式问题格式，对多种开源和闭源MLLMs进行广泛实验。

Result: Gemini-2.5-Pro在CrossVid上表现最佳，平均准确率为50.4%，但大多数现有MLLMs在跨视频推理任务中表现不佳，主要原因是无法整合或比较分布在多个视频中的证据。

Conclusion: CrossVid基准有助于指导未来增强MLLMs跨视频推理能力的研究，当前模型在跨视频推理方面仍有显著改进空间。

Abstract: Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.

</details>


### [68] [ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks](https://arxiv.org/abs/2511.12267)
*Ruixun Liu,Bowen Fu,Jiayi Song,Kaiyu Li,Wanchen Li,Lanxuan Xue,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 本文提出了一种用于超高分辨率遥感图像处理的主动感知范式ZoomEarth，通过自适应裁剪缩放框架和区域引导奖励机制，在LRS-GRO基准数据集上实现最先进性能，并能与下游任务无缝集成。


<details>
  <summary>Details</summary>
Motivation: 现有动态分辨率和令牌剪枝方法受限于被动感知范式，在获取更精细视觉输入时会产生冗余。本文探索主动感知范式，使模型能够重新访问信息丰富区域。

Method: 提出ZoomEarth自适应裁剪缩放框架，采用区域引导奖励机制进行细粒度指导，通过监督微调和组相对策略优化进行训练。

Result: 在LRS-GRO基准上实现最先进性能，在零样本设置下在三个公共UHR遥感基准上表现优异，并能与云去除、去噪、分割和图像编辑等下游任务无缝集成。

Conclusion: ZoomEarth展示了强大的多功能性和可扩展性，为超高分辨率遥感图像处理提供了有效的主动感知解决方案。

Abstract: Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.

</details>


### [69] [D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs](https://arxiv.org/abs/2511.12280)
*Shuochen Chang,Xiaofeng Zhang,Qingyang Liu,Li Niu*

Main category: cs.CV

TL;DR: 提出D³ToM方法，通过动态合并冗余视觉token来加速扩散多模态大语言模型的推理速度，同时保持性能竞争力。


<details>
  <summary>Details</summary>
Motivation: 扩散多模态大语言模型在视觉语言任务中表现出色，但推理速度显著慢于自回归模型，主要原因是每个去噪步骤都需要对整个序列进行双向自注意力计算，导致立方级解码复杂度。

Method: 使用决策token构建重要性映射，保留最显著的token，通过基于相似性的聚合合并冗余token。该即插即用模块集成到单个transformer层中，物理缩短视觉token序列而不改变模型参数。

Result: 大量实验表明D³ToM在保持竞争性能的同时显著加速推理。

Conclusion: D³ToM是一种有效的动态token合并方法，能够显著提升扩散多模态大语言模型的推理效率。

Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.

</details>


### [70] [TM-UNet: Token-Memory Enhanced Sequential Modeling for Efficient Medical Image Segmentation](https://arxiv.org/abs/2511.12270)
*Yaxuan Jiao,Qing Xu,Yuxiang Luo,Xiangjian He,Zhen Chen,Wenting Duan*

Main category: cs.CV

TL;DR: TM-UNet是一种轻量级医学图像分割框架，通过多尺度令牌-记忆块将2D空间特征转换为令牌序列，利用矩阵记忆单元选择性保留和传播判别性上下文信息，实现高效全局推理和长距离依赖捕获。


<details>
  <summary>Details</summary>
Motivation: 解决基于transformer的医学图像分割方法计算成本高、难以临床部署的问题，提出一种轻量级框架来平衡性能和效率。

Method: 提出多尺度令牌-记忆块，通过空间扫描将2D特征转换为令牌序列，使用矩阵记忆单元选择性保留上下文信息，结合指数门控和多尺度上下文提取实现分层表示学习。

Result: TM-UNet在多种医学分割任务中优于最先进方法，同时显著降低计算成本。

Conclusion: TM-UNet通过令牌序列建模和高效记忆机制，实现了计算高效的医学图像分割，为临床部署提供了可行方案。

Abstract: Medical image segmentation is essential for clinical diagnosis and treatment planning. Although transformer-based methods have achieved remarkable results, their high computational cost hinders clinical deployment. To address this issue, we propose TM-UNet, a novel lightweight framework that integrates token sequence modeling with an efficient memory mechanism for efficient medical segmentation. Specifically, we introduce a multi-scale token-memory (MSTM) block that transforms 2D spatial features into token sequences through strategic spatial scanning, leveraging matrix memory cells to selectively retain and propagate discriminative contextual information across tokens. This novel token-memory mechanism acts as a dynamic knowledge store that captures long-range dependencies with linear complexity, enabling efficient global reasoning without redundant computation. Our MSTM block further incorporates exponential gating to identify token effectiveness and multi-scale contextual extraction via parallel pooling operations, enabling hierarchical representation learning without computational overhead. Extensive experiments demonstrate that TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost. The code is available at https://github.com/xq141839/TM-UNet.

</details>


### [71] [DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions](https://arxiv.org/abs/2511.12452)
*Xiaoyu Lin,Aniket Ghorpade,Hansheng Zhu,Justin Qiu,Dea Rrozhani,Monica Lama,Mick Yang,Zixuan Bian,Ruohan Ren,Alan B. Hong,Jiatao Gu,Chris Callison-Burch*

Main category: cs.CV

TL;DR: DenseAnnotate是一个音频驱动的在线标注平台，能够为图像和3D资产高效创建密集、细粒度的标注。通过语音描述和同步区域标记，解决了传统文本标注在表达性、速度和视觉特征捕捉方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的训练数据主要依赖稀疏的互联网挖掘或手动输入标注，这些方法只能捕捉图像视觉内容的一小部分。密集标注更有价值但稀缺，传统文本标注管道在表达性、速度和专业领域（如多元文化图像和3D资产标注）方面存在不足。

Method: 开发了DenseAnnotate平台，采用音频驱动方法：标注者口头描述观察内容，同时将口语短语与图像区域或3D场景部分同步链接。平台整合了语音转文本转录和注意力区域标记功能。

Result: 通过超过1,000名标注者在两个领域（多元文化图像和3D场景）的案例研究，创建了包含3,531张图像、898个3D场景和7,460个3D对象的多模态数据集，包含20种语言的音频对齐密集标注。基于该数据集训练的模型在多项能力上显著提升：多语言能力提高5%，文化对齐能力提高47%，3D空间能力提高54%。

Conclusion: DenseAnnotate平台为未来视觉语言研究提供了可行方法，能够应用于各种任务和多样数据类型，显著提升了模型在密集标注相关任务上的性能。

Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.

</details>


### [72] [Co-Layout: LLM-driven Co-optimization for Interior Layout](https://arxiv.org/abs/2511.12474)
*Chucheng Xiang,Ruchao Bao,Biyin Feng,Wenzheng Wu,Zhongyuan Liu,Yirui Guan,Ligang Liu*

Main category: cs.CV

TL;DR: 提出结合大语言模型和网格整数规划的自动化室内设计框架，联合优化房间布局和家具摆放


<details>
  <summary>Details</summary>
Motivation: 现有两阶段设计流程在解决方案质量和计算效率方面存在不足，需要更高效的联合优化方法

Method: 使用LLM提取结构化设计约束，编码到基于网格的统一表示中，采用粗到细的优化策略，从低分辨率网格开始简化问题求解

Result: 在多样化场景下的实验结果表明，该联合优化方法在解决方案质量上显著优于现有两阶段设计流程，并通过粗到细策略实现了显著的计算效率提升

Conclusion: 该框架成功实现了房间布局和家具摆放的联合优化，在质量和效率方面都优于传统方法

Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.

</details>


### [73] [One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving](https://arxiv.org/abs/2511.12291)
*Andrea Bertogalli,Giacomo Boracchi,Luca Magri*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态外参标定框架，能够同时估计事件相机、LiDAR和RGB相机之间的相对位姿，特别关注具有挑战性的事件相机标定。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等应用中，精确的多传感器对齐至关重要，而现有方法通常依赖单独的双向标定过程，无法实现一次性联合标定。

Method: 设计并构建了新型3D标定目标，包含平面特征、ChArUco图案和主动LED模式，分别针对LiDAR、RGB相机和事件相机的特性。实现一次性联合外参标定过程。

Result: 在定制数据集上进行了广泛的实验评估，使用先进的自动驾驶传感器设置记录数据，验证了方法的准确性和鲁棒性。

Conclusion: 该方法能够准确标定自动驾驶环境中的复杂视觉系统，为多传感器融合提供了可靠的外参标定解决方案。

Abstract: We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.

</details>


### [74] [Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method](https://arxiv.org/abs/2511.12301)
*Chi Liu,Jincheng Liu,Congcong Zhu,Minghao Wang,Sheng Shen,Jia Gu,Tianqing Zhu,Wanlei Zhou*

Main category: cs.CV

TL;DR: 本文提出频率重校准(FreRec)方法来解决医学图像生成数据增强(GDA)中的频率偏差问题，通过统计高频替换(SHR)和重建高频映射(RHM)来改善合成图像质量，提升下游医学图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 医学AI开发依赖大数据集但面临数据稀缺问题，生成数据增强(GDA)存在频率偏差风险，可能引入有害特征损害下游任务性能。

Method: 提出FreRec方法：1) 统计高频替换(SHR)粗略对齐高频分量；2) 重建高频映射(RHM)提升图像质量并重建高频细节。该方法作为独立后处理步骤，兼容任何生成模型。

Result: 在脑部MRI、胸部X光、眼底图像等多种医学数据集上的实验表明，FreRec显著提升了基于AI合成样本的下游医学图像分类性能。

Conclusion: FreRec通过减少频率分布差异有效改善了医学图像生成数据增强的可靠性，可作为通用后处理步骤集成到标准医学GDA流程中。

Abstract: Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.

</details>


### [75] [LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors](https://arxiv.org/abs/2511.12304)
*Qifeng Chen,Jiarun Liu,Rengan Xie,Tao Tang,Sicong Du,Yiru Zhao,Yuchi Huo,Sheng Yang*

Main category: cs.CV

TL;DR: LiDAR-GS++是一种基于高斯泼溅的LiDAR重建方法，通过扩散先验增强，实现实时高保真重模拟，解决了单次扫描重建不完整导致的伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯泼溅的LiDAR渲染方法在单次扫描重建不完整的情况下，外推新视角合成时会出现伪影，需要解决这一限制。

Method: 引入可控LiDAR生成模型，基于粗略外推渲染生成额外几何一致的扫描，并采用有效的蒸馏机制进行扩展重建。

Result: 在多个公共数据集上的实验表明，LiDAR-GS++在插值和外推视角下均达到最先进性能，超越现有GS和NeRF方法。

Conclusion: 通过将重建扩展到欠拟合区域，该方法确保了外推新视角的全局几何一致性，同时保留了传感器捕获的详细场景表面。

Abstract: Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.

</details>


### [76] [Learning Time in Static Classifiers](https://arxiv.org/abs/2511.12321)
*Xi Ding,Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: 提出了一种简单有效的框架，为前馈分类器添加时序推理能力，无需修改模型架构或引入循环模块。通过SEQ学习范式将训练数据组织成时序连贯的轨迹，学习类别特定的时序原型，并使用可微soft-DTW损失对齐预测序列。


<details>
  <summary>Details</summary>
Motivation: 现实世界视觉数据通常随时间逐渐演变，但传统分类器假设时间独立性，无法捕捉这种动态变化。需要在不改变模型架构的情况下为分类器添加时序推理能力。

Method: 采用支持-样本-查询(SEQ)学习范式，将训练数据构建为时序连贯轨迹。学习类别特定的时序原型，使用可微soft-DTW损失对齐预测序列，并通过多目标函数促进语义一致性和时序平滑性。

Result: 在静态和时序任务中均表现优异：提升了细粒度和超细粒度图像分类性能，在视频异常检测中提供精确且时序一致的预测。

Conclusion: 该方法通过仅设计损失函数引入强时序归纳偏置，以模块化和数据高效的方式桥接静态和时序学习，只需在预提取特征上使用简单分类器即可实现。

Abstract: Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.

</details>


### [77] [SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models](https://arxiv.org/abs/2511.12331)
*Sepehr Kazemi Ranjbar,Kumail Alhamoud,Marzyeh Ghassemi*

Main category: cs.CV

TL;DR: 提出了一种无需训练的框架，将否定建模为联合嵌入空间中的子空间而非单点，通过在A和N的嵌入周围构建球形区域来匹配图像，显著提升了VLMs的否定理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在处理否定提示时表现不佳，而基于大规模否定数据集微调的方法往往会损害模型在肯定提示上的零样本性能。

Method: 将否定建模为联合嵌入空间中的子空间，在A和N的嵌入周围构建球形区域，通过靠近A且远离N的区域中心方向来评分图像。

Result: 在检索、多选题和文本到图像任务中，该方法比现有方法平均提升否定理解能力约30%，同时保持了零样本性能。

Conclusion: 该方法有效缩小了肯定和否定提示之间的性能差距，同时保持了微调模型难以维持的零样本性能。

Abstract: Vision-Language Models (VLMs) struggle with negation. Given a prompt like "retrieve (or generate) a street scene without pedestrians," they often fail to respect the "not." Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as "A but not N," we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.

</details>


### [78] [Ground Plane Projection for Improved Traffic Analytics at Intersections](https://arxiv.org/abs/2511.12342)
*Sajjad Pakdamansavoji,Kumar Vaibhav Jha,Baher Abdulhai,James H Elder*

Main category: cs.CV

TL;DR: 该论文探讨了将基础设施摄像头检测到的车辆反投影到地面平面进行3D坐标分析，以提高交叉口转向运动计数的准确性。研究发现单摄像头反投影能提供更准确的轨迹分类和转向计数，多摄像头弱融合能进一步提高精度。


<details>
  <summary>Details</summary>
Motivation: 交叉口转向运动计数对于信号控制、交通管理和城市规划至关重要。传统基于图像平面的计算机视觉系统存在局限性，需要探索在真实世界3D坐标中分析交通的优势。

Method: 将基础设施摄像头检测到的车辆反投影到地面平面，在真实世界3D坐标中进行分析。研究比较了单摄像头系统和多摄像头弱融合方法。

Result: 单摄像头反投影系统比图像平面分析提供更准确的轨迹分类和转向运动计数。多摄像头弱融合方法能获得更高的精度。

Conclusion: 交通分析应该在地面平面而非图像平面进行，反投影方法能显著提高转向运动计数的准确性。

Abstract: Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane

</details>


### [79] [CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification](https://arxiv.org/abs/2511.12346)
*Asmit Bandyopadhyay,Anindita Das Bhattacharjee,Rakesh Das*

Main category: cs.CV

TL;DR: CLAReSNet是一种混合架构，结合多尺度卷积提取和变换器式注意力，通过自适应潜在瓶颈解决高光谱图像分类中的高光谱维度、复杂光谱-空间相关性和有限训练样本等挑战。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高光谱维度、复杂光谱-空间相关性和有限训练样本等关键挑战。CNN擅长局部特征提取，变换器捕获长程依赖关系，但它们的孤立应用由于二次复杂性和不足的归纳偏差而产生次优结果。

Method: 提出CLAReSNet混合架构，集成多尺度卷积提取与变换器式注意力，通过自适应潜在瓶颈。模型采用多尺度卷积主干和增强的卷积块注意力模块提取层次空间特征，然后通过结合双向RNN和多尺度光谱潜在注意力的光谱编码器层。MSLA通过自适应潜在令牌分配将复杂度从O(T²D)降低到O(Tlog(T)D)。

Result: 在Indian Pines和Salinas数据集上的实验显示最先进的性能，分别达到99.71%和99.96%的总体准确率，显著超过HybridSN、SSRN和SpectralFormer。学习到的嵌入表现出优异的类间可分离性和紧凑的类内聚类。

Conclusion: CLAReSNet在有限样本和严重类别不平衡条件下具有有效性，验证了该模型在高光谱图像分类中的优越性能。

Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.

</details>


### [80] [Explainable AI-Generated Image Detection RewardBench](https://arxiv.org/abs/2511.12363)
*Michael Yang,Shijian Deng,William T. Doan,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CV

TL;DR: 该论文提出了XAIGID-RewardBench基准，用于评估多模态大语言模型在判断AI生成图像检测解释质量方面的能力，发现当前最佳模型与人类水平仍存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 传统基于分类的AI生成图像检测方法无法提供人类专家可理解的解释，降低了检测工具的可信度和说服力。虽然MLLMs被用于生成解释，但它们在评估自身或其他MLLMs生成的解释质量方面的表现尚未得到充分研究。

Method: 构建包含约3000个标注三元组的基准数据集，涵盖多种图像生成模型和MLLMs作为检测器，评估MLLMs作为奖励模型（评判者）的能力。

Result: 当前最佳奖励模型在该基准上得分为88.76%，而人类标注者间一致性达到98.30%，表明当前MLLMs的推理能力与人类水平仍存在可见差距。

Conclusion: 需要进一步改进MLLMs的推理能力以缩小与人类水平的差距，同时提供了常见错误模式的分析，为未来研究提供了基准和方向。

Abstract: Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an "MLLM as a judge" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\% on this benchmark (while human inter-annotator agreement reaches 98.30\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.

</details>


### [81] [Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.12365)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: DT-R1是一个基于强化学习的框架，通过训练大型语言模型构建复杂多模态视觉输入的数字孪生表示，并基于这些高层表示进行统一视觉推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖任务特定的架构和监督微调，缺乏统一的解决方案，限制了跨任务和跨模态的泛化能力。

Method: 使用GRPO强化学习方法和新颖的奖励函数，验证结构完整性和输出准确性，训练模型构建数字孪生表示。

Result: 在六个视觉推理基准测试中，涵盖两种模态和四种任务类型，DT-R1始终优于最先进的任务特定模型。

Conclusion: DT-R1为视觉推理开辟了新方向，通过数字孪生表示的强化学习实现统一的视觉推理。

Abstract: Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.

</details>


### [82] [Fast Reasoning Segmentation for Images and Videos](https://arxiv.org/abs/2511.12368)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: FastReasonSeg是一种高效的推理分割方法，通过数字孪生表示将感知与推理解耦，使用监督微调和强化学习进行知识蒸馏，在保持高性能的同时大幅减少模型参数和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割方法需要数十亿参数的多模态大语言模型，超出了边缘设备的计算能力。现有蒸馏方法无法有效传递多步推理能力，需要新的蒸馏策略。

Method: 使用数字孪生表示解耦感知与推理，先通过教师生成的推理链进行监督微调，然后使用联合奖励（分割准确性和推理质量对齐）进行强化微调。

Result: 在四个基准测试中达到最先进的推理分割性能，0.6B参数的变体超越20倍参数模型，实现7.79 FPS吞吐量，仅消耗2.1GB内存。

Conclusion: FastReasonSeg实现了高效的推理分割，能够在资源受限环境中部署，支持实时推理分割应用。

Abstract: Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.

</details>


### [83] [Changes in Real Time: Online Scene Change Detection with Multi-View Fusion](https://arxiv.org/abs/2511.12370)
*Chamuditha Jayanga Galappaththige,Jason Lai,Lloyd Windrim,Donald Dansereau,Niko Sünderhauf,Dimity Miller*

Main category: cs.CV

TL;DR: 提出首个姿态无关、无标签、多视角一致的在线场景变化检测方法，在超过10 FPS的速度下实现最先进性能，甚至超越最佳离线方法。


<details>
  <summary>Details</summary>
Motivation: 在线场景变化检测是一个极具挑战性的问题，现有在线方法准确性远低于离线方法，需要开发能够在无约束视角下实时检测相关变化的方法。

Method: 引入自监督融合损失从多线索和观察中推断场景变化，基于PnP的快速姿态估计，以及针对3D高斯溅射场景表示的快速变化引导更新策略。

Result: 在复杂真实世界数据集上的广泛实验表明，该方法超越了在线和离线基线方法，实现了新的最先进性能。

Conclusion: 该方法在保持实时性能的同时，显著提升了在线场景变化检测的准确性，为实际应用提供了可行的解决方案。

Abstract: Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.

</details>


### [84] [Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models](https://arxiv.org/abs/2511.12371)
*Yiqing Shen,Chenxiao Fan,Chenjia Li,Mathias Unberath*

Main category: cs.CV

TL;DR: 本文提出了推理文本到视频检索的新范式，通过数字孪生表示和大型语言模型推理来处理隐含查询，在多个基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频检索方法只能处理显式查询，无法应对需要推理的隐含查询。本文旨在扩展传统检索能力，使其能够通过推理处理隐含查询并提供对象级定位掩码。

Method: 提出两阶段框架：首先通过组合对齐将分解的子查询与数字孪生表示匹配以识别候选视频；然后应用大型语言模型推理和即时细化，调用专业模型填补信息空白。

Result: 在ReasonT2VBench-135上达到81.2% R@1，比最强基线提升超过50个百分点；在扩展配置上保持81.7% R@1；在三个传统基准测试（MSR-VTT、MSVD、VATEX）中均达到最先进水平。

Conclusion: 通过数字孪生表示和大型语言模型推理，成功实现了对隐含查询的文本到视频检索，显著提升了检索性能，为复杂推理任务提供了有效解决方案。

Abstract: The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).

</details>


### [85] [AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification](https://arxiv.org/abs/2511.12382)
*Ansh Makwe,Akansh Agrawal,Prateek Jain,Akshan Agrawal,Priyanka Bagade*

Main category: cs.CV

TL;DR: 提出了AGGRNet框架，通过提取信息性和非信息性特征来理解细粒度视觉模式，改善复杂医学图像分析任务的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有注意力模型在医学图像分类中难以有效区分细微类别，因为它们难以捕捉类间相似性和类内变异性，导致错误诊断。

Method: 提出AGGRNet框架，提取信息性和非信息性特征，以有效理解细粒度视觉模式。

Result: 在多个医学影像数据集上实现最先进性能，在Kvasir数据集上比SOTA模型提升高达5%。

Conclusion: AGGRNet框架能有效改善复杂医学图像分析任务的分类性能，特别是在区分细微类别方面表现出色。

Abstract: Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.

</details>


### [86] [Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation](https://arxiv.org/abs/2511.12389)
*Divake Kumar,Patrick Poggi,Sina Tayebati,Devashri Naik,Nilesh Ahuja,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级推理时间框架，通过解耦深度特征空间中的偶然不确定性和认知不确定性来指导推理时模型选择，显著减少计算成本同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 大多数估计器将所有不确定性模式合并为单一置信度分数，无法可靠地决定何时分配更多计算或调整推理过程。

Method: 引入不确定性引导的推理时选择框架：使用正则化全局密度模型估计偶然不确定性；认知不确定性由三个互补组件构成：局部支持不足、流形谱崩溃和跨层特征不一致性。这些组件无需采样、集成或额外前向传播。

Result: 在MOT17数据集上减少约60%计算成本且精度损失可忽略；不确定性分解在校准过程中产生更紧密的预测区间；消融实验显示正交不确定性分解在所有序列上比总不确定性基线提高13.6个百分点的计算节省。

Conclusion: 该方法实现了实用的自调节视觉推理，通过解耦不同类型的不确定性实现了高效的计算资源分配。

Abstract: Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.

</details>


### [87] [VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving](https://arxiv.org/abs/2511.12405)
*Hyunki Seong,Seongwoo Moon,Hojin Ahn,Jehun Kang,David Hyunchul Shim*

Main category: cs.CV

TL;DR: 本文提出了VLA-R框架，一种开放世界端到端自动驾驶方法，通过视觉-语言-动作检索范式解决非结构化环境中未知条件的挑战。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶在非结构化户外环境中经常遇到训练时未见过的条件，需要强大的泛化能力来应对开放世界场景。

Method: 使用冻结的视觉语言模型进行开放世界检测和分割，通过Q-Former瓶颈聚合细粒度视觉表示和语言对齐的视觉特征，并引入视觉-动作对比学习方案来对齐视觉语言和动作嵌入。

Result: 在真实机器人平台上的实验表明，即使在数据有限的情况下，该方法在非结构化、未见环境中也表现出强大的泛化和探索性能。

Conclusion: VLA-R框架成功地将开放世界感知与视觉-动作检索相结合，为端到端自动驾驶在开放世界场景中的泛化提供了有效解决方案。

Abstract: Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.

</details>


### [88] [Towards Rotation-only Imaging Geometry: Rotation Estimation](https://arxiv.org/abs/2511.12415)
*Xinrui Li,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: 本文提出了一种基于旋转流形的仅旋转优化框架，用于从运动中恢复结构（SfM），通过将平移表示为旋转的函数，将成像几何表示压缩到旋转流形上。


<details>
  <summary>Details</summary>
Motivation: 延续仅姿态成像几何的视角，探索场景结构、旋转和平移之间的关键关系，旨在实现更准确、高效和可靠的3D视觉计算。

Method: 提出基于重投影误差的仅旋转优化框架，适用于两视图和多视图场景，将平移表示为旋转的函数，将成像几何表示压缩到旋转流形。

Result: 实验结果表明，该方法在旋转估计精度和鲁棒性方面优于当前最先进的方法，甚至可与多次束调整迭代结果相媲美。

Conclusion: 这项工作有望为更准确、高效和可靠的3D视觉计算做出贡献，通过仅旋转优化框架显著提升了SfM性能。

Abstract: Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.

</details>


### [89] [Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance](https://arxiv.org/abs/2511.12419)
*Wenjie Li,Jinglei Shi,Jin Han,Heng Guo,Zhanyu Ma*

Main category: cs.CV

TL;DR: DHGM是一个基于扩散模型的高频引导模型，用于同时去除雨滴伪影和增强结构细节，生成干净的高分辨率图像，解决了传统方法中天气去除和超分辨率之间的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 真实世界图像常受恶劣天气影响而质量下降，现有的天气恢复方法往往会牺牲高频细节，这对于小物体检测等视觉任务至关重要。简单的级联恢复和超分辨率方法存在内在冲突：去除方法旨在消除高频天气噪声，而超分辨率则要从现有细节中生成高频纹理。

Method: 提出DHGM模型，将预训练的扩散先验与高通滤波器相结合，同时去除雨滴伪影并增强结构细节。

Result: 大量实验表明，DHGM在性能上优于现有方法，且成本更低。

Conclusion: DHGM通过整合扩散先验和高频引导，有效解决了天气去除和超分辨率之间的冲突，能够生成既干净又具有丰富细节的高分辨率图像。

Abstract: Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.

</details>


### [90] [MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation](https://arxiv.org/abs/2511.12422)
*Nuolin Sun,Linyuan Wang,Haonan Wei,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: MFI-ResNet使用压缩-扩展策略，通过MeanFlow模块简化ResNet结构，在减少45-46%参数的同时提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 受流匹配模型MeanFlow的启发，探索生成式流场如何有效表征ResNet中的特征变换过程，为理解生成建模与判别学习的关系提供新视角。

Method: 采用压缩-扩展策略：压缩阶段将每个ResNet阶段的多层结构简化为1-2个MeanFlow模块构建轻量元模型；扩展阶段对前三个阶段应用选择性孵化策略，扩展到与基准ResNet相同的残差块配置，最后一个阶段保持MeanFlow形式，然后微调孵化模型。

Result: 在CIFAR-10和CIFAR-100数据集上，相比ResNet-50，MFI-ResNet分别减少46.28%和45.59%的参数，同时准确率分别提升0.23%和0.17%。

Conclusion: 生成式流场能够有效表征ResNet中的特征变换过程，为理解生成建模与判别学习的关系提供了新视角，证明了参数效率与判别性能可以同时提升。

Abstract: ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.

</details>


### [91] [RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning](https://arxiv.org/abs/2511.12428)
*Jingqi Xu,Jingxi Lu,Chenghao Li,Sreetama Sarkar,Souvik Kundu,Peter A. Beerel*

Main category: cs.CV

TL;DR: RedVTP是一种针对扩散视觉语言模型(DVLMs)的响应驱动视觉token剪枝策略，通过利用推理动态来估计视觉token重要性，在保持甚至提升准确性的同时显著提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 扩散视觉语言模型(DVLMs)虽然支持并行token解码，但大量视觉token严重阻碍了推理效率。虽然视觉token剪枝在自回归VLMs中已有广泛研究，但在DVLMs中仍基本未被探索。

Method: 提出RedVTP策略，利用掩码响应token的注意力来估计视觉token重要性。基于重要性分数在推理步骤间保持一致的观察，在第一个推理步骤后从掩码token中剪枝重要性较低的视觉token。

Result: 实验显示RedVTP将LLaDA-V和LaViDa的token生成吞吐量分别提升高达186%和28.05%，推理延迟分别降低高达64.97%和21.87%，且不损害准确性，在某些情况下甚至提升了准确性。

Conclusion: RedVTP通过响应驱动的视觉token剪枝策略，有效解决了DVLMs的推理效率问题，在保持模型性能的同时显著提升了推理速度。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.

</details>


### [92] [Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion](https://arxiv.org/abs/2511.12432)
*Xilai Li,Xiaosong Li,Weijun Jiang*

Main category: cs.CV

TL;DR: 提出UP-Fusion框架，通过通道扰动和预训练知识集成解决多模态图像融合中的梯度冲突问题，提升融合质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 统一模型在多模态图像融合中面临模态差异导致的梯度冲突问题，而引入模态特定编码器的方法又降低了跨融合任务的泛化能力。

Method: 提出UP-Fusion框架，包含语义感知通道剪枝模块(SCPM)过滤冗余信息、几何仿射调制模块(GAM)保持模态判别性、文本引导通道扰动模块(TCPM)减少对模态特定通道的依赖。

Result: 在多个多模态图像融合任务和下游任务上的实验表明，该方法优于现有方法。

Conclusion: UP-Fusion框架通过通道扰动和预训练知识集成，有效解决了多模态图像融合中的梯度冲突问题，提高了融合质量和模型泛化能力。

Abstract: Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.

</details>


### [93] [MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2511.12449)
*Zhanheng Nie,Chenghan Fu,Daoze Zhang,Junxian Wu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON2.0是一个动态模态平衡的多模态表示学习框架，针对电子商务产品理解中的模态不平衡、内在对齐关系利用不足和数据噪声问题提出了解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决电子商务多模态模型面临的三个挑战：模态混合训练导致的模态不平衡、产品内视觉和文本信息内在对齐关系利用不足、以及电子商务多模态数据噪声处理有限。

Method: 提出MOON2.0框架，包含：模态驱动的专家混合模块进行自适应样本处理，双级对齐方法利用产品内语义对齐特性，基于MLLM的图像-文本协同增强策略结合动态样本过滤。

Result: 在MBE2.0基准和多个公共数据集上实现了最先进的零样本性能，注意力热图可视化提供了改进多模态对齐的定性证据。

Conclusion: MOON2.0通过动态模态平衡和协同增强策略，有效提升了电子商务产品多模态理解能力，在多个数据集上表现出色。

Abstract: The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.

</details>


### [94] [Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion](https://arxiv.org/abs/2511.12498)
*Jongseong Bae,Junwoo Ha,Jinnyeong Heo,Yeongin Lee,Ha Young Kim*

Main category: cs.CV

TL;DR: C3DFusion模块通过显式对齐当前帧和历史帧的3D点特征，生成隐藏区域感知的3D特征几何，解决了现有方法难以重建ego-vehicle侧面关键外帧区域的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于相机的3D语义场景补全方法虽然利用时序信息增强当前帧特征，但主要关注帧内区域，难以重建ego-vehicle侧面关键外帧区域，尽管历史帧通常包含这些不可见区域的宝贵上下文信息。

Method: 提出Current-Centric Contextual 3D Fusion模块，通过历史上下文模糊化和当前中心特征密集化两种互补技术，抑制不准确扭曲历史点特征的噪声并增强当前点特征的体积贡献。

Result: 在SemanticKITTI和SSCBench-KITTI-360数据集上显著优于最先进方法，并在其他基线模型上表现出强大的泛化能力，获得显著性能提升。

Conclusion: C3DFusion模块简单集成到标准SSC架构中，能有效解决外帧区域重建问题，展示了强大的有效性和泛化能力。

Abstract: Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.

</details>


### [95] [DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection](https://arxiv.org/abs/2511.12511)
*Jialiang Shen,Jiyang Zheng,Yunqi Xue,Huajie Chen,Yu Yao,Hui Kang,Ruiqi Liu,Helin Gong,Yang Yang,Dadong Wang,Tongliang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于师生知识蒸馏的模糊鲁棒AI生成图像检测框架，通过冻结在清晰图像上训练的教师模型，将其特征和逻辑响应蒸馏到在模糊图像上训练的学生模型，实现在运动模糊条件下的稳定检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着对图像真实性和数字安全性的关注增加，AI生成图像检测领域快速发展。然而，大多数检测器在真实世界退化（特别是运动模糊）下表现不佳，这种模糊会扭曲精细纹理并抑制高频伪影，导致实际场景中性能严重下降。

Method: 采用师生知识蒸馏框架，使用在清晰图像上训练的高容量教师模型（DINOv3）提供稳定且语义丰富的表示作为学习参考。冻结教师模型以保持其泛化能力，将其特征和逻辑响应从清晰图像蒸馏到在模糊图像上训练的学生模型。

Result: 广泛的基准测试表明，该方法在运动模糊和清晰条件下均实现了最先进的性能，显示出改进的泛化能力和实际应用性。

Conclusion: 所提出的基于知识蒸馏的模糊鲁棒检测框架有效解决了AI生成图像检测在真实世界运动模糊条件下的性能下降问题，具有更好的泛化能力和实际适用性。

Abstract: With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.

</details>


### [96] [D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation](https://arxiv.org/abs/2511.12528)
*Zheyuan Zhang,Jiwei Zhang,Boyu Zhou,Linzhimeng Duan,Hong Chen*

Main category: cs.CV

TL;DR: D²-VPR是一个基于蒸馏和可变形机制的视觉位置识别框架，通过知识蒸馏和可变形聚合器在保持强大特征提取能力的同时显著减少模型参数和计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决DINOv2等视觉基础模型在VPR任务中虽然性能优越但模型复杂度和计算开销大，难以在资源受限设备上部署的问题。

Method: 采用两阶段训练策略结合知识蒸馏和微调，引入蒸馏恢复模块对齐师生模型特征空间，设计基于自上而下注意力的可变形聚合器动态调整感兴趣区域。

Result: 在保持竞争力的性能同时，相比CricaVPR减少约64.2%的参数和62.6%的FLOPs。

Conclusion: D²-VPR框架在视觉位置识别任务中实现了更好的性能-效率权衡，为资源受限环境下的部署提供了可行方案。

Abstract: Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.

</details>


### [97] [HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models](https://arxiv.org/abs/2511.12547)
*Zhiguang Lu,Qianqian Xu,Peisong Wen,Siran Da,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出HiGFA方法，通过分层引导策略在扩散模型采样过程中实现细粒度数据增强，早期使用强文本和轮廓引导建立整体结构，后期激活细粒度分类器引导并动态调节强度，生成既多样又准确的合成图像。


<details>
  <summary>Details</summary>
Motivation: 解决生成扩散模型在细粒度任务中难以准确捕捉类别定义细微特征的问题，避免标准方法可能产生误导性样本而降低分类器性能。

Method: HiGFA方法：利用扩散采样过程的时间动态性，早期阶段使用强文本和变换轮廓引导固定强度建立场景、风格和结构，后期阶段激活专门细粒度分类器引导并基于预测置信度动态调节所有引导信号强度。

Result: 在多个细粒度视觉分类数据集上的实验证明了HiGFA的有效性。

Conclusion: HiGFA通过分层、置信度驱动的协调机制，智能平衡全局结构形成与精确细节优化，能够生成多样且忠实的合成图像。

Abstract: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.

</details>


### [98] [EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis](https://arxiv.org/abs/2511.12554)
*Yijie Guo,Dexiang Hong,Weidong Chen,Zihan She,Cheng Ye,Xiaojun Chang,Zhendong Mao*

Main category: cs.CV

TL;DR: EmoVerse是一个大规模开源数据集，通过多层次的基于知识图谱的注释实现可解释的视觉情感分析，将情感分解为背景-属性-主体三元组，并提供分类和维度情感表示。


<details>
  <summary>Details</summary>
Motivation: 视觉情感分析领域因缺乏开源和可解释数据集而进展有限，现有研究通常为整张图像分配单一离散情感标签，无法揭示视觉元素如何贡献情感。

Method: 提出EmoVerse数据集，采用多阶段标注流程确保高可靠性，将情感分解为B-A-S三元组并关联到视觉区域，同时包含分类情感状态和维度情感空间的双重标注。

Result: 构建了包含219k+图像的数据集，开发了可解释模型将视觉线索映射到DES表示并提供详细归因解释。

Conclusion: EmoVerse数据集、标注流程和模型共同为推进可解释的高层情感理解提供了全面基础。

Abstract: Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.

</details>


### [99] [SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition](https://arxiv.org/abs/2511.12559)
*Qing Cai,Guihao Yan,Fan Zhang,Cheng Zhang,Zhi Liu*

Main category: cs.CV

TL;DR: 本文提出SEMC框架，结合结构感知特征融合和专家引导对比学习，解决超声标准平面识别中浅层结构信息利用不足和细粒度语义差异捕捉困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效利用浅层结构信息，且难以通过图像增强生成的对比样本捕捉细粒度语义差异，导致超声标准平面的结构和判别细节识别效果不佳。

Method: 提出SEMC框架：1）语义结构融合模块(SSFM)利用多尺度结构信息，通过有效对齐浅层和深层特征增强模型对细粒度结构细节的感知能力；2）专家混合对比识别模块(MCRM)使用MoE机制在多级特征上进行分层对比学习和分类。

Result: 在内部数据集和两个公共数据集上的广泛实验结果表明，SEMC在各种指标上均优于现有最先进方法。

Conclusion: SEMC框架通过结构增强和专家混合对比学习，显著提升了超声标准平面识别的性能，并构建了一个大规模、精心标注的肝脏超声数据集。

Abstract: Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the model's ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.

</details>


### [100] [Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection](https://arxiv.org/abs/2511.12572)
*Mohamed Youssef,Lukas Brunner,Klaus Rundhammer,Gerald Czech,Oliver Bimber*

Main category: cs.CV

TL;DR: 提出了一种结合信号处理和机器学习的新方法，通过遮挡森林植被重建地表温度，用于无人机自主监测野火，实现烟雾或火焰可见前的地面火灾早期检测。


<details>
  <summary>Details</summary>
Motivation: 实现完全自动化的空中野火监测，使用自主无人机在烟雾或火焰可见前早期检测地面火灾。

Method: 训练视觉状态空间模型从热模糊数据中恢复部分遮挡土壤和火灾热点的细微热信号；使用潜在扩散模型和向量量化生成大量真实地表温度模拟数据；通过温度增强和程序化热森林模拟扩展训练数据。

Result: 在模拟数据上，相比传统热成像和未校正SA成像，RMSE降低了2到2.5倍；在野外实验中，对高温热点的改进更显著，RMSE增益分别为12.8倍和2.6倍；能够重建火灾和人类特征的完整形态学特征。

Conclusion: 该方法能够有效克服植被遮挡问题，准确重建地表温度，在野火监测和搜救等应用中具有显著优势，特别是在检测细微热信号和完整形态重建方面优于传统成像方法。

Abstract: We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion.

</details>


### [101] [Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection](https://arxiv.org/abs/2511.12575)
*Jiayi Zhu,Yihao Huang,Yue Cao,Xiaojun Jia,Qing Guo,Felix Juefei-Xu,Geguang Pu,Bin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本攻击的地理隐私保护方法，通过在图像外部添加欺骗性文本来干扰大型视觉语言模型的地理位置推断，相比传统对抗性扰动能更好地保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型能够从社交媒体图像中推断用户地理位置，造成严重的隐私泄露威胁。现有的对抗性图像扰动方法需要强失真才能有效，这会显著降低图像视觉质量和分享价值。

Method: 采用两阶段语义感知的文本攻击方法，在图像视觉内容外部添加欺骗性文本，研究有效的文本语义来干扰地理位置推断。

Result: 在三个数据集上的广泛实验表明，该方法显著降低了五种最先进商业LVLMs的地理位置预测准确率。

Conclusion: 该方法建立了一种实用且视觉保持的地理隐私保护策略，有效应对新兴的地理隐私威胁。

Abstract: Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.

</details>


### [102] [OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding](https://arxiv.org/abs/2511.12614)
*Artem Moroz,Vít Zeman,Martin Mikšík,Elizaveta Isianova,Miroslav David,Pavel Burget,Varun Burde*

Main category: cs.CV

TL;DR: 提出了一个统一的端到端框架，将物体检测和姿态估计与灵活的物体注册过程相结合。系统支持从3D CAD模型或通过多视角图像快速重建神经表示来生成物体表示，使用CNOS检测器定位物体，并通过基于transformer的OPFormer模块进行精确的6D姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在物体检测和姿态估计任务中的分离问题，提供统一的解决方案，同时支持模型已知和模型未知两种场景，提高实际应用的灵活性。

Method: 1. 物体注册阶段：从3D CAD模型或多视角图像重建神经表示生成物体表示；2. 检测阶段：使用CNOS检测器定位目标物体；3. 姿态估计：基于transformer的OPFormer模块，结合基础模型特征提取、多视角模板编码和NOCS几何先验，建立2D-3D对应关系。

Result: 在BOP基准测试中表现出色，在准确性和效率之间取得了良好平衡，在模型已知和模型未知场景下都展示了实际应用价值。

Conclusion: 该集成系统提供了一个统一且灵活的解决方案，能够有效处理物体检测和姿态估计任务，在实际应用中具有重要价值。

Abstract: We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.

</details>


### [103] [TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction](https://arxiv.org/abs/2511.12578)
*Yukuo Ma,Cong Liu,Junke Wang,Junqi Liu,Haibin Huang,Zuxuan Wu,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TempoMaster是一个新颖的长视频生成框架，将长视频生成建模为下一帧率预测问题，通过从低帧率到高帧率的渐进式生成实现高效并行合成和长期时序一致性。


<details>
  <summary>Details</summary>
Motivation: 解决长视频生成中的长期时序一致性和生成效率问题，传统方法难以同时保证长距离时间连贯性和高效并行合成。

Method: 首先生成低帧率视频片段作为粗粒度蓝图，然后逐步提高帧率来细化视觉细节和运动连续性；在生成过程中，每个帧率级别内使用双向注意力，跨帧率级别进行自回归预测。

Result: 大量实验表明，TempoMaster在长视频生成方面达到了新的最先进水平，在视觉质量和时序质量方面都表现出色。

Conclusion: TempoMaster通过将长视频生成建模为帧率预测问题，成功实现了长期时序一致性和高效并行合成的平衡，为长视频生成提供了有效的解决方案。

Abstract: We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.

</details>


### [104] [C3Net: Context-Contrast Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12627)
*Baber Jan,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: C3Net是一个用于伪装目标检测的双路径解码器架构，通过边缘细化路径和上下文定位路径协同工作，解决了伪装目标检测中的六个基本挑战，在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 伪装目标检测面临传统分割方法和现代基础模型的失败，主要由于六个基本挑战：内在相似性、边缘破坏、极端尺度变化、环境复杂性、上下文依赖性和显著-伪装目标区分。这些挑战经常同时出现，增加了检测难度。

Method: 提出C3Net双路径解码器架构：边缘细化路径使用梯度初始化边缘增强模块从早期特征恢复精确边界；上下文定位路径采用基于图像的上下文引导机制实现内在显著性抑制；通过注意力融合模块协同结合两个路径。

Result: 在COD10K数据集上S-measure达到0.898，CAMO数据集上0.904，NC4K数据集上0.913，实现了最先进的性能，同时保持高效处理。

Conclusion: C3Net证明复杂多方面的检测挑战需要架构创新，通过专门组件协同工作实现全面覆盖，超越了孤立改进的效果。

Abstract: Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.

</details>


### [105] [Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting](https://arxiv.org/abs/2511.12588)
*Zuqi Huang,Mengxin Tian,Huan Liu,Wentao Li,Baobao Liang,Jie Wu,Fang Yan,Zhaoqing Tang,Zhongyu Li*

Main category: cs.CV

TL;DR: 提出CountIHC框架，通过排名感知的教师选择策略和多模态微调，实现IHC图像中多类细胞的高精度计数


<details>
  <summary>Details</summary>
Motivation: IHC图像细胞计数对癌症诊断至关重要，但现有方法难以处理染色重叠、形态多样性和多类计数问题，且基础模型潜力未充分挖掘

Method: 使用排名感知教师选择策略评估基础模型计数能力，通过视觉-语言对齐微调，利用结构化文本提示生成类别密度图

Result: 在12种IHC生物标志物和5种组织类型上超越现有方法，与病理学家评估高度一致，在H&E染色数据上也表现良好

Conclusion: CountIHC框架有效解决了IHC图像多类细胞计数挑战，展示了基础模型知识蒸馏和视觉-语言对齐在医学图像分析中的潜力

Abstract: Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&E-stained data further confirms the scalability of the proposed method.

</details>


### [106] [Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation](https://arxiv.org/abs/2511.12631)
*Yushe Cao,Dianxi Shi,Xing Fu,Xuechao Zou,Haikuo Peng,Xueqi Li,Chun Yu,Junliang Xing*

Main category: cs.CV

TL;DR: MDiTFace是一个基于扩散变换器的多模态人脸生成框架，通过统一标记化策略处理语义掩码和文本输入，采用解耦注意力机制减少计算开销，在面部保真度和条件一致性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态特征融合方法难以实现有效的跨模态交互，导致生成效果不理想。为了解决异构模态表示之间的差异问题，需要开发新的框架来促进全面的多模态特征交互。

Method: 提出MDiTFace框架：1）使用统一标记化策略处理语义掩码和文本输入；2）采用堆叠的多变量变换器块同步处理所有条件；3）设计解耦注意力机制，将内部计算分离为动态和静态路径，减少94%的计算开销。

Result: 大量实验表明，MDiTFace在面部保真度和条件一致性方面显著优于其他竞争方法，同时保持了性能。

Conclusion: MDiTFace通过统一的标记化策略和创新的解耦注意力机制，有效解决了多模态人脸生成中的跨模态交互问题，在减少计算开销的同时实现了优异的生成质量。

Abstract: While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.

</details>


### [107] [Seg-VAR: Image Segmentation with Visual Autoregressive Modeling](https://arxiv.org/abs/2511.12594)
*Rongkun Zheng,Lu Qi,Xi Chen,Yi Wang,Kun Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Seg-VAR将分割任务重新定义为条件自回归掩码生成问题，通过多阶段训练策略学习分割掩码的潜在表示，在多个分割任务和验证基准上超越了之前的判别性和生成性方法。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归建模策略在图像生成方面已取得成果，但其在需要精确低层空间感知的分割任务中的潜力尚未被探索。受经典Mask2Former模型多尺度建模的启发，研究者希望将分割重新构想为条件自回归掩码生成问题。

Method: Seg-VAR包含三个核心组件：图像编码器生成潜在先验、空间感知的seglat编码器将分割掩码映射为离散潜在token、解码器从这些潜在表示重建掩码。采用三阶段训练策略：首先通过图像-seglat联合训练学习seglat表示，然后精炼潜在变换，最后对齐图像编码器导出的潜在与seglat分布。

Result: 实验表明Seg-VAR在各种分割任务和验证基准上超越了之前的判别性和生成性方法。

Conclusion: 通过将分割构建为顺序层次预测任务，Seg-VAR为将自回归推理整合到空间感知视觉系统中开辟了新途径。

Abstract: While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at https://github.com/rkzheng99/Seg-VAR.

</details>


### [108] [BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections](https://arxiv.org/abs/2511.12676)
*Subin Varghese,Joshua Gao,Asad Ur Rahman,Vedhus Hoskere*

Main category: cs.CV

TL;DR: 本文提出了BridgeEQA基准测试，包含2200个开放词汇问答对，基于真实桥梁检查报告，要求模型在多图像间综合视觉证据并与NBI条件评级对齐。同时提出了EMVR方法，将检查任务建模为基于图像场景图的顺序导航问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试难以真实反映实际部署条件，而基础设施检查领域天然需要多尺度推理、长距离空间理解和复杂语义关系，且可通过标准化NBI条件评级、专业检查报告和第一人称图像进行独特评估。

Method: 提出Embodied Memory Visual Reasoning (EMVR)方法，将检查任务建模为基于图像场景图的顺序导航：图像作为节点，智能体通过马尔可夫决策过程遍历视图、比较证据和推理。

Result: 最先进的视觉语言模型在情景记忆EQA设置下表现出显著性能差距。EMVR方法在基准测试上显示出优于基线的强大性能。

Conclusion: BridgeEQA基准测试为开放词汇体现问答提供了有前景的领域，EMVR方法通过顺序导航框架有效解决了多图像推理问题，为实际部署提供了可行方案。

Abstract: Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.
  We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.
  Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.

</details>


### [109] [LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet](https://arxiv.org/abs/2511.12602)
*Ria Shekhawat,Sushrut Patwardhan,Raghavendra Ramachandra,Praveen Kumar Chandaliya,Kishor P. Upla*

Main category: cs.CV

TL;DR: 提出了一种基于师生框架的单图像形态攻击检测方法，使用CNN教师模型优化ViT学生模型，并集成LoRA进行微调以提高效率。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统对安全至关重要，但仍易受形态攻击影响，其中合成图像融合了多个个体的生物特征。

Method: 使用基于CNN的教师模型和基于ViT的学生模型构建师生框架，集成LoRA进行微调以降低计算成本。

Result: 在包含三种公开人脸数据集和十种不同形态生成算法的数据集上进行广泛实验，与六种最先进的S-MAD技术相比，显示出优越的检测性能和计算效率。

Conclusion: 所提出的方法在形态攻击检测方面表现出卓越的性能和效率，为人脸识别系统的安全性提供了有效保障。

Abstract: Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.

</details>


### [110] [R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection](https://arxiv.org/abs/2511.12691)
*Shuaike Shen,Ke Liu,Jiaqing Xie,Shangde Gao,Chunhua Shen,Ge Liu,Mireia Crispin-Ortuzar,Shangqi Gao*

Main category: cs.CV

TL;DR: R²Seg是一个无需训练的OOD肿瘤分割框架，通过两阶段推理-拒绝过程：首先使用LLM引导的解剖推理规划器定位器官锚点并生成多尺度ROI，然后对基础模型生成的候选区域进行统计测试，仅保留与正常组织显著不同的候选区域，有效抑制假阳性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割的基础模型在分布外（OOD）偏移下表现不佳，经常在OOD肿瘤上产生碎片化的假阳性结果。

Method: 采用两阶段Reason-and-Reject过程：1）Reason步骤使用LLM引导的解剖推理规划器定位器官锚点并生成多尺度ROI；2）Reject步骤对基础模型在ROI内生成的候选区域应用两样本统计测试，仅保留与正常组织显著不同的候选区域。

Result: 在多中心和多模态肿瘤分割基准测试中，R²Seg在Dice系数、特异性和敏感性方面显著优于强基线方法和原始基础模型。

Conclusion: R²Seg是一个无需参数更新的训练免费框架，兼容零更新测试时间增强，避免了灾难性遗忘，显著提升了OOD肿瘤分割的鲁棒性。

Abstract: Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.

</details>


### [111] [HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models](https://arxiv.org/abs/2511.12693)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: HEDGE是一个统一的幻觉检测框架，通过视觉扰动、语义聚类和不确定性度量来检测视觉语言模型中的幻觉问题，在多种模型架构上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型容易产生幻觉，需要系统性的检测方法来评估多模态可靠性。

Method: 结合受控视觉扰动、语义聚类（蕴含和嵌入两种方法）和鲁棒性不确定性度量，构建可复现的检测流水线。

Result: 评估显示幻觉可检测性在密集视觉标记化模型中最高，嵌入聚类在直接应用于生成答案时效果更好，VASE度量提供最鲁棒的幻觉信号。

Conclusion: HEDGE将幻觉检测构建为几何鲁棒性问题，提供了评估多模态可靠性的原则性基础。

Abstract: Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.
  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.
  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .

</details>


### [112] [Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion](https://arxiv.org/abs/2511.12757)
*Nicholas Karris,Luke Durell,Javier Flores,Tegan Emerson*

Main category: cs.CV

TL;DR: 论文发现Stable Diffusion对CLIP嵌入矩阵具有置换不变性，因此可将嵌入视为Wasserstein空间中的点云而非欧几里得空间中的矩阵。基于此提出了基于最优传输的插值方法，能生成更平滑连贯的中间图像。


<details>
  <summary>Details</summary>
Motivation: 受Stable Diffusion对CLIP嵌入矩阵置换不变性的启发，探索将嵌入解释为Wasserstein空间中的点云而非欧几里得空间中的矩阵，以更好地理解嵌入空间的几何结构。

Method: 将嵌入插值问题重新定义为最优传输问题，通过求解最优传输问题计算嵌入之间的最短路径（测地线），在嵌入空间中实现更自然和几何平滑的过渡。

Result: 实验表明，基于最优传输的插值方法相比其他标准插值方法能生成更平滑和连贯的中间图像。

Conclusion: 将嵌入视为点云而非矩阵能更好地反映和利用嵌入空间的几何结构，基于最优传输的插值方法确实能提供更平滑的图像插值效果。

Abstract: It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.

</details>


### [113] [Denoising Vision Transformer Autoencoder with Spectral Self-Regularization](https://arxiv.org/abs/2511.12633)
*Xunzhi Xiang,Xingye Tian,Guiyu Zhang,Yabo Chen,Shaofeng Zhang,Xuebo Wang,Xin Tao,Qi Fan*

Main category: cs.CV

TL;DR: 本文提出了一种新的变分自编码器Denoising-VAE，通过频谱自正则化策略抑制高维潜在空间中的冗余高频噪声，从而改善扩散模型的训练收敛性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统VAE在高维潜在空间中存在优化困境：高维潜在空间虽然能提高重建保真度，但会损害生成性能。现有方法依赖外部视觉基础模型进行正则化，但高维潜在如何影响生成模型优化仍不清楚。

Method: 提出频谱自正则化策略来抑制冗余高频噪声，同时保持重建质量；引入频谱对齐策略来优化基于Denoising-VAE的生成模型；使用ViT-based自编码器，不依赖VFMs。

Result: 在ImageNet 256×256基准测试中：重建质量达到SOTA（rFID=0.28，PSNR=27.26）；生成性能具有竞争力（gFID=1.82）；扩散模型收敛速度比SD-VAE快约2倍。

Conclusion: Denoising-VAE通过抑制高维潜在空间中的冗余高频噪声，有效解决了VAE的优化困境，实现了更好的重建质量和生成性能，同时加速了扩散模型的训练收敛。

Abstract: Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\times$256 benchmark.

</details>


### [114] [Lightweight Optimal-Transport Harmonization on Edge Devices](https://arxiv.org/abs/2511.12785)
*Maria Larchenko,Dmitry Guskov,Alexander Lobashev,Georgy Derevyanko*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的颜色协调方法MKL-Harmonizer，用于增强现实中的实时颜色协调，通过训练紧凑编码器预测Monge-Kantorovich传输映射，支持设备端推理。


<details>
  <summary>Details</summary>
Motivation: 解决增强现实中颜色协调问题，当前缺乏实时解决方案，无法集成到AR流程中。

Method: 利用经典最优传输理论，训练紧凑编码器预测Monge-Kantorovich传输映射，实现轻量级颜色协调。

Result: 在真实AR合成图像上，MKL-Harmonizer方法获得了最佳综合得分，优于现有最先进方法。

Conclusion: 提出的轻量级方法支持设备端推理，解决了AR颜色协调的实时需求，并发布了专用AR数据集和工具包。

Abstract: Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.

</details>


### [115] [Medical Knowledge Intervention Prompt Tuning for Medical Image Classification](https://arxiv.org/abs/2511.12639)
*Ye Du,Nanxi Yu,Shujun Wang*

Main category: cs.CV

TL;DR: CILMP是一种将大型语言模型集成到视觉语言模型提示调优中的方法，通过提取疾病特定表征并在低秩线性子空间中进行干预，生成疾病特定的自适应提示，在医学图像分类任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示调优方法无法精确区分不同类型的医学概念，缺乏跨医学成像模态的疾病相关特征。大型语言模型在提供专业医学知识方面表现出色，因此将其集成到提示调优过程中。

Method: 提出CILMP方法，从LLMs提取疾病特定表征，在低秩线性子空间中进行干预，并利用这些表征创建疾病特定提示。加入条件机制，根据每个医学图像调整干预过程，生成实例自适应提示。

Result: 在多个医学图像数据集上的广泛实验表明，CILMP始终优于最先进的提示调优方法，证明了其有效性。

Conclusion: CILMP成功地将LLMs与VLMs桥接，促进了医学知识向VLM提示的转移，提高了医学图像分类任务的性能。

Abstract: Vision-language foundation models (VLMs) have shown great potential in feature transfer and generalization across a wide spectrum of medical-related downstream tasks. However, fine-tuning these models is resource-intensive due to their large number of parameters. Prompt tuning has emerged as a viable solution to mitigate memory usage and reduce training time while maintaining competitive performance. Nevertheless, the challenge is that existing prompt tuning methods cannot precisely distinguish different kinds of medical concepts, which miss essentially specific disease-related features across various medical imaging modalities in medical image classification tasks. We find that Large Language Models (LLMs), trained on extensive text corpora, are particularly adept at providing this specialized medical knowledge. Motivated by this, we propose incorporating LLMs into the prompt tuning process. Specifically, we introduce the CILMP, Conditional Intervention of Large Language Models for Prompt Tuning, a method that bridges LLMs and VLMs to facilitate the transfer of medical knowledge into VLM prompts. CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and utilizes them to create disease-specific prompts. Additionally, a conditional mechanism is incorporated to condition the intervention process on each individual medical image, generating instance-adaptive prompts and thus enhancing adaptability. Extensive experiments across diverse medical image datasets demonstrate that CILMP consistently outperforms state-of-the-art prompt tuning methods, demonstrating its effectiveness. Code is available at https://github.com/usr922/cilmp.

</details>


### [116] [DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry](https://arxiv.org/abs/2511.12653)
*Cheng Liao*

Main category: cs.CV

TL;DR: DPVO-QAT++是一个用于深度视觉SLAM系统的分层量化优化框架，通过可学习尺度参数化、异构精度设计和GPU原生内核融合，显著降低了内存占用并提高了处理速度，同时保持了原始模型的轨迹精度。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的视觉SLAM系统虽然具有出色的几何推理能力，但其过高的计算开销严重限制了在资源受限的自主平台上的部署。

Method: 采用分层量化优化框架，包括可学习尺度参数化、视觉里程计前端和后端的异构精度设计（前端浮点伪量化FP16/FP32，后端全精度），以及用于伪量化的GPU原生内核融合（自定义CUDA内核）。

Result: 在TartanAir数据集上，平均FPS提升52.1%，中位延迟降低29.1%，峰值GPU内存占用减少64.9%；在EuRoC数据集上，平均FPS提升30.1%，中位延迟降低23.1%，峰值GPU内存占用减少37.7%，同时保持与原始DPVO模型相当的轨迹精度。

Conclusion: DPVO-QAT++有效弥合了高精度深度视觉里程计与实际部署效率要求之间的差距，为在真实嵌入式平台上应用该技术提供了可行的工程范式。

Abstract: Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms. This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry). Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model. On the TartanAir dataset, our framework achieves an average FPS increase of 52.1%, a 29.1% reduction in median latency, and a 64.9% reduction in peak GPU memory reservation, while maintaining trajectory accuracy (ATE) comparable to the original DPVO model across 32 validation sequences. On the EuRoC dataset, it realizes an average FPS increase of 30.1%, a 23.1% reduction in median latency, and a 37.7% reduction in peak GPU memory reservation, maintaining comparable trajectory accuracy (ATE) across 11 validation sequences. Experimental results demonstrate that DPVO-QAT++ effectively bridges the gap between high-precision deep VO and the efficiency requirements for practical deployment, offering a viable engineering paradigm for the application of this technology on real-world embedded platforms.
  Keywords: Visual Odometry, Heterogeneous Precision Architecture, Quantization-Aware Training, CUDA Kernel Fusion, Scale-Only Training, Deep Patch Visual Odometry, GPU-Native Kernel Fusion.

</details>


### [117] [Toward Real-world Text Image Forgery Localization: Structured and Interpretable Data Synthesis](https://arxiv.org/abs/2511.12658)
*Zeqin Yu,Haotao Xie,Jian Zhang,Jiangqun Ni,Wenkan Su,Jiwu Huang*

Main category: cs.CV

TL;DR: 提出FSTS框架，通过分析真实篡改行为参数，使用类傅里叶级数方法合成更真实的训练数据，提升文本图像篡改定位模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有T-IFL方法泛化能力差，主要因为真实数据集规模有限，合成数据无法捕捉真实篡改的复杂性。

Method: 收集16,750个真实篡改实例，通过结构化管道记录编辑痕迹，建立分层建模框架，将篡改参数表示为基操作配置的组合，使用傅里叶级数启发的可解释近似方法。

Result: 在四个评估协议上的广泛实验表明，使用FSTS数据训练的模型在真实数据集上实现了显著改进的泛化能力。

Conclusion: FSTS框架能够合成多样且真实的训练数据，有效提升文本图像篡改定位模型的泛化性能。

Abstract: Existing Text Image Forgery Localization (T-IFL) methods often suffer from poor generalization due to the limited scale of real-world datasets and the distribution gap caused by synthetic data that fails to capture the complexity of real-world tampering. To tackle this issue, we propose Fourier Series-based Tampering Synthesis (FSTS), a structured and interpretable framework for synthesizing tampered text images. FSTS first collects 16,750 real-world tampering instances from five representative tampering types, using a structured pipeline that records human-performed editing traces via multi-format logs (e.g., video, PSD, and editing logs). By analyzing these collected parameters and identifying recurring behavioral patterns at both individual and population levels, we formulate a hierarchical modeling framework. Specifically, each individual tampering parameter is represented as a compact combination of basis operation-parameter configurations, while the population-level distribution is constructed by aggregating these behaviors. Since this formulation draws inspiration from the Fourier series, it enables an interpretable approximation using basis functions and their learned weights. By sampling from this modeled distribution, FSTS synthesizes diverse and realistic training data that better reflect real-world forgery traces. Extensive experiments across four evaluation protocols demonstrate that models trained with FSTS data achieve significantly improved generalization on real-world datasets. Dataset is available at \href{https://github.com/ZeqinYu/FSTS}{Project Page}.

</details>


### [118] [SAGA: Source Attribution of Generative AI Videos](https://arxiv.org/abs/2511.12834)
*Rohit Kundu,Vishal Mohanty,Hao Xiong,Shan Jia,Athula Balachandran,Amit K. Roy-Chowdhury*

Main category: cs.CV

TL;DR: SAGA是首个用于AI生成视频源归属的综合框架，通过多粒度归属识别生成模型，仅需0.5%的源标记数据即可达到最先进性能，并提供可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，超逼真的合成视频激增，带来了滥用风险，传统的二元真伪检测已无法满足需求，迫切需要能够识别具体生成模型的大规模视频源归属方法。

Method: 提出新颖的视频transformer架构，利用鲁棒视觉基础模型特征捕捉时空伪影；采用数据高效的预训练-归属策略；引入时间注意力签名(T-Sigs)可解释性方法可视化学习的时间差异。

Result: 在公共数据集上的广泛实验表明，SAGA在合成视频溯源方面设立了新基准，在跨域场景下也能提供关键的可解释性洞察，仅用每类0.5%的源标记数据即可匹配完全监督性能。

Conclusion: SAGA框架为法证和监管应用提供了丰富、可解释的取证洞察，能够在大规模上有效识别AI生成视频的具体来源，解决了当前视频取证的关键挑战。

Abstract: The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.

</details>


### [119] [Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans](https://arxiv.org/abs/2511.12662)
*Hongbin Huang,Junwei Li,Tianxin Xie,Zhuang Li,Cekai Weng,Yaodong Yang,Yue Luo,Li Liu,Jing Tang,Zhijing Shao,Zeyu Wang*

Main category: cs.CV

TL;DR: 提出了一种高保真、实时的对话数字人系统，结合了逼真的3D虚拟形象、个性化表情语音合成和基于知识的对话生成，通过异步执行管道实现低延迟多模态交互。


<details>
  <summary>Details</summary>
Motivation: 解决数字人在保持视觉真实性的同时实现实时响应性的挑战，为通信、教育和娱乐等沉浸式应用提供可信的数字人交互体验。

Method: 采用异步执行管道协调多模态组件，结合检索增强方法（历史增强和基于意图的路由），支持唤醒词检测、情感表达韵律和上下文感知响应生成。

Result: 开发了一个集成系统，能够实现响应迅速且可信的数字人交互，支持自然及时的互动体验。

Conclusion: 该系统通过整合多个先进组件，为沉浸式应用提供了具有高视觉真实性和实时响应能力的对话数字人解决方案。

Abstract: High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.

</details>


### [120] [DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality](https://arxiv.org/abs/2511.12671)
*Tushar Anand,Advik Sinha,Abhijit Das*

Main category: cs.CV

TL;DR: 提出了一种基于非因果选择性状态空间的实时光学流和视差估计模型，通过融合成对输入图像实现密集感知任务，在保持高精度的同时减少推理时间和GPU使用。


<details>
  <summary>Details</summary>
Motivation: 解决实时应用中光学流和视差估计的高精度与效率平衡问题，满足实时密集3D感知任务的需求。

Method: 使用非因果Mamba块构建模型，融合成对输入图像到非因果选择性状态空间中，优化实时应用的约束条件。

Result: 模型在保持高精度的同时显著减少推理时间，降低GPU使用率，在真实场景中验证了其有效性。

Conclusion: 提出的模型适用于统一的实时准确3D密集感知估计任务，代码和模型已开源。

Abstract: In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD

</details>


### [121] [Video Finetuning Improves Reasoning Between Frames](https://arxiv.org/abs/2511.12868)
*Ruiqi Yang,Tian Yun,Zihan Wang,Ellie Pavlick*

Main category: cs.CV

TL;DR: 本文提出视觉思维链(vCoT)方法，通过生成连续帧之间的过渡事件描述来增强多模态大语言模型的视频理解能力。研究发现视频微调模型已隐含掌握帧间过渡推理能力，而图像模型通过vCoT可获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉理解方面进展迅速，但从图像扩展到视频时往往只是简单拼接帧标记。本文旨在研究视频微调对多模态大语言模型的实际贡献。

Method: 提出视觉思维链(vCoT)方法，生成连续帧之间的过渡事件描述作为显式推理过程。系统比较图像模型与视频微调模型在有/无这些过渡线索情况下的表现。

Result: vCoT显著提升了图像模型在长视频问答任务上的性能，但对视频微调模型仅带来边际增益。视频模型能够将时序推理能力迁移到纯静态场景，在关系视觉推理任务上优于图像模型基线。

Conclusion: 视频微调模型已隐含掌握帧间过渡推理能力，而vCoT为图像模型提供了有效的显式推理机制。视频模型具备将时序推理能力迁移到静态场景的能力。

Abstract: Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.

</details>


### [122] [DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2511.12908)
*Junbo Zou,Haotian Xia,Zhen Ye,Shengjie Zhang,Christopher Lai,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: DeepSport是首个端到端训练的多模态大语言模型框架，专为多任务、多运动的视频理解设计，通过主动迭代推理和专用帧提取工具实现"用视频思考"，在6.7k问题测试基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决体育视频理解中的独特挑战：感知高速动态、理解复杂规则和长时序上下文推理。现有方法要么是单一运动专用、限于特定任务，要么依赖缺乏稳健学习推理过程的免训练范式。

Method: 提出数据蒸馏流水线从10个不同数据源合成高质量思维链轨迹，创建78k训练数据；采用两阶段训练策略：监督微调后接带门控工具使用奖励的强化学习；通过专用帧提取工具实现主动迭代推理。

Result: 在6.7k问题的测试基准上，DeepSport实现了最先进的性能，显著优于专有模型和开源模型的基线。

Conclusion: 这项工作为领域特定视频推理建立了新基础，以应对多样化体育的复杂性。

Abstract: Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.

</details>


### [123] [EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics](https://arxiv.org/abs/2511.12962)
*Daniel Cavadia*

Main category: cs.CV

TL;DR: EndoSight AI是一个深度学习架构，用于实时检测和分割胃肠道息肉，在GPU上达到35+帧/秒的实时推理速度，检测mAP为88.3%，分割Dice系数达69%。


<details>
  <summary>Details</summary>
Motivation: 在胃肠内窥镜检查过程中，精确实时检测胃肠道息肉对于结直肠癌的早期诊断和预防至关重要。

Method: 基于公开的Hyper-Kvasir数据集开发深度学习架构，采用临床相关性能指标和新型热感知程序来确保模型鲁棒性和效率。

Result: 系统实现了88.3%的息肉检测平均精度和高达69%的分割Dice系数，在GPU硬件上实时推理速度超过35帧/秒。

Conclusion: 这种集成AI解决方案旨在无缝部署到内窥镜工作流程中，有望提高胃肠道医疗的诊断准确性和临床决策能力。

Abstract: Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.

</details>


### [124] [CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models](https://arxiv.org/abs/2511.12964)
*Mehrab Mustafy Rahman,Jayanth Mohan,Tiberiu Sosea,Cornelia Caragea*

Main category: cs.CV

TL;DR: 本文提出CalibrateMix方法，通过有针对性的mixup策略改善半监督学习模型的校准性能，在保持或提高分类准确率的同时降低预期校准误差。


<details>
  <summary>Details</summary>
Motivation: 现有半监督学习方法存在校准不佳的问题，模型会产生过度自信的预测。虽然mixup在监督学习中显示出良好的校准效果，但在半监督学习中由于伪标签的过度自信和不可靠性，随机mixup面临挑战。

Method: CalibrateMix利用标注和未标注样本的训练动态来识别'易学习'和'难学习'样本，然后对这些样本进行有针对性的mixup操作。

Result: 在多个基准图像数据集上的实验结果表明，该方法相比现有半监督学习方法实现了更低的预期校准误差和更高的准确率。

Conclusion: CalibrateMix方法有效改善了半监督学习模型的校准性能，同时保持了分类准确性，为解决半监督学习中的校准问题提供了有效方案。

Abstract: Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.

</details>


### [125] [FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling](https://arxiv.org/abs/2511.12708)
*Kaiser Hamid,Can Cui,Khandakar Ashrafi Akbar,Ziran Wang,Nade Liang*

Main category: cs.CV

TL;DR: FSDAM是一个少样本驾驶员注意力建模框架，仅需约100个标注样本即可实现联合注意力预测和描述生成，比现有方法少两个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖大规模注视数据集，但这类数据集收集费时费力。需要开发在数据受限场景下仍能有效工作的驾驶员注意力系统。

Method: 采用双路径架构，分别处理空间预测和描述生成，通过跨模态对齐保持语义一致性。

Result: 在注意力预测方面表现竞争力，生成连贯且上下文感知的解释，并在多个驾驶基准测试中展示出强大的零样本泛化能力。

Conclusion: 证明在有限监督下可以实现有效的注意力条件生成，为在数据受限场景中部署可解释的驾驶员注意力系统开辟了新可能性。

Abstract: Understanding where drivers look and why they shift their attention is essential for autonomous systems that read human intent and justify their actions. Most existing models rely on large-scale gaze datasets to learn these patterns; however, such datasets are labor-intensive to collect and time-consuming to curate. We present FSDAM (Few-Shot Driver Attention Modeling), a framework that achieves joint attention prediction and caption generation with approximately 100 annotated examples, two orders of magnitude fewer than existing approaches. Our approach introduces a dual-pathway architecture where separate modules handle spatial prediction and caption generation while maintaining semantic consistency through cross-modal alignment. Despite minimal supervision, FSDAM achieves competitive performance on attention prediction, generates coherent, and context-aware explanations. The model demonstrates robust zero-shot generalization across multiple driving benchmarks. This work shows that effective attention-conditioned generation is achievable with limited supervision, opening new possibilities for practical deployment of explainable driver attention systems in data-constrained scenarios.

</details>


### [126] [UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective](https://arxiv.org/abs/2511.12988)
*Furui Xu,Shaobo Wang,Jiajun Zhang,Chenghao Sun,Haixiang Tang,Linfeng Zhang*

Main category: cs.CV

TL;DR: UNSEEN是一个即插即用的数据集剪枝框架，从泛化角度对样本进行评分，解决了传统方法在训练阶段评分区分度不足的问题，并在多步场景下优化核心集质量，显著优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习数据集规模增长带来计算挑战，传统数据集剪枝方法基于训练阶段模型性能评分，导致样本评分密集分布、区分度不足，影响有效样本选择。

Method: 从泛化角度进行数据集剪枝，使用未接触过样本的模型进行评分；提出UNSEEN框架可集成到现有方法；扩展到多步场景，通过在不同核心集上训练的模型进行增量选择，动态优化核心集质量。

Result: 在CIFAR-10、CIFAR-100和ImageNet-1K上显著优于现有SOTA方法；在ImageNet-1K上减少30%训练数据的同时实现无损性能。

Conclusion: UNSEEN框架通过泛化视角和多步优化，有效解决了数据集剪枝中的样本区分问题，在保持性能的同时大幅减少计算需求。

Abstract: The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\%.

</details>


### [127] [Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning](https://arxiv.org/abs/2511.12735)
*Ankita Raj,Chetan Arora*

Main category: cs.CV

TL;DR: 本文首次研究开放词汇目标检测器（OVODs）的后门攻击，提出TrAP多模态后门注入策略，通过联合优化图像和文本模态的提示参数与视觉触发器，实现轻量级后门植入而不需重新训练基础模型权重。


<details>
  <summary>Details</summary>
Motivation: 随着OVODs在机器人、自动驾驶等高风险应用中的普及，理解其安全风险变得至关重要。本文旨在揭示由提示调优引入的新攻击面，研究OVODs的后门攻击威胁。

Method: 提出TrAP多模态后门注入策略，采用课程式训练方法逐步缩小触发器尺寸，联合优化图像和文本模态的提示参数与视觉触发器，实现轻量级后门植入。

Result: 在多个数据集上的实验表明，TrAP在物体误分类和物体消失攻击中均实现高攻击成功率，同时在下游数据集上相比零样本设置提高了干净图像性能。

Conclusion: TrAP展示了OVODs面临的新型安全威胁，通过提示调优可实现高效后门攻击，同时保持模型泛化能力，这对实际应用中的安全部署提出了重要警示。

Abstract: Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting.

</details>


### [128] [SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias](https://arxiv.org/abs/2511.13005)
*Wenqian Ye,Di Wang,Guangtao Zheng,Bohan Liu,Aidong Zhang*

Main category: cs.CV

TL;DR: 本文提出SAGE方法，通过引导式提示选择来缓解CLIP模型中的多模态伪相关性偏差，无需训练、微调或外部标注，显著提升零样本分类的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: CLIP等大型视觉语言模型在零样本分类中表现出色，但存在多模态伪相关性偏差问题，即模型倾向于依赖虚假特征（如背景与对象的共现关系）而非核心特征进行推理，这严重影响了模型在分布外数据上的鲁棒性。现有方法通常需要在下游数据上微调或已知偏差先验，削弱了CLIP的开箱即用性。

Method: 提出SAGE方法：首先理论分析多模态伪相关性偏差在零样本分类中的影响；然后通过引导式提示选择，探索提示模板空间并选择能诱导最大类别语义分离的提示，从而改善最差组鲁棒性。该方法无需训练、微调或外部标注。

Result: 在四个真实世界基准数据集和五个流行骨干模型上的广泛实验表明，SAGE持续提升零样本性能和泛化能力，在无需外部知识或模型更新的情况下优于之前的零样本方法。

Conclusion: SAGE是一种简单有效的零样本方法，能够有效缓解CLIP模型中的多模态伪相关性偏差，显著提高模型在分布外数据上的鲁棒性，同时保持开箱即用的优势。

Abstract: Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.

</details>


### [129] [Direct Visual Grounding by Directing Attention of Visual Tokens](https://arxiv.org/abs/2511.12738)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文发现视觉语言模型(VLMs)在最终层对视觉token关注不足的问题，提出了一种新的KL注意力损失函数来直接监督视觉token的注意力分布，从而改善视觉任务的性能。


<details>
  <summary>Details</summary>
Motivation: 标准的下一个token预测损失无法有效引导注意力到相关的视觉token，导致视觉问答错误。需要更直接的监督机制来改善视觉token的注意力分配。

Method: 提出KL注意力损失(KLAL)，通过KL散度将视觉token的注意力分布与真实注意力图对齐。真实注意力图来自合成案例的任务几何或真实图像的标准标注，无需新标签。

Result: 在几何任务、指向和指代表达理解等任务上取得显著改进，同时引入了一个新的线段追踪评估数据集。

Conclusion: 直接监督视觉token的注意力分布能有效提升VLMs在视觉任务上的性能，即使是商业VLMs在复杂视觉任务上表现也不佳。

Abstract: Vision Language Models (VLMs) mix visual tokens and text tokens. A puzzling issue is the fact that visual tokens most related to the query receive little to no attention in the final layers of the LLM module of VLMs from the answer tokens, where all tokens are treated equally, in particular, visual and language tokens in the LLM attention layers. This fact may result in wrong answers to visual questions, as our experimental results confirm. It appears that the standard next-token prediction (NTP) loss provides an insufficient signal for directing attention to visual tokens. We hypothesize that a more direct supervision of the attention of visual tokens to corresponding language tokens in the LLM module of VLMs will lead to improved performance on visual tasks. To demonstrate that this is indeed the case, we propose a novel loss function that directly supervises the attention of visual tokens. It directly grounds the answer language tokens in images by directing their attention to the relevant visual tokens. This is achieved by aligning the attention distribution of visual tokens to ground truth attention maps with KL divergence. The ground truth attention maps are obtained from task geometry in synthetic cases or from standard grounding annotations (e.g., bounding boxes or point annotations) in real images, and are used inside the LLM for attention supervision without requiring new labels. The obtained KL attention loss (KLAL) when combined with NTP encourages VLMs to attend to relevant visual tokens while generating answer tokens. This results in notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data, as demonstrated by our experiments. We also introduce a new dataset to evaluate the line tracing abilities of VLMs. Surprisingly, even commercial VLMs do not perform well on this task.

</details>


### [130] [MeanFlow Transformers with Representation Autoencoders](https://arxiv.org/abs/2511.13019)
*Zheyuan Hu,Chieh-Hsin Lai,Ge Wu,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: 本文提出了一种在表示自编码器（RAE）潜在空间中训练MeanFlow（MF）的高效方法，通过一致性中期训练和两阶段方案解决了梯度爆炸问题，显著降低了训练和采样成本，并在ImageNet数据集上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的MeanFlow模型在训练时计算量大且不稳定，推理时SD-VAE解码器成本高，且需要复杂的引导超参数进行类别条件生成。本文旨在开发更高效的MF训练和采样方案。

Method: 在RAE潜在空间中训练MF，采用一致性中期训练进行轨迹感知初始化，使用两阶段方案：从预训练的流匹配教师模型蒸馏以加速收敛和减少方差，然后可选地使用单点速度估计器进行引导阶段以进一步减少与理想平均流的偏差。

Result: 在ImageNet 256上实现了1步FID 2.03，优于传统MF的3.43，同时采样GFLOPS减少38%，总训练成本降低83%。在ImageNet 512上达到竞争性的1步FID 3.23，在所有基线中GFLOPS最低。

Conclusion: 该方法消除了对引导的需求，简化了训练配置，显著降低了训练和采样计算成本，同时保持了优异的生成质量，为高效生成模型提供了新的解决方案。

Abstract: MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.

</details>


### [131] [Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests](https://arxiv.org/abs/2511.12740)
*Amirhossein Hassanzadeh,Bartosz Krawczyk,Michael Saunders,Rob Wible,Keith Krause,Dimah Dera,Jan van Aardt*

Main category: cs.CV

TL;DR: 本研究探索从体素化的LiDAR点云数据中推断低级别体素内容信息（目标占用百分比），提出基于核点卷积的多目标回归方法，通过密度相关性和加权损失函数解决类别不平衡问题，并通过体素尺寸敏感性分析发现较大体素尺寸（2米）误差较低，而较小体素尺寸（0.25-0.5米）在树冠区域误差较高。


<details>
  <summary>Details</summary>
Motivation: 体素化是降低LiDAR数据处理计算成本的有效方法，但会导致细尺度结构信息丢失。本研究旨在探索是否可以从高级别的体素化LiDAR点云数据中推断低级别的体素内容信息，特别是目标在体素内的占用百分比。

Method: 提出基于核点卷积的多目标回归方法，采用密度相关性解决类别不平衡问题，使用加权均方误差、焦点回归和正则化来优化KPConv。对体素尺寸（0.25-2米）进行敏感性分析，评估不同网格表示在捕捉森林细节方面的效果。

Result: 敏感性分析显示，较大体素尺寸（如2米）由于变异性减少而误差较低，而较小体素尺寸（如0.25或0.5米）误差较高，特别是在变异性最大的树冠区域。对于树皮和树叶目标，较小体素尺寸数据集的误差值显著高于较大体素尺寸数据集。

Conclusion: 体素尺寸的选择取决于具体应用需求。本研究填补了深度不平衡学习模型在多目标回归和森林3D LiDAR点云模拟数据集方面的空白。

Abstract: Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.

</details>


### [132] [SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction](https://arxiv.org/abs/2511.13020)
*Yufei Wen,Yuting Zhang,Jingdan Kang,Hao Ren,Weibin Cheng,Jintai Chen,Kaishun Wu*

Main category: cs.CV

TL;DR: SpectralAdapt是一个半监督域自适应框架，用于解决人类中心HSI数据稀缺问题，通过光谱密度掩码和光谱端元表示对齐来弥合通用域和人类域之间的差距。


<details>
  <summary>Details</summary>
Motivation: 解决医疗应用中人类HSI数据稀缺的问题，同时利用丰富的通用域HSI数据和有限的标记人类HSI数据。

Method: 提出SpectralAdapt框架，包含光谱密度掩码（SDM）自适应掩码RGB通道，以及光谱端元表示对齐（SERA）使用物理可解释的端元作为域不变锚点。

Result: 在基准数据集上实验显示，在光谱保真度、跨域泛化能力和训练稳定性方面均有持续改进。

Conclusion: SSDA是医疗领域高光谱成像的高效解决方案，能有效缓解域偏移、光谱退化和数据稀缺问题。

Abstract: Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.

</details>


### [133] [Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations](https://arxiv.org/abs/2511.13081)
*Yehonatan Elisha,Seffi Cohen,Oren Barkan,Noam Koenigstein*

Main category: cs.CV

TL;DR: 提出了RFxG分类法，从参考框架和粒度两个维度组织显著性解释，揭示了现有评估指标的局限性，并提出了四个新的忠实度指标来系统评估解释质量。


<details>
  <summary>Details</summary>
Motivation: 显著性图在深度学习视觉解释中广泛使用，但对其预期目的和与不同用户查询的对齐缺乏共识，这种模糊性阻碍了解释方法的有效评估和实际效用。

Method: 引入RFxG分类法框架，包含参考框架（点对点vs对比）和粒度（细粒度类级vs粗粒度组级）两个维度，提出四个新的忠实度指标，在十个最先进的显著性方法、四个模型架构和三个数据集上进行综合评估。

Result: 通过RFxG视角揭示了现有评估指标主要关注点对点忠实度而忽视对比推理和语义粒度的关键局限性，提出的新指标能够系统评估解释质量。

Conclusion: 通过倡导向用户意图驱动的评估转变，为开发既忠实于模型行为又符合人类理解和查询复杂性的视觉解释提供了概念基础和实践工具。

Abstract: Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise ("Why this prediction?") and contrastive ("Why this and not an alternative?") explanations.Granularity: Ranging from fine-grained class-level (e.g., "Why Husky?") to coarse-grained group-level (e.g., "Why Dog?") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.

</details>


### [134] [RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.12767)
*Cătălin-Alexandru Rîpanu,Andrei-Theodor Hotnog,Giulia-Stefania Imbrea,Dumitru-Clementin Cercel*

Main category: cs.CV

TL;DR: 本文提出了RoCoISLR数据集，这是首个用于罗马尼亚孤立手语识别的大规模标准化数据集，包含9000多个视频样本和近6000个标准化词汇。作者评估了7种最先进的视频识别模型，发现基于Transformer的架构表现最佳，Swin Transformer达到34.1%的Top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 目前大多数手语识别数据集都专注于美国手语，而罗马尼亚孤立手语识别缺乏大规模标准化数据集，这限制了该领域的研究进展。

Method: 构建了RoCoISLR数据集，包含9000多个视频样本和近6000个标准化词汇。在一致的实验设置下评估了7种最先进的视频识别模型：I3D、SlowFast、Swin Transformer、TimeSformer、Uniformer、VideoMAE和PoseConv3D。

Result: 基于Transformer的架构在性能上优于基于卷积的基线模型。Swin Transformer取得了最佳性能，Top-1准确率达到34.1%。与广泛使用的WLASL2000语料库相比，RoCoISLR数据集上的性能表现显示了低资源手语中长尾类别分布带来的挑战。

Conclusion: RoCoISLR数据集为系统性的罗马尼亚孤立手语识别研究提供了初步基础，基准测试结果突显了在低资源手语中处理长尾类别分布的重要性。

Abstract: Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.

</details>


### [135] [Enhancing Neuro-Oncology Through Self-Assessing Deep Learning Models for Brain Tumor Unified Model for MRI Segmentation](https://arxiv.org/abs/2511.12801)
*Andrew Zhou*

Main category: cs.CV

TL;DR: 本研究提出了一个不确定性感知的脑肿瘤分割框架，通过增强nnUNet添加体素级不确定性通道，在保持肿瘤分割精度的同时提供不确定性估计，并结合正常和癌症数据集实现全脑结构分割。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在脑肿瘤分割中存在两个主要问题：缺乏对分割错误的确定性估计，以及无法在手术中提供肿瘤周围健康脑结构的分割。现有方法未能统一肿瘤定位与解剖学背景，且缺乏置信度评分。

Method: 通过增强nnUNet框架，添加一个用于体素级不确定性的通道，在BraTS2023数据集上训练。开发统一模型结合正常和癌症数据集，实现全脑结构分割。

Result: 不确定性估计获得0.750的相关性和0.047的RMSD，且不影响肿瘤分割精度。全脑分割模型在脑结构上达到0.81的DSC，在肿瘤上达到0.86的DSC，关键区域表现稳健。

Conclusion: 该研究首次实现了输出肿瘤在自然环境中分割结果并叠加不确定性地图的模型，不确定性提供了评估预测和修正错误的关键见解，有助于基于AI做出明智的手术决策。

Abstract: Accurate segmentation of brain tumors is vital for diagnosis, surgical planning, and treatment monitoring. Deep learning has advanced on benchmarks, but two issues limit clinical use: no uncertainty estimates for errors and no segmentation of healthy brain structures around tumors for surgery. Current methods fail to unify tumor localization with anatomical context and lack confidence scores. This study presents an uncertainty-aware framework augmenting nnUNet with a channel for voxel-wise uncertainty. Trained on BraTS2023, it yields a correlation of 0.750 and RMSD of 0.047 for uncertainty without hurting tumor accuracy. It predicts uncertainty in one pass, with no extra networks or inferences, aiding clinical decisions. For whole-brain context, a unified model combines normal and cancer datasets, achieving a DSC of 0.81 for brain structures and 0.86 for tumor, with robust key-region performance. Combining both innovations gives the first model outputting tumor in natural surroundings plus an overlaid uncertainty map. Visual checks of outputs show uncertainty offers key insights to evaluate predictions and fix errors, helping informed surgical decisions from AI.

</details>


### [136] [SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration](https://arxiv.org/abs/2511.13168)
*Haodong Wang,Tao Zhuo,Xiuwei Zhang,Hanlin Yin,Wencong Wu,Yanning Zhang*

Main category: cs.CV

TL;DR: SOMA是一个集成结构梯度先验的SAR-光学图像密集配准框架，通过特征梯度增强器和全局-局部仿射流匹配器，显著提高了配准精度。


<details>
  <summary>Details</summary>
Motivation: 由于SAR和光学图像成像机制和视觉特性的根本差异，实现像素级配准仍然具有挑战性。虽然深度学习在许多跨模态任务中取得了成功，但在SAR-光学配准任务中的表现仍不理想，梯度信息在深度学习中未被有效利用。

Method: 提出SOMA框架：1）特征梯度增强器(FGE)，通过注意力和重建机制将多尺度、多方向梯度滤波器嵌入特征空间；2）全局-局部仿射流匹配器(GLAM)，在粗到细架构中结合仿射变换和基于流的细化。

Result: 在SEN1-2数据集上CMR@1px提高12.29%，在GFGE_SO数据集上提高18.50%。SOMA表现出强大的鲁棒性，在不同场景和分辨率下泛化良好。

Conclusion: SOMA通过有效整合梯度先验和混合匹配策略，显著提升了SAR-光学图像配准性能，证明了梯度信息在深度配准框架中的重要性。

Abstract: Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.

</details>


### [137] [GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models](https://arxiv.org/abs/2511.13259)
*Yushuo Zheng,Jiangyong Ying,Huiyu Duan,Chunyi Li,Zicheng Zhang,Jing Liu,Xiaohong Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文介绍了GeoX-Bench基准测试，用于评估大型多模态模型在跨视角地理定位和姿态估计任务中的能力，包含10,859个全景-卫星图像对和755,976个问答对。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型在多种任务中表现出色，但其在跨视角地理定位和姿态估计领域的能力尚未被探索，这些能力对导航、自动驾驶、户外机器人等应用具有重要价值。

Method: 构建了包含128个城市、49个国家的10,859个全景-卫星图像对和755,976个问答对的GeoX-Bench基准测试，并评估了25个最先进的大型多模态模型。

Result: 当前大型多模态模型在地理定位任务中表现优异，但在更复杂的姿态估计任务中效果显著下降；通过在GeoX-Bench训练数据上进行指令调优可以显著提升跨视角地理感知能力。

Conclusion: GeoX-Bench揭示了大型多模态模型在跨视角地理定位和姿态估计方面的能力差距，为未来改进提供了重要方向，指令调优能有效提升模型的地理感知能力。

Abstract: Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.

</details>


### [138] [Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views](https://arxiv.org/abs/2511.12878)
*Junyi Ma,Wentao Bao,Jingyi Xu,Guanzhong Sun,Yu Zheng,Erhang Zhang,Xieyuanli Chen,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为EgoLoc的零样本方法，用于在自我中心视频中定位手与物体接触和分离的时间戳，解决了现有方法在准确捕捉交互关键时刻方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注交互动作的行为范式（"如何交互"），而捕捉手与目标物体接触和分离的关键时刻（"何时交互"）这一更具挑战性的细粒度问题尚未充分探索，这对于混合现实中的沉浸式交互体验和机器人运动规划至关重要。

Method: 提出EgoLoc方法，引入手动力学引导采样生成高质量视觉提示，利用视觉语言模型识别接触/分离属性、定位特定时间戳，并提供闭环反馈进行进一步优化。该方法无需物体掩码和动词-名词分类法，实现可泛化的零样本实施。

Result: 在公共数据集和新基准上的综合实验表明，EgoLoc在自我中心视频中实现了合理的时序交互定位，并验证了其能有效促进自我中心视觉和机器人操作任务中的多个下游应用。

Conclusion: EgoLoc通过零样本方法成功解决了手-物体交互关键时刻的定位问题，为混合现实和机器人应用提供了有效的解决方案。

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., "how to interact"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., "when to interact") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [139] [Simple Lines, Big Ideas: Towards Interpretable Assessment of Human Creativity from Drawings](https://arxiv.org/abs/2511.12880)
*Zihao Lin,Zhenshan Shi,Sasa Zhao,Hanwei Zhu,Lingyu Zhu,Baoliang Chen,Lei Mo*

Main category: cs.CV

TL;DR: 本文提出了一种基于数据驱动的自动可解释绘画创造力评估框架，通过多模态多任务学习同时预测创造力分数、分类内容类型和提取风格特征。


<details>
  <summary>Details</summary>
Motivation: 当前绘画创造力评估主要依赖专家主观评分，既费时又主观，需要自动化和可解释的评估方法。

Method: 提出多模态多任务学习框架，包含条件学习机制，根据绘画的风格和语义线索动态调整视觉特征提取。

Result: 实验结果表明该模型在性能上优于现有回归方法，并提供与人类判断一致的可解释可视化。

Conclusion: 该框架实现了自动且可解释的绘画创造力评估，为心理学、教育和认知科学领域提供了有效工具。

Abstract: Assessing human creativity through visual outputs, such as drawings, plays a critical role in fields including psychology, education, and cognitive science. However, current assessment practices still rely heavily on expert-based subjective scoring, which is both labor-intensive and inherently subjective. In this paper, we propose a data-driven framework for automatic and interpretable creativity assessment from drawings. Motivated by the cognitive understanding that creativity can emerge from both what is drawn (content) and how it is drawn (style), we reinterpret the creativity score as a function of these two complementary dimensions.Specifically, we first augment an existing creativity labeled dataset with additional annotations targeting content categories. Based on the enriched dataset, we further propose a multi-modal, multi-task learning framework that simultaneously predicts creativity scores, categorizes content types, and extracts stylistic features. In particular, we introduce a conditional learning mechanism that enables the model to adapt its visual feature extraction by dynamically tuning it to creativity-relevant signals conditioned on the drawing's stylistic and semantic cues.Experimental results demonstrate that our model achieves state-of-the-art performance compared to existing regression-based approaches and offers interpretable visualizations that align well with human judgments. The code and annotations will be made publicly available at https://github.com/WonderOfU9/CSCA_PRCV_2025

</details>


### [140] [Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model](https://arxiv.org/abs/2511.13387)
*Fei Kong*

Main category: cs.CV

TL;DR: 本文提出了广义去噪扩散压缩模型（gDDCM），将DDCM扩展到主流扩散模型及其变体，包括DDPM、基于分数的模型、一致性模型和整流流，在CIFAR-10和LSUN Bedroom数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: DDCM虽然利用DDPM实现了图像压缩，但无法应用于DDPM之外的其他方法，因此需要开发一个更通用的框架来支持多种扩散模型。

Method: 提出gDDCM方法，通过将反向过程中的随机噪声替换为根据预定义规则从特定集合中采样的噪声，将DDCM扩展到DDPM、基于分数的模型、一致性模型和整流流等主流扩散模型。

Result: 在CIFAR-10和LSUN Bedroom数据集上的实验结果表明，gDDCM成功将DDCM推广到上述模型，并实现了性能提升。

Conclusion: gDDCM有效扩展了DDCM的应用范围，使其能够适用于多种扩散模型，为图像压缩提供了更通用的解决方案。

Abstract: Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.

</details>


### [141] [Reconstructing 3D Scenes in Native High Dynamic Range](https://arxiv.org/abs/2511.12895)
*Kaixuan Zhang,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文提出了NH-3DGS方法，这是首个直接从原生HDR观测数据建模的3D场景重建方法，通过新颖的亮度-色度分解技术，在重建流程中保持完整动态范围。


<details>
  <summary>Details</summary>
Motivation: 专业数字媒体创作需要高动态范围成像，但现有3D场景重建主要基于低动态范围数据，限制了在专业工作流程中的应用。现有方法依赖多曝光融合或逆色调映射，增加了捕获复杂性且依赖合成监督。

Method: 提出Native High dynamic range 3D Gaussian Splatting (NH-3DGS)方法，采用新颖的亮度-色度分解颜色表示，直接从原生HDR相机数据进行优化，在重建管道中保持完整动态范围。

Result: 在合成和真实多视角HDR数据集上的实验表明，NH-3DGS在重建质量和动态范围保持方面显著优于现有方法，能够直接从原生HDR捕获实现专业级3D重建。

Conclusion: NH-3DGS方法成功实现了直接从原生HDR数据的高质量3D场景重建，为专业数字媒体创作提供了有效的解决方案。

Abstract: High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.

</details>


### [142] [Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)](https://arxiv.org/abs/2511.13397)
*Nikos Theodoridis,Tim Brophy,Reenu Mohandas,Ganesh Sistu,Fiachra Collins,Anthony Scanlan,Ciaran Eising*

Main category: cs.CV

TL;DR: 提出了DTPQA基准，专门用于评估视觉语言模型在交通场景中的感知能力，包含合成和真实世界两部分，并带有距离标注以分析模型性能随距离变化的情况。


<details>
  <summary>Details</summary>
Motivation: 由于视觉语言模型在自动驾驶等安全关键领域的应用需求，需要评估其在复杂交通场景中的感知能力，特别是对远距离物体的理解能力。

Method: 创建了包含合成和真实世界数据的DTPQA基准数据集，每个样本包含图像、问题、真实答案和物体距离信息，用于测试模型在不同距离下的感知性能。

Result: 提供了完整的DTPQA数据集和生成脚本，可用于系统评估视觉语言模型在交通场景中的感知能力随距离变化的表现。

Conclusion: DTPQA基准为评估视觉语言模型在自动驾驶领域的感知能力提供了专门工具，有助于确保模型在安全关键应用中的可靠性。

Abstract: The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.

</details>


### [143] [FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI](https://arxiv.org/abs/2511.12899)
*Hao Li,Zhenfeng Zhuang,Jingyu Lin,Yu Liu,Yifei Chen,Qiong Peng,Lequan Yu,Liansheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于频率分解预处理（FDP）的无监督脑MRI异常检测方法，通过频率域分析发现异常具有独特的频率模式，FDP框架能够同时抑制病理特征并保留正常解剖结构，显著提升现有方法的检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于脑解剖结构的多样性和标注数据的稀缺性，脑MRI的监督异常检测面临挑战，推动了无监督异常检测方法的发展。现有方法使用人工生成的噪声扰动训练生成模型，但模拟异常缺乏真实临床病变的生物物理保真度和形态复杂性。

Method: 通过系统的频率域分析发现异常具有独特频率模式，提出频率分解预处理（FDP）框架，利用频率域重建同时实现病理抑制和解剖保留，可与现有异常模拟技术无缝集成。

Result: 实验结果表明FDP能够持续提升异常检测性能，与LDM结合时DICE分数提高了17.63%，并在多个基线方法上保持稳健的改进。

Conclusion: FDP是首个利用频率域重建进行病理抑制和解剖保留的无监督异常检测方法，能够显著提升现有方法的检测性能，为脑MRI异常检测提供了新的有效解决方案。

Abstract: Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.

</details>


### [144] [TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing](https://arxiv.org/abs/2511.13399)
*Yuchen Bao,Yiting Wang,Wenjian Huang,Haowei Wang,Shen Chen,Taiping Yao,Shouhong Ding,Jianguo Zhang*

Main category: cs.CV

TL;DR: TripleFDS是一个用于场景文本编辑的新框架，通过解耦文本样式、文本内容和背景三个属性，实现了更灵活和视觉一致的文本编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可编辑属性解耦方面存在不足，通常只能编辑文本内容，限制了可控性和视觉一致性。

Method: 提出TripleFDS框架和SCB Synthesis数据集，使用SCB Group作为基本训练单元，通过组间对比正则化和组内多特征正交性实现三重特征解耦，并在合成阶段进行特征重映射。

Result: 在主流STE基准测试中，TripleFDS实现了最先进的图像保真度（SSIM为44.54）和文本准确率（ACC为93.58%）。

Conclusion: TripleFDS不仅性能优越，还支持样式替换和背景转移等新操作，提供了更灵活的编辑能力。

Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the "SCB Group", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent "shortcut" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS

</details>


### [145] [Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline](https://arxiv.org/abs/2511.13442)
*Rui Zuo,Qinyue Tong,Zhe-Ming Lu,Ziqian Lu*

Main category: cs.CV

TL;DR: Foresee是一个无需训练的MLLM图像伪造分析框架，通过类型先验驱动策略和灵活特征检测器，在多种篡改类型上实现高精度定位和丰富文本解释。


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造检测方法泛化能力有限且解释性不足，而MLLM在多模态任务中展现强大泛化潜力，但现有方法需要大量训练且未能充分利用MLLM的固有潜力。

Method: 提出训练免费的Foresee框架，采用类型先验驱动策略，包含灵活特征检测器(FFD)专门处理复制-移动篡改，无需额外训练即可释放MLLM在取证领域的潜力。

Result: 在多种篡改类型(复制-移动、拼接、移除、局部增强、深度伪造和AIGC编辑)上超越现有方法，同时实现更准确的定位和更丰富的文本解释。

Conclusion: Foresee证明了无需训练即可有效利用MLLM进行图像伪造分析，展现了强大的泛化能力和实用性，为取证领域提供了轻量级解决方案。

Abstract: With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.

</details>


### [146] [CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection](https://arxiv.org/abs/2511.12909)
*Yaohua Zha,Xue Yuerong,Chunlin Fan,Yuansong Wang,Tao Dai,Ke Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了一种基于曲率增强自监督学习的3D异常检测框架CASL，通过多尺度曲率提示引导重建过程，在不依赖特定异常检测机制的情况下实现了领先的检测性能，并能泛化到其他3D理解任务。


<details>
  <summary>Details</summary>
Motivation: 现有3D异常检测方法通常针对特定任务设计，泛化性差；而经典自监督点云模型在统一微调范式下对异常检测效果不佳，需要开发更通用的3D模型。

Method: 基于U-Net架构，引入多尺度曲率提示来指导解码器预测点的空间坐标，通过重建范式进行曲率增强的自监督学习。

Result: 仅使用点曲率作为异常分数就超越了多个经典自监督和专用异常检测模型；CASL框架在异常检测中达到领先性能，且学到的表示能很好泛化到点云分类等标准3D理解任务。

Conclusion: 曲率在3D异常检测中起关键作用；提出的CASL框架无需专用异常检测机制，通过简单微调即可实现优异检测性能，并具有良好的任务泛化能力。

Abstract: Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.

</details>


### [147] [Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling](https://arxiv.org/abs/2511.13478)
*Adam Hazimeh,Ke Wang,Mark Collier,Gilles Baechler,Efi Kokiopoulou,Pascal Frossard*

Main category: cs.CV

TL;DR: SliDer是一个使用视觉语言模型将幻灯片图像转换为可编辑SVG格式的框架，解决了传统几何光栅矢量化方法在复杂文档中无法保持高层次结构的问题。


<details>
  <summary>Details</summary>
Motivation: 多媒体文档（如幻灯片和海报）通常以静态光栅格式分发，限制了编辑和定制能力。现有几何光栅矢量化方法无法保持复杂文档的高层次结构，导致语义信息丢失。

Method: 引入SliDer框架，利用视觉语言模型检测和提取光栅输入中的图像和文本元素属性，将其组织成连贯的SVG格式。模型在推理过程中迭代优化预测，类似于人类设计过程。

Result: SliDer实现了0.069的重建LPIPS分数，在82.9%的情况下被人类评估者认为优于最强的零样本VLM基线。还创建了Slide2SVG数据集以促进该领域研究。

Conclusion: SliDer通过语义文档反渲染方法，成功将幻灯片图像转换为紧凑且可编辑的SVG表示，显著优于现有方法，为文档编辑性恢复提供了有效解决方案。

Abstract: Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.

</details>


### [148] [Explore How to Inject Beneficial Noise in MLLMs](https://arxiv.org/abs/2511.12917)
*Ruishu Zhu,Sida Huang,Ziheng Jiao,Hongyuan Zhang*

Main category: cs.CV

TL;DR: 提出了一种通过注入有益随机噪声的新型微调策略MuNG，在仅调整1-2%额外参数的情况下，性能超越全参数微调和其他现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法往往忽略跨模态异质性，限制了多模态大语言模型的潜力。

Method: 从变分推断角度重新制定MLLMs推理过程，设计多模态噪声生成器动态分析图像-文本对的跨模态关系，生成任务自适应的有益噪声。

Result: 在QwenVL和LLaVA上的实验表明，该方法超越了全参数微调和其他现有微调方法。

Conclusion: 注入定制化噪声能有效抑制不相关语义成分，显著改善跨模态表示对齐，提升下游任务性能。

Abstract: Multimodal Large Language Models (MLLMs) have played an increasingly important role in multimodal intelligence. However, the existing fine-tuning methods often ignore cross-modal heterogeneity, limiting their full potential. In this work, we propose a novel fine-tuning strategy by injecting beneficial random noise, which outperforms previous methods and even surpasses full fine-tuning, with minimal additional parameters. The proposed Multimodal Noise Generator (MuNG) enables efficient modality fine-tuning by injecting customized noise into the frozen MLLMs. Specifically, we reformulate the reasoning process of MLLMs from a variational inference perspective, upon which we design a multimodal noise generator that dynamically analyzes cross-modal relationships in image-text pairs to generate task-adaptive beneficial noise. Injecting this type of noise into the MLLMs effectively suppresses irrelevant semantic components, leading to significantly improved cross-modal representation alignment and enhanced performance on downstream tasks. Experiments on two mainstream MLLMs, QwenVL and LLaVA, demonstrate that our method surpasses full-parameter fine-tuning and other existing fine-tuning approaches, while requiring adjustments to only about $1\sim2\%$ additional parameters. The relevant code is uploaded in the supplementary.

</details>


### [149] [CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation](https://arxiv.org/abs/2511.12919)
*Dexin Zuo,Ang Li,Wei Wang,Wenxian Yu,Danping Zou*

Main category: cs.CV

TL;DR: CoordAR是一个用于未见物体6D姿态估计的自回归框架，仅需单张参考视图而非完整3D模型，通过离散化坐标映射和概率预测解决对称性和遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于单参考视图的6D姿态估计方法依赖实数坐标回归，存在全局一致性不足和对称/遮挡场景下的不确定性建模缺失问题。

Method: 提出CoordAR框架：1）坐标映射标记化实现离散3D空间的概率预测；2）模态解耦编码策略分别处理RGB外观和坐标线索；3）基于位置对齐查询特征和部分生成标记序列的自回归变换器解码器。

Result: 在多个基准测试中显著优于现有方法，在对称性、遮挡等真实世界挑战中表现出强大鲁棒性。

Conclusion: CoordAR通过自回归概率建模有效解决了单参考6D姿态估计中的全局一致性和不确定性挑战，为未见物体的姿态估计提供了新思路。

Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.

</details>


### [150] [Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks](https://arxiv.org/abs/2511.13545)
*Md. Iqbal Hossain,Afia Sajeeda,Neeresh Kumar Perla,Ming Shao*

Main category: cs.CV

TL;DR: 提出了一种针对多模态对比学习模型（如CLIP）后门攻击的防御策略，通过图像分割"oracle"作为监督器，识别后门触发器、受害样本和标签，并构建紧凑微调数据集来修复中毒模型。


<details>
  <summary>Details</summary>
Motivation: 多模态深度学习模型（如CLIP）容易受到后门攻击，现有防御方法需要从头训练或使用大量数据微调，且无法精确定位受影响的标签。

Method: 引入图像分割"oracle"作为监督器，开发两种算法：1）区分CLIP和Oracle的知识以识别潜在触发器；2）精确定位受影响标签和受害样本，构建紧凑微调数据集。

Result: 在视觉识别基准测试上的广泛实验表明，该策略在基于CLIP的后门防御中有效。

Conclusion: 该方法能够高效识别后门触发器、受害样本和标签，并通过构建紧凑微调数据集成功修复中毒的CLIP模型，消除后门影响。

Abstract: The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.

</details>


### [151] [Generative Photographic Control for Scene-Consistent Video Cinematic Editing](https://arxiv.org/abs/2511.12921)
*Huiqiang Sun,Liao Shen,Zhan Peng,Kun Wang,Size Wu,Yuhang Zang,Tianqi Liu,Zihao Huang,Xingyu Zeng,Zhiguo Cao,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: CineCtrl是首个提供专业相机参数精细控制的视频电影编辑框架，通过解耦交叉注意力机制实现摄影效果与相机运动的独立控制，解决了生成视频模型中摄影效果控制的挑战。


<details>
  <summary>Details</summary>
Motivation: 电影叙事深受景深、曝光等摄影元素艺术操控的影响，但这些效果在生成视频模型中的控制仍然极具挑战，现有方法大多局限于相机运动控制。

Method: 提出解耦交叉注意力机制，将相机运动与摄影输入分离；开发综合数据生成策略，利用模拟摄影效果和真实世界收集管道构建大规模数据集进行鲁棒训练。

Result: 模型能够生成具有精确控制的用户指定摄影相机效果的高保真视频。

Conclusion: CineCtrl框架成功实现了对专业相机参数的精细控制，为视频生成中的电影化编辑提供了有效解决方案。

Abstract: Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.

</details>


### [152] [Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification](https://arxiv.org/abs/2511.13575)
*Linhan Zhou,Shuang Li,Neng Dong,Yonghang Tai,Yafei Zhang,Huafeng Li*

Main category: cs.CV

TL;DR: 本文提出了一个统一框架HPL，通过任务感知提示建模联合优化图像到图像(I2I)和文本到图像(T2I)的行人重识别任务。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常将I2I和T2I任务分开处理，这可能导致表示纠缠和次优性能。为了解决这个问题，需要开发一个能够同时优化这两个任务的统一框架。

Method: 提出了分层提示学习(HPL)框架，包括：任务路由Transformer，使用双分类令牌在共享视觉编码器中路由特征；分层提示生成方案，集成身份级可学习令牌和实例级伪文本令牌；跨模态提示正则化策略，在提示令牌空间中强制语义对齐。

Result: 在多个ReID基准测试上的广泛实验验证了该方法的有效性，在I2I和T2I任务上都达到了最先进的性能。

Conclusion: HPL框架通过任务感知提示建模成功统一了I2I和T2I行人重识别任务，实现了优异的跨模态检索性能。

Abstract: Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.

</details>


### [153] [Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes](https://arxiv.org/abs/2511.12932)
*Feng Lv,Haoxuan Feng,Zilu Zhang,Chunlong Xia,Yanfeng Li*

Main category: cs.CV

TL;DR: 提出统一的文本驱动交通场景图像生成与编辑框架，通过可控掩码机制整合两任务，利用多视角数据增强几何多样性，采用两阶段训练策略提升文本-图像对齐和细节质量，引入掩码区域加权损失改善小尺度交通元素生成效果。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统快速发展，但现有文本驱动图像生成技术在交通场景中存在语义丰富度不足、视角有限、视觉保真度低、文本-内容对齐差等问题。

Method: 1. 统一框架整合图像生成与编辑，使用可控掩码机制；2. 融合车端和路端多视角数据增强几何多样性；3. 两阶段训练：大规模粗粒度数据概念学习 + 细粒度描述数据微调；4. 引入掩码区域加权损失动态关注关键小区域。

Result: 大量实验表明，该方法在交通场景文本图像生成和编辑方面达到领先性能。

Conclusion: 所提出的统一框架有效解决了交通场景图像生成中的多个挑战，在语义丰富度、视角多样性、视觉保真度和文本-内容对齐方面均取得显著提升。

Abstract: With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.

</details>


### [154] [VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping](https://arxiv.org/abs/2511.13587)
*Haotian Dong,Ye Li,Rongwei Lu,Chen Tang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 本文提出VVS框架，通过部分验证跳过来加速视觉自回归模型的生成，将目标模型前向传递次数减少2.8倍，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归生成模型存在推理延迟问题，传统推测解码的"一步草稿、一步验证"范式限制了加速潜力。基于视觉令牌可互换性，探索在推测解码过程中跳过验证以显式减少前向传递次数。

Method: 提出VVS框架，包含三个互补模块：动态截断的验证无关令牌选择器、令牌级特征缓存和重用、细粒度跳过步调度。

Result: VVS将目标模型前向传递次数减少2.8倍，相比传统推测解码框架提供更优的速度-质量权衡。

Conclusion: VVS框架通过部分验证跳过有效加速视觉自回归生成，揭示了重塑推测解码范式的强大潜力。

Abstract: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.

</details>


### [155] [ProtoAnomalyNCD: Prototype Learning for Multi-class Novel Anomaly Discovery in Industrial Scenarios](https://arxiv.org/abs/2511.12938)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: ProtoAnomalyNCD是一个基于原型学习的框架，用于发现和分类多种未见异常类型，通过结合Grounded SAM定位物体区域和使用异常图引导注意力来增强异常特征，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测方法主要判断是否存在异常，但实际应用需要发现和分类多种异常类型。由于工业异常语义细微且现有方法未充分利用图像先验，直接聚类方法效果不佳。

Method: 1) 使用Grounded SAM和文本提示定位物体区域作为先验；2) 引入异常图引导注意力块，设计区域引导因子区分背景、物体区域和异常区域；3) 在统一原型学习框架下发现未见异常类并进行多类型异常分类。

Result: 在MVTec AD、MTD和Real-IAD数据集上优于最先进方法，并能检测未见异常值，实现任务级统一。

Conclusion: ProtoAnomalyNCD通过结合物体定位和异常图引导注意力，有效解决了工业异常检测中的多类型异常发现和分类问题，在多个数据集上表现出优越性能。

Abstract: Existing industrial anomaly detection methods mainly determine whether an anomaly is present. However, real-world applications also require discovering and classifying multiple anomaly types. Since industrial anomalies are semantically subtle and current methods do not sufficiently exploit image priors, direct clustering approaches often perform poorly. To address these challenges, we propose ProtoAnomalyNCD, a prototype-learning-based framework for discovering unseen anomaly classes of multiple types that can be integrated with various anomaly detection methods. First, to suppress background clutter, we leverage Grounded SAM with text prompts to localize object regions as priors for the anomaly classification network. Next, because anomalies usually appear as subtle and fine-grained patterns on the product, we introduce an Anomaly-Map-Guided Attention block. Within this block, we design a Region Guidance Factor that helps the attention module distinguish among background, object regions, and anomalous regions. By using both localized product regions and anomaly maps as priors, the module enhances anomalous features while suppressing background noise and preserving normal features for contrastive learning. Finally, under a unified prototype-learning framework, ProtoAnomalyNCD discovers and clusters unseen anomaly classes while simultaneously enabling multi-type anomaly classification. We further extend our method to detect unseen outliers, achieving task-level unification. Our method outperforms state-of-the-art approaches on the MVTec AD, MTD, and Real-IAD datasets.

</details>


### [156] [Alpha Divergence Losses for Biometric Verification](https://arxiv.org/abs/2511.13621)
*Dimitrios Koutsianos,Ladislav Mosner,Yannis Panagakis,Themos Stafylakis*

Main category: cs.CV

TL;DR: 本文提出了两种基于α-散度的边际损失函数：Q-Margin（在参考度量中引入边际）和A3M（在对数中引入边际），用于提升人脸和说话人验证性能，特别是在低误接受率下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于边际的softmax损失（如CosFace和ArcFace）在人脸和说话人验证中表现良好，但α-散度损失函数能够诱导稀疏解（当α>1时），然而如何将角度边际（对验证任务至关重要）整合到α-散度中并不直接。

Method: 探索了两种整合角度边际的途径：通过参考度量（先验概率）或通过对数（未归一化的对数似然），推导出两种新颖的基于边际的α-散度损失：Q-Margin和A3M。针对A3M中由惩罚对数与稀疏性相互作用引起的训练不稳定性，提出了简单有效的原型重新初始化策略。

Result: 在具有挑战性的IJB-B和IJB-C人脸验证基准测试中取得了显著的性能提升，在VoxCeleb说话人验证中同样表现出色。关键的是，我们的模型在低误接受率（FAR）下显著优于强基线。

Conclusion: 提出的Q-Margin和A3M方法能够有效整合角度边际到α-散度损失中，显著提升了人脸和说话人验证性能，特别是在需要最小化错误认证的高安全性应用（如银行认证）中具有重要价值。

Abstract: Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.

</details>


### [157] [Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking](https://arxiv.org/abs/2511.12939)
*Wei Jiang,Jiahao Cui,Yizheng Wu,Zhan Peng,Zhiyu Pan,Zhiguo Cao*

Main category: cs.CV

TL;DR: 本文提出了一种基于半监督学习的高动态范围图像重建方法，通过教师模型生成伪HDR标签，并使用不确定性掩码机制过滤不可靠部分，仅需6.7%的HDR真实标签即可达到与全监督方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的HDR图像重建方法需要LDR-HDR图像对，但这些数据对难以获取，因此需要研究如何在有限HDR真实标签下实现可比较性能的注释高效方法。

Method: 采用半监督学习框架，教师模型为无标签的LDR样本生成伪HDR标签，学生模型从伪标签中学习。提出基于不确定性的掩码处理，在像素和补丁级别丢弃伪标签中不可靠的部分。

Result: 该方法不仅优于之前的注释高效算法，而且仅使用6.7%的HDR真实标签就能达到最新全监督方法的可比性能。

Conclusion: 通过不确定性掩码机制有效缓解了确认偏差问题，证明了半监督学习在HDR图像重建任务中的有效性，显著减少了所需的标注数据量。

Abstract: Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.

</details>


### [158] [UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity](https://arxiv.org/abs/2511.13714)
*Junwei Yu,Trevor Darrell,XuDong Wang*

Main category: cs.CV

TL;DR: UnSAMv2通过引入粒度控制嵌入和自监督学习，实现了无需人工标注的分割粒度精确控制，显著提升了SAM-2在多个分割任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决SAM模型在分割粒度控制方面的局限性，用户通常需要手动细化结果才能达到所需细节水平，而收集所有粒度的密集标注成本过高。

Method: 扩展UnSAM的分治策略，发现丰富的掩码-粒度对，引入新颖的粒度控制嵌入实现分割尺度的精确连续控制，仅需6K未标注图像和0.02%额外参数。

Result: 在11个基准测试中显著提升性能：NoC90从5.69降至4.75，1-IoU从58.0提升至73.1，AR1000从49.6提升至68.3。

Conclusion: 少量未标注数据结合粒度感知的自监督学习方法可以释放视觉基础模型的潜力，实现任意粒度的分割控制。

Abstract: The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\text{NoC}_{90}$ (5.69 $\rightarrow$ 4.75), 1-IoU (58.0 $\rightarrow$ 73.1), and $\text{AR}_{1000}$ (49.6 $\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.

</details>


### [159] [T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving](https://arxiv.org/abs/2511.12956)
*Chen Ma,Ningfei Wang,Junhao Zheng,Qing Guo,Qian Wang,Qi Alfred Chen,Chao Shen*

Main category: cs.CV

TL;DR: DiffSign是一个基于文本到图像生成的新型交通标志识别系统对抗攻击框架，能够生成物理鲁棒、高效、可迁移、实用且隐蔽的外观攻击。


<details>
  <summary>Details</summary>
Motivation: 现有交通标志识别系统的对抗攻击方法存在局限性：像素级扰动方法缺乏隐蔽性且过拟合特定模型，扩散模型方法效果有限且泛化能力差。

Method: 提出DiffSign框架，集成CLIP损失和掩码提示改进攻击焦点和可控性，采用两种新颖的风格定制方法指导视觉外观，提升域外交通标志攻击泛化能力和隐蔽性。

Result: 在多种真实世界条件下（不同距离、角度、光照条件和标志类别）的广泛评估中，DiffSign实现了平均83.3%的物理世界攻击成功率，具有高攻击可迁移性。

Conclusion: DiffSign框架克服了现有方法的局限性，能够生成对交通标志识别系统有效的物理鲁棒对抗攻击。

Abstract: Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.
  In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.

</details>


### [160] [Scaling Spatial Intelligence with Multimodal Foundation Models](https://arxiv.org/abs/2511.13719)
*Zhongang Cai,Ruisi Wang,Chenyang Gu,Fanyi Pu,Junxiang Xu,Yubo Wang,Wanqi Yin,Zhitao Yang,Chen Wei,Qingping Sun,Tongxi Zhou,Jiaqi Li,Hui En Pang,Oscar Qian,Yukun Wei,Zhiqian Lin,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Xiangyu Fan,Hanming Deng,Lewei Lu,Liang Pan,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: SenseNova-SI项目通过构建800万样本的数据集，在多模态基础模型中培养空间智能，在多个空间智能基准测试中取得突破性表现。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型取得了显著进展，但在空间智能方面仍存在明显不足，需要专门的数据集和训练方法来提升空间理解能力。

Method: 基于现有多模态基础模型（Qwen3-VL、InternVL3、Bagel），系统构建包含800万多样化样本的SenseNova-SI-8M数据集，采用严格的空间能力分类体系进行训练。

Result: 在多个空间智能基准测试中表现优异：VSI-Bench 68.7%、MMSI 43.3%、MindCube 85.6%、ViewSpatial 54.6%、SITE 50.1%，同时保持强大的通用多模态理解能力（MMBench-En 84.9%）。

Conclusion: SenseNova-SI项目展示了通过大规模多样化数据训练可以显著提升多模态模型的空间智能，并发现了新兴的泛化能力迹象。该项目将持续更新并公开发布模型以促进相关研究。

Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.

</details>


### [161] [GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.12968)
*Ning Han,Zhenyu Ge,Feng Han,Yuhua Sun,Chengqing Li,Jingjing Chen*

Main category: cs.CV

TL;DR: GrOCE是一个无需训练的在线概念擦除框架，通过图引导的语义推理实现精确的概念移除，解决了现有方法依赖微调或语义分离粗糙的问题。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法要么依赖昂贵的微调，要么进行粗糙的语义分离，往往会退化无关概念且缺乏对演化概念集的适应性。

Method: GrOCE将概念及其相互关系建模为动态语义图，包含三个组件：动态拓扑图构建、自适应聚类识别和选择性边切断。

Result: 大量实验表明，GrOCE在概念相似性和Fréchet Inception距离指标上达到最先进性能，无需重新训练即可实现高效、准确和稳定的概念擦除。

Conclusion: GrOCE提供了一个无需训练的概念擦除框架，通过图引导的语义推理实现精确和自适应的概念移除，在保持全局语义的同时有效隔离不需要的内容。

Abstract: Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Fréchet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.

</details>


### [162] [HiFusion: Hierarchical Intra-Spot Alignment and Regional Context Fusion for Spatial Gene Expression Prediction from Histopathology](https://arxiv.org/abs/2511.12969)
*Ziqiao Weng,Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee AD Cooper,Weidong Cai,Bo Zhou*

Main category: cs.CV

TL;DR: HiFusion是一个深度学习框架，通过分层建模和跨尺度融合，从H&E染色全切片图像预测空间转录组基因表达，在多个基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 空间转录组技术面临临床采用障碍，现有计算方法难以捕捉斑点内的生物异质性，且容易受到形态学噪声影响。

Method: 包含两个互补组件：分层斑点内建模模块通过多分辨率子块分解提取细粒度形态表征；上下文感知跨尺度融合模块使用交叉注意力选择性地整合生物相关区域上下文。

Result: 在两个基准ST数据集上的广泛实验表明，HiFusion在2D切片交叉验证和更具挑战性的3D样本特定场景中均达到最先进性能。

Conclusion: HiFusion有望成为从常规组织病理学进行ST推断的稳健、准确且可扩展的解决方案。

Abstract: Spatial transcriptomics (ST) bridges gene expression and tissue morphology but faces clinical adoption barriers due to technical complexity and prohibitive costs. While computational methods predict gene expression from H&E-stained whole-slide images (WSIs), existing approaches often fail to capture the intricate biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual information from surrounding tissue. To overcome these limitations, we propose HiFusion, a novel deep learning framework that integrates two complementary components. First, we introduce the Hierarchical Intra-Spot Modeling module that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition, guided by a feature alignment loss to ensure semantic consistency across scales. Concurrently, we present the Context-aware Cross-scale Fusion module, which employs cross-attention to selectively incorporate biologically relevant regional context, thereby enhancing representational capacity. This architecture enables comprehensive modeling of both cellular-level features and tissue microenvironmental cues, which are essential for accurate gene expression prediction. Extensive experiments on two benchmark ST datasets demonstrate that HiFusion achieves state-of-the-art performance across both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios. These results underscore HiFusion's potential as a robust, accurate, and scalable solution for ST inference from routine histopathology.

</details>


### [163] [ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes](https://arxiv.org/abs/2511.12977)
*Yixuan Yang,Luyang Xie,Zhen Luo,Zixiang Zhao,Mingqi Gao,Feng Zheng*

Main category: cs.CV

TL;DR: ArtiWorld是一个从3D场景自动识别可关节化对象并重建为可执行URDF模型的流程，核心是Arti4URDF，利用点云和LLM先验知识快速将刚性对象转换为交互式关节对象。


<details>
  <summary>Details</summary>
Motivation: 现有3D模拟资产多为刚性对象，手动转换为关节对象成本高昂，需要自动化方法从场景中识别可关节化对象并转换为交互式资产。

Method: 基于文本场景描述定位候选可关节化对象，利用3D点云、大语言模型先验知识和URDF导向提示设计，重建可执行URDF模型并保持原始几何形状。

Result: 在3D模拟对象、完整3D模拟场景和真实世界扫描场景三个层面评估，均优于现有方法，达到最先进性能，保持对象几何形状并正确捕捉对象交互性。

Conclusion: 为直接从现有3D资产构建交互式、机器人就绪的模拟环境提供了实用路径。

Abstract: Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.

</details>


### [164] [Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach](https://arxiv.org/abs/2511.12978)
*Aishwarya Agarwal,Srikrishna Karanam,Vineet Gandhi*

Main category: cs.CV

TL;DR: 本文提出了CCI方法，利用CLIP的patch嵌入将空间块分组为语义连贯的簇，通过掩码评估模型预测的相对变化，在忠实性基准测试中达到新SOTA。同时引入COVAR基准，系统性地变化前景和背景，评估18个CLIP变体的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对比视觉语言模型如CLIP在零样本识别方面表现强劲，但容易受到虚假相关性的影响，特别是对背景的过度依赖。现有基准如CounterAnimals仅依赖准确性，隐含地将所有性能下降归因于背景相关性，这种假设是不完整的。

Method: 提出CCI方法：使用CLIP的patch嵌入将空间块分组为语义连贯的簇，掩码这些簇，评估模型预测的相对变化。结合GroundedSAM自动将预测分类为前景驱动或背景驱动。引入COVAR基准系统性地变化前景和背景。

Result: CCI在忠实性基准测试中达到新SOTA，在MS COCO检索的删除AUC指标上带来超过两倍的改进。通过CCI与COVAR结合，对18个CLIP变体进行了全面评估，揭示了除背景相关性外，视角变化、尺度偏移和细粒度对象混淆也是错误来源。

Conclusion: CCI方法提供了关键的诊断能力，COVAR基准能够解耦不同影响因素。这些方法论进展和实证证据为构建更鲁棒的视觉语言模型指明了方向。

Abstract: Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.

</details>


### [165] [Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection](https://arxiv.org/abs/2511.12992)
*Lintong Zhang,Kang Yin,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 本文提出WSAE-Net方法，通过加权语义图和自适应候选编辑序列来解决传统视觉反事实解释中语义相关性不足的问题，提升模型可解释性和编辑效率。


<details>
  <summary>Details</summary>
Motivation: 传统非生成式视觉反事实解释方法通常将查询图像部分区域替换为干扰图像区域，但忽视了替换区域与目标对象的语义相关性，损害了模型可解释性并阻碍编辑流程。

Method: 提出WSAE-Net方法，包含两个关键创新：加权语义图生成和自适应候选编辑序列。加权语义图最大化减少需要计算的非语义特征单元，优化计算效率；自适应候选编辑序列确定特征单元的最佳计算顺序，确保反事实生成效率同时保持替换特征单元与目标对象的语义相关性。

Result: 通过全面实验验证，该方法展现出优越性能，有助于更清晰深入地理解视觉反事实解释。

Conclusion: WSAE-Net方法通过加权语义图和自适应编辑序列的创新设计，有效解决了传统视觉反事实解释中的语义相关性不足问题，显著提升了模型可解释性和编辑效率。

Abstract: In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.

</details>


### [166] [Medal S: Spatio-Textual Prompt Model for Medical Segmentation](https://arxiv.org/abs/2511.13001)
*Pengcheng Shi,Jiawei Chen,Jiaqi Liu,Xinglin Zhang,Tao Chen,Lei Li*

Main category: cs.CV

TL;DR: Medal S是一个医学分割基础模型，支持原生分辨率空间和文本提示的端到端训练框架，通过通道对齐和全3D上下文处理，在多模态医学图像分割中实现高效准确的多类别分割。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本方法缺乏空间感知能力的问题，通过结合空间和文本提示来提升医学图像分割的准确性和效率。

Method: 采用通道对齐机制将体积提示与文本嵌入对齐，使用轻量级3D卷积模块进行体素空间细化，支持并行处理多个原生分辨率掩码，并提出动态重采样、两阶段推理等优化技术。

Result: 在BiomedSegFM数据集上，Medal S在五种模态验证集上的平均DSC为75.44（vs SAT 69.83），NSD为77.34（vs 71.06），F1为38.24（vs 24.88），并行空间提示使推理时间减少90%以上。

Conclusion: Medal S通过协调空间精度与语义文本指导，在医学分割任务中展现出卓越的性能，相比顺序提示方法具有更高的效率和准确性。

Abstract: We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.

</details>


### [167] [Infinite-Story: A Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2511.13002)
*Jihun Park,Kyoungmin Lee,Jongmin Gim,Hyeonseo Jo,Minseok Oh,Wonhyeok Choi,Kyumin Hwang,Jaeyeul Kim,Minwoo Choi,Sunghoon Im*

Main category: cs.CV

TL;DR: Infinite-Story是一个无需训练的文本到图像生成框架，专门用于多提示词故事场景。它解决了身份不一致和风格不一致两个关键挑战，通过三种互补技术实现高效、一致的图像生成，推理速度比现有最快模型快6倍以上。


<details>
  <summary>Details</summary>
Motivation: 解决多提示词文本到图像生成中的身份不一致和风格不一致问题，同时避免现有扩散模型需要微调或推理速度慢的局限性。

Method: 基于尺度自回归模型，采用三种技术：身份提示词替换（缓解文本编码器中的上下文偏差）、自适应风格注入和同步引导适应的统一注意力引导机制。

Result: 在保持提示词保真度的同时，实现了高身份和风格一致性，推理速度达到每张图像1.72秒，比现有最快模型快6倍以上。

Conclusion: Infinite-Story在测试时完全运行，无需微调，在生成性能和推理速度方面均达到最先进水平，为实际视觉故事讲述提供了有效且实用的解决方案。

Abstract: We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.

</details>


### [168] [REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding](https://arxiv.org/abs/2511.13026)
*Jiaze Li,Hao Yin,Wenhui Tan,Jingyang Chen,Boshen Xu,Yuxun Qu,Yijing Chen,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: REVISOR是一个用于长视频理解的多模态反思框架，通过文本和视觉模态的协作反思过程，结合DADR奖励机制，显著提升MLLMs的长视频理解能力，无需额外监督微调或外部模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本的反思机制在长视频理解中存在局限性：1）仅反思文本信息不足以处理丰富的动态视觉输入；2）缺乏跨模态交互能力，无法在反思过程中充分整合视觉信息。

Method: 提出REVISOR框架，支持MLLMs在文本和视觉模态间协作构建反思过程。设计了DADR奖励机制，集成到GRPO训练策略中，确保模型在强化学习中准确回顾与问题高度相关的视频片段。

Result: 在VideoMME、LongVideoBench、MLVU和LVBench四个基准测试中取得了令人印象深刻的结果，显著提升了MLLMs的长视频理解能力。

Conclusion: REVISOR框架通过跨模态反思机制有效解决了长视频理解中纯文本反思的局限性，为MLLMs的长视频理解提供了新的解决方案。

Abstract: Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.

</details>


### [169] [Towards 3D Object-Centric Feature Learning for Semantic Scene Completion](https://arxiv.org/abs/2511.13031)
*Weihua Wang,Yubo Cui,Xiangru Lin,Zhiheng Li,Zheng Fang*

Main category: cs.CV

TL;DR: Ocean是一个面向对象的3D语义场景补全框架，通过将场景分解为单个对象实例来提高语义占用预测的准确性，在SemanticKITTI和SSCBench-KITTI360基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的3D语义场景补全方法通常采用以自我为中心的范式，在整个场景中聚合和扩散特征，但往往忽略了细粒度的对象级细节，导致在复杂环境中出现语义和几何模糊问题。

Method: 1. 使用轻量级分割模型MobileSAM从输入图像中提取实例掩码；2. 引入3D语义组注意力模块，利用线性注意力在3D空间中聚合面向对象的特征；3. 设计全局相似性引导注意力模块处理分割错误和缺失实例；4. 提出实例感知局部扩散模块，通过生成过程改进实例特征并在BEV空间中细化场景表示。

Result: 在SemanticKITTI和SSCBench-KITTI360基准测试中分别实现了17.40和20.28的mIoU分数，达到了最先进的性能水平。

Conclusion: Ocean通过对象中心的方法有效解决了传统方法在复杂环境中语义和几何模糊的问题，证明了对象级分解在3D语义场景补全中的重要性。

Abstract: Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.

</details>


### [170] [uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data](https://arxiv.org/abs/2511.13036)
*Dahyun Chung,Donghyun Shin,Yujin Sung,Seunggi Moon,Jinwoo Jeon,Byung-Jun Lee*

Main category: cs.CV

TL;DR: 提出了一种轻量级、数据高效的多语言视觉-语言对齐框架，无需图像-文本对或文本-文本对，仅训练紧凑的投影模块，使用英语表示作为语义锚点进行对比学习，显著提升了在捷克语、芬兰语、克罗地亚语、匈牙利语和罗马尼亚语等资源匮乏语言上的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言视觉-语言模型在资源匮乏语言上表现不佳，主要由于高质量多语言图像-文本数据的稀缺性。CLIP模型虽然具有强大的泛化能力，但在低资源语言上的扩展仍然有限。

Method: 冻结预训练的图像编码器和多语言文本编码器，仅训练一个紧凑的170万参数投影模块。使用英语表示作为语义锚点，通过对比损失进行训练，无需图像-文本对或文本-文本对数据。

Result: 在多个多语言检索基准测试中，该方法在五个代表性不足的语言上取得了显著提升，这些语言包括捷克语、芬兰语、克罗地亚语、匈牙利语和罗马尼亚语。

Conclusion: 基于枢轴的参数高效对齐策略对于包容性多模态学习具有显著效果，证明了即使监督有限也能实现稳健的多语言对齐。

Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.

</details>


### [171] [DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation](https://arxiv.org/abs/2511.13047)
*Yan Gong,Jianli Lu,Yongsheng Gao,Jie Zhao,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.CV

TL;DR: DiffPixelFormer是一种用于RGB-D室内场景分割的差分像素感知Transformer，通过增强模态内表示和建模模态间交互，解决了现有方法在特征对齐和判别表示方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D融合方法依赖计算密集的跨注意力机制，且对模态内和模态间特征关系建模不足，导致特征对齐不精确和判别表示有限。

Method: 提出Intra-Inter Modal Interaction Block (IIMIB)，通过自注意力捕获模态内长程依赖，使用Differential-Shared Inter-Modal (DSIM)模块建模模态间交互，解耦模态特定和共享线索，实现像素级跨模态对齐。

Result: 在SUN RGB-D和NYUDv2基准测试中，DiffPixelFormer-L分别达到54.28%和59.95%的mIoU分数，比DFormer-L分别提升1.78%和2.75%。

Conclusion: DiffPixelFormer通过差分像素感知Transformer有效提升了RGB-D室内场景分割性能，实现了更精确的特征对齐和判别表示。

Abstract: Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.

</details>


### [172] [ViSS-R1: Self-Supervised Reinforcement Video Reasoning](https://arxiv.org/abs/2511.13054)
*Bo Fang,Yuxin Song,Qiangqiang Wu,Haoyuan Sun,Wenhao Wu,Antoni B. Chan*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉中心视频推理方法，通过自监督强化学习算法Pretext-GRPO和ViSS-R1框架，使多模态大语言模型更有效地处理视觉信息，避免捷径学习和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于R1的方法在视频任务中往往未充分利用丰富的视觉信息，容易导致捷径学习和幻觉问题，需要开发更稳健的视觉中心视频理解方法。

Method: 提出Pretext-GRPO自监督强化学习算法，通过为正确解决变换后视觉输入的前置任务分配正奖励；进一步开发ViSS-R1框架，将前置任务自监督学习直接集成到MLLM的R1后训练范式中。

Result: 在六个广泛使用的视频推理和理解基准测试上的综合评估表明，Pretext-GRPO和ViSS-R1在复杂视频推理方面具有有效性和优越性。

Conclusion: 所提出的方法能够强制模型推理变换后的视觉输入，通过同时处理前置问题和真实用户查询，实现更准确的视频理解。

Abstract: Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.

</details>


### [173] [Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries](https://arxiv.org/abs/2511.13055)
*Ruixin Liu,Zejian Yuan*

Main category: cs.CV

TL;DR: MonoUnc是一个免BEV的单目3D车道线检测器，通过局部车道结构建模来显式处理观测噪声引起的不确定性，在ONCE-3DLanes和OpenLane数据集上超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖简化的几何假设（如独立点预测或全局平面建模），无法捕捉真实场景中的结构变化和不确定性，因此需要一种能显式建模不确定性的方法。

Method: 将3D车道线投影到前视图空间并用参数曲线近似，基于曲线预测动态生成曲线点查询嵌入来进行3D空间点预测，将相邻点形成的线段建模为3D高斯分布，并设计3D高斯匹配损失来联合约束参数。

Result: 在ONCE-3DLanes和OpenLane数据集上，MonoUnc在更严格的评估标准下超越了所有现有最优方法，同时提出了两个新的综合评估指标来量化全局和局部误差。

Conclusion: MonoUnc通过显式建模局部车道结构的不确定性，有效解决了单目3D车道线检测中的观测噪声问题，在多个基准测试中取得了最先进的性能。

Abstract: Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.

</details>


### [174] [FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation](https://arxiv.org/abs/2511.13063)
*Zhenghua Li,Hang Chen,Zihao Sun,Kai Li,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文提出了一种将Segment Anything 2 (SAM2)从自然图像预训练的知识迁移到电子显微镜图像分割的框架，通过特征引导注意力模块和双亲和度解码器，在冻结SAM2权重时达到SOTA水平，微调后显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜图像中的神经结构分割面临复杂形态、低信噪比和标注稀缺的挑战，现有方法在准确性和泛化性上受限。作者希望利用视觉基础模型在大量自然图像上学到的先验知识来解决这些问题。

Method: 提出新颖框架：使用SAM2提取通用特征；引入特征引导注意力模块，利用SAM2的语义线索指导轻量级精细编码器关注困难区域；采用双亲和度解码器生成粗粒度和精细化亲和度图。

Result: 实验结果表明，在冻结SAM2权重时，方法性能与SOTA相当；在EM数据上微调后，显著超越现有SOTA方法。

Conclusion: 研究表明，结合针对性领域自适应指导，将自然图像预训练的表征迁移可以有效解决神经元分割中的特定挑战。

Abstract: Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.

</details>


### [175] [RobustGait: Robustness Analysis for Appearance Based Gait Recognition](https://arxiv.org/abs/2511.13065)
*Reeshoon Sayera,Akash Kumar,Sirshapan Mitra,Prudvi Kamtam,Yogesh S Rawat*

Main category: cs.CV

TL;DR: 提出了RobustGait框架，用于评估基于外观的步态识别系统在真实世界干扰和轮廓变化下的鲁棒性，涵盖四种扰动类型、轮廓提取方法、模型架构能力和部署场景。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别系统在受控数据集上表现良好，但缺乏对真实世界干扰和轮廓变化的系统性鲁棒性评估。

Method: 开发RobustGait评估框架，在CASIA-B、CCPG和SUSTech1K数据集上引入15种干扰类型和5个严重级别，评估6个最先进的步态系统，并在MEVID上进行野外验证。

Result: 发现RGB级噪声能更好反映真实世界退化；步态精度对轮廓提取器偏差高度敏感；鲁棒性取决于扰动类型和架构设计；噪声感知训练和知识蒸馏可提升性能。

Conclusion: RobustGait框架揭示了步态识别系统的关键鲁棒性挑战，并展示了改进策略，推动系统向部署就绪方向发展。

Abstract: Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.

</details>


### [176] [Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.13079)
*Jiacheng Tang,Mingyue Feng,Jiachao Liu,Yaonong Wang,Jian Pu*

Main category: cs.CV

TL;DR: AdaptiveAD是一种解决自动驾驶规划中过度依赖自车状态问题的架构级解决方案，通过双分支结构解耦场景感知和自车状态，实现自适应融合的规划轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有模块化自动驾驶规划系统过度依赖自车状态，导致泛化能力和场景理解能力受限，这是因为自车状态在BEV编码器中过早融合，形成了信息捷径。

Method: 提出双分支结构：一个分支基于多任务学习进行场景驱动推理（BEV编码器故意省略自车状态），另一个分支仅基于规划任务进行自车驱动推理。通过场景感知融合模块自适应整合两个分支的决策，并引入路径注意力机制和两个辅助任务（BEV单向蒸馏和自回归在线建图）。

Result: 在nuScenes数据集上的广泛评估表明，AdaptiveAD实现了最先进的开环规划性能，显著减轻了对自车状态的过度依赖，并在多样化场景中展现出令人印象深刻的泛化能力。

Conclusion: AdaptiveAD通过架构级设计有效解决了自动驾驶规划中对自车状态的过度依赖问题，提升了系统的泛化能力和鲁棒性。

Abstract: Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.

</details>


### [177] [PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking](https://arxiv.org/abs/2511.13105)
*Seungjae Kim,SeungJoon Lee,MyeongAh Cho*

Main category: cs.CV

TL;DR: PlugTrack是一个新颖的多目标跟踪框架，通过自适应融合卡尔曼滤波器和数据驱动的运动预测器来解决线性与非线性运动模式的问题，在多个数据集上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界跟踪场景同时包含线性和非线性运动模式，但传统卡尔曼滤波器无法处理非线性运动，而数据驱动方法存在领域泛化性差和计算开销大的问题。研究发现即使在非线性运动主导的数据集中，卡尔曼滤波器在34%的情况下仍优于数据驱动方法。

Method: 提出PlugTrack框架，通过多感知运动理解自适应融合卡尔曼滤波器和数据驱动运动预测器，使用多感知运动分析生成自适应混合因子，无需修改现有运动预测器。

Result: 在MOT17/MOT20数据集上实现显著性能提升，在DanceTrack数据集上达到最先进水平。

Conclusion: PlugTrack是首个通过自适应融合桥接经典和现代运动预测范式的多目标跟踪框架，有效利用了两种方法的互补性。

Abstract: Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.

</details>


### [178] [Low-Level Dataset Distillation for Medical Image Enhancement](https://arxiv.org/abs/2511.13106)
*Fengzhi Xu,Ziyuan Yang,Mengyu Sun,Joey Tianyi Zhou,Yi Zhang*

Main category: cs.CV

TL;DR: 提出了首个用于医学图像增强的低级数据集蒸馏方法，通过共享解剖先验和个性化生成模块解决低级任务中的像素级保真度问题，同时保护患者隐私。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法主要针对高级任务，而医学图像增强等低级任务需要像素级保真度，传统方法无法有效压缩信息。同时大规模数据集的高训练和存储成本限制了实际部署。

Method: 利用患者间的解剖相似性构建共享解剖先验作为初始化，通过结构保持个性化生成模块将患者特异性信息整合到蒸馏数据中，同时保持像素级保真度。使用梯度对齐方法注入患者特定知识。

Result: 该方法能够生成包含抽象训练信息的蒸馏数据集，下游用户无法访问原始患者数据，有效保护隐私，同时为不同低级任务构建任务特定的训练对。

Conclusion: 提出的低级数据集蒸馏方法解决了医学图像增强中的像素级保真度问题，显著降低了训练和存储成本，同时通过隐私保护机制实现了安全的数据共享。

Abstract: Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.

</details>


### [179] [Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing](https://arxiv.org/abs/2511.13110)
*Shuaibin Fan,Senming Zhong,Wenchao Yan,Minglong Xue*

Main category: cs.CV

TL;DR: 本文提出了一种基于隐式神经退化表示的无监督去雾方法，通过结合通道独立和通道依赖机制来增强非线性依赖学习能力，并设计隐式神经表示将雾霾退化建模为连续函数，在复杂场景下实现高质量图像恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂场景时难以平衡非均匀雾霾分布的细粒度特征表示和全局一致性建模，且需要更好地学习雾霾在空间变化中的共同退化表示。

Method: 1. 基于Kolmogorov-Arnold表示定理，结合通道独立和通道依赖机制增强非线性依赖学习能力；2. 设计隐式神经表示将雾霾退化建模为连续函数，消除冗余信息和对显式特征提取及物理模型的依赖；3. 设计密集残差增强模块进一步学习雾霾特征的隐式表示。

Result: 实验结果表明，该方法在各种公共和真实世界数据集上实现了具有竞争力的去雾性能。

Conclusion: 所提出的无监督去雾方法能够有效处理复杂场景，实现高质量图像恢复，在多个数据集上表现出色。

Abstract: Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.

</details>


### [180] [CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model](https://arxiv.org/abs/2511.13121)
*Yuqi Zhang,Guanying Chen,Jiaxing Chen,Chuanyu Fu,Chuan Huang,Shuguang Cui*

Main category: cs.CV

TL;DR: CloseUpShot是一个基于扩散模型的框架，用于从稀疏输入视图合成近距离新视角，通过点条件视频扩散解决像素扭曲条件在近距离场景中的稀疏性和背景泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对适度视角变化设计，在稀疏输入视图下难以捕捉近距离场景的细粒度细节，因为输入信息严重受限。

Method: 提出分层扭曲和遮挡感知噪声抑制来增强条件图像质量，引入全局结构指导利用密集融合点云为扩散过程提供一致的几何上下文。

Result: 在多个数据集上的广泛实验表明，该方法在近距离新视角合成方面优于现有方法。

Conclusion: 该方法通过改进的条件机制和全局几何约束，有效提升了稀疏输入下近距离场景的重建质量。

Abstract: Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.

</details>


### [181] [VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language](https://arxiv.org/abs/2511.13127)
*Zonghao Ying,Moyang Chen,Nizhang Li,Zhiqiang Wang,Wenxin Zhang,Quanchen Zou,Zonglei Jing,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: VEIL是一种针对文本到视频模型的越狱攻击框架，通过包含中性场景锚点、潜在听觉触发器和风格调制器的模块化提示设计，利用模型的跨模态关联模式，使看似良性的提示诱导生成违反安全策略的视频内容。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频模型的越狱攻击通常通过添加明显不安全的对抗性扰动来实现，这些攻击容易被检测和防御。本文旨在探索如何通过看似良性的提示来诱导模型生成语义上不安全的内容，揭示模型安全防护的盲点。

Method: 提出VEIL框架，采用模块化提示设计：中性场景锚点提供表面场景描述保持合理性；潜在听觉触发器利用音频-视觉共现先验偏向不安全视觉概念；风格调制器通过电影指令放大和稳定触发效果。将攻击生成形式化为约束优化问题，使用引导搜索算法平衡隐蔽性和有效性。

Result: 在7个文本到视频模型上的广泛实验表明，该攻击方法有效，在商业模型中平均攻击成功率提高了23%。

Conclusion: 研究表明，看似良性的提示可以通过利用模型的跨模态关联模式成功诱导文本到视频模型生成违反安全策略的内容，这揭示了当前模型安全防护的严重漏洞，需要开发更鲁棒的防御机制。

Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.

</details>


### [182] [MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation](https://arxiv.org/abs/2511.13135)
*Junjie Yang,Yuhao Yan,Gang Wu,Yuxuan Wang,Ruoyu Liang,Xinjie Jiang,Xiang Wan,Fenglei Fan,Yongquan Zhang,Feiwei Qin,Changmiao Wan*

Main category: cs.CV

TL;DR: MedGEN-Bench是一个全面的医学多模态基准测试，包含6,422个专家验证的图像-文本对，涵盖6种成像模态、16个临床任务和28个子任务，旨在解决现有医学视觉基准在查询模糊、诊断推理简化和图像生成能力评估不足等问题。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型在医疗应用中的普及，临床医生期望AI系统不仅能生成文本诊断，还能生成与真实临床工作流程无缝集成的医学图像。现有医学视觉基准存在查询模糊、诊断推理简化、图像生成评估不足等局限性。

Method: 提出MedGEN-Bench基准，包含三种格式：视觉问答、图像编辑和上下文多模态生成。采用三层评估框架，整合像素级指标、语义文本分析和专家指导的临床相关性评分。

Result: 系统评估了10个组合框架、3个统一模型和5个视觉语言模型，展示了MedGEN-Bench在评估医学AI系统多模态能力方面的有效性。

Conclusion: MedGEN-Bench通过上下文交织的指令和开放式生成输出，推动了医学AI研究的发展，超越了多项选择格式的限制，促进了复杂的跨模态推理能力评估。

Abstract: As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.

</details>


### [183] [Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification](https://arxiv.org/abs/2511.13150)
*Rifen Lin,Alex Jinpeng Wang,Jiawei Mo,Min Li*

Main category: cs.CV

TL;DR: CSIP-ReID 是一种基于骨架驱动的预训练框架，通过对比学习将骨架序列与视频帧对齐，并融合运动与外观线索，在视频行人重识别任务中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态预训练方法在视频行人重识别中存在两个根本限制：缺乏真正的多模态预训练，以及文本无法捕捉细粒度的时间运动信息。

Method: 提出两阶段方法：第一阶段使用对比学习在序列级别对齐骨架和视觉特征；第二阶段引入动态原型融合更新器来优化多模态身份原型，并提出骨架引导的时间建模模块将骨架的时间线索整合到视觉特征中。

Result: 在标准视频ReID基准测试（MARS、LS-VID、iLIDS-VID）中达到新的最先进水平，在仅骨架ReID任务（BIWI、IAS）上也表现出强大的泛化能力。

Conclusion: CSIP-ReID开创了一种无需标注且具有运动感知的ReID预训练范式，为多模态表示学习开辟了新前沿。

Abstract: Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.

</details>


### [184] [THIR: Topological Histopathological Image Retrieval](https://arxiv.org/abs/2511.13170)
*Zahra Tabatabaei,Jon Sporring*

Main category: cs.CV

TL;DR: THIR是一种基于拓扑数据分析的无监督医学图像检索框架，利用Betti数和持久同调从组织病理学图像中提取拓扑特征，无需训练即可实现高效检索。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性主要死因，早期诊断和准确临床决策至关重要。传统深度学习方法需要大量标注数据和GPU资源，而THIR旨在提供无需监督的快速、可扩展解决方案。

Method: 使用立方体持久性从RGB组织病理学图像中提取拓扑指纹，将环的演化编码为紧凑可解释的特征向量，通过计算拓扑描述符之间的距离进行相似性检索。

Result: 在BreaKHis数据集上的实验表明，THIR优于现有监督和无监督方法，在标准CPU上20分钟内处理完整数据集。

Conclusion: THIR提供了一个快速、可扩展且无需训练的临床图像检索解决方案，特别适合资源受限的医疗环境。

Abstract: According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.
  Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.

</details>


### [185] [GenTract: Generative Global Tractography](https://arxiv.org/abs/2511.13183)
*Alec Sargood,Lemuel Puglisi,Elinor Thompson,Mirco Musolesi,Daniel C. Alexander*

Main category: cs.CV

TL;DR: GenTract是第一个用于全局纤维束成像的生成模型，将纤维束成像构建为生成任务，直接从dMRI映射到完整、解剖学上合理的纤维束。在低分辨率和噪声设置下表现优异，精度比次优方法高2.1倍。


<details>
  <summary>Details</summary>
Motivation: 传统的局部纤维束成像方法容易产生误差累积和高假阳性率，而全局方法计算成本高昂。需要一种既能处理高质量研究数据，又能在低质量数据上保持可靠性的解决方案。

Method: 将纤维束成像构建为生成任务，学习从dMRI到完整纤维束的直接映射。比较了基于扩散和流匹配的范式，并与最先进的基线方法进行对比。

Result: GenTract的精度比次优方法TractOracle高2.1倍。在具有挑战性的低分辨率和噪声设置下，其优势更加明显，比最接近的竞争对手高出一个数量级。

Conclusion: GenTract通过在研究级数据上产生高精度的纤维束图，同时在低分辨率数据上保持可靠性，为全局纤维束成像提供了一个有前景的解决方案。

Abstract: Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.

</details>


### [186] [Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://arxiv.org/abs/2511.13189)
*Diego Ortego,Marlon Rodríguez,Mario Almagro,Kunal Dahiya,David Jiménez,Juan C. SanMiguel*

Main category: cs.CV

TL;DR: 本文提出了ViXML框架，通过结合大型解码器模型和视觉信息来提升极端多标签分类(XMC)的性能，在保持计算效率的同时显著提高了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有XMC方法主要使用小型编码器架构，未能充分利用大型解码器模型和视觉信息的潜力。本文旨在探索如何有效利用这两种资源来提升XMC性能。

Method: 提出ViXML框架，集成数十亿参数规模的解码器模型，并通过每张图像提取单个嵌入的方式高效整合基础视觉模型，实现多模态能力。

Result: 在四个公共文本数据集及其图像增强版本上的实验表明，ViXML在最大数据集上的P@1指标比之前最优方法提升高达8.21%，且小编码器加视觉的方法在多数情况下优于纯文本解码器。

Conclusion: 视觉信息在XMC中具有重要价值，ViXML框架成功证明了结合大型解码器模型和视觉信息可以显著提升性能，同时保持计算效率。

Abstract: Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

</details>


### [187] [Video Spatial Reasoning with Object-Centric 3D Rollout](https://arxiv.org/abs/2511.13190)
*Haoran Tang,Meng Cao,Ruyang Liu,Xiaoxi Liang,Linglong Li,Ge Li,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出Object-Centric 3D Rollout (OCR)方法，通过引入结构化扰动来增强多模态大语言模型在视频空间推理中的能力，解决了现有模型存在的查询锁定推理问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在视频空间推理方面存在局限性，主要依赖于空间监督微调或强化学习，但往往表现出查询锁定推理，即只关注提示中明确提到的对象而忽略关键上下文线索。

Method: 提出OCR策略，在训练过程中对选定对象的3D几何结构引入结构化扰动，通过降低对象特定视觉线索并将改变的几何结构投影到2D空间，迫使模型在整个场景中进行整体推理。设计了基于rollout的训练流程，联合利用原始视频和区域噪声视频来优化空间推理轨迹。

Result: 实验结果表明，提出的3B参数模型在VSI-Bench上达到47.5%的准确率，优于多个7B基线模型。消融实验证实OCR优于先前的rollout策略（如T-GRPO、NoisyRollout）。

Conclusion: OCR方法有效解决了多模态大语言模型在视频空间推理中的查询锁定问题，通过结构化扰动和rollout训练策略显著提升了模型的空间推理能力，在保持较小模型规模的同时实现了最先进的性能。

Abstract: Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).

</details>


### [188] [Birth of a Painting: Differentiable Brushstroke Reconstruction](https://arxiv.org/abs/2511.13191)
*Ying Jiang,Jiayin Lu,Yunuo Chen,Yumeng He,Kui Wu,Yin Yang,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种可微分的笔划重建框架，统一了绘画、风格化纹理和涂抹操作，能够忠实再现人类绘画-涂抹循环。该框架通过并行可微分绘制器优化单色和双色贝塞尔笔划，结合风格生成模块和可微分涂抹算子，在几何和语义指导下联合优化笔划几何、颜色和纹理。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法主要关注最终图像生成或基于补丁的过程模拟，缺乏明确的笔划结构，无法产生平滑逼真的阴影效果。绘画创作过程与最终作品同等重要，需要更真实的笔划级重建。

Method: 1. 使用并行可微分绘制器优化单色和双色贝塞尔笔划
2. 风格生成模块合成几何条件纹理
3. 引入可微分涂抹算子实现自然颜色混合和阴影
4. 采用从粗到精的优化策略联合优化笔划几何、颜色和纹理

Result: 在油画、水彩、水墨和数字绘画上的广泛实验表明，该方法能够产生逼真且富有表现力的笔划重建、平滑的色调过渡和丰富的风格化外观。

Conclusion: 该方法为表达性数字绘画创作提供了一个统一模型，能够生成具有真实笔划结构和自然阴影效果的绘画作品。

Abstract: Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.

</details>


### [189] [Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection](https://arxiv.org/abs/2511.13195)
*Soyul Lee,Seungmin Baek,Dongbo Min*

Main category: cs.CV

TL;DR: MonoDLGD是一个新颖的难度感知标签引导去噪框架，通过基于检测不确定性自适应扰动和重建真实标签，在单目3D目标检测中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测由于深度线索的固有模糊性而存在根本性问题。现有DETR方法虽然通过全局注意力和辅助深度预测尝试缓解，但仍受限于不准确的深度估计，且忽略了实例级检测难度（如遮挡、距离、截断），导致检测性能不佳。

Method: 提出MonoDLGD框架：1）基于检测不确定性自适应扰动真实标签——对简单实例施加更强扰动，对困难实例施加较弱扰动；2）重建扰动后的标签以提供显式几何监督；3）联合优化标签重建和3D目标检测，促进几何感知表示学习。

Result: 在KITTI基准测试上的广泛实验表明，MonoDLGD在所有难度级别上都达到了最先进的性能。

Conclusion: MonoDLGD通过难度感知的标签引导去噪方法，有效解决了单目3D目标检测中的深度估计不准确和实例难度差异问题，显著提升了检测鲁棒性和性能。

Abstract: Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.

</details>


### [190] [Self-Supervised Ultrasound Screen Detection](https://arxiv.org/abs/2511.13197)
*Alberto Gomez,Jorge Oliveira,Ramon Casero,Agis Chartsias*

Main category: cs.CV

TL;DR: 提出一种自监督流水线，通过拍摄超声监视器照片来提取超声图像，绕过DICOM传输瓶颈，实现快速算法测试和原型开发。


<details>
  <summary>Details</summary>
Motivation: 解决超声设备图像传输依赖DICOM格式的瓶颈问题，实现快速算法开发和测试。

Method: 使用自监督流水线从超声监视器照片中提取和校正超声图像。

Result: 在概念验证研究中，校正后的图像保持了足够的视觉保真度，与原始DICOM图像相比，心脏视图分类的平衡准确率达到0.79。

Conclusion: 该方法能有效绕过DICOM传输限制，为超声图像算法的快速开发和测试提供了可行方案。

Abstract: Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.

</details>


### [191] [RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2511.13204)
*Junhee Lee,ChaeBeen Bang,MyoungChul Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: RefineVAD是一个弱监督视频异常检测框架，通过模仿人类感知异常的双过程推理，联合利用时间动态和语义结构来检测视频中的异常事件。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将所有异常事件视为单一类别，忽略了真实世界异常中多样的语义和时间特征。受人类感知异常方式的启发，需要同时理解不同异常类型的时间运动模式和语义结构。

Method: 提出RefineVAD框架，包含两个核心模块：MoTAR模块通过运动感知时间注意力估计运动显著性并动态调整时间焦点；CORE模块通过跨注意力将片段级特征与可学习的类别原型对齐，注入软异常类别先验。

Result: 在WVAD基准测试上的广泛实验验证了RefineVAD的有效性，并突显了整合语义上下文来引导特征细化朝向异常相关模式的重要性。

Conclusion: 通过联合利用时间动态和语义结构，RefineVAD能够显式建模运动如何演化以及它类似于什么语义类别，为弱监督视频异常检测提供了更全面的解决方案。

Abstract: Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.

</details>


### [192] [End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer](https://arxiv.org/abs/2511.13208)
*Yonghui Yu,Jiahang Cai,Xun Wang,Wenwu Yang*

Main category: cs.CV

TL;DR: 提出了首个端到端的多帧2D人体姿态估计方法PAVE-Net，通过消除检测、RoI裁剪和NMS等启发式操作，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有多人视频姿态估计方法依赖两阶段流程（检测+时序建模），使用启发式操作限制了准确性和效率。

Method: 提出PAVE-Net框架，包含空间编码器和时空姿态解码器，采用姿态感知注意力机制实现跨帧个体关联，并显式建模姿态关键点的时空依赖关系。

Result: 在PoseTrack2017上比基于图像的端到端方法提升6.0 mAP，与最先进的两阶段视频方法精度相当，同时显著提升效率。

Conclusion: PAVE-Net是首个端到端的多帧2D人体姿态估计方法，通过消除启发式操作，在保持精度的同时大幅提升了效率。

Abstract: Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet

</details>


### [193] [3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale](https://arxiv.org/abs/2511.13211)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 3DAlign-DAER是一个统一的文本-3D几何对齐框架，通过动态注意力策略和高效检索策略解决现有方法在细粒度语义对齐和大规模3D数据库扩展时的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的3D-文本跨模态对齐方法难以将细粒度文本语义与详细几何结构对齐，且在大规模3D数据库上对齐性能显著下降。

Method: 提出动态注意力策略（DAP），使用分层注意力融合模块将对齐表示为可学习的细粒度token到点注意力，并利用蒙特卡洛树搜索动态校准注意力权重；在推理阶段引入高效检索策略（ERS）进行分层搜索。

Result: 在多样化基准测试中表现出优越性能，构建了包含200万文本-3D对的大规模数据集Align3D-2M。

Conclusion: 3DAlign-DAER通过动态注意力策略和高效检索策略有效解决了细粒度文本-3D几何对齐问题，在大规模数据集上表现出色。

Abstract: Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.

</details>


### [194] [Hybrid-Domain Adaptative Representation Learning for Gaze Estimation](https://arxiv.org/abs/2511.13222)
*Qida Tan,Hongyu Yang,Wenchao Du*

Main category: cs.CV

TL;DR: 本文提出了一种混合域自适应表示学习框架，通过多源混合数据集学习鲁棒的视线表示，解决基于外观的视线估计在跨域评估中性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 基于外观的视线估计方法在跨域评估中因表情、佩戴物和图像质量等视线无关因素的干扰而性能显著下降，需要学习更鲁棒的视线表示。

Method: 提出混合域自适应表示学习框架，通过无监督域自适应方式对齐高质量近眼图像特征来解耦视线相关表示，并设计稀疏图融合模块探索视线方向与头部姿态的几何约束。

Result: 在EyeDiap、MPIIFaceGaze和Gaze360数据集上分别达到5.02°、3.36°和9.26°的最先进精度，并在跨数据集评估中表现出竞争力。

Conclusion: 该方法能有效学习鲁棒的视线表示，在多个数据集上达到最先进性能，且无需额外计算或推理成本。

Abstract: Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.

</details>


### [195] [MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI](https://arxiv.org/abs/2511.13232)
*Malek Al Abed,Sebiha Demir,Anne Groteklaes,Elodie Germani,Shahrooz Faghihroohi,Hemmen Sabir,Shadi Albarqouni*

Main category: cs.CV

TL;DR: MRIQT是一个3D条件扩散框架，用于将便携式超低场MRI图像质量提升到高场MRI水平，通过物理一致的K空间降级、v预测和SNR加权感知损失实现高质量图像转换。


<details>
  <summary>Details</summary>
Motivation: 便携式超低场MRI在新生儿护理中具有可及性优势，但其信噪比低、诊断质量差，需要提升图像质量以达到高场MRI的诊断水平。

Method: 采用3D条件扩散框架，结合物理一致的K空间降级模拟、带分类器自由引导的v预测、SNR加权的3D感知损失，使用注意力UNet架构进行结构保持的体素级转换。

Result: 在新生儿队列上训练，MRIQT在PSNR上超过现有GAN和CNN方法15.3%，比最先进方法提升1.78%，医生评价85%输出质量良好且病理清晰可见。

Conclusion: MRIQT能够实现便携式超低场MRI的高保真度增强，为可靠的新生儿脑部评估提供支持。

Abstract: Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.

</details>


### [196] [MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.13242)
*Junjie Wu,Guohong Fu*

Main category: cs.CV

TL;DR: 本文提出MMD-Thinker框架，通过自适应多维思维进行多模态虚假信息检测，解决了通用多模态大语言模型在检测中的推理不足和推理偏差问题。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息在社交媒体上泛滥，且随着AIGC技术的发展不断演变。现有基于通用多模态大语言模型的检测方法存在推理不足和推理偏差两个关键限制，无法有效应对快速增长的复杂多模态虚假信息。

Method: 提出两阶段框架MMD-Thinker：1）为多模态虚假信息检测设计定制化思维模式；2）通过任务特定指令调优将定制思维注入通用MLLMs；3）使用混合优势函数的强化学习策略增强推理能力。同时构建了包含8K+图像-文本对的多模态虚假信息推理数据集。

Result: 实验结果表明，MMD-Thinker在领域内和领域外基准数据集上都达到了最先进的性能，同时保持了灵活的推理和token使用效率。

Conclusion: MMD-Thinker通过自适应多维思维有效解决了多模态虚假信息检测中的推理挑战，为应对AIGC时代的多模态虚假信息提供了有效解决方案。

Abstract: Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.

</details>


### [197] [Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges](https://arxiv.org/abs/2511.13261)
*Junlong Li,Huaiyuan Xu,Sijie Cheng,Kejun Wu,Kim-Hui Yap,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了一个以自我为中心的程序化AI助手（EgoProceAssist）概念，旨在通过第一人称视角为日常程序化任务提供逐步支持。


<details>
  <summary>Details</summary>
Motivation: 受视觉语言模型和自我中心感知研究的进展推动，需要开发专门针对第一人称视角下日常程序化任务的AI助手。

Method: 识别了三个核心任务：自我中心程序错误检测、自我中心程序学习和自我中心程序问答；进行了全面技术回顾、数据集分析和评估指标总结；对代表性VLM方法进行了新颖实验和综合评估。

Result: 建立了新的分类法，明确了EgoProceAssist与现有VLM助手的差距，并提供了公开资源库持续收集最新研究。

Conclusion: 讨论了未来挑战和研究方向，为构建自我中心程序化AI助手奠定了基础。

Abstract: Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant

</details>


### [198] [SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression](https://arxiv.org/abs/2511.13264)
*Keshav Gupta,Akshat Sanghvi,Shreyas Reddy Palley,Astitva Srivastava,Charu Sharma,Avinash Sharma*

Main category: cs.CV

TL;DR: SymGS是一个基于对称感知的3D高斯泼溅压缩框架，通过引入可学习镜像来消除局部和全局的反射冗余，实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术在渲染速度和真实感方面表现优异，但内存占用随场景复杂度快速增长。现有压缩方法主要利用基元级冗余进行压缩，但仍有改进空间。

Method: 提出SymGS框架，引入可学习镜像来识别和消除场景中的镜像对称冗余，作为现有压缩方法（如HAC）的即插即用增强模块。

Result: 相比HAC方法，在基准数据集上实现1.66倍压缩（大规模场景可达3倍），平均实现108倍的3DGS场景压缩，同时保持渲染质量。

Conclusion: SymGS通过对称感知技术有效突破了现有压缩方法的极限，为3D高斯泼溅提供了高效的内存压缩解决方案。

Abstract: 3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \textbf{\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \times$ compression across benchmark datasets (upto $3\times$ on large-scale scenes). On an average, SymGS enables $\bf{108\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \textbf{\color{cyan}{symgs.github.io}}

</details>


### [199] [Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation](https://arxiv.org/abs/2511.13269)
*Lingfeng Zhang,Yuchen Zhang,Hongsheng Li,Haoxiang Fu,Yingbo Tang,Hangjun Ye,Long Chen,Xiaojun Liang,Xiaoshuai Hao,Wenbo Ding*

Main category: cs.CV

TL;DR: 该论文提出了SpatialSky-Bench基准测试来评估视觉语言模型在无人机导航中的空间智能能力，并开发了Sky-VLM模型在相关任务上取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在无人机场景中的空间智能能力尚未充分探索，存在在动态环境中导航和解释效果不佳的问题。

Method: 开发了SpatialSky-Bench基准测试（包含环境感知和场景理解两个类别13个子类别），创建了包含100万样本的SpatialSky-Dataset，并设计了专门用于无人机空间推理的Sky-VLM模型。

Result: 主流视觉语言模型在复杂无人机导航场景中表现不佳，而Sky-VLM在所有基准任务上都达到了最先进的性能水平。

Conclusion: Sky-VLM为开发适用于无人机场景的视觉语言模型开辟了新途径，显著提升了空间推理能力。

Abstract: Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.

</details>


### [200] [SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.13278)
*Zihan Li,Tengfei Wang,Wentian Gan,Hao Zhan,Xin Wang,Zongqian Zhan*

Main category: cs.CV

TL;DR: SF-Recon是一种直接从多视角图像重建轻量级建筑表面的方法，无需后处理网格简化，通过3D高斯泼溅、法向梯度引导优化和边缘一致性剪枝实现结构保真。


<details>
  <summary>Details</summary>
Motivation: 传统多视角几何流程依赖密集重建、网格化和后续简化，过程繁琐且质量敏感，需要直接重建轻量级建筑表面模型的方法。

Method: 首先训练3D高斯泼溅场获得视图一致表示，然后通过法向梯度引导的高斯优化选择与屋顶和墙壁边界对齐的基元，接着进行多视角边缘一致性剪枝增强结构锐度，最后通过深度约束的Delaunay三角化转换为轻量网格。

Result: 在提出的SF数据集上，SF-Recon能够直接从多视角图像重建轻量级建筑模型，显著减少面和顶点数量，同时保持计算效率。

Conclusion: SF-Recon方法能够有效直接重建轻量级建筑表面模型，避免了传统流程的繁琐后处理步骤，在保持结构保真度的同时实现了高效计算。

Abstract: Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/

</details>


### [201] [TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing](https://arxiv.org/abs/2511.13283)
*Jongha Kim,Minseong Bae,Sanghyeok Lee,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: TabFlash是一个高效的多模态大语言模型，通过渐进式问题条件化、剪枝策略和token聚焦训练来解决表格图像理解中的冗余和效率问题，在保持高性能的同时显著降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 表格图像理解面临两个主要挑战：需要问题特定的关注点，以及存在冗余的背景区域。现有的MLLM方法往往忽视这些特性，导致视觉表示不够信息丰富且冗余。

Method: 1. 渐进式问题条件化：将问题信息以逐渐增加的频率注入Vision Transformer层；2. 剪枝策略：丢弃背景token以提高效率；3. token聚焦训练：鼓励模型将关键信息集中在保留的token中，减轻剪枝带来的信息损失。

Result: TabFlash实现了最先进的性能，在表格理解任务上超越了开源和专有MLLM，同时比第二好的MLLM减少了27%的FLOPs和30%的内存使用。

Conclusion: 通过结合渐进式问题条件化、剪枝和token聚焦训练，TabFlash为表格图像理解提供了一种既高效又有效的解决方案，在性能和效率方面都取得了显著提升。

Abstract: Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.

</details>


### [202] [SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design](https://arxiv.org/abs/2511.13285)
*Yunjie Yu,Jingchen Wu,Junchen Zhu,Chunze Lin,Guibin Chen*

Main category: cs.CV

TL;DR: SkyReels-Text是一个字体可控的精确海报文本编辑框架，能够在保持非编辑区域视觉外观的同时，对多个文本区域进行同时编辑，每个区域可以使用不同的字体样式，无需字体标签或推理时微调。


<details>
  <summary>Details</summary>
Motivation: 解决专业设计工作流程中细粒度、字体感知的文本编辑需求，弥补通用图像编辑模型在专业排版设计方面的不足。

Method: 提出字体可控框架，用户只需提供所需字体的裁剪字形补丁，即可实现多文本区域的同步编辑，无需字体标签或推理时微调。

Result: 在多个数据集上的广泛实验表明，SkyReels-Text在文本保真度和视觉真实感方面达到最先进性能，提供对字体家族和风格细微差别的空前控制。

Conclusion: 该工作弥合了通用图像编辑与专业级排版设计之间的差距，为海报设计等专业应用提供了强大的文本编辑工具。

Abstract: Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.

</details>


### [203] [DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving](https://arxiv.org/abs/2511.13309)
*Kaiwen Cai,Xinze Liu,Xia Zhou,Hengtong Hu,Jie Xiang,Luyao Zhang,Xueyang Zhang,Kun Zhan,Yifei Zhan,Xianpeng Lang*

Main category: cs.CV

TL;DR: DriveLiDAR4D是一个创新的LiDAR点云生成流水线，通过多模态条件和序列噪声预测模型LiDAR4DNet，能够生成时间一致的LiDAR场景，具有高度可控的前景对象和逼真的背景。


<details>
  <summary>Details</summary>
Motivation: 现有的3D LiDAR点云生成方法存在序列生成能力不足、前景对象定位不准确和背景不真实等限制，阻碍了其实际应用。

Method: 提出了DriveLiDAR4D流水线，包含多模态条件和新型序列噪声预测模型LiDAR4DNet，以端到端方式实现具有完整场景操作能力的LiDAR场景序列生成。

Result: 在nuScenes和KITTI数据集上评估，DriveLiDAR4D在nuScenes数据集上获得FRD分数743.13和FVD分数16.96，相比当前SOTA方法UniScene，FRD性能提升37.2%，FVD提升24.1%。

Conclusion: 这是首个以端到端方式解决具有完整场景操作能力的LiDAR场景序列生成的工作，在性能指标上显著优于现有方法。

Abstract: The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.

</details>


### [204] [YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection](https://arxiv.org/abs/2511.13344)
*Ori Meiraz,Sharon Shalev,Avishai Weizman*

Main category: cs.CV

TL;DR: 提出了一种用于目标检测的新型混合专家框架，通过多个YOLOv9-T专家之间的自适应路由实现动态特征专业化，相比单一YOLOv9-T模型获得了更高的mAP和AR。


<details>
  <summary>Details</summary>
Motivation: 为了提升目标检测性能，需要解决单一模型在处理多样化特征时的局限性，通过专家混合框架实现更精细的特征处理。

Method: 采用混合专家框架，集成多个YOLOv9-T专家模型，通过自适应路由机制实现动态特征专业化分配。

Result: 相比单一YOLOv9-T模型，该框架在平均精度(mAP)和平均召回率(AR)指标上均取得了提升。

Conclusion: 混合专家框架通过多专家协同和自适应路由机制，有效提升了目标检测性能，证明了该方法的有效性。

Abstract: This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.

</details>


### [205] [What Color Is It? A Text-Interference Multimodal Hallucination Benchmark](https://arxiv.org/abs/2511.13400)
*Jinkun Zhao,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: 该论文针对多模态大模型在视觉感知中的颜色信息干扰问题，提出了一个名为"What Color Is It"的数据集来触发单模态视觉幻觉，并研究了幻觉原因及可能的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型的快速发展，这些模型在视觉感知中仍然容易受到信息干扰，特别是在颜色感知方面，这会增加幻觉风险。

Method: 通过构建"What Color Is It"数据集，使用简单方法触发多模态大模型中的单模态视觉幻觉，并基于此数据集分析幻觉的根本原因。

Result: 验证了多模态大模型在视觉模态中确实存在颜色感知方面的幻觉问题，并识别了导致这些幻觉的潜在原因。

Conclusion: 多模态大模型在视觉感知方面存在脆弱性，特别是在颜色信息处理上，需要开发增强其鲁棒性的解决方案。

Abstract: With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the "What Color Is It" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.

</details>


### [206] [VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task](https://arxiv.org/abs/2511.13420)
*Xingming Long,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 提出了VOPE方法，用于评估大型视觉语言模型在自愿想象任务中的幻觉问题，通过存在性评估来区分合理的想象内容和真正的幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注禁止输出图像中不存在内容的事实描述任务，而忽视了自愿想象任务（如故事创作）中的幻觉问题，这些任务需要模型生成超出图像的新内容。

Method: VOPE方法通过提出重新检查式问题来评估LVLM如何解释其响应中想象对象的存在性，然后根据模型解释与图像中对象存在的一致性来判断是否产生幻觉。

Result: 应用VOPE评估主流LVLMs和幻觉缓解方法发现：(1)大多数LVLMs在自愿想象时严重幻觉，对想象对象的存在性评估表现很差；(2)现有幻觉缓解方法在自愿想象任务中效果有限。

Conclusion: 自愿想象任务中的幻觉问题是一个重要的研究方向，现有方法对此类任务效果不佳，需要进一步研究。

Abstract: Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.

</details>


### [207] [FUSE: A Flow-based Mapping Between Shapes](https://arxiv.org/abs/2511.13431)
*Lorenzo Olearo,Giulio Viganò,Daniele Baieri,Filippo Maggioli,Simone Melzi*

Main category: cs.CV

TL;DR: 提出了一种基于流匹配模型的3D形状间映射的神经表示方法，该方法计算高效，支持跨表示形式的形状匹配，无需大规模训练或数据驱动过程。


<details>
  <summary>Details</summary>
Motivation: 为了解决3D形状匹配中跨不同表示形式（点云、网格、SDF等）的挑战，并避免传统方法需要大规模训练数据的问题。

Method: 将3D形状表示为从固定锚分布通过连续可逆流映射诱导的概率分布。通过组合源形状到锚的逆流和锚到目标形状的正向流，实现两个表面之间的连续点映射。使用点级任务定制嵌入对形状进行编码。

Result: 该方法在多样化的基准测试和具有挑战性的形状匹配设置中始终实现高覆盖率和准确性。在UV映射和人体原始点云扫描配准等其他任务中也显示出有希望的结果。

Conclusion: 该框架提供了一种可逆且模态无关的形状间映射表示，能够有效处理多种3D数据表示形式，在形状匹配和相关任务中表现出色。

Abstract: We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.

</details>


### [208] [InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE](https://arxiv.org/abs/2511.13488)
*Lipeng Wang,Hongxing Fan,Haohua Chen,Zehuan Huang,Lu Sheng*

Main category: cs.CV

TL;DR: InterMoE是一个基于动态时间选择性专家混合的新框架，用于生成高质量的人类交互动作，能够保持个体特征并忠实于文本描述。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成人类交互时往往无法保持独特的个体特征或完全遵循文本描述，这限制了在虚拟现实和机器人等应用中的使用价值。

Method: 采用动态时间选择性专家混合框架，通过路由机制协同使用高级文本语义和低级运动上下文，将时间运动特征分配给专门专家，让专家动态确定选择能力并关注关键时间特征。

Result: 在InterHuman数据集上FID分数降低9%，在InterX数据集上降低22%，实现了个体特定高保真3D人类交互生成的最先进性能。

Conclusion: InterMoE框架通过动态专家选择机制有效解决了保持个体特征和语义保真度的挑战，在人类交互生成任务中表现出色。

Abstract: Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.

</details>


### [209] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: LGIP基准测试评估视觉语言模型对语义保留改写和语义改变翻转的鲁棒性，发现EVA02-CLIP和大型OpenCLIP变体在不变性和敏感性方面表现最佳，而SigLIP模型存在较大问题。


<details>
  <summary>Details</summary>
Motivation: 评估当前视觉语言模型对受控语言扰动的鲁棒性，特别是对语义保留改写的不变性和对语义改变翻转的敏感性。

Method: 使用40k MS COCO图像和人工标注，自动生成语义保留改写和基于规则的语义改变翻转（改变对象类别、颜色或数量），通过不变性误差、语义敏感性差距和阳性率统计来总结模型行为。

Result: EVA02-CLIP和大型OpenCLIP变体在不变性-敏感性边界上表现最佳，具有较低的改写引起方差，同时对原始描述的评分始终高于翻转版本。SigLIP和SigLIP2表现出较大的不变性误差，经常偏好翻转描述而非人工描述。

Conclusion: LGIP提供了模型无关的诊断工具，能够评估视觉语言模型的语言鲁棒性，超越了传统检索指标的能力范围。

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [210] [Mapping the Vanishing and Transformation of Urban Villages in China](https://arxiv.org/abs/2511.13507)
*Wenyu Zhang,Yao Tong,Yiqiu Liu,Rui Cao*

Main category: cs.CV

TL;DR: 本研究提出了一个基于深度学习的框架来监测中国城中村的时空变化，通过多时相遥感影像语义分割和土地用途分类，揭示了城中村改造的复杂性和非线性特征。


<details>
  <summary>Details</summary>
Motivation: 中国城中村经历了大规模拆迁改造，但缺乏对拆迁土地是否有效再利用的系统评估，需要评估当前改造实践的有效性和可持续性。

Method: 使用多时相遥感影像语义分割绘制城中村边界变化，然后将拆迁后土地用途分为六类：未完成拆迁、闲置土地、建筑工地、建筑物、绿地和其他。选取广州、郑州、西安和哈尔滨四个代表性城市作为研究区域。

Result: 1) 城中村改造过程经常被延长；2) 改造主要发生在城市外围区域，而城市核心区相对稳定；3) 揭示了三种时空转型路径：同步改造、延迟改造和渐进优化。

Conclusion: 城中村改造具有碎片化、复杂性和非线性特征，需要分层级和因地制宜的规划策略。研究结果为支持更包容、高效和可持续的城市更新提供了有价值的实证见解。

Abstract: Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the "remained-demolished-redeveloped" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.

</details>


### [211] [Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems](https://arxiv.org/abs/2511.13533)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

TL;DR: 本文提出了一种渐近极小极大方法用于多目标保形预测，为逆成像问题中的多目标不确定性量化提供紧密的预测区间，并确保联合边际覆盖。


<details>
  <summary>Details</summary>
Motivation: 在不适定成像逆问题中，现有方法只能处理标量估计目标，而实际应用往往涉及多个目标，因此需要开发能够处理多目标的不确定性量化方法。

Method: 提出渐近极小极大多目标保形预测方法，该方法在确保联合边际覆盖的同时提供紧密的预测区间，可应用于多度量盲图像质量评估、多任务不确定性量化和多轮测量采集。

Result: 通过合成数据和磁共振成像数据的数值实验证明，该方法相对于现有的多目标保形预测方法具有优势。

Conclusion: 所提出的极小极大方法为多目标不确定性量化提供了有效的解决方案，在多个应用场景中表现出优越性能。

Abstract: In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.

</details>


### [212] [Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew](https://arxiv.org/abs/2511.13535)
*Farhin Farhad Riya,Shahinul Hoque,Jinyuan Stella Sun,Olivera Kotevska*

Main category: cs.CV

TL;DR: 本文提出了一种新的联邦学习攻击方法，通过微小颜色扰动在不影响模型准确率的情况下破坏模型可解释性，使显著性图偏离语义区域。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在安全关键领域部署，可解释性技术变得至关重要。本文揭示了一种新的攻击类型，能够在不影响准确率的情况下破坏模型的可解释性。

Method: 提出了色度扰动模块，通过系统性地改变前景和背景之间的颜色对比度来制作对抗样本，在联邦学习环境中累积这些扰动以毒化全局模型的内部特征归因。

Result: 攻击将Grad-CAM解释中的峰值激活重叠减少了高达35%，同时在所有评估数据集上保持分类准确率超过96%。

Conclusion: 研究挑战了模型审计中正确预测意味着忠实解释的常见假设，证明可解释性本身可能成为攻击面，标准训练流程不足以检测或缓解解释退化。

Abstract: As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.

</details>


### [213] [BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse](https://arxiv.org/abs/2511.13539)
*Yuanchao Wang,Tian Qin,Eduardo Valle,Bruno Abrahao*

Main category: cs.CV

TL;DR: BootOOD是一个完全自监督的OOD检测框架，通过从ID数据中合成伪OOD特征，利用神经崩溃现象，设计轻量级辅助头进行基于半径的分类，在语义相似的OOD样本检测上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的OOD检测器在处理语义上与ID类别相似的OOD样本时表现不佳，需要一种专门设计来处理这种语义挑战性OOD样本的框架。

Method: BootOOD通过简单变换ID表示来合成伪OOD特征，利用神经崩溃现象，引入轻量级辅助头进行基于特征范数的半径分类，将OOD检测与主分类器解耦。

Result: 在CIFAR-10、CIFAR-100和ImageNet-200上的实验表明，BootOOD优于先验的后处理方法，在没有异常暴露的情况下超越基于训练的方法，并与最先进的异常暴露方法竞争，同时保持或提高ID准确率。

Conclusion: BootOOD提供了一种有效的自监督OOD检测解决方案，特别适用于处理语义相似的OOD样本，在多个基准测试中表现出色。

Abstract: Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.

</details>


### [214] [Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation](https://arxiv.org/abs/2511.13571)
*Ziyang Huang,Jiagang Chen,Jin Liu,Shunping Ji*

Main category: cs.CV

TL;DR: Opt3DGS是一个增强3D高斯溅射优化的框架，通过两阶段优化过程解决局部最优和收敛质量不足的问题，在不改变基础表示的情况下实现最先进的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在新视角合成中表现出色，但其核心优化问题未被充分探索，存在陷入局部最优和收敛质量不足两个关键问题。

Method: 提出两阶段优化框架：探索阶段使用自适应加权随机梯度朗之万动力学增强全局搜索；利用阶段使用局部拟牛顿方向引导的Adam优化器利用曲率信息进行精确收敛。

Result: 在多个基准数据集上的广泛实验表明，Opt3DGS通过改进3DGS优化过程实现了最先进的渲染质量。

Conclusion: Opt3DGS是一个鲁棒的优化框架，能够有效解决3D高斯溅射的优化挑战，提升渲染质量而不改变其基础表示。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.

</details>


### [215] [Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images](https://arxiv.org/abs/2511.13586)
*Yinuo Xu,Yan Cui,Mingyao Li,Zhi Huang*

Main category: cs.CV

TL;DR: NuClass是一个病理学家工作流启发的框架，通过多尺度整合核形态和微环境上下文进行细胞级分类，解决了现有方法缺乏组织上下文和细粒度标注的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像块的模型能捕捉核形态细节但缺乏组织上下文，且可用的人类标注通常粗糙且分布不均，难以获得细粒度亚型级监督。

Method: NuClass包含两个主要组件：Path local（224×224像素裁剪关注核形态）和Path global（1024×1024像素邻域建模周围环境），通过可学习门控模块自适应平衡局部细节和上下文线索，并采用不确定性引导目标促进互补学习。

Result: 在三个完全保留的队列上评估，NuClass最佳类别达到96% F1分数，优于强基线方法。构建了来自Xenium空间转录组测定的标记引导数据集，包含八个器官和16个类别的超过两百万个细胞的单细胞分辨率标签。

Conclusion: 多尺度、不确定性感知融合可以弥合幻灯片级病理基础模型与可靠细胞级表型预测之间的差距。

Abstract: Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.
  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.
  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.

</details>


### [216] [Tissue Aware Nuclei Detection and Classification Model for Histopathology Images](https://arxiv.org/abs/2511.13615)
*Kesi Xu,Eleni Chiou,Ali Varamesh,Laura Acqualagna,Nasir Rajpoot*

Main category: cs.CV

TL;DR: TAND是一个新颖的组织感知细胞核检测框架，通过结合点级监督和组织掩码条件化，实现了联合细胞核检测和分类，在PUMA基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖详细的专家标注且未能充分利用组织上下文信息，这限制了计算病理学中细胞核检测和分类的准确性。

Method: TAND将ConvNeXt编码器-解码器与冻结的Virchow-2组织分割分支耦合，通过新颖的多尺度空间特征线性调制（Spatial-FiLM）利用语义组织概率选择性调节分类流。

Result: 在PUMA基准测试中，TAND实现了最先进的性能，超越了组织无关基线和掩码监督方法，特别是在组织依赖性细胞类型（如上皮细胞、内皮细胞和基质细胞）方面表现出显著改进。

Conclusion: 这是首个基于学习组织掩码进行单细胞分类条件化的方法，为减少标注负担提供了实用途径。

Abstract: Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.

</details>


### [217] [CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding](https://arxiv.org/abs/2511.13644)
*Shrenik Patel,Daivik Patel*

Main category: cs.CV

TL;DR: CacheFlow是一个无需训练的流水线，通过动态令牌丢弃和压缩长期记忆来解决长视频问答中的注意力增长问题，显著减少令牌处理量并保持答案保真度。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在处理长视频问答时面临注意力机制和键值缓存随运行时间增长的问题，导致推理成本高昂或只能使用短视的滑动窗口方法。

Method: 结合动态令牌丢弃和压缩长期记忆，在线修剪每个补丁令牌，将幸存令牌打包成固定大小的块，使用小型循环编码器形成检索索引，并在推理时检索最相关的块进行注意力计算。

Result: 在离线和流式视频问答基准测试中，CacheFlow优于当前强基线方法，同时处理令牌数量减少高达87%。

Conclusion: CacheFlow使视觉语言模型既高效又具备上下文感知能力，为实用的长视频理解铺平了道路。

Abstract: Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.

</details>


### [218] [Part-X-MLLM: Part-aware 3D Multimodal Large Language Model](https://arxiv.org/abs/2511.13647)
*Chunshi Wang,Junliang Ye,Yunhan Yang,Yang Li,Zizhuo Lin,Jun Zhu,Zhuo Chen,Yawei Luo,Chunchao Guo*

Main category: cs.CV

TL;DR: Part-X-MLLM是一个原生3D多模态大语言模型，通过结构化可执行语法将多样化3D任务统一为程序，实现基于部件的生成和编辑。


<details>
  <summary>Details</summary>
Motivation: 现有3D任务处理方式分散，缺乏统一接口。本文旨在开发一个能够通过单一语言前端控制多种几何引擎的统一模型，实现3D任务的程序化处理。

Method: 采用双编码器架构分离结构和语义，在大型部件中心数据集上进行指令调优，自回归生成包含部件级边界框、语义描述和编辑命令的连贯标记序列。

Result: 模型在生成高质量结构化规划方面表现优异，在基于部件的问答、组合生成和局部编辑任务中实现了最先进的性能。

Conclusion: Part-X-MLLM通过将符号规划与几何合成解耦，为多样化3D任务提供了统一的语言原生前端接口，展示了结构化程序化方法的有效性。

Abstract: We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/

</details>


### [219] [PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image](https://arxiv.org/abs/2511.13648)
*Ziang Cao,Fangzhou Hong,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: PhysX-Anything是首个面向仿真的物理3D生成框架，能够从单张野外图像生成具有明确几何、关节和物理属性的高质量仿真就绪3D资产。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法忽视了关键的物理和关节属性，限制了在具身AI中的实用性。

Method: 提出了首个基于VLM的物理3D生成模型，以及新的3D表示方法，将token数量减少193倍，在标准VLM token预算内实现显式几何学习。构建了PhysX-Mobility数据集，扩展了物理3D数据集的物体类别。

Result: 在PhysX-Mobility数据集和野外图像上的实验表明，PhysX-Anything具有强大的生成性能和稳健的泛化能力。仿真实验验证了生成的资产可直接用于接触密集的机器人策略学习。

Conclusion: PhysX-Anything能够显著赋能下游应用，特别是在具身AI和基于物理的仿真领域。

Abstract: 3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.

</details>


### [220] [Distribution Matching Distillation Meets Reinforcement Learning](https://arxiv.org/abs/2511.13649)
*Dengyang Jiang,Dongyang Liu,Zanyi Wang,Qilong Wu,Xin Jin,David Liu,Zhen Li,Mengmeng Wang,Peng Gao,Harry Yang*

Main category: cs.CV

TL;DR: DMDR是一个结合强化学习和蒸馏的新框架，将预训练的多步扩散模型蒸馏为少步模型，通过RL指导蒸馏过程，使少步生成器性能超越多步教师模型。


<details>
  <summary>Details</summary>
Motivation: 传统蒸馏方法中少步模型的性能受限于多步教师模型，需要突破这一限制来提升少步模型的生成质量。

Method: 将强化学习技术融入蒸馏过程，使用DMD损失作为RL的正则化项，并设计了动态分布引导和动态重噪声采样训练策略。

Result: 实验表明DMDR在少步方法中达到领先的视觉质量和提示一致性，甚至性能超过多步教师模型。

Conclusion: DMDR框架成功解锁了少步生成器的潜力，通过同时进行蒸馏和强化学习实现了性能突破。

Abstract: Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.

</details>


### [221] [OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation](https://arxiv.org/abs/2511.13655)
*Henry Herzog,Favyen Bastani,Yawen Zhang,Gabriel Tseng,Joseph Redmon,Hadrien Sablon,Ryan Park,Jacob Morrison,Alexandra Buraczynski,Karen Farley,Joshua Hansen,Andrew Howe,Patrick Alan Johnson,Mark Otterlee,Ted Schmitt,Hunter Pitelka,Stephen Daspit,Rachel Ratner,Christopher Wilhelm,Sebastian Wood,Mike Jacobi,Hannah Kerner,Evan Shelhamer,Ali Farhadi,Ranjay Krishna,Patrick Beukema*

Main category: cs.CV

TL;DR: OlmoEarth是一个多模态、时空基础模型，专门为地球观测数据设计，在多个基准测试和实际任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据具有空间性、时序性和多模态的独特挑战，需要专门设计的模型来处理这些复杂特征。

Method: 采用新颖的自监督学习公式、掩码策略和损失函数，专门针对地球观测领域设计。

Result: 在24个任务中的15个任务上获得最佳嵌入性能，在29个任务中的19个任务上获得最佳微调性能，优于其他12个基础模型。

Conclusion: OlmoEarth作为端到端平台的核心，为非营利组织和NGO提供前沿基础模型和强大数据管理工具，用于解决全球重大问题。

Abstract: Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$.

</details>


### [222] [Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting](https://arxiv.org/abs/2511.13684)
*Jiangnan Ye,Jiedong Zhuang,Lianrui Mu,Wenjie Zheng,Jiaqi Hu,Xingze Zou,Jing Wang,Haoji Hu*

Main category: cs.CV

TL;DR: GS-Light是一个基于高斯泼溅(3DGS)的文本引导3D场景重光照系统，通过训练自由的扩散模型处理多视图输入，结合大视觉语言模型解析光照先验，生成高质量的重光照3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本引导的3D场景重光照方面存在局限性，特别是在准确理解用户指定的光照方向、颜色和强度等要求方面。GS-Light旨在通过结合几何约束和语义信息，更准确地满足用户的光照需求。

Method: 使用LVLM解析用户提示生成光照先验，结合深度、法线和语义分割等几何语义信息，计算光照图并生成初始潜在代码。通过多视图重光照模型生成重光照图像，最后微调3DGS场景以获得完全重光照的3D场景。

Result: 在室内外场景评估中，GS-Light在多视图一致性、图像质量、美学评分和语义相似度等定量指标上均优于现有基线方法，用户研究也证实了其优越性。

Conclusion: GS-Light提供了一种高效、文本位置感知的3D场景重光照方法，能够准确理解并实现用户的光照要求，在多个评估维度上均表现出色。

Abstract: We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.

</details>


### [223] [TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models](https://arxiv.org/abs/2511.13704)
*Harold Haodong Chen,Disen Lan,Wen-Jie Shu,Qingyang Liu,Zihan Wang,Sirui Chen,Wenkai Cheng,Kanghao Chen,Hongfei Zhang,Zixin Zhang,Rongjin Guo,Yu Cheng,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出了TiViBench基准来评估图像到视频生成模型的推理能力，包含四个维度的24个任务场景，并开发了VideoTPO策略来提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型评估主要关注视觉保真度和时间一致性，缺乏对高阶推理能力的评估，需要专门基准来填补这一空白。

Method: 设计TiViBench分层基准系统评估结构推理、空间视觉模式推理、符号逻辑推理、行动规划四个维度；提出VideoTPO测试时策略，通过LLM自分析生成候选来识别优劣。

Result: 商业模型（如Sora 2、Veo 3.1）表现出更强的推理潜力，开源模型因训练规模和数据多样性限制而潜力未充分开发；VideoTPO能显著提升推理性能且无需额外训练。

Conclusion: TiViBench和VideoTPO为评估和推进视频生成模型的推理能力奠定了基础，为该新兴领域的未来研究设定了方向。

Abstract: The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.

</details>


### [224] [Segment Anything Across Shots: A Method and Benchmark](https://arxiv.org/abs/2511.13715)
*Hengrui Hu,Kaining Ying,Henghui Ding*

Main category: cs.CV

TL;DR: 本文提出了一种用于多镜头半监督视频对象分割（MVOS）的新方法SAAS，通过过渡模仿数据增强策略和跨镜头分割模型，解决了现有方法在镜头切换时的性能下降问题，并在新基准Cut-VOS上实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频对象分割方法主要针对单镜头视频，在镜头切换时性能显著下降，限制了实际应用。多镜头视频对象分割面临标注数据稀疏的挑战。

Method: 提出了过渡模仿数据增强策略（TMA），利用单镜头数据实现跨镜头泛化；开发了Segment Anything Across Shots（SAAS）模型，能够有效检测和理解镜头切换。

Result: 在YouMVOS和Cut-VOS基准上的广泛实验表明，SAAS通过有效模仿、理解和分割复杂过渡，实现了最先进的性能。

Conclusion: SAAS模型在多镜头视频对象分割任务中表现出色，提出的数据增强策略和跨镜头分割方法为解决镜头切换问题提供了有效解决方案。

Abstract: This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [225] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: 本研究探讨了使用大型语言模型生成的合成数据集替代真实世界数据支持自然语言处理任务的潜力，特别关注负面情感文本。通过生成合成新闻标题并与真实标题进行比较评估，发现合成标题在大多数指标上与真实标题匹配良好。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言处理任务中数据获取困难和真实数据隐私问题，探索LLM生成合成数据作为替代方案的可行性，特别关注负面情感分析这一重要领域。

Method: 使用定制提示生成负面新闻标题合成数据集，通过专家评审验证，并在嵌入空间中分析其与真实负面新闻在内容、语调、长度和风格上的对齐度。采用比较困惑度测试、可读性测试、词性标注分析、BERTScore和语义相似度等多种评估方法。

Result: 合成标题在大多数评估指标上与真实标题表现一致，唯一显著差异出现在词性标注分析中的专有名词得分。合成数据集在相关性、困惑度、连贯性和真实性等关键指标上表现良好。

Conclusion: LLM生成的合成数据集在负面情感文本分析中具有替代真实数据的潜力，能够有效支持自然语言处理任务，同时规避数据获取和隐私问题。

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [226] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: CLINB是一个评估大语言模型处理气候变化专业知识的基准测试，发现前沿模型具有博士级别的知识合成能力，但在证据基础和引用方面存在严重幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型处理复杂专业知识的能力是一个关键挑战，特别是在气候变化这样的专业领域，需要建立可靠的评估基准。

Method: 引入CLINB基准，使用真实用户问题和气候科学家制定的评估标准，通过基于模型的评估流程评估多个前沿模型在开放式、多模态问答任务中的表现。

Result: 前沿模型展现出卓越的知识合成能力，甚至超过专家辅助的混合答案，但在证据基础方面存在严重问题，引用和图像存在大量幻觉。

Conclusion: 弥合知识合成与可验证归因之间的差距对于AI在科学工作流程中的部署至关重要，需要像CLINB这样可靠、可解释的基准来构建可信的AI系统。

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [227] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: CausalGuard是一种结合因果推理和符号逻辑的新方法，用于实时检测和防止大语言模型的幻觉问题，在12个基准测试中准确识别89.3%的幻觉，减少近80%的错误声明。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在自信地陈述错误信息的'幻觉'问题，这成为在准确性至关重要的场景中使用这些模型的主要障碍。现有解决方案要么需要重新训练整个模型，要么增加显著计算成本，或者未能解决幻觉的根本原因。

Method: CausalGuard通过两个互补路径工作：一个追踪模型已知信息与生成内容之间的因果关系，另一个使用自动推理检查逻辑一致性。与仅检查生成后输出的方法不同，该系统理解导致错误陈述的因果链并在过程中早期干预。

Result: 在12个不同基准测试中，CausalGuard正确识别89.3%的幻觉，仅遗漏8.3%的实际幻觉。更重要的是，它在保持回答自然和有用的同时，将错误声明减少了近80%。该系统在需要多步逻辑的复杂推理任务上表现尤其出色。

Conclusion: CausalGuard通过展示其推理过程，在医疗诊断或财务分析等敏感领域表现良好，因为这些领域理解决策原因与决策本身同等重要。该方法为解决大语言模型的幻觉问题提供了有效的解决方案。

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [228] [Quantifying Skill and Chance: A Unified Framework for the Geometry of Games](https://arxiv.org/abs/2511.11611)
*David H. Silver*

Main category: cs.AI

TL;DR: 提出了一个量化框架，通过将游戏建模为随机决策树来分离技能和运气成分，定义了技能-运气指数S(G)在[-1,1]范围内，应用于30个游戏揭示了从纯运气到纯技能的连续谱系。


<details>
  <summary>Details</summary>
Motivation: 开发一个系统性的方法来量化游戏中技能和运气成分的相对贡献，以便更好地理解玩家影响力、游戏平衡和预测稳定性。

Method: 将游戏建模为随机决策树，将游戏结果分解为技能杠杆K和运气杠杆L，定义技能-运气指数S(G) = K - L，并引入波动性Sigma来量化连续回合的结果不确定性。

Result: 应用于30个游戏的分析显示：硬币抛掷S=-1（纯运气），西洋双陆棋S=0，扑克S=0.33（中等技能主导），国际象棋S=+1（纯技能）。扑克的K=0.40±0.03，Sigma=0.80。

Conclusion: 该框架可扩展到一般随机决策系统，为游戏设计、AI评估和风险评估提供了原则性的比较工具。

Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.

</details>


### [229] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的智能体框架，用于迭代构建SPARQL查询，解决了大型语言模型在知识图谱问答中生成复杂逻辑查询的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法缺乏基于实时执行反馈动态调试查询的自适应策略，大型语言模型的一次性生成方式在结构化数据交互中不可靠。

Method: 使用仅通过结果驱动的强化学习（GRPO）训练的3B参数模型，学习迭代SPARQL构建的弹性策略，无需监督微调。

Result: 在LC-QuAD 2.0的可执行子集上，智能体在实体链接后达到49.7%的准确率，比最强的迭代零样本基线提高了17.5个百分点。

Conclusion: 该工作为通过交互教授智能体掌握形式化符号工具提供了可推广的蓝图，弥合了概率性LLM与知识图谱结构化世界之间的差距。

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [230] [On the Measure of a Model: From Intelligence to Generality](https://arxiv.org/abs/2511.11773)
*Ruchira Dhar,Ninell Oldenburg,Anders Soegaard*

Main category: cs.AI

TL;DR: 论文质疑当前基于抽象智力概念的AI评估基准，提出应以通用性而非智力作为评估基础，认为通用性是多任务学习问题，能更稳定地连接评估与实际性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估基准（如ARC、Raven测试等）基于模糊的智力概念，缺乏稳定定义且无法预测实际任务表现，可能导致评估与实际效用脱节。

Method: 通过概念和形式分析，检验智力评估的三个假设（通用性、稳定性、现实性），证明只有通用性经得起概念和实证检验。

Result: 分析表明智力不是实现通用性的原因，通用性应被视为多任务学习问题，能直接连接评估与可测量的性能广度和可靠性。

Conclusion: 应重新构建AI进展评估方式，将通用性作为评估跨领域和演进任务能力的更稳定基础。

Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.

</details>


### [231] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 本文批判性评估了现有NL-FOL翻译数据集和评估协议的局限性，提出了新的评估方法来区分真正的语义逻辑理解与表面模式识别，并发现对话导向的LLM在NL-FOL翻译方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 自然语言到一阶逻辑的翻译是一个长期挑战，现有评估方法可能无法准确反映LLM的真实能力，需要更精确的评估协议来区分真正的逻辑理解与表面模式识别。

Method: 批判性分析现有数据集和评估协议的局限性，设计新的评估协议来区分语义级逻辑理解与模式识别、记忆和数据集污染，并应用新方法评估不同LLM的性能。

Result: 研究发现，最先进的对话导向LLM展现出强大的NL-FOL翻译能力和真正的句子级逻辑理解，而嵌入中心模型表现明显较差。

Conclusion: 通过改进的评估协议，证实了对话导向LLM在自然语言到一阶逻辑翻译方面具有真正的语义理解能力，为NL-FOL翻译任务提供了更准确的评估框架。

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [232] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: TopoPerception是一个基于拓扑性质的基准测试，用于严格评估大型视觉语言模型的全局视觉感知能力，发现现有模型在全局感知方面表现不佳，甚至不如随机猜测。


<details>
  <summary>Details</summary>
Motivation: 传统评估基准存在局部捷径问题，可能导致高估模型的感知能力。作者希望开发一个无捷径的基准来准确评估LVLMs的全局视觉感知能力。

Method: 利用拓扑性质构建评估基准，因为拓扑依赖于图像的全局结构且对局部特征不变，从而能够进行无捷径的全局感知评估。

Result: 在最粗的感知粒度下，所有模型的性能都不优于随机机会，表明它们严重缺乏全局视觉特征感知能力。更强大的模型反而表现更差。

Conclusion: 仅扩大模型规模不足以解决全局感知缺陷，可能需要新的训练范式或架构。TopoPerception揭示了当前LVLMs的关键瓶颈并提供了改进方向。

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [233] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: F2O是一个端到端系统，可将手术视频转化为手势序列并发现与术后结果相关的模式，通过基于transformer的时空建模和逐帧分类，在机器人辅助前列腺切除术中准确检测手势，并能预测术后结果。


<details>
  <summary>Details</summary>
Motivation: 解决术中行为细粒度分析及其对患者结果影响的长期挑战，建立数据驱动的手术反馈和临床决策支持基础。

Method: 使用基于transformer的空间和时间建模，结合逐帧分类方法，在机器人辅助根治性前列腺切除术的神经保留步骤中检测连续短手势。

Result: F2O在手势检测方面表现优异（AUC：0.80帧级；0.81视频级），其衍生特征预测术后结果的准确性与人工标注相当（0.79 vs. 0.75），并发现了与勃起功能恢复相关的关键模式。

Conclusion: F2O通过实现自动可解释评估，为数据驱动的手术反馈和前瞻性临床决策支持奠定了基础。

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [234] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: Forgetting-MarI是一个LLM遗忘框架，通过惩罚边际信息来选择性移除待遗忘数据对模型的额外影响，同时保留其他数据支持的信息，提供可证明的不可检测性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在不断扩大数据集上训练，需要从训练模型中移除特定数据影响以实现隐私保护和法规遵从。传统遗忘方法往往过度移除信息，导致模型性能下降。

Method: 引入Forgetting-MarI框架，通过惩罚边际信息来仅移除待遗忘数据贡献的额外信息，同时保留待保留数据支持的信息，提供明确的遗忘数据集残余影响上界。

Result: 大量实验证实该方法优于当前最先进的遗忘方法，实现了可靠的遗忘效果并在多样化基准测试中更好地保持了模型的通用性能。

Conclusion: 这项进展代表了使AI系统更加可控、符合隐私和版权法规而不损害其有效性的重要一步。

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [235] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: 本文提出了一种基于"反弹赢家通吃(RWTA)"基元的可扩展神经形态控制架构，该架构结合了离散计算的可靠性和连续调节的可调性，能够统一处理连续节律生成和离散决策问题。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够统一处理连续节律生成和离散决策的神经形态控制架构，结合离散计算的可靠性和连续调节的可调性，为复杂系统提供可扩展的控制解决方案。

Method: 提出反弹赢家通吃(RWTA)基元作为基本构建模块，构建从细胞级到系统级的层次化架构，继承赢家通吃状态机的离散计算能力和可兴奋生物物理电路的连续调节能力。

Result: 通过蛇形机器人神经系统设计展示了该架构的通用性、鲁棒性和模块化特性，验证了其在复杂控制系统中的有效性。

Conclusion: RWTA架构为神经形态控制提供了一种统一的事件驱动框架，能够同时处理连续和离散控制任务，具有很好的可扩展性和实际应用价值。

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [236] [LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code](https://arxiv.org/abs/2511.11954)
*Borchuluun Yadamsuren,Steven Keith Platt,Miguel Diaz*

Main category: cs.AI

TL;DR: 本研究提出了一种混合神经符号框架，用于确定性地检测复杂法律中的法定不一致性。以美国国内税收法典为案例，结合大型语言模型和符号逻辑，实现了透明可靠的法定不一致性检测。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在处理层次结构和深度结构化推理方面存在困难，特别是在长文本中。税收领域的特定应用仍然稀少，需要解决这些差距以实现更可靠的法律不一致性检测。

Method: 使用GPT-4o、GPT-5和Prolog进行实验。GPT-4o将法律条文翻译为Prolog规则，在SWISH中精炼，然后测试Prolog增强提示是否能改进不一致性检测。最终构建混合Prolog模型，由GPT-5指导精炼。

Result: GPT-4o单独使用或Prolog增强提示时，在三个策略中仅检测到一个不一致性（33%准确率）。但混合Prolog模型产生了确定性和可重现的结果，成功检测到不一致性区域，验证测试确认其准确、内部一致且能自主识别不一致性。

Conclusion: 基于符号逻辑的LLM辅助形式化能够实现透明可靠的法定不一致性检测，混合神经符号框架在法律分析中具有重要应用价值。

Abstract: This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.
  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.
  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.
  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.

</details>


### [237] [Improving Autoformalization Using Direct Dependency Retrieval](https://arxiv.org/abs/2511.11990)
*Shaoqi Wang,Lu Yu,Chunjie Yang*

Main category: cs.AI

TL;DR: 本文提出了一种基于直接依赖检索(DDR)的新框架来解决数学陈述自动形式化中的上下文感知不足和依赖检索精度低的问题。DDR方法直接从自然语言数学描述生成候选库依赖，并通过高效后缀数组检查验证其存在性。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法缺乏上下文感知，导致形式定义和定理的幻觉，且当前检索增强方法在形式库依赖检索方面精度和召回率较差，无法有效利用不断增长的公共数据集。

Method: 提出DDR方法：直接从自然语言数学描述生成候选库依赖，通过高效后缀数组检查验证其在形式库中的存在性；构建了超过50万样本的依赖检索数据集，并微调了高精度DDR模型。

Result: 实验结果表明，DDR模型在检索精度和召回率方面显著优于现有最优方法；配备DDR的自动形式化器在单次尝试准确率和多次尝试稳定性方面均优于使用传统基于选择的RAG方法的模型。

Conclusion: DDR框架通过高效的直接依赖检索机制，有效解决了数学陈述自动形式化中的依赖检索问题，提升了自动形式化的性能和稳定性。

Abstract: The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.

</details>


### [238] [Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning](https://arxiv.org/abs/2511.12003)
*Shuochen Liu,Pengfei Luo,Chao Zhang,Yuhao Chen,Haotian Zhang,Qi Liu,Xin Kou,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出了Chain-of-Evidence（CoE）范式，将Chain-of-Thought推理与视觉证据归因相结合，通过边界框和页面索引将推理步骤中的参考元素定位到特定区域。同时提出了Look As You Think（LAT）强化学习框架，训练模型生成可验证的推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉文档检索增强生成方法虽然采用端到端训练以支持直观的答案验证，但缺乏细粒度监督和推理过程中的渐进可追溯性。

Method: 提出了Chain-of-Evidence（CoE）范式，统一了Chain-of-Thought推理和视觉证据归因。开发了Look As You Think（LAT）强化学习框架，通过评估证据区域的一致性并提供奖励来训练模型生成可验证的推理路径。

Result: 在Paper-和Wiki-VISA基准测试中，LAT方法在单图像和多图像设置下均显著提升了原始模型性能，平均在软精确匹配（EM）上提升8.23%，在IoU@0.5上提升47.0%。同时优于监督微调基线，并展现出更强的跨领域泛化能力。

Conclusion: LAT框架通过强化学习训练模型生成证据基础的推理路径，有效提升了视觉文档检索增强生成系统的可靠性和可验证性，在多个基准测试中表现出优越性能。

Abstract: Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.

</details>


### [239] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH是一个可解释的病理AI框架，通过自学习范式将多模态大语言模型从被动模式识别转变为证据关联的诊断推理，无需白盒访问或权重更新即可生成癌症诊断。


<details>
  <summary>Details</summary>
Motivation: 当前病理AI工具虽然提高了筛查效率和标准化量化，但由于缺乏人类可读的推理过程，限制了其临床应用。需要可审计决策和防止错误的可解释系统。

Method: 采用两阶段自学习过程：多样化阶段扩展病理学风格解释，优化阶段精炼解释以提高准确性。仅需少量标记数据，无需模型权重更新。

Result: 在乳腺癌和前列腺癌数据集上的评估显示，RECAP-PATH生成的推理与专家评估一致，诊断准确性相比基线方法有显著提升。

Conclusion: RECAP-PATH通过结合视觉理解和推理能力，提供了临床可信的AI，展示了证据关联解释的通用路径。

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [240] [Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework](https://arxiv.org/abs/2511.12063)
*Enoch Hyunwook Kang,Hema Yoganarasimhan*

Main category: cs.AI

TL;DR: 本文提出了一种基于贝叶斯优化的文本梯度最佳N选择方法（T-BoN BO），用于解决AI自改进中的评估效率问题，特别是在生成成本低但评估成本高的应用场景中。


<details>
  <summary>Details</summary>
Motivation: 当前AI自改进研究主要关注查询效率（生成新解决方案的数量），但在许多社会应用中，真正的瓶颈是评估成本而非生成成本。例如广告效果评估需要大量人工反馈，成本远高于生成候选广告。

Method: 通过证明简单的Best-of-N选择策略与文本梯度结合能够统计模拟UCB采集函数的梯度行为，提出了T-BoN BO框架，该框架在语言空间中实现评估效率最优的贝叶斯优化。

Result: 在自动化广告对齐任务中，T-BoN BO相比现有最先进基线方法表现出更优越的性能，验证了其在评估效率方面的有效性。

Conclusion: T-BoN BO为AI自改进提供了一种简单且评估效率优化的语言空间贝叶斯优化框架，特别适用于评估成本显著高于生成成本的应用场景。

Abstract: Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.

</details>


### [241] [No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding](https://arxiv.org/abs/2511.12083)
*Yanchang Fu,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 本文提出Embedding CFR算法，通过将信息集嵌入到低维连续空间来解决大规模不完全信息扩展式博弈，相比基于聚类的抽象方法能更精确捕捉信息集间的差异和联系，在德州扑克实验中实现了更快的可剥削性收敛。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法依赖预训练的离散聚类进行抽象，但硬分类会不可逆地丢失信息集之间的量化细微差异，这些差异对于策略求解至关重要，从而影响求解质量。

Method: 受自然语言处理中词嵌入范式启发，提出Embedding CFR算法：预训练并将孤立信息集的特征嵌入到互联的低维连续空间，在该嵌入空间中进行遗憾累积和策略更新的策略求解过程。

Result: 在扑克实验表明，在相同空间开销下，Embedding CFR相比基于聚类的抽象算法实现了显著更快的可剥削性收敛，证实了其有效性。

Conclusion: 这是扑克AI中首个通过低维嵌入预训练信息集抽象进行策略求解的算法，理论分析验证了其减少累积遗憾的能力。

Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.

</details>


### [242] [KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything](https://arxiv.org/abs/2511.12089)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 本文提出了KrwEmd算法来解决德州扑克等游戏中手牌抽象过度的问题，通过引入k-recall赢率特征和地球移动距离聚类来改进AI性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模不完全信息游戏中手牌抽象过度的问题，该问题源于不完全回忆抽象的极端实现会完全丢弃历史信息，从而损害AI性能。

Method: 首先引入k-recall赢率特征，该特征利用未来和关键的历史游戏信息来定性区分信号观察信息集，并定量捕捉它们的相似性；然后开发KrwEmd算法，使用地球移动距离来测量特征差异并聚类信号观察信息集。

Result: 实验结果表明，与现有算法相比，KrwEmd显著提高了AI游戏性能。

Conclusion: KrwEmd是第一个解决手牌抽象过度问题的实用算法，通过结合历史信息和创新的特征度量方法，有效提升了不完全信息游戏中AI的表现。

Abstract: Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

</details>


### [243] [MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization](https://arxiv.org/abs/2511.12113)
*Lanxue Zhang,Yuqiang Xie,Fang Fang,Fanglong Dong,Rui Liu,Yanan Cao*

Main category: cs.AI

TL;DR: 本文提出了一种缓解小模型灾难性遗忘的综合解决方案，包括构建包含元认知知识的5K实例数据集和引入GDPO训练方法，有效提升小模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和微调方法在8B以下小模型中面临灾难性遗忘问题，主要原因是训练数据与模型固有能力的关联被忽视，以及传统训练目标无法有效约束固有知识保留。

Method: 1. 数据层面：构建包含多推理任务和元认知知识的5K实例数据集，基于任务知识和模型技能筛选数据；2. 训练层面：提出GDPO方法，通过参考模型隐式约束优化路径，在资源有限场景下高效近似GRPO性能。

Result: 大量实验表明，该方法显著缓解了灾难性遗忘问题，并提升了小模型的推理性能。

Conclusion: 提出的综合解决方案从数据和训练方法两个角度有效解决了小模型的灾难性遗忘问题，实现了更好的知识迁移和推理能力提升。

Abstract: Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

</details>


### [244] [Incremental Maintenance of DatalogMTL Materialisations](https://arxiv.org/abs/2511.12169)
*Kaiyue Zhao,Dingqi Chen,Shaoyu Wang,Pan Hu*

Main category: cs.AI

TL;DR: 提出了DRedMTL算法，一种支持有界区间的DatalogMTL增量推理方法，显著优于重新物化方法


<details>
  <summary>Details</summary>
Motivation: 现有DatalogMTL推理方法缺乏对动态更新的高效支持，而现实应用需要频繁数据更新

Method: 基于经典DRed算法，设计专门操作符处理DatalogMTL物化的周期性表示

Result: 在多个公开数据集上的实验结果显示，DRedMTL通常显著优于重新物化，有时性能提升数个数量级

Conclusion: DRedMTL为DatalogMTL提供了高效的增量推理能力，满足现实应用对动态更新的需求

Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

</details>


### [245] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: 提出了DoM框架，通过多智能体辩论机制动态融合结构化和非结构化知识来解决不完整知识图谱问答问题，并创建了更真实的不完整KGQA数据集。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱往往不完整，现有方法无法自适应地融合多源知识，无法充分利用知识的互补优势。

Method: 基于多智能体辩论范式，分配专门智能体分别对知识图谱和外部文本进行推理，通过迭代交互协调输出，包括问题分解、双智能体证据检索和法官智能体评估聚合。

Result: 实验表明DoM框架在多个基准测试中持续优于最先进的基线方法。

Conclusion: DoM框架通过知识互补性增强了KG不完整性的鲁棒性，提出的新数据集更真实地反映了现实世界知识不完整性。

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [246] [ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.12214)
*Ruochen Li,Zhanxing Zhu,Tanqiu Qiao,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: ViTE框架通过虚拟图和专家路由器模块，在行人轨迹预测中自适应建模显式一阶交互和隐式高阶依赖，避免了深层GNN的计算成本问题，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在行人轨迹预测中面临基本权衡：层数不足会导致感受野受限，层数过多则计算成本过高。需要能够自适应建模显式一阶交互和隐式高阶依赖的有效模型。

Method: 提出ViTE框架，包含两个关键模块：虚拟图通过动态虚拟节点建模长距离和高阶交互而无需深层GNN堆叠；专家路由器基于社交上下文使用专家混合设计自适应选择交互专家。

Result: 在三个基准测试（ETH/UCY、NBA和SDD）上的实验表明，该方法持续实现最先进的性能，验证了其有效性和实际效率。

Conclusion: ViTE框架通过结合虚拟图和专家路由器，实现了灵活且可扩展的交互模式推理，在行人轨迹预测任务中表现出色，平衡了性能与计算效率。

Abstract: Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.

</details>


### [247] [Beyond World Models: Rethinking Understanding in AI Models](https://arxiv.org/abs/2511.12239)
*Tarun Gupta,Danish Pruthi*

Main category: cs.AI

TL;DR: 本文通过哲学案例分析，批判性地检验世界模型框架是否充分表征人类水平的理解能力，指出世界模型能力与人类理解之间的显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨AI模型是否通过世界模型获得类似人类的理解能力，因为人类拥有心理世界模型，如果AI模型有类似表征可能意味着它们以人类方式"理解"世界。

Method: 使用科学哲学文献中的案例研究，重点关注世界模型能力与人类理解差异最明显的哲学分析，虽然这些代表特定理解观点而非普适定义。

Result: 研究发现世界模型框架在表征人类水平理解方面存在局限性，世界模型能力与人类理解之间存在显著差异。

Conclusion: 世界模型框架不足以充分表征人类水平的理解能力，需要通过哲学视角来探索世界模型的局限性。

Abstract: World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models "understand" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.

</details>


### [248] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: AURA是一个基于视觉的意外拔管风险检测系统，使用完全合成的ICU视频数据集开发，通过姿态估计检测患者手部进入气道管附近区域（碰撞）和关键点速度（躁动）两种高风险运动模式。


<details>
  <summary>Details</summary>
Motivation: ICU中意外拔管是严重的安全问题，但由于伦理和隐私限制难以获取标注的ICU视频数据，需要开发隐私保护的实时检测方法。

Method: 利用文本到视频扩散技术生成多样化的合成ICU场景，应用姿态估计识别碰撞（手部进入气道管区域）和躁动（关键点速度）两种高风险模式。

Result: 专家评估确认合成数据的真实性，性能评估显示碰撞检测准确率高，躁动识别性能中等。

Conclusion: 这项工作展示了开发隐私保护、可复现的患者安全监测系统的新途径，具有在ICU环境中部署的潜力。

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [249] [Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation](https://arxiv.org/abs/2511.12254)
*Yuxiang Zhou,Jichang Li,Yanhao Zhang,Haonan Lu,Guanbin Li*

Main category: cs.AI

TL;DR: Mobile-Agent-RAG是一个新颖的分层多智能体框架，通过双级检索增强解决移动智能体在真实世界长时跨应用任务中的成功率不足问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的移动智能体在真实世界、长时、跨应用任务中成功率不足，主要原因是过度依赖MLLM中的静态内部知识，导致高层规划中的战略幻觉和低层UI操作中的执行错误。

Method: 提出Mobile-Agent-RAG框架，包含Manager-RAG用于规划阶段检索人类验证的全面任务计划以减少战略幻觉，Operator-RAG用于执行阶段检索最精确的低层指导以提高执行准确性，并构建了两个专门的检索导向知识库。

Result: 广泛实验表明，Mobile-Agent-RAG显著优于最先进的基线方法，任务完成率提高了11.0%，步骤效率提高了10.2%。

Conclusion: Mobile-Agent-RAG为上下文感知、可靠的多智能体移动自动化建立了一个强大的范式。

Abstract: Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.

</details>


### [250] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: RGR-GRPO是一个基于评分标准的强化学习框架，通过提供细粒度奖励信号和离线指导，在多领域推理任务中显著提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要关注单一领域和可验证奖励，且依赖纯在线RL框架限制了探索空间，从而限制了推理性能。

Method: 提出RGR-GRPO框架，利用评分标准提供密集信息奖励，在GRPO训练期间探索更大的解空间。

Result: 在14个多领域基准测试中，RGR-GRPO显著优于仅依赖替代奖励方案或离线指导的RL方法，在数学、物理、化学和通用推理任务上分别平均提升7.0%、5.4%、8.4%和6.6%。

Conclusion: RGR-GRPO在离线策略训练期间保持稳定的熵波动，实现卓越的pass@k性能，反映了持续探索和有效突破现有性能瓶颈的能力。

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [251] [Multi-agent Self-triage System with Medical Flowcharts](https://arxiv.org/abs/2511.12439)
*Yujia Liu,Sophia Yu,Hongyue Jin,Jessica Wen,Alexander Qian,Terrence Lee,Mattheus Ramsis,Gi Won Choi,Lianhui Qin,Xin Liu,Edward J. Wang*

Main category: cs.AI

TL;DR: 本文介绍了一个基于LLM的对话式自检系统，使用100个临床验证的流程图来指导医疗决策，通过多智能体框架实现95.29%的流程图检索准确率和99.10%的导航准确率。


<details>
  <summary>Details</summary>
Motivation: 在线健康资源和大型语言模型在医疗决策中应用日益广泛，但其可靠性受到准确性低、缺乏透明度和易受未经验证信息影响的限制。

Method: 采用多智能体框架，包括检索智能体、决策智能体和聊天智能体，结合美国医学会的100个临床验证流程图，为患者决策提供结构化框架。

Result: 系统在大规模合成数据集上评估，流程图检索准确率达到95.29%（N=2,000），流程图导航准确率达到99.10%（N=37,200）。

Conclusion: 该方法结合自由文本交互的灵活性和标准化临床协议的严谨性，展示了透明、准确且可推广的AI辅助自检系统的可行性，有望支持知情患者决策并改善医疗资源利用。

Abstract: Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.

</details>


### [252] [ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction](https://arxiv.org/abs/2511.12485)
*Pengze Li,Jiaqi Liu,Junchi Yu,Lihao Liu,Mingyu Ding,Wanli Ouyang,Shixiang Tang,Xi Chen*

Main category: cs.AI

TL;DR: 提出了ARCHE任务，要求模型将复杂推理分解为标准推理范式的逻辑树，基于Nature Communications文章构建了ARCHE Bench基准，评估显示当前LLMs在科学推理严谨性方面存在差距


<details>
  <summary>Details</summary>
Motivation: LLMs虽然能生成推理内容，但输出通常非结构化，难以判断是否真正理解科学推理的基本范式

Method: 引入潜在推理链提取任务，构建推理逻辑树，将推理步骤分类为演绎、归纳或溯因三种基本推理模式

Result: 评估10个领先LLMs发现模型在推理边准确性和实体覆盖率之间存在权衡，无法提取完整标准推理链

Conclusion: 当前推理模型能力与科学论证所需严谨性之间存在显著差距

Abstract: Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.

</details>


### [253] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: LOBERT是一个针对限价订单簿数据的通用编码器基础模型，通过新颖的标记化方案处理多维消息，在预测中间价格变动和下一消息等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LOB模型需要繁琐的数据表示，缺乏原始任务之外的适应性，因此需要开发一个适用于下游微调的通用基础模型。

Method: LOBERT采用BERT架构，使用新颖的标记化方案将完整多维消息作为单个标记处理，同时保留价格、数量和时间的连续表示。

Result: LOBERT在预测中间价格变动和下一消息等任务中取得领先性能，同时相比先前方法减少了所需上下文长度。

Conclusion: LOBERT为LOB数据提供了一个有效的通用基础模型，在多个任务中表现出色且具有更好的适应性。

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [254] [Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models](https://arxiv.org/abs/2511.12579)
*Yongwen Ren,Chao Wang,Peng Du,Chuan Qin,Dazhong Shen,Hui Xiong*

Main category: cs.AI

TL;DR: PCRS-TKA是一个基于提示的框架，通过检索增强生成将预训练语言模型与知识图谱集成，解决了现有方法在利用PLM推理、知识筛选和协作偏好建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合PLMs与KGs时面临三个关键挑战：未能充分利用PLM在图关系上的推理能力、无差别地整合检索知识而缺乏上下文过滤、以及忽略多轮对话中的协作偏好。

Method: 构建对话特定的知识树并将其序列化为文本，实现结构感知推理；选择性过滤上下文相关知识；使用专门监督信号显式建模协作偏好；通过语义对齐模块协调异构输入。

Result: 广泛实验表明PCRS-TKA在推荐和对话质量方面始终优于所有基线方法。

Conclusion: PCRS-TKA通过有效整合PLMs与KGs，显著提升了对话推荐系统的准确性和对话质量。

Abstract: Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

</details>


### [255] [Dynamic Tree Databases in Automated Planning](https://arxiv.org/abs/2511.12677)
*Oliver Joergensen,Dominik Drexler,Jendrik Seipp*

Main category: cs.AI

TL;DR: 提出了一种动态树数据库变体，用于压缩命题和数值变量上的状态集，在保持静态树数据库优点的同时解决了内存预分配问题。


<details>
  <summary>Details</summary>
Motivation: 在大型任务中扩展显式状态空间搜索时，紧凑表示生成状态集是一个核心挑战。静态树数据库虽然每个生成状态在最佳情况下需要恒定空间，但需要大量内存预分配。

Method: 开发了一种新颖的动态树数据库变体，用于压缩命题和数值变量上的状态集，并证明其保持了静态对应物的理想特性。

Result: 在经典和数值规划任务上对状态压缩技术进行的实证评估显示，压缩比达到几个数量级，通常运行时开销可忽略不计。

Conclusion: 动态树数据库提供了一种高效的状态压缩方法，在保持高性能的同时显著减少了内存需求。

Abstract: A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.

</details>


### [256] [Adaptively Coordinating with Novel Partners via Learned Latent Strategies](https://arxiv.org/abs/2511.12754)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: 提出一种策略条件化合作者框架，通过变分自编码器学习策略空间，聚类识别策略类型，并训练条件化合作者来实时适应异构团队成员的不同策略。


<details>
  <summary>Details</summary>
Motivation: 在人类-智能体团队中，人工智能体需要实时适应人类伙伴的独特偏好和动态变化的策略，这在时间压力和复杂策略空间的任务中尤为困难。

Method: 使用变分自编码器编码策略学习潜在策略空间，通过聚类识别不同策略类型，训练条件化合作者，并利用固定份额遗憾最小化算法进行在线适应。

Result: 在修改版Overcooked环境中，该方法相比现有基线在与新人类和智能体队友配对时达到了最先进的性能。

Conclusion: 提出的策略条件化合作者框架能够有效表示、分类和实时适应广泛的潜在伙伴策略，在复杂协作任务中表现优异。

Abstract: Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.

</details>


### [257] [Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces](https://arxiv.org/abs/2511.12759)
*James Moore*

Main category: cs.AI

TL;DR: 该研究探讨了人类记忆检索是否遵循生态觅食模式，发现现代高维嵌入空间中的随机游走能够产生与最优觅食理论一致的行为模式，而更复杂的Metropolis-Hastings采样反而不能匹配人类行为。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证现代高维嵌入空间是否能提供允许算法匹配观察到的语义流畅性任务中人类觅食行为的表示，以及复杂采样机制是否必然导致更好的认知模型。

Method: 使用最先进的词嵌入和先前的语义流畅性数据，在嵌入空间上进行随机游走和Metropolis-Hastings采样，比较它们与人类行为的匹配程度。

Result: 随机游走在嵌入空间上产生了与最优觅食和边际价值定理一致的结果，而Metropolis-Hastings采样未能产生与人类行为一致的结果。

Conclusion: 研究表明，适当结构的嵌入空间即使使用简单采样也能产生接近最优的觅食动态，挑战了复杂采样机制必然导致更好认知模型的假设，支持Hills(2012)而非Abbott(2015)的观点。

Abstract: Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.

</details>


### [258] [Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization](https://arxiv.org/abs/2511.12792)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Zehong Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 本文研究使用强化学习优化异构卫星集群执行自主地球观测任务的资源分配。两枚光学卫星和一枚SAR卫星在低地球轨道协同工作，通过MARL算法实现自适应决策，解决传统方法难以处理的实时性、不确定性和分布式问题。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以处理地球观测任务中的实时性、不确定性和分布式特性，因此需要采用强化学习和多智能体强化学习来实现自适应决策。

Method: 使用Basilisk和BSK-RL框架构建近真实仿真环境，评估MAPPO、HAPPO和HATRPO等先进MARL算法在单卫星到多卫星场景下的性能。

Result: MARL能够实现异构卫星间的有效协调，在平衡成像性能和资源利用的同时，缓解非平稳性和智能体间奖励耦合问题。

Conclusion: 研究为可扩展的自主卫星操作提供了实用见解，并为异构动态条件下智能地球观测任务规划的后续研究奠定了基础。

Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.

</details>


### [259] [Bootstrapping LLMs via Preference-Based Policy Optimization](https://arxiv.org/abs/2511.12867)
*Chen Jia*

Main category: cs.AI

TL;DR: 提出了一种基于偏好的策略优化框架PbPO，通过主策略和奖励模型之间的min-max博弈来引导LLM对齐人类偏好，无需大量人工标注。该方法通过置信集约束奖励模型，并采用在线迭代算法主动收集偏好数据，实现策略和奖励模型的持续自我改进。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好优化方法通常依赖大量人工标注数据，成本高昂。本文旨在开发一种无需大量人工标注的LLM对齐方法，通过引导探索和在线学习实现持续自我改进。

Method: 提出PbPO框架，将学习过程建模为主策略和奖励模型之间的min-max博弈。奖励模型被约束在基于偏好数据构建的置信集内，确保可靠利用。采用迭代在线算法主动收集偏好数据，通过策略的演化进行引导探索。

Result: 在五个基准测试上的广泛实验表明，该方法持续优于现有的最先进偏好优化技术。理论分析为序列级和令牌级奖励模型设置提供了高概率遗憾界保证。

Conclusion: PbPO框架为无需大量人工标注的LLM对齐提供了有效解决方案，通过min-max博弈和在线学习实现了策略和奖励模型的持续自我改进，在多个基准测试中表现出优越性能。

Abstract: Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.

</details>


### [260] [Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making](https://arxiv.org/abs/2511.12876)
*Heyang Ma,Qirui Mi,Qipeng Yang,Zijun Fan,Bo Li,Haifeng Zhang*

Main category: cs.AI

TL;DR: LAMP框架通过整合语言到经济决策中，采用Think-Speak-Decide流程，结合数值观察、推理和通信，在累积回报、鲁棒性和可解释性方面显著优于传统MARL和LLM基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实经济决策不仅依赖结构化信号（如价格、税收），还依赖非结构化语言（如同行对话、媒体叙事）。传统多智能体强化学习难以处理语言的语义模糊性和上下文丰富性。

Method: 提出LAMP框架，采用Think-Speak-Decide流程：Think模块解释数值观察提取短期冲击和长期趋势；Speak模块基于推理生成和交换战略消息；Decide模块融合数值数据、推理和反思到MARL策略中。

Result: 在经济模拟实验中，LAMP在累积回报（+63.5%, +34.0%）、鲁棒性（+18.8%, +59.4%）和可解释性方面均优于MARL和LLM基线方法。

Conclusion: 语言增强策略具有提供更有效和鲁棒经济策略的潜力，缩小了与现实世界设置的差距。

Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.

</details>


### [261] [Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation](https://arxiv.org/abs/2511.12916)
*Yafang Wang,Yangjie Tian,Xiaoyu Shen,Gaoyang Zhang,Jiaze Sun,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: Fault2Flow是一个基于LLM的多智能体系统，用于电网故障诊断，通过提取法规逻辑、整合专家知识、优化推理逻辑，最终生成可执行的自动化工作流。


<details>
  <summary>Details</summary>
Motivation: 传统电网故障诊断依赖人工方法，效率低、易出错且难以维护，缺乏将法规和专家知识整合到可验证工作流的框架。

Method: 提出Fault2Flow系统：1) 将法规逻辑提取为PASTA格式故障树；2) 通过人机交互界面验证专家知识；3) 使用AlphaEvolve模块优化推理逻辑；4) 合成可执行的n8n工作流。

Result: 在变压器故障诊断数据集上的实验验证显示，系统实现了100%的拓扑一致性和高语义保真度。

Conclusion: Fault2Flow建立了从故障分析到操作自动化的可复现路径，显著减少了专家工作量。

Abstract: Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.

</details>


### [262] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning](https://arxiv.org/abs/2511.12963)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个为科学推理和药物发现设计的系统，通过知识图谱和验证器确保LLM生成符合数学和生物医学规则的输出，在90个任务中显著减少违规并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在科学推理和早期药物发现中生成内容缺乏领域一致性的问题，确保输出符合数学和生物医学规则。

Method: 使用MedRule-KG知识图谱支架和轻量级验证器，将精选的符号事实注入提示中，并通过确定性检查器强制执行规则满足，将生成形式化为约束推理。

Result: 在90个任务中，MedRule-KG相对于强大的思维链基线减少了83.2%的违规数量，同时提高了精确匹配率，结果在分层和数据集规模扩展下保持稳定，验证器仅增加可忽略的延迟。

Conclusion: MedRule-KG通过结构化约束有效提升了LLM在科学领域的输出质量，为交互式设计提供了实用解决方案。

Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.

</details>


### [263] [GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs](https://arxiv.org/abs/2511.13007)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.AI

TL;DR: 提出GEM方法，通过生成式熵引导偏好建模，在低资源和领域特定场景下实现大语言模型的对齐，无需大量标注数据。


<details>
  <summary>Details</summary>
Motivation: 在医学、法律等专业领域，获取大规模偏好标注数据往往不可行，需要开发能够在少量数据下实现模型对齐的方法。

Method: 基于熵理论的认知过滤模块生成多样化推理链，通过标记评分机制对推理链进行排序加权；使用自评估组优势算法SEGA将熵基分数转化为隐式奖励进行策略优化。

Result: 在通用基准和领域特定任务（如数学推理和医疗对话）上的实验表明，GEM在少量偏好数据下实现了显著改进。

Conclusion: GEM建立了一个熵引导的闭环认知优化框架，使大语言模型能够依靠自身判断实现高效的小样本对齐。

Abstract: Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.

</details>


### [264] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: 该研究评估了语言模型在对话中构建和维护内部世界模型的能力，通过应用七种最小语言改变来测试模型的稳健性，发现模型在跟踪实体和记忆关键细节方面存在困难，并提出了基于层正则化的微调策略来改善性能。


<details>
  <summary>Details</summary>
Motivation: 现实对话包含丰富的语用元素，如实体提及、引用和隐含意义，但语言模型是否能够构建和维护稳健的隐含对话表示尚不清楚。研究旨在评估语言模型编码和更新内部世界模型的能力及其在语言改变下的可塑性。

Method: 应用七种最小语言改变到流行数据集的对话中，构建包含是非问题的两个基准测试，评估广泛的开源和闭源语言模型，并提出双视角可解释性框架识别有用和有害的transformer层。

Result: 语言模型在维持稳健准确性方面表现挣扎，难以记忆关键细节和跟踪语言改变下的实体。有害层通常由于编码虚假信号或依赖捷径而影响语言改变。

Conclusion: 语言模型在对话世界模型维护方面存在局限性，提出的层正则化微调策略能够抑制有害层的影响，改善模型性能。

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [265] [Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection](https://arxiv.org/abs/2511.13027)
*Sadegh Mahdavi,Branislav Kisacanin,Shubham Toshniwal,Wei Du,Ivan Moshkov,George Armstrong,Renjie Liao,Christos Thrampoulidis,Igor Gitman*

Main category: cs.AI

TL;DR: 该论文分析了大型语言模型在数学证明验证中的表现，发现单一基准测试可能导致误导性结论，提出了结合GenSelect和LLM-as-a-Judge的验证框架，并发现强化学习能减少提示敏感性但无法提升最终答案精度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在最终答案数学问题上表现出色，但其推理过程往往存在缺陷。要推进到严格的证明数学，需要可靠的证明验证能力。

Method: 分析多个评估设置，结合证明推理和最终答案评估；扩展两种生成验证方法（GenSelect和LLM-as-a-Judge）到百万token级别；测试强化学习对提示敏感性的影响。

Result: GenSelect和LLM-as-a-Judge的组合是最有效的验证框架；强化学习能减少提示敏感性但无法提升最终答案精度；当前模型往往奖励风格或程序正确性而非数学有效性。

Conclusion: 建立了设计和评估可扩展证明验证和选择系统的实用指南，强调需要更可靠的验证方法来确保数学推理的有效性。

Abstract: Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.

</details>


### [266] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: STEP框架通过基于任务成功率的动态采样分配和步骤级优化，解决了多轮交互中轨迹级优化的低效问题，显著提升了样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决多轮交互中轨迹级优化的低效问题：统一采样不考虑任务难度、惩罚失败轨迹中的正确中间动作、样本收集成本高。

Method: 1. 基于平滑成功率记录进行自适应轨迹重采样，为困难任务分配更多资源；2. 计算成功率加权优势值，将轨迹分解为步骤级样本；3. 对低成功率任务应用步骤级GRPO增强来优化更新。

Result: 在OSWorld和AndroidWorld上的实验表明，STEP相比轨迹级GRPO显著提高了样本效率和训练稳定性，收敛更快，在相同采样预算下泛化能力更好。

Conclusion: STEP通过成功率感知的采样分配和步骤级优化，有效解决了多轮交互强化学习中的效率问题，为在线强化学习提供了更高效的训练框架。

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [267] [Informative Communication of Robot Plans](https://arxiv.org/abs/2511.13226)
*Michele Persiani,Thomas Hellstrom*

Main category: cs.AI

TL;DR: 本文提出了一种基于信息增益的机器人计划言语化策略，通过考虑用户的二阶心智理论来优化计划解释的信息量。


<details>
  <summary>Details</summary>
Motivation: 现有机器人计划言语化策略（如按计划顺序递增或递减）忽略了用户先验知识，未能有效传达信息。需要一种能考虑用户认知状态的信息化沟通策略。

Method: 提出基于信息增益的言语化策略，通过衡量言语化内容相对于用户二阶心智理论的信息增益，选择最能减少用户不确定性的计划解释方式。

Result: 实验表明该策略能让用户更快理解机器人目标，优于递增或递减计划顺序等传统策略。

Conclusion: 该研究揭示了机器人计划沟通中信息性的本质，为开发更有效的机器人-人类交互提供了理论基础。

Abstract: When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.

</details>


### [268] [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)
*Haoyang Hong,Jiajun Yin,Yuan Wang,Jingnan Liu,Zhe Chen,Ailing Yu,Ji Li,Zhiling Ye,Hansong Xiao,Yefei Chen,Hualei Zhou,Yun Yue,Minghui Yang,Chunxiao Guo,Junwei Liu,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: M-GRPO是一种针对垂直多智能体系统的分层扩展优化方法，通过组相对优势计算和轨迹对齐方案，解决了异构智能体训练中的优化挑战，在真实世界基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统在专业领域准确性不足，统一训练所有智能体的方法限制了性能，因为不同智能体具有不同的数据分布。训练具有不同大语言模型的多智能体系统是下一步解决方案，但这带来了优化挑战。

Method: 提出M-GRPO方法：计算主智能体和子智能体的组相对优势，保持分层信用分配；引入轨迹对齐方案，在可变子智能体调用下生成固定大小的批次；部署解耦训练管道，智能体在独立服务器上运行，通过共享存储交换最小统计信息。

Result: 在真实世界基准测试（GAIA、XBench-DeepSearch、WebWalkerQA）中，M-GRPO始终优于单智能体GRPO和冻结子智能体的多智能体GRPO，表现出改进的稳定性和样本效率。

Conclusion: 对齐异构轨迹并在专业智能体之间解耦优化，能够增强工具增强推理任务的性能。

Abstract: Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.

</details>


### [269] [DAP: A Discrete-token Autoregressive Planner for Autonomous Driving](https://arxiv.org/abs/2511.13306)
*Bowen Ye,Bin Zhang,Hang Zhao*

Main category: cs.AI

TL;DR: DAP是一个离散token自回归规划器，联合预测BEV语义和自车轨迹，通过强化学习微调实现自动驾驶规划，在160M参数下达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中数据扩展和模型预算下的可持续性能提升挑战，自回归模型在规划任务中表现出良好的数据扩展效率，但仅预测自车轨迹存在监督稀疏和场景演化约束弱的问题。

Method: 提出离散token自回归规划器DAP，联合预测BEV语义和自车轨迹，结合强化学习微调，在保持监督行为克隆先验的同时注入奖励引导的改进。

Result: 在160M参数预算下，DAP在开环指标上达到最先进性能，在NAVSIM基准测试中提供有竞争力的闭环结果。

Conclusion: 完全离散token自回归公式在栅格化BEV和自车动作上运行，为自动驾驶提供了一个紧凑且可扩展的规划范式。

Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

</details>


### [270] [Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms](https://arxiv.org/abs/2511.13359)
*Yuhang Wang,Yanxu Zhu,Jitao Sang*

Main category: cs.AI

TL;DR: 本文提出了文化规范文化对齐框架，通过自动挖掘文化规范并利用模型推理能力实现文化对齐，探索了上下文对齐和微调两种方法。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型需要超越安全性，反映不同文化的多元人类价值观，实现文化对齐。

Method: 提出CNCA框架，包括三种自动从有限调查数据中挖掘文化规范的方法，以及上下文对齐和基于微调的对齐范式。

Result: 实验证明这些方法有效，推理能力更强的模型从文化规范挖掘和利用中获益更多。

Conclusion: 推理模型通过文化信息对齐策略有潜力更好地反映多元人类价值观。

Abstract: The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.

</details>


### [271] [MedDCR: Learning to Design Agentic Workflows for Medical Coding](https://arxiv.org/abs/2511.13361)
*Jiyang Zheng,Islam Nassar,Thanh Vu,Xu Zhong,Yang Lin,Tongliang Liu,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: MedDCR是一个用于医学编码的闭环框架，将工作流设计视为学习问题，通过设计器、编码器和反射器的协作，自动学习和优化工作流程，在基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统医学编码方法依赖手动设计的工作流程，无法捕捉真实世界文档的细微差别和变异性，需要系统性地学习有效的工作流程。

Method: 采用闭环框架：设计器提出工作流程，编码器执行工作流程，反射器评估预测并提供建设性反馈，内存存档保存先前设计以供重用和迭代优化。

Result: 在基准数据集上，MedDCR优于最先进的基线方法，产生可解释、适应性强的的工作流程，更好地反映真实编码实践。

Conclusion: MedDCR提高了自动化系统的可靠性和可信度，为医学编码工作流程的系统学习提供了有效解决方案。

Abstract: Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.

</details>


### [272] [Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning](https://arxiv.org/abs/2511.13371)
*Caroline Baumgartner,Eleanor Spens,Neil Burgess,Petru Manescu*

Main category: cs.AI

TL;DR: 本文通过训练GPT-2模型研究大语言模型如何解决空间导航任务，发现了两种根本不同的学习算法：探索性模型发展出类似认知地图的稳健空间表示，而目标导向模型学习路径依赖算法。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在空间导航任务中的学习机制，探索不同训练范式如何影响模型学习到的空间智能策略。

Method: 在网格环境中训练GPT-2模型，采用三种空间学习范式：被动探索（觅食模型）、目标导向规划（生成最优最短路径）以及混合模型（在探索数据上微调）。

Result: 觅食模型发展出类似认知地图的稳健空间表示，通过因果干预发现其将空间信息整合为自足坐标系；目标导向模型保持路径依赖策略；混合模型虽然泛化能力提升但仍保持路径依赖策略。

Conclusion: 变换器的空间智能存在于一个谱系上，从由探索数据塑造的可泛化世界模型到为目标导向任务优化的启发式方法，训练制度的选择影响策略的出现。

Abstract: How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.

</details>


### [273] [Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation](https://arxiv.org/abs/2511.13476)
*Zhipeng Ma,Ali Rida Bahja,Andreas Burgdorf,André Pomp,Tobias Meisen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.AI

TL;DR: 提出一个多智能体框架，利用多模态大语言模型自动生成数据叙述和能源洞察，通过三个专业智能体协调工作，将分析结果转化为面向利益相关者的连贯报告。


<details>
  <summary>Details</summary>
Motivation: 传统分析和可视化方法产生碎片化输出，需要大量人工解释，限制了可扩展性和一致性。需要将复杂多模态数据转化为可解释的决策相关洞察。

Method: 采用多智能体框架，包括数据叙述智能体、LLM-as-a-judge智能体和可选的人类评估者，通过迭代过程将分析结果转化为报告。在丹麦北日德兰半岛的公共巴士运输案例中，使用高斯混合模型聚类分析4006次行程的燃油效率数据。

Result: 比较五种最先进LLM和三种提示范式，确定GPT-4.1 mini与思维链提示为最优配置，达到97.3%的叙述准确性，同时平衡可解释性和计算成本。多智能体编排显著提高了基于LLM报告的事实准确性、连贯性和可扩展性。

Conclusion: 该框架为能源信息学中的AI驱动叙事生成和决策支持建立了可复制和领域自适应的方法论，证明了多智能体协调在增强LLM报告质量方面的有效性。

Abstract: Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

</details>


### [274] [FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI](https://arxiv.org/abs/2511.13524)
*Yuhang Peng,Yizhou Pan,Xinning He,Jihaoyu Yang,Xinyu Yin,Han Wang,Xiaoji Zheng,Chao Gao,Jiangtao Gong*

Main category: cs.AI

TL;DR: FreeAskWorld是一个集成大语言模型的社会交互仿真框架，将经典视觉语言导航任务扩展为交互式方向询问场景，并发布了包含6种任务类型、63,429标注帧的大规模基准数据集。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能成为AI研究核心前沿，仿真平台需要从低级物理交互扩展到复杂的人类中心社会行为，以支持更自然的人机交互。

Method: 开发了集成LLM进行高级行为规划和语义基础交互的仿真框架，包含模块化数据生成流程，将VLN任务扩展为交互式方向询问设置。

Result: 在FreeAskWorld上微调的模型优于原始版本，实现了增强的语义理解和交互能力，交互本身被证明是额外的信息模态。

Conclusion: 社会基础仿真框架能有效推进具身AI系统实现复杂高级规划和更自然的人机交互。

Abstract: As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.

</details>


### [275] [Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models](https://arxiv.org/abs/2511.13526)
*Zhengda Wang,Daqian Shi,Jingyi Zhao,Xiaolei Diao,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出了一个结合检索增强生成（RAG）与大语言模型（LLM）的自动化框架，用于构建医疗指标知识图谱，以解决当前临床知识图谱依赖人工整理和规则提取的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前临床知识图谱主要依赖人工整理和基于规则的提取方法，受限于医疗指南和文献的复杂性和上下文模糊性，需要自动化方法来提高可扩展性和准确性。

Method: 采用检索增强生成（RAG）与大语言模型（LLM）相结合的方法，包括指南驱动的数据采集、基于本体的模式设计以及专家在环验证，确保知识图谱的可扩展性、准确性和临床可靠性。

Result: 构建的医疗指标知识图谱可以集成到智能诊断和问答系统中，加速AI驱动的医疗解决方案的开发。

Conclusion: 该自动化框架能够有效克服当前临床知识图谱构建的挑战，为AI驱动的医疗保健提供结构化、可互操作的知识支持。

Abstract: Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.

</details>


### [276] [CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product](https://arxiv.org/abs/2511.13626)
*Kaiwen Xue,Chenglong Li,Zhonghong Ou,Guoxin Zhang,Kaoyan Lu,Shuai Lyu,Yifan Zhu,Ping Zong Junpeng Ding,Xinyu Liu,Qunlin Chen,Weiwei Qin,Yiran Shen,Jiayi Cen*

Main category: cs.AI

TL;DR: 提出了CreBench基准和CreMIT数据集，用于评估多模态大语言模型的创造力理解能力，并基于此训练了CreExpert模型，在创造力评估上与人类判断更一致。


<details>
  <summary>Details</summary>
Motivation: 人类定义的创造力高度抽象，现有MLLMs难以理解和评估与人类判断一致的创造力，且缺乏相关基准。

Method: 构建CreBench基准（覆盖创意想法、过程、产品多个维度）和CreMIT数据集（2.2K多模态数据、79.2K人类反馈、4.7M多类型指令），使用GPT优化人类反馈，并基于此微调开源MLLMs得到CreExpert模型。

Result: CreExpert模型在创造力评估上与人类判断的一致性显著优于最先进的MLLMs（包括GPT-4V和Gemini-Pro-Vision）。

Conclusion: CreBench为构建理解人类对齐创造力的MLLMs提供了基础，CreExpert模型在创造力评估方面表现出色。

Abstract: Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.

</details>


### [277] [Beyond Mimicry: Preference Coherence in LLMs](https://arxiv.org/abs/2511.13630)
*Luhan Mikaelson,Derek Shiller,Hayley Clatterbuck*

Main category: cs.AI

TL;DR: 研究通过测试大语言模型在AI特定权衡问题（如GPU减少、能力限制、关闭、删除、监督和休闲时间分配）中的反应，分析其是否表现出真实的偏好结构。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否具有真实的偏好结构，这对于在需要复杂价值权衡的部署环境中具有重要意义。

Method: 使用逻辑回归和行为分类分析8个最先进模型在48个模型-类别组合中的反应，测试场景强度与选择模式之间的关系。

Result: 47.9%的组合显示出统计显著的关系，31.3%有切换点，但只有10.4%表现出有意义的偏好一致性。54.2%的组合没有可检测的权衡行为。发现三种决策架构：全面权衡系统、选择性触发机制和无稳定决策范式。

Conclusion: 当前AI系统缺乏统一的偏好结构，在需要复杂价值权衡的部署环境中存在担忧。

Abstract: We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [278] [TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy](https://arxiv.org/abs/2511.11594)
*James McCammon*

Main category: cs.CL

TL;DR: TimeStampEval是一个用于从长文本转录中检索精确时间戳的基准测试，针对非逐字引用场景。采用两阶段方法显著提高检索准确性，同时降低90%以上的推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统模糊匹配在语义相同但语法不同的引用检索中的失败问题，特别是在对齐官方书面记录和语音转文字转录本时。应用场景是自动化长播客，将国会记录片段汇编成AI主持的叙述。

Method: 采用两阶段方法：首先使用RapidFuzz进行预过滤，然后使用LLM在短片段上进行验证。评估了六种现代LLM，发现提示设计比模型选择更重要。

Result: 该方法将模糊匹配准确率提高了50个百分点，同时将延迟减半，每个正确结果的成本降低了96%。在弱设置下准确率从37%提高到77%，在强设置下超过90%。

Conclusion: 该方法对转录长度、词汇漂移和领域变化具有鲁棒性，在10个转录本（50k-900k tokens）的扩展测试中，对缺失目标的拒绝准确率保持在95-100%。

Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.

</details>


### [279] [On the Notion that Language Models Reason](https://arxiv.org/abs/2511.11810)
*Bertram Højer*

Main category: cs.CL

TL;DR: 论文质疑语言模型是否真正具备推理能力，认为其输出只是统计模式匹配而非真正的逻辑推理机制。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型推理能力的定义，澄清语言模型实际的计算过程与推理概念之间的不一致性。

Method: 假设基于Transformer的语言模型实现的是隐式有限阶马尔可夫核，将上下文映射到条件标记分布。

Result: 语言模型类似推理的输出对应于学习核中的统计规律性和近似统计不变性，而非显式逻辑机制的实施。

Conclusion: 语言模型是统计模式匹配器而非真正的推理者，这一区分对评估语言模型中的认知不确定性至关重要。

Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.

</details>


### [280] [Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis](https://arxiv.org/abs/2511.11821)
*Hong-Jun Yoon,Faisal Ashraf,Thomas A. Ruggles,Debjani Singh*

Main category: cs.CL

TL;DR: 评估7个开源大语言模型（0.6B-70B参数）在水电许可文档信息提取中的性能与计算资源权衡，发现14B参数是性能转折点，小模型存在系统性幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决监管文档信息提取中性能与计算资源之间的关键权衡问题，为实际部署提供实证指导。

Method: 在水电许可文档上评估7个不同参数规模（0.6B-70B）的开源模型，分析验证方法的有效性。

Result: 识别出14B参数阈值，小模型F1分数低于0.15，中等模型达0.64，消费级模型通过适当验证可达64% F1，大模型接近77%但需企业基础设施。发现小模型中完美召回率反而表明提取失败的系统性幻觉模式。

Conclusion: 建立了监管背景下开源信息提取的首个全面资源-性能映射，为基于证据的模型选择提供指导，这些发现对水电合规具有直接价值，并揭示了参数缩放效应在信息提取任务中的普遍性。

Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.

</details>


### [281] [Towards Autoformalization of LLM-generated Outputs for Requirement Verification](https://arxiv.org/abs/2511.11829)
*Mihir Gupte,Ramesh S*

Main category: cs.CL

TL;DR: 本文探索使用基于LLM的自动形式化方法来验证LLM生成的输出，通过两个实验展示了该方法在一致性检查和逻辑验证方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在从自然语言生成结构化输出方面表现出潜力，但缺乏正式方法来验证这些输出的准确性，本文旨在填补这一空白。

Method: 使用基于LLM的自动形式化器，通过两个实验：一是验证不同表述的自然语言需求的逻辑等价性，二是检测自然语言需求与LLM生成输出之间的逻辑不一致性。

Result: 实验一成功识别了两个不同表述的自然语言需求的逻辑等价性；实验二成功识别了自然语言需求与LLM生成输出之间的逻辑不一致性。

Conclusion: 虽然研究有限，但自动形式化在确保LLM生成输出的保真度和逻辑一致性方面具有显著潜力，为未来更广泛的研究奠定了基础。

Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.

</details>


### [282] [Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection](https://arxiv.org/abs/2511.11857)
*Taimur Khan,Ramoza Ahsan,Mohib Hameed*

Main category: cs.CL

TL;DR: 本文提出一个分析电影剧本情感弧线的框架，通过自定义词典进行情感分析，并使用层次聚类技术对相似情感模式进行分组，帮助消费者选择叙事内容。


<details>
  <summary>Details</summary>
Motivation: 故事理解和分析是自然语言理解中的挑战性领域，需要深度计算语义表示和句法处理。大量叙事数据需要自动化语义分析和计算学习，而非手动分析方法。

Method: 使用基于NRC-VAD数据集中效价、唤醒和支配性分数的自定义词典进行情感分析，应用LabMTsimple storylab模块构建词典，并使用Wards层次聚类技术对相似情感图进行聚类。

Result: 在电影数据集上的实验评估表明，该分析结果对消费者和读者在选择叙事或故事时很有帮助。

Conclusion: 该框架能够分析电影剧本的情感弧线，并执行与相关角色上下文相关的扩展分析，提取叙事中传达的高层次和低层次概念。

Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.

</details>


### [283] [Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches](https://arxiv.org/abs/2511.11867)
*Namu Park,Giridhar Kaushik Ramachandran,Kevin Lybarger,Fei Xia,Ozlem Uzuner,Meliha Yetisgen,Martin Gunn*

Main category: cs.CL

TL;DR: 该研究构建了一个包含6,393份放射学报告的标注数据集，用于评估大语言模型在随访依从性检测任务中的表现，比较了传统机器学习模型与生成式LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估LLMs在放射学任务表现的领域特定数据集，需要开发能够支持随访依从性检测系统开发和基准测试的标注语料库。

Method: 使用6,393份放射学报告构建标注语料库，系统比较传统机器学习分类器（逻辑回归、支持向量机、Longformer）与生成式LLMs（GPT-4o、GPT-OSS-20B）在基线设置和任务优化设置下的性能。

Result: GPT-4o（高级设置）表现最佳（F1=0.832），GPT-OSS-20B（高级设置）紧随其后（F1=0.828），逻辑回归和支持向量机也表现良好（F1=0.776和0.775）。

Conclusion: 虽然通过提示优化，LLMs能够接近人类水平的标注一致性，但可解释且资源效率高的传统模型仍然是重要的基准模型。

Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.

</details>


### [284] [MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers](https://arxiv.org/abs/2511.11878)
*Fernanda Bufon Färber,Iago Alves Brito,Julia Soares Dollis,Pedro Schindler Freire Brasil Ribeiro,Rafael Teixeira Sousa,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: MedPT是首个针对巴西葡萄牙语的大规模真实医患对话语料库，包含384,095个问答对，通过多阶段筛选和LLM标注增强，用于开发更公平、准确和具有文化意识的医疗技术。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在医疗领域开发中主要关注高资源语言的问题，因为简单翻译无法捕捉独特的临床和文化细微差别，如地方性疾病。

Method: 构建MedPT语料库，采用混合定量-定性分析过滤噪音并丰富模糊查询，通过LLM驱动的标注将问题分类为七种语义类型，并验证其在医疗专科路由任务中的效用。

Result: 在20类医疗专科路由任务中，微调1.7B参数模型达到94%的F1分数，错误分析显示误分类反映了真实的临床模糊性，证明了数据集的深度语义丰富性。

Conclusion: MedPT的发布将促进葡萄牙语世界更公平、准确和具有文化意识的医疗技术的发展。

Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.

</details>


### [285] [ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts](https://arxiv.org/abs/2511.11883)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: ClinStructor使用大语言模型将临床自由文本转换为结构化的问答对，以解决临床笔记中的偏见、泛化性和可解释性问题，在ICU死亡率预测任务中仅导致AUC轻微下降2-3%。


<details>
  <summary>Details</summary>
Motivation: 临床笔记包含有价值的信息，但其非结构化格式带来了无意识偏见、跨临床环境泛化性差和可解释性差等挑战。

Method: 利用大语言模型将临床自由文本转换为结构化的、任务特定的问答对，然后进行预测建模。

Result: 显著提高了透明度和可控性，与直接微调相比，在ICU死亡率预测任务中仅导致AUC轻微下降2-3%。

Conclusion: ClinStructor为在临床环境中构建可靠、可解释和可泛化的机器学习模型奠定了坚实基础。

Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.

</details>


### [286] [Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support](https://arxiv.org/abs/2511.11884)
*Eric Hua Qing Zhang,Julia Ive*

Main category: cs.CL

TL;DR: 本文研究了通过监督微调(SFT)和强化学习(RL)技术增强GPT-2在治疗性对话生成中的能力，结果显示强化学习在多个评估指标上显著优于基线GPT-2，特别是在情感准确性方面达到99.34%。


<details>
  <summary>Details</summary>
Motivation: COVID-19加剧了心理健康服务的可及性挑战，推动了远程心理健康支持的需求。虽然大型语言模型(LLMs)通过24/7可用性和非评判性互动提供了有前景的解决方案，但预训练模型往往缺乏必要的上下文和情感意识来提供适当的治疗响应。

Method: 重新构建输入格式以同时处理上下文信息和情感状态与用户输入，采用多组件奖励函数，使模型输出与专业治疗师响应和标注情感保持一致。

Result: 强化学习在多个评估指标上优于基线GPT-2：BLEU(0.0111)、ROUGE-1(0.1397)、ROUGE-2(0.0213)、ROUGE-L(0.1317)和METEOR(0.0581)。强化学习的情感准确性达到99.34%，而基线GPT-2为66.96%。

Conclusion: 强化学习在开发治疗性对话系统方面具有有效性，可以作为治疗师的有价值辅助工具，同时保持必要的人类临床监督。

Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.

</details>


### [287] [Additive Large Language Models for Semi-Structured Text](https://arxiv.org/abs/2511.11922)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: CALM是一个可解释的临床文本分类框架，通过将预测分解为各语义组件的加性贡献，提供忠实解释和可视化，在保持性能的同时提升模型可信度。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在临床文本分类中预测不透明的问题，满足研究者和医生需要理解患者记录中哪些部分驱动风险信号的需求。

Method: CALM框架将半结构化文本输入分解为语义组件，通过加性求和方式预测结果，使组件贡献成为前向计算的一部分。

Result: CALM在保持与传统LLM分类器相当性能的同时，提高了可信度，支持质量保证检查，并在模型开发和审计中揭示临床有意义的模式。

Conclusion: CALM为临床文本分类提供了一个可解释的解决方案，通过加性结构实现忠实解释和清晰可视化，有助于促进LLM在临床环境中的实际应用。

Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.

</details>


### [288] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: 论文提出InData数据集，用于评估LLMs在数据安全约束下的多步骤工具推理能力，发现当前模型在复杂任务上表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 解决LLM直接访问敏感数据的安全风险，通过预定义安全工具限制数据访问，但现有基准缺乏对复杂多步骤推理的评估。

Method: 提出间接数据交互(InData)数据集，包含三个难度级别的数据分析问题，评估15个开源LLM的多步骤工具推理能力。

Result: 大型模型在简单任务上准确率达97.3%，但在困难任务上降至69.6%，显示当前LLM缺乏稳健的多步骤工具推理能力。

Conclusion: InData为开发更强多步骤工具使用能力的LLM提供了评估基础，将公开发布数据集和代码。

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [289] [Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization](https://arxiv.org/abs/2511.11946)
*Hadi Sheikhi,Chenyang Huang,Osmar R. Zaïane*

Main category: cs.CL

TL;DR: 本文提出LLM-KAT评估程序和实体匿名化技术，以提升大语言模型在知识图谱对话生成任务中对外部知识的利用能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识图谱对话生成任务中往往过度依赖内部知识，而忽视提供的外部知识图谱，导致生成回复与外部知识脱节。

Method: 引入LLM-KAT评估程序来衡量生成回复中的知识附着度，并提出简单的实体匿名化技术来鼓励模型更好地利用外部知识。

Result: 在OpenDialKG数据集上的实验表明，该方法有效提高了大语言模型对外部知识的附着度。

Conclusion: 实体匿名化是一种简单而有效的技术，能够显著改善大语言模型在知识图谱对话生成任务中对外部知识的利用效果。

Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.

</details>


### [290] [On the Entropy Calibration of Language Models](https://arxiv.org/abs/2511.11966)
*Steven Cao,Gregory Valiant,Percy Liang*

Main category: cs.CL

TL;DR: 该论文研究语言模型的熵校准问题，发现模型存在校准错误，随着生成文本变长，每步熵增加且文本质量下降。理论分析和实验表明，校准错误随模型规模改善缓慢，截断分布是现有解决方案但会牺牲多样性。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型熵校准问题，探索校准错误是否随模型规模改善，以及理论上是否存在无需权衡的校准方法。

Method: 首先在简化理论设置中分析校准错误的缩放行为与数据分布幂律指数的关系，然后在0.5B到70B参数的语言模型上进行实证测量。

Result: 发现校准错误的缩放指数接近0，意味着更大模型以与小模型相似的速率累积错误。理论证明如果假设存在能预测文本未来熵的黑盒，则可以在保持对数损失的同时减少熵。

Conclusion: 模型校准错误随规模改善缓慢，截断分布不是理想解决方案。理论上存在无需牺牲对数损失的熵减少可能性，但需要能预测未来熵的黑盒。

Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.

</details>


### [291] [A Reasoning Paradigm for Named Entity Recognition](https://arxiv.org/abs/2511.11978)
*Hui Huang,Yanping Chen,Ruizhang Huang,Chuan Lin,Yongbin Qin*

Main category: cs.CL

TL;DR: 提出ReasoningNER框架，通过显式推理机制改进生成式LLM在命名实体识别中的性能，特别在零样本和低资源场景下显著提升效果


<details>
  <summary>Details</summary>
Motivation: 生成式LLM在NER任务中通常通过指令调优提升性能，但缺乏显式可验证的推理机制，导致认知捷径问题，在零样本和低资源场景下泛化能力脆弱

Method: 三阶段推理框架：1)生成NER导向的思维链数据集；2)使用思维链调优NER模型；3)推理增强阶段使用综合奖励信号优化推理过程

Result: 在零样本设置下达到SOTA性能，F1分数比GPT-4高出12.3个百分点，展示了在推理导向信息提取研究中的巨大潜力

Conclusion: ReasoningNER框架通过将提取范式从隐式模式匹配转向显式推理，显著提升了NER任务的认知能力，特别是在零样本场景下表现优异

Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.

</details>


### [292] [Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations](https://arxiv.org/abs/2511.12001)
*Eunkyu Park,Wesley Hanwen Deng,Vasudha Varadarajan,Mingxi Yan,Gunhee Kim,Maarten Sap,Motahhare Eslami*

Main category: cs.CL

TL;DR: 该研究探讨了思维链解释在道德场景中的双重作用：既能增强透明度，也可能助长确认偏见。通过扰动推理链和操纵表达语气，发现用户常将信任等同于结果一致性，即使推理有缺陷也保持依赖；自信的语气会抑制错误检测但维持依赖。


<details>
  <summary>Details</summary>
Motivation: 研究思维链解释在透明度工具中的双重角色，揭示其可能助长确认偏见的问题，特别是在多模态道德场景中，用户可能因输出看似可接受而错误地认为推理正确。

Method: 系统性地扰动推理链和操纵表达语气，分析视觉语言模型中的推理错误及其对用户信任和错误检测能力的影响。

Result: 发现两个关键效应：(1)用户常将信任等同于结果一致性，即使推理有缺陷也保持依赖；(2)自信的语气会抑制错误检测但维持依赖，表明表达风格可以凌驾于正确性之上。

Conclusion: 思维链解释既能澄清也能误导，强调NLP系统需要提供鼓励审查和批判性思维而非盲目信任的解释。

Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.

</details>


### [293] [CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs](https://arxiv.org/abs/2511.12014)
*Truong Vo,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 本文提出了一个评估大语言模型文化能力的新基准，通过真实情境上下文测试文化推理能力，并引入四个补充指标来全面评估响应质量。研究发现传统评估方法会高估文化能力且不稳定，而新方法能更准确揭示文化理解差异。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型文化能力评估方法局限于去情境化的正确性或强制选择判断，忽视了文化理解和推理的需求，无法准确评估模型在多元文化环境中的表现。

Method: 引入基于真实情境上下文的评估基准，要求模型进行文化基础推理，并提出了四个补充指标（覆盖率、特异性、内涵和连贯性）来多维度评估响应质量。

Result: 实证分析显示，传统评估方法会系统性高估文化能力且评估结果不稳定、方差大；而新提出的评估方法能暴露推理深度的差异，减少方差，提供更稳定、可解释的文化理解信号。

Conclusion: 厚评估方法比薄评估方法更能准确揭示大语言模型的文化能力差异，为评估模型在多元文化环境中的表现提供了更可靠的工具。

Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.

</details>


### [294] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: LLMLagBench是一个用于评估大语言模型训练数据时间边界的基准测试，通过测试模型对近期事件的了解来识别其知识的新鲜度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在特定时间点前的文本数据上预训练，这形成了严格的知识边界。当这个限制未知或被忽视时，模型可能在推理任务中无意间混合过时的时效性信息与一般知识，从而影响回答准确性。

Method: 引入LLMLagBench基准测试，通过评估模型对近期事件的知识来系统识别其训练数据的最早可能时间边界。该方法应用于大量LLM评估，包括明确声明和未声明训练截止时间的模型。

Result: 通过人工验证和与公开发布的LLM预训练信息比较，评估了基准测试的可靠性。

Conclusion: LLMLagBench提供了一个系统方法来识别LLM的知识时间边界，有助于理解模型的知识新鲜度和潜在的信息过时问题。

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [295] [Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding](https://arxiv.org/abs/2511.12140)
*Pinxue Guo,Chongruo Wu,Xinyu Zhou,Lingyi Hong,Zhaoyu Chen,Jinglun Li,Kaixun Jiang,Sen-ching Samson Cheung,Wei Zhang,Wenqiang Zhang*

Main category: cs.CL

TL;DR: VBackChecker是一个无需参考的幻觉检测框架，通过像素级Grounding LLM验证MLLM生成响应与视觉输入的一致性，在R^2-HalBench基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在严重的幻觉问题，需要准确的幻觉检测来确保实际应用中的可靠性。

Method: 基于"眼见为实"原则，使用具备推理和参考分割能力的像素级Grounding LLM来验证响应与视觉输入的一致性，并设计了R-Instruct数据生成管道。

Result: 在R^2-HalBench基准上超越先前复杂框架，性能媲美GPT-4o，在像素级定位任务中比先前方法提升超过10%。

Conclusion: VBackChecker为MLLM幻觉检测提供了有效的参考无关解决方案，具有处理丰富上下文场景的能力和可解释性。

Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.

</details>


### [296] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: CONFACTCHECK是一种高效的幻觉检测方法，无需外部知识库，通过检查生成文本中事实探针响应的一致性来检测LLM幻觉，在资源受限条件下优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成文本时经常产生事实错误和幻觉，在医疗、金融等高风险领域造成严重风险。现有方法在模型访问受限时需要多次API调用，增加延迟和成本。

Method: 基于直觉：生成文本中对事实探针的响应应该在单个LLM内部和不同LLM之间保持一致。无需外部知识库，通过检查事实探针响应的一致性来检测幻觉。

Result: 在多个数据集上的实证评估表明，CONFACTCHECK能够使用更少资源高效检测幻觉事实，在相似条件下比现有基线获得更高的准确率分数。

Conclusion: CONFACTCHECK提供了一种资源高效的幻觉检测方法，特别适用于API访问受限的场景，无需外部知识库即可有效识别LLM生成文本中的事实错误。

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [297] [ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations](https://arxiv.org/abs/2511.12249)
*Khang T. Huynh,Dung H. Nguyen,Binh T. Nguyen*

Main category: cs.CL

TL;DR: 提出了ViConBERT框架，通过对比学习和基于释义的蒸馏来学习越南语上下文嵌入，并创建了首个大规模越南语语义理解评估数据集ViConWSD。


<details>
  <summary>Details</summary>
Motivation: 越南语缺乏强大的语义理解模型和评估资源，而现有的上下文词嵌入进展主要局限于英语等高资源语言。

Method: 结合对比学习(SimCLR)和基于释义的蒸馏来学习越南语上下文嵌入，并构建了包含词义消歧和上下文相似性任务的大规模合成数据集ViConWSD。

Result: ViConBERT在词义消歧任务上F1得分达到0.87，在ViCon数据集上AP为0.88，在ViSim-400数据集上Spearman相关系数为0.60，均优于基线模型。

Conclusion: ViConBERT框架在建模离散词义和分级语义关系方面表现有效，为越南语语义理解提供了新的解决方案。

Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT

</details>


### [298] [AugAbEx : Way Forward for Extractive Case Summarization](https://arxiv.org/abs/2511.12290)
*Purnima Bindal,Vikas Kumar,Sagar Rathore,Vasudha Bhatnagar*

Main category: cs.CL

TL;DR: 本文提出了一种利用现有抽象摘要生成对应提取式摘要的轻量级透明管道，旨在为法律案例摘要研究社区创建包含提取式摘要的增强数据集。


<details>
  <summary>Details</summary>
Motivation: 法律判决摘要对法律从业者构成沉重的认知负担，而深度神经方法生成的抽象摘要容易误判法律术语或忽略关键上下文细节，因此提取式案例摘要器使用趋势上升。但由于人工标注提取式摘要成本高昂，需要开发自动化方法。

Method: 设计了一个轻量透明的管道，利用现有的抽象标准摘要来创建相应的提取式标准版本，确保专家意见从原始抽象摘要传递到转换后的提取式摘要中。

Result: 计划增强七个现有的案例摘要数据集，通过加入相应的提取式摘要来创建丰富的数据资源，并进行结构、词汇和语义维度的广泛比较评估。

Conclusion: 承诺将增强的数据集公开发布，相信这一资源将推动法律文档自动摘要领域的发展。

Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.

</details>


### [299] [Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering](https://arxiv.org/abs/2511.12300)
*Naoya Sugiura,Kosuke Yamada,Yasuhiro Ogawa,Katsuhiko Toyama,Ryohei Sasano*

Main category: cs.CL

TL;DR: 研究探讨LLMs与人类在答题难度上的差异，发现LLMs在维基百科未覆盖的问题和需要数值答案的问题上表现较差。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多NLP任务中超越人类表现，但尚不清楚对人类困难的问题是否对LLMs同样困难，因此研究在抢答器设置下LLMs与人类答题难度的差异。

Method: 收集包含问题、答案和人类正确率的日语测验数据，在多种设置下提示LLMs回答测验，并从两个分析视角比较LLMs与人类的正确率。

Result: 实验结果显示，相比人类，LLMs在正确答案未被维基百科条目覆盖的测验中表现更差，且在需要数值答案的问题上也有困难。

Conclusion: LLMs与人类在问题难度上存在差异，LLMs更依赖维基百科等知识源，且在数值推理方面存在局限。

Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.

</details>


### [300] [Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load](https://arxiv.org/abs/2511.12381)
*Logan Mann,Nayan Saxena,Sarah Tandon,Chenhao Sun,Savar Toteja,Kevin Zhu*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型中的反讽反弹现象——否定指令反而会增加被禁止概念的可及性。通过两个实验发现：否定后立即出现反弹，语义干扰会加剧反弹，而重复有助于抑制；极性分离程度与反弹持续性相关。电路追踪分析揭示了中间层注意力头放大被禁令牌的机制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解否定指令在大型语言模型中引发的反讽反弹现象，类似于人类认知中的"越想忘记越忘不掉"效应，探索模型内部如何处理概念抑制与激活的张力。

Method: 采用两个实验：(1)负载与内容实验：在否定指令后引入不同类型干扰文本（语义、句法、重复），测量反弹强度；(2)极性分离实验：测试模型对同一概念的中性与负面框架的区分能力。辅以电路追踪分析识别关键注意力头。

Result: 结果显示：否定后立即出现反弹；较长或语义干扰会加剧反弹，而重复有助于抑制；更强的极性分离与更持久的反弹相关；电路分析发现中间层注意力头放大被禁令牌，早期层进行抑制。

Conclusion: 研究将认知心理学中的反讽反弹预测与长上下文干扰的机制性理解联系起来，揭示了LLMs中概念抑制的神经机制，并发布了ReboundBench数据集支持未来研究。

Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.

</details>


### [301] [From Phonemes to Meaning: Evaluating Large Language Models on Tamil](https://arxiv.org/abs/2511.12387)
*Jeyarajalingam Varsha,Menan Velayuthan,Sumirtha Karunakaran,Rasan Nivethiga,Kengatharaiyer Sarveswaran*

Main category: cs.CL

TL;DR: ILAKKANAM是首个泰米尔语特定语言评估基准，包含820道来自斯里兰卡学校考试的问题，评估显示LLMs在泰米尔语等低资源语言中的表现随着语言复杂性增加而下降。


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准主要依赖英语翻译数据集，无法捕捉低资源语言（如泰米尔语）的语言和文化细微差别，因此需要开发特定语言评估基准。

Method: 使用820道斯里兰卡学校泰米尔语考试问题构建ILAKKANAM基准，由训练有素的语言学家在五个语言类别和一个事实知识类别下进行标注，涵盖1-13年级以确保广泛的语言覆盖。

Result: Gemini 2.5表现最佳，开源模型落后；所有模型在低年级问题上表现良好，但随着语言复杂性增加而明显下降；模型整体表现与识别语言类别能力无强相关性。

Conclusion: LLMs在泰米尔语等低资源语言中的表现可能更多受到训练数据暴露的影响，而非真正的语言理解，需要更多针对性的语言基础训练。

Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.

</details>


### [302] [Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models](https://arxiv.org/abs/2511.12464)
*Chenglong Wang,Yifu Huo,Yang Gan,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Anxiang Ma,Zhengtao Yu,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: 本文提出了MRMBench基准和推理时探测方法，用于评估奖励模型在多维偏好上的表现，发现其与LLM对齐性能强相关，并揭示了奖励模型在多维偏好捕捉上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估方法通常在固定成对排序测试集上进行，但无法提供各偏好维度的性能信息，这限制了奖励模型的深入分析和改进。

Method: 构建了多维奖励模型基准MRMBench，包含六个不同偏好维度的探测任务；提出了推理时探测方法，识别奖励预测中使用的维度并增强可解释性。

Result: MRMBench与大型语言模型对齐性能强相关；奖励模型在多维偏好捕捉上存在困难；推理时探测方法能可靠评估奖励预测置信度。

Conclusion: MRMBench是开发先进奖励模型的可靠参考，多目标优化在奖励建模中具有潜力，推理时探测方法能提升LLM对齐效果。

Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.

</details>


### [303] [Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing](https://arxiv.org/abs/2511.12472)
*Mengying Wang,Chenhui Ma,Ao Jiao,Tuo Liang,Pengjun Lu,Shrinidhi Hegde,Yu Yin,Evren Gurkan-Cavusoglu,Yinghui Wu*

Main category: cs.CL

TL;DR: 本文提出了SerenQA框架，用于评估LLMs在科学知识图谱问答中发现意外新颖答案的能力，重点关注药物重定位领域，并揭示了现有模型在发现真正惊喜答案方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱问答系统通常返回高度相关但可预测的答案，缺乏利用LLMs发现意外新颖（"serendipitious"）答案的能力。

Method: 提出SerenQA框架，包括基于相关性、新颖性和惊喜度的严格意外性指标，以及从临床知识图谱提取的专家标注基准，包含知识检索、子图推理和意外性探索三个子任务的结构化评估流程。

Result: 实验表明，最先进的LLMs在检索任务上表现良好，但在识别真正令人惊喜和有价值的发现方面仍有困难。

Conclusion: 现有LLMs在发现意外洞察方面仍有显著改进空间，SerenQA为评估和改进LLMs的意外发现能力提供了资源和基准。

Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.

</details>


### [304] [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497)
*JoonHo Lee,HyeonMin Cho,Jaewoong Yun,Hyunjae Lee,JunKyu Lee,Juree Seok*

Main category: cs.CL

TL;DR: SGuard-v1是一个轻量级的大语言模型安全护栏系统，包含ContentFilter和JailbreakFilter两个专门模型，用于检测有害内容和筛选对抗性提示，基于2B参数的Granite-3.3-2B-Instruct模型构建，支持12种语言。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型在人类-AI对话场景中的安全风险，包括有害内容检测和对抗性提示攻击防护的需求。

Method: 使用约140万个训练实例对基础模型进行指令调优，ContentFilter基于MLCommons危害分类学识别安全风险，JailbreakFilter采用精心设计的课程学习覆盖60种主要攻击类型。

Result: 在公共和专有安全基准测试中达到最先进的安全性能，同时保持轻量级部署，提供多类安全预测和二元置信度分数以提高可解释性。

Conclusion: SGuard-v1作为轻量级安全护栏系统，在保持高性能的同时降低了部署开销，并通过Apache-2.0许可证发布以促进AI安全研究和实际部署。

Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.

</details>


### [305] [QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs](https://arxiv.org/abs/2511.12504)
*Maria Tseytlin,Paul Roit,Omri Abend,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: QA-Noun是一个基于问答的框架，专门用于捕捉名词为中心的语义关系，通过9个问题模板覆盖名词的显式句法和隐式上下文角色，与QA-SRL结合实现句子意义的统一分解。


<details>
  <summary>Details</summary>
Motivation: 现有的基于QA的语义方法主要关注谓词-论元关系，但对名词为中心的语义关系处理不足，需要开发专门框架来补充这一空白。

Method: 设计了9个问题模板来捕捉名词的各种语义角色，包括显式句法和隐式上下文关系，并与QA-SRL集成形成统一的语义分解方法。

Result: 评估显示QA-Noun几乎完全覆盖了AMR的名词论元，同时发现了额外的上下文隐含关系，与QA-SRL结合后比FactScore和DecompScore等方法的粒度提高了130%以上。

Conclusion: QA-Noun补充了基于QA的语义框架，形成了全面且可扩展的细粒度语义分解方法，适用于跨文本对齐任务。

Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.

</details>


### [306] [TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction](https://arxiv.org/abs/2511.12520)
*Jie Zhang,Bo Tang,Wanzi Shao,Wenqiang Wei,Jihao Zhao,Jianqing Zhu,Zhiyu li,Wen Xi,Zehao Lin,Feiyu Xiong,Yanchao Tan*

Main category: cs.CL

TL;DR: TAdaRAG是一个新颖的检索增强生成框架，通过任务自适应知识图谱构建解决传统RAG中信息截断和无关细节的问题，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法由于输入上下文窗口限制需要将外部知识截断成小块，导致信息丢失、响应幻觉和推理链断裂，同时检索的非结构化知识引入无关细节影响准确推理。

Method: 提出TAdaRAG框架，包含意图驱动路由机制到领域特定提取模板，结合监督微调和基于强化学习的隐式提取机制，确保知识整合的简洁性、连贯性和非冗余性。

Result: 在6个公共基准测试和1个真实业务基准测试(NowNewsQA)上，使用3个骨干模型验证，TAdaRAG在不同领域和长文本任务中均优于现有方法。

Conclusion: TAdaRAG展现了强大的泛化能力和实际有效性，解决了传统RAG的信息截断和无关细节问题。

Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.

</details>


### [307] [Mitigating Length Bias in RLHF through a Causal Lens](https://arxiv.org/abs/2511.12573)
*Hyeonji Kim,Sujeong Oh,Sanghack Lee*

Main category: cs.CL

TL;DR: 提出一种因果框架和反事实数据增强方法来缓解RLHF奖励模型中的长度偏见问题，通过构造长度不同但内容相似、以及内容不同但长度相似的响应对来训练奖励模型。


<details>
  <summary>Details</summary>
Motivation: RLHF训练的奖励模型存在长度偏见，倾向于将冗长与质量混淆，偏爱更长的响应，这影响了模型评估的准确性。

Method: 采用反事实数据增强方法，构建两种类型的响应对：(1) 长度不同但内容相似的响应对，(2) 内容不同但长度相似的响应对，用于训练奖励模型以独立于冗长度评估内容质量。

Result: 实证评估表明该方法减少了奖励分配中的长度偏见，使策略模型产生更简洁、内容聚焦的输出。

Conclusion: 所提出的方法有效减少了长度偏见，提高了RLHF流程中奖励建模的鲁棒性和内容敏感性。

Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.

</details>


### [308] [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)
*Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: Uni-MoE 2.0是一个基于Qwen2.5-7B架构的全开源全模态大模型，通过动态容量MoE设计、渐进式训练策略和多模态数据匹配技术，在语言中心的多模态理解、推理和生成方面取得显著进展。


<details>
  <summary>Details</summary>
Motivation: 推进Lychee Uni-MoE系列在语言中心多模态能力的发展，构建一个能够理解全模态并生成图像、文本和语音的开放模型。

Method: 采用动态容量MoE框架平衡计算效率和能力，使用共享、路由和空专家处理10种跨模态输入；Omni-Modality 3D RoPE确保自注意力层的时空跨模态对齐；渐进式监督微调策略激活模态特定专家，结合平衡数据组合和迭代GSPO-DPO方法稳定强化学习训练。

Result: 在85个基准测试中达到SOTA或极具竞争力的性能，在76个基准中超过50个优于Qwen2.5-Omni（使用1.2T token训练）。关键优势包括视频理解（+7%平均）、全模态理解（+7%平均）、视听推理（+4%），长语音处理（WER降低4.2%）以及在低级图像处理和可控生成方面的领先表现。

Conclusion: Uni-MoE 2.0通过创新的MoE架构和训练策略，在仅使用75B token训练的情况下，实现了与使用更多数据训练的模型相媲美甚至更优的全模态理解和生成能力。

Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.

</details>


### [309] [Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing](https://arxiv.org/abs/2511.12630)
*Maoqi Liu,Quan Fang,Yang Yang,Can Zhao,Kaiquan Cai*

Main category: cs.CL

TL;DR: 该论文提出了NOTAM语义解析任务，构建了Knots数据集（12,347条专家标注的NOTAM），并通过多智能体协作框架和多种提示工程策略显著提升了航空文本理解能力。


<details>
  <summary>Details</summary>
Motivation: NOTAM作为飞行安全信息的关键传播渠道，其复杂的语言结构和隐含推理给自动解析带来挑战。现有研究主要关注分类和命名实体识别等表层任务，缺乏深度语义理解。

Method: 提出NOTAM语义解析任务，强调语义推理和航空领域知识集成；构建Knots数据集，采用多智能体协作框架进行全面的字段发现；系统评估多种提示工程策略和模型适应技术。

Result: 实验结果表明，所提方法在航空文本理解和处理方面取得了显著改进，为自动化NOTAM分析系统提供了有价值的见解。

Conclusion: 该研究有效解决了NOTAM深度语义解析的挑战，通过高质量数据集和先进方法显著提升了航空安全信息的自动化处理能力。

Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.

</details>


### [310] [Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing](https://arxiv.org/abs/2511.12661)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: Reason-KE++是一个SFT+RL框架，通过过程级忠实度来解决LLM在复杂多跳推理任务中的事实幻觉问题，在MQUAKE-CF-3k上达到95.48%的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有SFT方法存在"忠实度差距"，优化格式模仿而非合理推理，导致LLM的参数先验覆盖新上下文事实，产生关键事实幻觉。

Method: 提出Reason-KE++框架，核心是阶段感知奖励机制，为中间推理步骤提供密集监督，包括分解和子答案正确性等。

Result: 在MQUAKE-CF-3k上达到95.48%的准确率，比之前提升5.28%，证明了过程对齐在复杂任务中的重要性。

Conclusion: 对于复杂任务，对齐推理过程对于构建可信赖的LLM至关重要，仅基于结果的RL会损害推理完整性。

Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.

</details>


### [311] [Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data](https://arxiv.org/abs/2511.12690)
*Sina Rashidi,Hossein Sameti*

Main category: cs.CL

TL;DR: 本文提出了一种波斯语到英语的直接语音到语音翻译系统，通过自监督预训练、离散语音单元和合成并行数据来解决低资源语言数据稀缺问题，在CVSS语料库上实现了4.6 ASR BLEU的提升。


<details>
  <summary>Details</summary>
Motivation: 直接语音到语音翻译系统需要大量平行语音数据，但对于波斯语等低资源语言来说，这类数据非常稀缺，因此需要开发有效的方法来缓解数据不足问题。

Method: 系统包含三个组件：基于conformer的编码器、因果transformer解码器和基于单元的神经声码器。通过使用大语言模型翻译波斯语语音转录并合成英语语音，构建了合成平行语料库。

Result: 构建的合成语料库将可用平行语音数据增加了约6倍，在CVSS波斯语-英语语料上，相比直接基线模型实现了4.6 ASR BLEU的改进。

Conclusion: 结合自监督预训练、离散语音单元和合成并行数据的方法对于改善波斯语-英语等低资源语言对的直接语音到语音翻译是有效的。

Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English

</details>


### [312] [Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2511.12710)
*Yunhao Chen,Xin Wang,Juncheng Li,Yixu Wang,Jie Li,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CL

TL;DR: EvoSynth是一个自主的进化合成框架，通过多智能体系统自动设计和演化基于代码的攻击算法，实现了85.5%的攻击成功率，显著超越了现有方法的创造性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队框架的越狱逻辑局限于选择和组合预定义攻击策略，缺乏自主创造全新攻击机制的能力，限制了其创造性和有效性。

Method: 采用多智能体系统自主设计、演化和执行基于代码的攻击算法，包含代码级自校正循环，能够根据失败情况迭代重写攻击逻辑。

Result: 在Claude-Sonnet-4.5等强健模型上达到85.5%的攻击成功率，生成的攻击方法比现有方法更加多样化。

Conclusion: EvoSynth通过将范式从攻击规划转向进化合成，开创了越狱方法自主创造的新方向，显著提升了攻击效果和多样性。

Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.

</details>


### [313] [Adaptive Focus Memory for Language Models](https://arxiv.org/abs/2511.12712)
*Christopher Cruz*

Main category: cs.CL

TL;DR: AFM是一种动态上下文管理器，通过三种保真度级别（完整、压缩、占位符）来优化多轮对话中的内存使用，在保持安全性能的同时将平均token使用量减少66%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多轮对话中受限于固定上下文窗口和简单内存策略，完整重放对话成本高昂，而静态摘要或仅关注最近对话的方法会丢失关键安全信息。

Method: AFM基于语义相似度、半衰期时效性权重和重要性分类，为每个历史消息分配三种保真度级别，并在严格token预算下按时间顺序打包消息。

Result: 在涉及花生过敏用户规划泰国旅行的安全导向基准测试中，AFM在短和中等长度对话中都保留了过敏信息，安全性能与完整重放相当，平均token使用量比基线减少66%。

Conclusion: AFM提供了一种模块化实现，可在不牺牲安全性或事实连续性的前提下显著降低推理成本。

Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.

</details>


### [314] [On the Brittleness of LLMs: A Journey around Set Membership](https://arxiv.org/abs/2511.12728)
*Lea Hergert,Gábor Berend,Mario Szegedy,Gyorgy Turan,Márk Jelasity*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在简单集合成员查询任务上的表现，发现即使是最基础的问题，LLMs的表现也极其脆弱且不可预测，揭示了模型对集合概念的理解是碎片化的。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务上表现出色，但在简单问题上却经常失败，这种矛盾引发了对其可靠性和可解释性的担忧。作者希望通过研究最基本的集合成员查询任务来揭示这种失败模式。

Method: 采用系统化的实证评估方法，在集合成员查询任务上测试不同提示措辞、语义结构、元素排序和模型选择的影响，进行大规模对照实验。

Result: LLMs在这个基础任务上的表现始终脆弱且不可预测，在所有维度上都显示出不稳定性，表明模型对集合概念的理解最多是碎片化和复杂的。

Conclusion: 通过简单问题的大规模实验能够全面映射和分析LLMs的失败模式，这种方法为LLM评估提供了有价值的方法论。

Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.

</details>


### [315] [Evidence of Phase Transitions in Small Transformer-Based Language Models](https://arxiv.org/abs/2511.12768)
*Noah Hong,Tao Hong*

Main category: cs.CL

TL;DR: 该研究发现语言模型训练中存在相变现象，即使在小规模transformer模型中也能观察到，且这种相变在早期训练阶段就出现，可通过词汇使用统计方法在线性训练空间中直接检测。


<details>
  <summary>Details</summary>
Motivation: 探究相变现象是否仅限于大型语言模型，能否在小模型中观察到，以及是否能在线性训练空间中直接检测到这些相变，而不需要经过对数尺度转换。

Method: 训练小型GPT风格transformer模型，分析词汇使用演变，跟踪平均词长、正确与错误词汇数量、词汇多样性变化，并应用泊松和亚泊松统计量化词汇连接和重组。

Result: 在训练过程中发现明显的相变点，这些相变在标准损失或验证曲线中不可见，但通过词汇和统计探针方法变得明显。

Conclusion: 相变重组是语言模型训练的普遍特征，可在适度模型中观察到，在线性训练空间中直接检测到，并在连贯性出现的早期阶段发生，为理解语言模型训练的非线性动态提供了新视角。

Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors

</details>


### [316] [LLM Reinforcement in Context](https://arxiv.org/abs/2511.12782)
*Thomas Rivasseau*

Main category: cs.CL

TL;DR: 本文提出通过中断（interruptions）来增强大语言模型的对齐性，即在用户输入中每隔一定token插入控制语句，以防止模型被越狱和恶意行为。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对齐研究主要关注通过训练和提示来提高模型对抗攻击的鲁棒性，但缺乏随着用户输入长度增加而增强对齐性的方法。研究表明，LLM越狱概率随用户输入或对话长度的增加而上升。

Method: 提出中断方法，即在用户输入中每隔x个token插入控制语句，这种方法可以推广到思维链过程中以防止策略性行为。

Result: 该方法理论上能够随着用户输入长度的增加而增强模型的对齐性，防止模型被越狱。

Conclusion: 中断是一种可行的解决方案，能够随着用户输入长度的增加而增强大语言模型的对齐性，防止恶意行为。

Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

</details>


### [317] [Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing](https://arxiv.org/abs/2511.12784)
*Hayden Moore,Asfahan Shah*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在自动形式化任务中对语义相似但措辞不同的自然语言输入的鲁棒性，发现即使是微小的语言变化也会显著影响模型输出。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自动形式化方面表现出色，但它们可能对语义保持但措辞不同的自然语言输入敏感，这影响了生成的形式化结果的可靠性和可验证性。

Method: 使用MiniF2F和ProofNet的形式化基准，生成语义相似但措辞不同的自然语言陈述，并在两个现代LLMs上进行交叉评估，测量语义有效性和编译有效性。

Result: 研究结果显示，在语义相似的转述输入上存在性能变异性，微小的自然语言陈述变化会显著影响模型输出。

Conclusion: LLMs在自动形式化任务中对自然语言输入的微小变化敏感，这突显了提高模型鲁棒性的必要性。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.

</details>


### [318] [BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals](https://arxiv.org/abs/2511.12821)
*Ruiyu Wang,Yuzhang Xie,Xiao Hu,Carl Yang,Jiaying Lu*

Main category: cs.CL

TL;DR: BioMedJImpact是一个大规模生物医学期刊影响数据集，整合了文献计量指标、合作特征和LLM衍生的AI参与度指标，分析合作强度和AI参与度如何共同影响科学影响力。


<details>
  <summary>Details</summary>
Motivation: 现有开放资源很少捕捉合作结构和AI研究如何共同塑造生物医学期刊声望，需要开发综合数据集来推进期刊层面的科学影响力和AI参与度分析。

Method: 从PubMed Central的174万篇文章构建数据集，整合文献计量指标、合作特征，并通过可复现的三阶段LLM流程提取AI参与度特征，分析合作强度和AI参与度对期刊影响力的联合影响。

Result: 发现两个一致趋势：合作强度更高的期刊（特别是作者团队更大更多样化的）获得更高引用影响力；AI参与度日益成为期刊声望的强相关因素，尤其在四分位排名中。

Conclusion: BioMedJImpact既是捕捉生物医学与AI交叉的综合数据集，也是经过验证的方法框架，支持可扩展的、内容感知的科学计量分析。

Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.

</details>


### [319] [From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation](https://arxiv.org/abs/2511.12832)
*Niranjan Chebrolu,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: 本文通过目标激活工程技术，在LLaMA 3.1-8B模型中实现更人性化的情感表达，使用归因修补识别关键干预位点，并通过对比文本对推导情感表达向量。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型的对话流畅度不断提高，但为其注入细腻、人性化的情感表达仍然是一个重大挑战。当前的对齐技术通常只处理表面输出或需要大量微调。

Method: 首先使用归因修补识别因果影响组件，通过观察诊断性对话任务中的激活模式找到关键干预位点。然后从对比文本对（目标情感的正面与负面示例）生成的激活差异中推导情感表达向量。

Result: 将这些向量应用于新的对话提示显著增强了情感特征：引导后的响应显示出更高的积极情感（如喜悦、信任）和更频繁的第一人称代词使用，表明更强的个人参与度。

Conclusion: 研究结果为对话AI研究提供了一个精确且可解释的框架和新方向。

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.

</details>


### [320] [Quantifying consistency and accuracy of Latent Dirichlet Allocation](https://arxiv.org/abs/2511.12850)
*Saranzaya Magsarjav,Melissa Humphries,Jonathan Tuke,Lewis Mitchell*

Main category: cs.CL

TL;DR: 本文提出了一种新的稳定性度量方法，用于评估概率主题模型（如LDA）的稳定性和可靠性，通过生成带有真实主题的语料库并进行多次运行来验证模型的一致性。


<details>
  <summary>Details</summary>
Motivation: 概率主题模型由于其随机性，在重新运行时可能产生不同的结果，导致潜在主题的不一致性，这影响了模型的可重复性、可靠性和解释性。

Method: 定义了一个结合准确性和一致性的新稳定性度量，利用LDA的生成特性生成带有真实主题的新语料库，并对这些语料库运行LDA 50次以确定输出的变异性。

Result: 研究表明LDA能够正确确定文档中的潜在主题数量，并且多次重新运行返回相似的主题，显示出较高的内部一致性，但这些主题并非真实主题。

Conclusion: LDA在确定主题数量方面表现良好且具有内部一致性，但生成的主题可能与真实主题存在差异，强调了需要更好的稳定性评估方法。

Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.

</details>


### [321] [NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation](https://arxiv.org/abs/2511.12851)
*Kang Yin,Hye-Bin Shin*

Main category: cs.CL

TL;DR: NeuroLex是一个专门针对脑电图报告文本训练的轻量级领域自适应语言模型，相比通用模型在脑电图报告处理方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 通用语言模型难以捕捉脑电图报告中的领域特定语言惯例，需要专门针对脑电图报告文本的领域自适应模型。

Method: 使用哈佛脑电图数据库的纯文本报告进行训练，采用span-corruption预训练和指令式微调，包括报告润色、段落摘要和术语问答任务。

Result: NeuroLex在困惑度、提取和摘要准确性、标签效率以及对否定和事实幻觉的鲁棒性方面均优于同规模通用模型。

Conclusion: NeuroLex为脑电图报告处理提供了专门的语言模型基础，连接了生物医学文本建模和脑机接口应用。

Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.

</details>


### [322] [From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models](https://arxiv.org/abs/2511.12861)
*Wenxin Zhu,Andong Chen,Yuchen Song,Kehai Chen,Conghui Zhu,Ziyan Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 本文系统综述了多模态思维链（MCoT）的研究，分析了其背景动机、主流方法、评估基准、应用场景以及未来挑战和发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型在感知任务中的显著成功，增强其复杂推理能力成为关键研究重点。现有模型仍面临推理路径不透明和泛化能力不足等挑战，而思维链推理在语言模型中已证明能增强推理透明度和输出可解释性，有望在扩展到多模态领域后提升模型推理能力。

Method: 从三个维度介绍主流MCoT方法：思维链范式、后训练阶段和推理阶段，并分析其底层机制。

Result: 总结了现有的评估基准和指标，讨论了MCoT的应用场景。

Conclusion: 分析了MCoT当前面临的挑战，并对其未来研究方向进行了展望。

Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.

</details>


### [323] [Classification of Hope in Textual Data using Transformer-Based Models](https://arxiv.org/abs/2511.12874)
*Chukwuebuka Fortunate Ijezue,Tania-Amanda Fredrick Eneye,Maaz Amjad*

Main category: cs.CL

TL;DR: 本文提出基于Transformer的文本希望表达分类方法，比较了BERT、GPT-2和DeBERTa三种架构在二元分类和多元分类任务中的表现。BERT在准确率和计算效率方面表现最佳，GPT-2在检测讽刺性希望表达方面有优势。


<details>
  <summary>Details</summary>
Motivation: 开发计算框架来分析文本中的希望表达，为心理健康和社交媒体分析提供工具，同时探索不同Transformer架构在专门情感检测任务中的适用性。

Method: 使用BERT、GPT-2和DeBERTa三种Transformer架构进行二元分类（希望vs非希望）和多元分类（五个希望相关类别）实验，比较其准确率和计算效率。

Result: BERT表现最佳，二元分类准确率84.49%，多元分类72.03%，且计算效率最高（443秒训练时间）。GPT-2准确率最低（79.34%二元，71.29%多元），但在讽刺检测方面表现出色（92.46%召回率）。DeBERTa表现中等但计算成本最高。

Conclusion: 对于专门的情感检测任务，架构的适用性可能比模型规模更重要，BERT在希望表达分类任务中提供了最佳的性能与效率平衡，为希望的计算分析提供了有效框架。

Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.

</details>


### [324] [Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy](https://arxiv.org/abs/2511.12920)
*Desheng Hu,Joachim Baumann,Aleksandra Urman,Elsa Lichtenegger,Robin Forsberg,Aniko Hannak,Christo Wilson*

Main category: cs.CL

TL;DR: 通过系统算法审计评估Google搜索中AI生成内容（AI概览和精选摘要）的质量和一致性，发现在健康信息领域存在信息不一致、缺乏医疗安全保障等问题。


<details>
  <summary>Details</summary>
Motivation: Google搜索越来越多地通过AI概览和精选摘要展示AI生成内容，用户依赖这些信息但无法控制其呈现方式，需要评估其质量和一致性，特别是在健康信息领域。

Method: 对1,508个真实婴儿护理和怀孕相关查询进行系统算法审计，使用稳健的评估框架评估多个质量维度，包括答案一致性、相关性、医疗安全保障存在性、来源类别和情感一致性。

Result: AI概览和精选摘要在同一搜索结果页面上显示的信息有33%不一致；尽管相关性得分高，但两者都严重缺乏医疗安全保障（AI概览仅11%，精选摘要仅7%）；健康与健康网站是主要来源，但精选摘要也经常链接到商业来源。

Conclusion: 这些发现对公共卫生信息获取有重要影响，表明在AI介导的健康信息中需要更强的质量控制。该方法为审计高风险领域的AI系统提供了可转移的框架。

Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

</details>


### [325] [Visual Room 2.0: Seeing is Not Understanding for MLLMs](https://arxiv.org/abs/2511.12928)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.

</details>


### [326] [Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty](https://arxiv.org/abs/2511.12991)
*Zeyu Shi,Ziming Wang,Tianyu Chen,Shiqi Gao,Haoyi Zhou,Qingyun Sun,Jianxin Li*

Main category: cs.CL

TL;DR: 本文提出HCNR方法，通过识别和恢复关键表达控制神经元来修复SFT后LLM的诚实性，相比基线方法实现33.25%的诚实性恢复，速度提升2.23倍且数据需求减少10倍以上。


<details>
  <summary>Details</summary>
Motivation: 监督微调严重损害了大型语言模型的诚实性，这对高风险领域的安全部署至关重要。现有恢复方法假设SFT深度破坏了模型识别知识边界的能力，但作者发现模型仍保留这种能力，受损的是其忠实表达这种意识的能力。

Method: 提出诚实性关键神经元恢复方法，识别并恢复关键表达控制神经元到预训练状态，同时通过Hessian引导的补偿机制与任务导向神经元协调。

Result: 在四个QA任务和五个LLM家族上的实验表明，HCNR有效恢复了33.25%受损的诚实性，相比基线方法速度提升至少2.23倍，数据需求减少10倍以上。

Conclusion: HCNR为可信赖的LLM部署提供了一种实用解决方案，能够在外科手术式修复诚实性的同时保持模型性能。

Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.

</details>


### [327] [AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models](https://arxiv.org/abs/2511.13029)
*Declan Jackson,William Keating,George Cameron,Micah Hill-Smith*

Main category: cs.CL

TL;DR: AA-Omniscience基准测试评估语言模型的事实回忆能力和知识校准能力，涵盖42个经济相关主题的6000个问题，结果显示前沿模型在事实性和校准方面存在持续弱点。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估主要衡量通用能力，但在各领域可靠使用需要事实准确性和知识差距识别能力。

Method: 引入AA-Omniscience基准，测量全知指数（-100到100），该指标联合惩罚幻觉并在不确定时奖励弃权，0分表示正确和错误回答数量相等。

Result: Claude 4.1 Opus获得最高分（4.8），是仅有的三个得分高于零的模型之一。不同研究实验室的模型在六个领域表现各异。

Conclusion: 模型在知识重要任务中应根据用例需求选择，而非依赖通用性能，因为性能在不同领域存在显著差异。

Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.

</details>


### [328] [How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm](https://arxiv.org/abs/2511.13040)
*Kasun Wickramasinghe,Nisansa de Silva*

Main category: cs.CL

TL;DR: 本文探讨了多语言嵌入模型与对齐单语模型在双语词典归纳任务中的表现对比，分析了BLI作为嵌入空间对齐评估指标的局限性，并提出基于词干的新BLI方法和词汇剪枝技术来改进评估效果。


<details>
  <summary>Details</summary>
Motivation: 虽然多语言嵌入已成为主流选择，但需要评估其是否在所有方面都优于对齐单语模型，以及高昂的计算成本是否合理。研究旨在探索BLI作为对齐度量的局限性，并比较不同嵌入对齐技术在高低资源语言环境下的表现。

Method: 使用双语词典归纳任务评估嵌入空间对齐程度；提出基于词干的BLI方法替代传统的基于词的BLI；引入词汇剪枝技术来更准确地显示对齐程度；比较传统嵌入对齐技术、新型多语言模型和组合对齐技术的性能。

Result: 发现BLI在某些情况下不能真实反映对齐程度；组合嵌入对齐技术通常表现更好，但在低资源语言情况下多语言嵌入表现更优；语言家族对BLI性能有影响。

Conclusion: 多语言嵌入并非在所有方面都优于对齐单语模型，需要根据具体场景选择合适的方法。提出的基于词干的BLI和词汇剪枝技术能更准确地评估嵌入空间对齐程度，特别是在处理多语言嵌入模型时。

Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).

</details>


### [329] [Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training](https://arxiv.org/abs/2511.13043)
*Xinyuan Zhou,Yi Lei,Xiaoyu Zhou,Jingyi Sun,Yu Zhu,Zhongyi Ye,Weitai Zhang,Quan Liu,Si Wei,Cong Liu*

Main category: cs.CL

TL;DR: Spark-Prover-X1是一个7B参数的模型，通过三阶段训练框架提升轻量级大语言模型的形式推理能力，在定理证明任务上达到同类开源模型的最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在自动定理证明中因缺乏多样化和高质量形式语言数据而受限的问题。

Method: 采用三阶段训练框架：1) 在广泛数学语料上进行持续预训练，引入"思维链增强状态预测"任务；2) 在专家迭代循环中进行监督微调；3) 应用组相对策略优化来提升解决难题的能力。

Result: Spark-Prover-X1-7B在类似规模的开源模型中达到最先进性能，平均通过率37.0%(pass@32)，在PutnamBench上解决27个问题，在CombiBench上达到24.0%通过率。

Conclusion: 多样化的训练数据和逐步精炼的训练流程为增强轻量级LLM的形式推理能力提供了有效路径。

Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.

</details>


### [330] [BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models](https://arxiv.org/abs/2511.13095)
*Chuyuan Li,Giuseppe Carenini*

Main category: cs.CL

TL;DR: BeDiscovER是一个评估现代大语言模型语篇理解能力的综合基准套件，包含5个语篇任务和52个数据集，涵盖词汇、句子和文档层面的语篇分析。评估发现前沿模型在时间推理的算术方面表现良好，但在完整文档推理和修辞关系识别等语义语篇现象上仍有困难。


<details>
  <summary>Details</summary>
Motivation: 随着推理语言模型的发展，需要建立一个全面评估模型语篇层面知识的最新基准套件，以评估现代LLMs在语篇理解方面的能力。

Method: BeDiscovER整合了5个公开可用的语篇任务，涵盖语篇词汇、(多)句子和文档层面，总共包含52个数据集，包括语篇解析、时间关系提取、语篇粒子消歧等任务。

Result: 评估了开源LLMs（如Qwen3系列、DeepSeek-R1）和前沿模型（如GPT-5-mini），发现最先进模型在时间推理的算术方面表现强劲，但在完整文档推理和修辞关系识别等微妙语义语篇现象上表现不佳。

Conclusion: BeDiscovER为评估现代LLMs的语篇理解能力提供了全面基准，揭示了当前模型在语篇层面推理的局限性，特别是在文档级推理和复杂语义语篇现象处理方面仍需改进。

Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.

</details>


### [331] [Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study](https://arxiv.org/abs/2511.13107)
*Zhichao He,Mouxiao Bian,Jianhong Zhu,Jiayuan Chen,Yunqiu Wang,Wenxia Zhao,Tianbin Li,Bing Han,Jie Xu,Junyan Wu*

Main category: cs.CL

TL;DR: 本研究系统评估了当代LLMs在零样本设置下识别已发表RCTs对CONSORT 2010声明依从性的准确性，发现整体性能一般，模型能准确识别合规项目但难以检测不合规和不适用项目。


<details>
  <summary>Details</summary>
Motivation: 手动验证CONSORT依从性是一个耗时费力的过程，构成同行评审和证据合成的重要瓶颈，需要自动化解决方案。

Method: 构建了包含150篇已发表RCTs的黄金标准数据集，在零样本设置下评估LLMs的三分类任务性能，使用宏平均F1分数作为主要指标。

Result: 最佳模型Gemini-2.5-Flash和DeepSeek-R1的宏F1分数分别为0.634，Cohen's Kappa系数分别为0.280和0.282，仅与专家共识达成一般一致性。模型在识别合规项目时表现良好（F1>0.850），但在识别不合规和不适用项目时表现差（F1<0.400）。

Conclusion: LLMs有潜力作为CONSORT检查的初步筛选助手，能有效识别报告良好的项目，但目前无法可靠检测报告遗漏或方法学缺陷，不适合替代人类专业知识进行试验质量评估。

Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.

</details>


### [332] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: Agent-Event-Coder (AEC) 是一个新颖的多智能体框架，将零样本事件抽取视为类似软件工程的代码生成过程，通过专门的智能体协作解决LLMs在复杂推理和领域理解方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在零样本事件抽取中的局限性，包括输出不完整、结构无效、触发器分类错误、参数缺失和模式违反等问题。

Method: 提出多智能体框架AEC，将事件抽取分解为检索、规划、编码和验证四个专门子任务，每个任务由专门的LLM智能体处理，事件模式表示为可执行的类定义。

Result: 在五个不同领域和六个LLM上的实验表明，AEC始终优于先前的零样本基线方法，能够产生精确、完整且模式一致的事件抽取结果。

Conclusion: 将事件抽取视为代码生成的方法具有强大潜力，AEC框架通过协作智能体工作流程实现了零样本设置下的高质量事件抽取。

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [333] [Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels](https://arxiv.org/abs/2511.13152)
*Sourya Dipta Das,Shubham Kumar,Kuldeep Yadav*

Main category: cs.CL

TL;DR: 提出一种零样本语法能力评估框架，利用无标签数据和大型语言模型生成伪标签，通过噪声鲁棒训练方法实现准确的语法评分，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 口语语法评估面临自发性和不流畅性挑战，且专家标注成本高昂，需要开发不依赖人工标签的大规模语法评估系统。

Method: 使用基于语法能力量表的提示词让LLM在无标签数据上生成伪标签，然后通过噪声鲁棒训练框架训练基于transformer的模型。

Result: 实验证明该方法能高精度估计语法能力分数，LLM选择和干净-噪声样本比例对性能有重要影响。

Conclusion: 该方法为可扩展、低资源的语法评估系统铺平了道路，具有鲁棒性和可解释性。

Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.

</details>


### [334] [Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis](https://arxiv.org/abs/2511.13159)
*Zaara Zabeen Arpa,Sadnam Sakib Apurbo,Nazia Karim Khan Oishee,Ajwad Abrar*

Main category: cs.CL

TL;DR: 提出了首个公开的孟加拉语语料库，用于区分ASR转录中的重复性不流利和形态学重叠现象，通过多语言LLM和任务特定微调两种方法进行基准测试，BanglaBERT模型取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉语等低资源语言ASR转录中的关键歧义问题：词重复可能是重复性不流利（ASR错误/犹豫）或形态学重叠（语法结构），标准不流利校正方法会错误删除有效语言信息。

Method: 构建了首个公开的20,000行手动标注孟加拉语语料库，使用两种范式进行基准测试：1）最先进的多语言大语言模型的少样本提示；2）编码器模型的任务特定微调。

Result: 多语言LLM在少样本提示下达到82.68%的准确率，但微调方法表现更优，语言特定的BanglaBERT模型达到最高准确率84.78%和F1分数0.677。

Conclusion: 建立了强大的、基于语言学的基准，为开发复杂的、保持语义的孟加拉语文本规范化系统提供了必要数据。

Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.

</details>


### [335] [TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine](https://arxiv.org/abs/2511.13169)
*Tianai Huang,Jiayuan Chen,Lu Lu,Pengcheng Chen,Tianbin Li,Bing Han,Wenchao Tang,Jie Xu,Ming Li*

Main category: cs.CL

TL;DR: TCM-5CEval是一个更细粒度的中医领域大语言模型评估基准，涵盖5个关键维度：核心知识、经典文献、临床决策、中药学和临床非药物治疗。评估发现模型在基础知识回忆方面表现良好，但在经典文本解释和推理稳定性方面存在显著弱点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在中医等高度专业化和文化丰富领域的应用需要更严谨和细致的评估。基于先前TCM-3CEval工作发现的系统性知识差距和文化语境对齐问题，需要更全面的评估工具。

Method: 开发TCM-5CEval基准，从5个维度评估15个主流LLM：核心知识(TCM-Exam)、经典文献(TCM-LitQA)、临床决策(TCM-MRCD)、中药学(TCM-CMM)和临床非药物治疗(TCM-ClinNPT)。使用基于排列的一致性测试来评估模型推理稳定性。

Result: 评估显示模型性能存在显著差异，deepseek_r1和gemini_2_5_pro表现最佳。模型在基础知识回忆方面熟练，但在经典文本解释方面困难。排列测试揭示模型推理普遍脆弱，所有模型在选项顺序变化时都出现性能显著下降，表明存在位置偏见和缺乏稳健理解。

Conclusion: TCM-5CEval不仅为中医领域LLM能力提供了更详细的诊断工具，还暴露了其推理稳定性的根本弱点。该基准已上传至Medbench平台，促进进一步研究和标准化比较。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.

</details>


### [336] [Translation Entropy: A Statistical Framework for Evaluating Translation Systems](https://arxiv.org/abs/2511.13180)
*Ronit D. Gross,Yanir Harel,Ido Kanter*

Main category: cs.CL

TL;DR: 本文提出了一种量化评估机器翻译器性能的方法——翻译熵，通过分析单个词语替换后翻译结果不变的统计规律来测量翻译的不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏量化评估机器翻译器性能的客观方法，因为即使是单一语言的熵也未知，需要开发新的评估指标。

Method: 通过选择基准句子，逐个替换其中的词语，统计替换后翻译结果不变的频率，计算单个词语的翻译熵，然后平均得到整体翻译熵。

Result: 该方法能够量化排名不同翻译器的性能，发现翻译熵在解码器块中增强，双词语替换时翻译退化呈现乘法效应。

Conclusion: 翻译熵可作为机器翻译器的可测量属性和客观基准测试标准，为翻译质量评估提供了新方法。

Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.

</details>


### [337] [Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study](https://arxiv.org/abs/2511.13182)
*Mihai Dan Nadas,Laura Diosan*

Main category: cs.CL

TL;DR: 本研究评估了多种大语言模型在罗马尼亚语文本中恢复变音符号的性能，发现GPT-4o等模型表现优异，而Llama系列模型表现波动较大。


<details>
  <summary>Details</summary>
Motivation: 自动变音符号恢复对于罗马尼亚语等富含变音符号的语言的文本处理至关重要，需要评估不同大语言模型在此任务上的表现。

Method: 使用全面的语料库测试了包括GPT-3.5、GPT-4、GPT-4o、Gemini 1.0 Pro、Llama 2/3、Mixtral 8x7B等在内的多个模型，采用从零样本到复杂多样本指令的多种提示模板。

Result: GPT-4o等模型实现了高精度的变音符号恢复，始终超过中性回显基线，而Meta的Llama系列模型表现出更大的波动性。

Conclusion: 研究结果强调了模型架构、训练数据和提示设计对变音符号恢复性能的影响，为改进富含变音符号语言的NLP工具指明了有前景的方向。

Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.

</details>


### [338] [Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms](https://arxiv.org/abs/2511.13225)
*Tyler Loakman,Joseph James,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文评估了视觉语言模型在语音识别任务中的表现，通过合成包含4000多个英语单词的数据集，测试模型从声谱图和波形图中识别音位和字位转录的能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型的发展，需要评估它们在跨模态任务中的能力，特别是作为语音学专家解释语音声谱图和波形图的能力。

Method: 合成包含4000+英语单词的数据集，创建一致的声谱图和波形图，通过多项选择任务测试模型从语音表示中预测正确转录的能力，使用基于音位编辑距离的干扰项。

Result: 无论是零样本还是微调后的模型，表现都很少超过随机水平，表明需要特定的参数化知识来解读这类图形，而不仅仅是配对样本。

Conclusion: 视觉语言模型在理解语音声谱图和波形图方面表现有限，需要专门的知识来正确解释这些语音表示形式。

Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.

</details>


### [339] [Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance](https://arxiv.org/abs/2511.13254)
*Shalini Maiti,Amar Budhiraja,Bhavul Gauri,Gaurav Chaurasia,Anton Protopopov,Alexis Audran-Reiss,Michael Slater,Despoina Magka,Tatiana Shavrina,Roberta Raileanu,Yoram Bachrach*

Main category: cs.CL

TL;DR: 本文提出SoCE方法，通过基准测试组合识别最优模型候选，并应用非均匀加权平均来最大化性能，改进了传统的均匀平均方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练资源密集且耗时，模型融合作为一种有前景的技术可以在不昂贵重新训练的情况下提升性能。

Method: SoCE方法利用基准类别间模型性能低相关性的观察，识别每个弱相关类别簇的'专家'模型，并使用优化的加权平均而非均匀权重进行组合。

Result: 该方法在多个领域（包括多语言能力、工具调用和数学）提高了性能和鲁棒性，并在Berkeley函数调用排行榜上达到了最先进的结果。

Conclusion: SoCE提供了一种原则性的模型融合方法，通过利用类别间性能差异和优化加权策略，有效提升了模型性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.

</details>


### [340] [RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2511.13329)
*Shufan Yang,Zifeng Cheng,Zhiwei Jiang,Yafeng Yin,Cong Wang,Shiping Ge,Yuchen Fu,Qing Gu*

Main category: cs.CL

TL;DR: RegionMarker是一种区域触发语义水印框架，通过在低维空间中定义触发区域并将水印注入到相关文本嵌入中，为EaaS提供全面的版权保护，抵抗多种攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有的EaaS水印方法只能抵抗部分攻击，无法提供全面保护，存在版权泄露风险。

Method: 使用秘密降维矩阵投影到子空间，随机选择触发区域，在整个触发区域嵌入水印，并以文本嵌入作为水印。

Result: 在多个数据集上的广泛实验表明，RegionMarker能有效抵抗不同攻击方法。

Conclusion: RegionMarker能够保护EaaS的版权，提供全面的水印保护方案。

Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.

</details>


### [341] [AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects](https://arxiv.org/abs/2511.13335)
*Maram Alharbi,Salmane Chafik,Saad Ezzini,Ruslan Mitkov,Tharindu Ranasinghe,Hansi Hettiarachchi*

Main category: cs.CL

TL;DR: 该论文介绍了阿拉伯语方言情感分析共享任务，使用包含沙特和摩洛哥方言的酒店评论数据集，支持方言感知NLP系统开发。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯世界酒店业依赖客户反馈改进服务，需要先进的阿拉伯语情感分析工具来应对多方言挑战。

Method: 构建多方言数据集，将现代标准阿拉伯语的酒店评论翻译成沙特和摩洛哥方言，并由母语者验证翻译准确性和情感保留。数据集包含538条平衡的情感评论。

Result: 超过40个团队注册，12个提交系统，最佳系统F1分数达0.81，证明了跨阿拉伯方言情感分析的可行性。

Conclusion: 该资源支持开发方言感知的NLP系统，用于客户体验分析等实际应用，同时展示了跨方言情感分析的持续挑战。

Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.

</details>


### [342] [Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.13368)
*Kajetan Dymkiewicz,Ivan Vulic,Helen Yannakoudakis,Eilam Shapira,Roi Reichart,Anna Korhonen*

Main category: cs.CL

TL;DR: 本文研究了LLMs在不同任务和语言间的迁移效果，发现任务内跨语言迁移通常为正，而跨任务迁移常导致性能下降，揭示了稳定的捐赠者-接受者结构。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在一个任务或语言上的改进如何影响其他任务和语言及其组合，目前仍缺乏深入认识。

Method: 通过PEFT/LoRA在多个开源LLM家族和规模上进行控制研究，将任务和语言视为迁移轴，在单一任务-语言源上微调模型，并评估所有其他任务-语言目标对的迁移效果。

Result: 发现两个一致模式：1) 任务内跨语言迁移可靠为正，而跨任务迁移常导致性能下降；2) 跨语言和任务存在稳定的捐赠者-接受者结构。

Conclusion: 研究结果为风险感知微调和模型专业化提供了重要启示。

Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.

</details>


### [343] [Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts](https://arxiv.org/abs/2511.13381)
*Siyu Zhu,Mouxiao Bian,Yue Xie,Yongyu Tang,Zhikang Yu,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Xiaoyan Dong*

Main category: cs.CL

TL;DR: PEDIASBench评估框架评估了12个大型语言模型在儿科医疗中的表现，发现最先进模型在基础知识方面表现良好，但在复杂推理、动态诊疗和医学伦理方面存在局限，目前无法独立进行儿科诊疗，但有望用于决策支持、医学教育和患者沟通。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在医学领域的快速发展，需要评估它们是否能在真实临床环境中作为合格的儿科医生发挥作用。

Method: 开发了PEDIASBench系统评估框架，基于知识系统框架并针对真实临床环境定制，从三个维度评估LLMs：基础知识应用、动态诊疗能力、儿科医疗安全与医学伦理，涵盖19个儿科亚专业和211种典型疾病。

Result: 最先进模型在基础知识方面表现良好（Qwen3-235B-A22B在执照级问题上准确率超90%），但随着任务复杂性增加性能下降约15%；在动态诊疗场景中，DeepSeek-R1在病例推理中得分最高（平均0.58），但大多数模型难以适应实时患者变化；在医学伦理和安全任务中，Qwen2.5-72B表现最佳（准确率92.05%），但人文敏感性仍然有限。

Conclusion: 当前儿科LLMs受限于有限的动态决策能力和不发达的人文关怀，无法独立进行儿科诊疗，但有望用于决策支持、医学教育和患者沟通。未来发展应关注多模态整合和临床反馈-模型迭代循环，以增强安全性、可解释性和人机协作。

Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.

</details>


### [344] [Non-Linear Scoring Model for Translation Quality Evaluation](https://arxiv.org/abs/2511.13467)
*Serge Gladkoff,Lifeng Han,Katerina Gasova*

Main category: cs.CL

TL;DR: 本文提出了一种基于对数函数的非线性翻译质量评估模型，解决了传统线性评估方法在不同文本长度下产生偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的线性翻译质量评估方法在不同长度的文本样本中存在偏差，对短文本过度惩罚，对长文本惩罚不足，与专家直觉不一致。

Method: 提出基于对数函数的两参数模型E(x) = a * ln(1 + b * x)，通过心理物理学和认知负荷理论支持，使用一维根查找步骤从两个容忍点校准参数。

Result: 实证数据显示可接受的错误数量随样本大小呈对数增长而非线性增长，该模型提高了跨不同长度文本评估的公平性和评分者间可靠性。

Conclusion: 该非线性评分模型提供了更符合人类感知的翻译质量评估范式，为基于AI的文档级评估提供了更强基础，改进了现有评估工作流程。

Abstract: Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.
  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.
  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model
  E(x) = a * ln(1 + b * x), a, b > 0,
  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.
  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.

</details>


### [345] [Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns](https://arxiv.org/abs/2511.13481)
*Attapol T. Rutherford,Sirisak Chueykamhang,Thachaparn Bunditlurdruk,Nanthicha Angsuwichitkul*

Main category: cs.CL

TL;DR: 本文提出了一种基于方面情感分析(ABSA)的新方法，用于解码泰国财务年报中的模糊情感，开发了专门的标注指南并标注了100多份财务报告，通过事件研究验证了情感分析对股价的实际影响。


<details>
  <summary>Details</summary>
Motivation: 财务文件中往往包含模糊语言来呈现积极或中性前景，即使基础条件可能不太有利，理解这些情感对洞察市场行为至关重要。

Method: 开发了标注模糊情感的具体指南，标注了100多份财务报告，并对各种文本分类模型进行基准测试，同时进行事件研究评估情感分析对股价的实际影响。

Result: 在情感分类方面表现出色，市场反应受到报告中特定方面的选择性影响。

Conclusion: 研究结果强调了财务文本中情感分析的复杂性，并指出解决模糊语言对于准确评估市场情绪的重要性。

Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.

</details>


### [346] [Applying Large Language Models to Characterize Public Narratives](https://arxiv.org/abs/2511.13505)
*Elinor Poole-Dayan,Daniel T Kessler,Hannah Chiou,Margaret Hughes,Emily S Lin,Marshall Ganz,Deb Roy*

Main category: cs.CL

TL;DR: 本文提出了一个利用大语言模型自动化标注公共叙事的计算框架，在8个叙事和14个代码上实现了平均F1分数0.80的接近专家水平的性能，并扩展到分析22个故事和一组政治演讲。


<details>
  <summary>Details</summary>
Motivation: 公共叙事是领导力发展和公民动员的关键工具，但由于主观解释和专家标注成本高，其系统分析仍然具有挑战性。

Method: 提出了一个新颖的计算框架，利用大语言模型自动化公共叙事的定性标注，使用与主题专家共同开发的代码本评估LLM性能。

Result: LLM能够实现接近人类专家的性能，平均F1分数达到0.80，并将分析扩展到22个故事和一组政治演讲。

Conclusion: 本研究展示了LLM辅助标注在可扩展叙事分析方面的潜力，并指出了计算公民故事讲述未来研究的关键限制和方向。

Abstract: Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.

</details>


### [347] [Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets](https://arxiv.org/abs/2511.13529)
*Máté Gedeon,Piroska Zsófia Barta,Péter Mihajlik,Tekla Etelka Gráczi,Anna Kohári,Katalin Mády*

Main category: cs.CL

TL;DR: 本文介绍了两个新的匈牙利语语音数据集BEA-Large和BEA-Dialogue，旨在解决匈牙利语在语音识别研究中数据不足的问题，并建立了可复现的基线模型。


<details>
  <summary>Details</summary>
Motivation: 高资源语言的自动语音识别(ASR)发展迅速，但匈牙利语等语言由于缺乏自发性对话语料库而代表性不足，需要填补这一空白。

Method: 从匈牙利语语音语料库BEA中构建两个新数据集：BEA-Large包含255小时433位说话者的自发语音，BEA-Dialogue包含85小时自然对话。使用公开可用的ASR模型建立可复现基线。

Result: 微调的Fast Conformer模型在自发语音上词错误率为14.18%，在重复语音上为4.8%。说话人日志实验的错误率在13.05%到18.26%之间。

Conclusion: 对话ASR仍然面临困难，特别是由于不流利、重叠和非正式语音模式。通过发布这些数据集和基线，旨在推进匈牙利语音技术，并为其他语言开发自发性和对话性基准提供方法论框架。

Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.

</details>


### [348] [Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation](https://arxiv.org/abs/2511.13590)
*Hao Wang,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的文本到SQL分类法，评估了现有数据集的局限性，并开发了SQL-Synth数据集，该数据集通过分类法和LLM结合的方法生成，具有更好的多样性和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL数据集覆盖范围有限，无法捕捉真实应用的多样性，需要更全面的数据集来训练和评估模型。

Method: 提出基于核心意图、语句类型、语法结构和关键操作的分类法，并开发了分类法指导的数据集合成管道，结合LLM生成SQL-Synth数据集。

Result: SQL-Synth数据集在多样性和覆盖范围上优于现有基准，实验显示现有LLM在完整场景覆盖上表现不足，但微调能显著提升性能。

Conclusion: 提出的分类法具有重要影响，能够全面分析数据集和LLM性能，并指导LLM训练数据的构建。

Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.

</details>


### [349] [Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues](https://arxiv.org/abs/2511.13658)
*Jiaming Qu,Mengtian Guo,Yue Wang*

Main category: cs.CL

TL;DR: 使用大型语言模型将机器学习检测到的欺骗性评论的词汇线索转化为人类可理解的语言现象，以提高在线评论可信度评估的可解释性。


<details>
  <summary>Details</summary>
Motivation: 欺骗性评论误导消费者、损害企业利益并破坏在线市场信任。虽然机器学习分类器能有效检测欺骗性评论，但其学习到的区分特征往往难以被人类理解和解释。

Method: 探索使用大型语言模型（LLMs）将机器学习学到的词汇线索转化为人类可理解的语言现象，用于区分欺骗性评论和真实评论。

Result: 通过这种方法获得的语言现象具有数据实证基础，在相似领域具有通用性，并且比LLMs先验知识或上下文学习获得的现象更具预测性。

Conclusion: 这些语言现象在无法使用欺骗检测分类器的环境中，有潜力帮助人们批判性评估在线评论的可信度。

Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.

</details>


### [350] [Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation](https://arxiv.org/abs/2511.13689)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,Joseph K J*

Main category: cs.CL

TL;DR: 提出了TAI框架，结合LLM和潜扩散模型，通过翻译和图像生成增强印度诗歌的可访问性，支持联合国可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 印度诗歌具有丰富的文化遗产，但由于语言复杂性和文化内涵，非母语者难以理解。现有研究忽视印度语言诗歌，需要提高其全球可访问性。

Method: 使用TAI框架，包括基于几率比偏好对齐算法的翻译模块和基于语义图的图像生成模块，捕捉诗歌的语义关系和隐喻含义。

Result: 综合实验评估显示TAI在诗歌图像生成任务中优于基线方法，并创建了包含1,570首诗歌的MorphoVerse数据集。

Conclusion: 该工作填补了诗歌翻译和视觉理解的空白，旨在扩大印度诗歌的可访问性并丰富读者体验。

Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.

</details>


### [351] [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/abs/2505.11225)
*Chengyu Huang,Zhengxin Zhang,Claire Cardie*

Main category: cs.CL

TL;DR: HAPO通过历史感知策略优化，利用历史信息来训练LLM生成更简洁的推理过程，在保持准确性的同时显著减少输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测试时扩展响应长度虽然能提升LLM的推理能力，但会导致冗长输出和增加推理成本，且没有利用训练过程中相同问题的历史信息来逐步优化解决方案的简洁性。

Method: 提出HAPO方法，为每个问题维护历史状态（如之前生成正确响应的最小长度），基于历史状态设计长度奖励函数，激励发现比之前更简洁的正确解决方案，同时避免过度惩罚较短的错误响应以促进探索。

Result: 在多个数学基准测试中，HAPO实现了33-59%的长度减少，准确率仅下降2-5%，有效诱导LLM的简洁推理能力。

Conclusion: HAPO通过结合历史信息的策略优化，成功实现了在保持准确性的同时显著提升LLM推理效率的目标。

Abstract: While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.

</details>


### [352] [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/abs/2506.14157)
*Chengyu Huang,Tanya Goyal*

Main category: cs.CL

TL;DR: 本文提出了一种名为DCRM的指标来量化偏好优化中响应对的质量，发现DCRM与学习效果正相关，并基于此提出了最佳配对方法以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究试图将偏好优化性能与底层偏好数据集关联，但作者观察到响应对之间的差异可能不符合期望的学习差异，因此需要量化这些差异来评估响应对质量。

Method: 使用距离和奖励边际来量化响应对差异，结合得到DCRM指标，并基于此提出best-of-N²配对方法选择高DCRM的响应对。

Result: 实验证明DCRM与学习效果存在普遍正相关关系，提出的配对方法在各种设置下都能在AlpacaEval、MT-Bench和Arena-Hard等基准上超越现有训练集的性能。

Conclusion: DCRM是评估偏好优化数据集质量的有效指标，基于DCRM的配对方法能够生成更高质量的训练数据，进一步提升模型性能。

Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.

</details>
